<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Future Network of Intelligence Institute</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<orgName type="institution" key="instit2">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Future Network of Intelligence Institute</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<orgName type="institution" key="instit2">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent Map</orgName>
								<address>
									<country>T Lab</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Future Network of Intelligence Institute</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<orgName type="institution" key="instit2">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Future Network of Intelligence Institute</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<orgName type="institution" key="instit2">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Future Network of Intelligence Institute</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<orgName type="institution" key="instit2">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Segmentation</term>
					<term>Multi-Modal</term>
					<term>Knowledge Distilla- tion</term>
					<term>LiDAR Point Clouds</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As camera and LiDAR sensors capture complementary information in autonomous driving, great efforts have been made to conduct semantic segmentation through multi-modality data fusion. However, fusion-based approaches require paired data, i.e., LiDAR point clouds and camera images with strict point-to-pixel mappings, as the inputs in both training and inference stages. It seriously hinders their application in practical scenarios. Thus, in this work, we propose the 2D Priors Assisted Semantic Segmentation (2DPASS) method, a general training scheme, to boost the representation learning on point clouds. The proposed 2DPASS method fully takes advantage of 2D images with rich appearance during training, and then conduct semantic segmentation without strict paired data constraints. In practice, by leveraging an auxiliary modal fusion and multi-scale fusion-to-single knowledge distillation (MSFSKD), 2DPASS acquires richer semantic and structural information from the multi-modal data, which are then distilled to the pure 3D network. As a result, our baseline model shows significant improvement with only point cloud inputs once equipped with the 2DPASS. Specifically, it achieves the state-of-the-arts on two large-scale recognized benchmarks (i.e., SemanticKITTI and NuScenes), i.e., ranking the top-1 in both single and multiple scan(s) competitions of SemanticKITTI. Code will be made available at https://github.com/yanx27/2DPASS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation plays a crucial role in large-scale outdoor scene understanding, which has broad applications in autonomous driving and robotics <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>.</p><p>In the past few years, the research community has devoted significant effort to understanding natural scenes using either camera images <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> or LiDAR point clouds <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> as the input. However, these single-modal methods inevitably face challenges in complex environments due to the inherent limitations of the Front-Camera Image and Perspective Projection 360? LiDAR Point Cloud Point Cloud in Camera Perspective <ref type="figure" target="#fig_7">Fig. 1</ref>. Limitation of fusion-based methods. When the self-driving car only has front-cameras with limited perspective such as SemanticKITTI <ref type="bibr" target="#b15">[16]</ref> dataset while the 360-degree LiDAR has a much larger sensing range, fusion-based methods that require strict alignment between camera and LiDAR can only identify a small proportion of the point cloud (see the red region).</p><p>input sensors. Concretely, cameras provide dense color information and finegrained texture, but they are ambiguous in depth sensing and unreliable in low light conditions. In contrast, LiDARs robustly offer accurate and wide-ranging depth information regardless of lighting variances but only capture sparse and textureless data. Since cameras and LiDARs complement each other, it is better to perceive the surrounding with both sensors. Recently, many commercial cars have been equipped with both cameras and LiDARs. This excites the research community to improve the semantic segmentation by fusing the information from two complementary sensors <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. These approaches first establish the mapping between 3D points and 2D pixels by projecting the point clouds onto the image planes using the sensor calibrations. Based on the point-to-pixel mapping, the models fuse the corresponding image features into the point features, which are further processed to obtain the final semantic scores. Despite the improvements, fusion-based methods have the following unavoidable limitations: 1) Due to the difference of FOVs (field of views) between cameras and LiDARs, the point-to-pixel mapping cannot be established for points that are out of the image planes. Typically, the FOVs of LiDAR and cameras only overlap in a small portion (see <ref type="figure" target="#fig_7">Fig. 1</ref>), which significantly limits the application of fusion-based methods. 2) Fusion-based methods consume more computational resources since they process both images and point clouds (through multitask or cascade manners) at runtime, which introduces a great burden on real-time applications.</p><p>To address the above two issues, we focus on improving semantic segmentation by leveraging both images and point clouds through an effective design in this work. Considering the sensors are moving in the scenes, the non-overlap part of the 360-degree LiDAR point clouds corresponding to image in the same timestamp (see the gray region of the right part in <ref type="figure" target="#fig_7">Fig. 1</ref>) can be covered by images from other time-stamp. Besides, the dense and structural information of images provides useful regularization for both seen and unseen point cloud regions. Based on these observations, we propose a "model-independent" training scheme, namely 2D Priors Assisted Semantic Segmentation (2DPASS), to enhance the representation learning of any 3D semantic segmentation networks with minor structure modification. In practice, on the one hand, for above-mentioned nonoverlap regions, 2DPASS takes pure point clouds as the inputs to train the segmentation model. On the other hand, for subregions with well-aligned pointto-pixel mappings, 2DPASS adopts an auxiliary multi-modal fusion to aggregate image and point features in each scale, and then aligns the 3D predictions with the fusion predictions. Unlike previous cross-modal alignment <ref type="bibr" target="#b16">[17]</ref> apt to contaminate the modal-specific information, we design a multi-scale fusion-to-single knowledge distillation (MSFSKD) strategy to transfer extra knowledge to the 3D model as well as retaining its modal-specific ability. Compared with fusion-based methods, our solution has the following preferable properties: 1) Generality: It can be easily integrated with any 3D segmentation model with minor structural modification; 2) Flexibility: The fusion module is only used during the training to enhance the 3D network. After training, the enhanced 3D model can be deployed without image inputs. 3) Effectively: Even with only a small section of overlapped multi-modality data, our method can significantly boost the performance. As a result, we evaluate 2DPASS with a simple yet strong baseline implemented with sparse convolutions <ref type="bibr" target="#b2">[3]</ref>. The experiments show 2DPASS brings noticeable improvements even over this strong baseline. Equipped with 2DPASS using multi-modal data, our model achieves the top-1 results on the single and multiple-scan leaderboards of SemanticKITTI <ref type="bibr" target="#b15">[16]</ref>. The state-of-the-art results on the NuScenes <ref type="bibr" target="#b17">[18]</ref> dataset further confirm the generality of our method.</p><p>In general, the main contributions are summarized as follows.</p><p>-We propose 2D Priors Assisted Semantic Segmentation (2DPASS) that assists 3D LiDAR semantic segmentation with 2D priors from cameras. To the best of our knowledge, 2DPASS is the first method that distills multi-modal knowledge to single point cloud modality for semantic segmentation. -Equipped with the proposed multi-scale fusion-to-single knowledge distillation (MSFSKS) strategy, 2DPASS achieves the significant performance gains on SemanticKITTI and NuScenes benchmarks, ranking the 1st on single and multiple tracks of SemanticKITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single-Sensor Methods</head><p>Camera-Based Methods. Camera-based semantic segmentation aims to predict the pixel-wise labels for input 2D images. FCN <ref type="bibr" target="#b18">[19]</ref> is the pioneer in semantic segmentation, which proposes an end-to-end fully convolutional architecture based on image classification networks. Recent works have achieved significant improvements via exploring multi-scale features learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, dilated convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>, and attention mechanisms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>. However, camera-only methods are ambiguous in depth sensing and not robust in low light conditions. LiDAR-Based Methods. The LiDAR data is generally represented as point clouds. There are several mainstreams to process point clouds with different representations. 1) Point-based methods approximate a permutation-invariant set function using a per-point Multi-Layer Perceptron (MLP). PointNet <ref type="bibr" target="#b23">[24]</ref> is the pioneer in this field. Later on, many studies design point-wise MLP <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, adaptive weight <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> and pseudo grid <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> based methods to extract local features of point clouds or exploit nonlocal operators <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> to learn long distance dependency. However, point-based methods are not efficient in the Li-DAR scenario since their sampling and grouping algorithms are generally timeconsuming. 2) Projection-based methods are very efficient approaches for Li-DAR point clouds. They project point clouds onto 2D pixels so that traditional CNN can play a normal role. Previous works project all points scanned by the rotating LiDAR onto 2D images by plane projection <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>, spherical projection <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> or both <ref type="bibr" target="#b38">[39]</ref>. However, the projection inevitably causes information loss. And the projection-based methods currently meet the bottleneck of the segmentation accuracy. 3) Most recent works adopt voxel-based frameworks since they balance the efficiency and effectiveness, where sparse convolution (Spar-seConv) <ref type="bibr" target="#b2">[3]</ref> are most commonly utilized. Compared to traditional voxel-based methods (i.e., 3DCNN) directly transforming all points into the 3D voxel grids, SparseConv only stores non-empty voxels in a Hash table and conducts convolution operations only on these non-empty voxels in a more efficient way. Recently, many studies have used SparseConv to design more powerful network architectures. Cylinder3D <ref type="bibr" target="#b39">[40]</ref> changes original grid voxels to cylinder ones and designs an asymmetrical network to boost the performance. AF 2 -S3Net <ref type="bibr" target="#b40">[41]</ref> applies multiple branches with different kernel sizes, aggregating multi-scale features via an attention mechanism. 4) Very recently, there is a trend of exploiting multirepresentation fusion methods. These methods combine multiple representations above (i.e., points, projection images, and voxels) and design feature fusion among different branches. Tang et.al. <ref type="bibr" target="#b9">[10]</ref> combines point-wise MLPs in each sparse convolution block to learn a point-voxel representation and uses NAS to search for a more efficient architecture. RPVNet <ref type="bibr" target="#b41">[42]</ref> proposes range-point-voxel fusion network to utilizes information from three representations. Nevertheless, these methods only take sparse and textureless LiDAR point clouds as inputs, thus appearance and texture in the camera images have not been fully utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-Sensor Methods</head><p>Multi-sensor methods attempt to fuse information from two complementary sensors and leverage the benefits of both camera and LiDAR <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. RGBAL <ref type="bibr" target="#b13">[14]</ref> converts RGB images to a polar-grid mapping representation and designs early and mid-level fusion strategies. PointPainting <ref type="bibr" target="#b14">[15]</ref> exploits the segmentation logits of images and projects them to the LiDAR space by bird's-eye projection <ref type="bibr" target="#b22">[23]</ref> or spherical projection <ref type="bibr" target="#b44">[45]</ref> for LiDAR network performance improvement. Recently, PMF <ref type="bibr" target="#b12">[13]</ref> exploits a collaborative fusion of two modalities in camera coordinates. However, these methods require multi-sensor inputs in both training and inference phases. Moreover, the paired multi-modality data is usually computation-intensive and unavailable in practical application.</p><p>Image from Camera</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Network</head><p>LiDAR Points Cloud </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-modal Knowledge Transfer</head><p>Knowledge distillation was initially proposed for compressing the large teacher network to a small student one <ref type="bibr" target="#b45">[46]</ref>. Over the past few years, several subsequent studies enhanced knowledge transferring through matching feature representations in different manners <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>. For instance, aligning attention maps <ref type="bibr" target="#b48">[49]</ref> and Jacobean matrixes <ref type="bibr" target="#b49">[50]</ref> were independently applied. With the development of multi-modal computer vision, recent research apply knowledge distillation to transfer priors across different modalities, e.g., exploiting extra 2D images in the training phase and improving the performance in the inference <ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref>. Specifically, <ref type="bibr" target="#b55">[56]</ref> introduces the 2D-assisted pre-training, <ref type="bibr" target="#b56">[57]</ref> inflates the kernels of 2D convolution to the 3D ones, and <ref type="bibr" target="#b57">[58]</ref> applies well-designed teacher-student framework. Inspired but different from the above, we transfer 2D knowledge through a multi-scale fusion-to-single manner, which additionally takes care of the modal-specific knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>This paper focuses on improving the LiDAR point cloud semantic segmentation, which aims to assign the semantic label to each point. To handle difficulties in large-scale outdoor LiDAR point clouds, i.e., sparsity, varying density, and lack of texture, we introduce the strong regularization and priors from 2D camera images through a fusion-to-single knowledge transferring.</p><p>The workflow of our 2D Priors Assisted Semantic Segmentation (2DPASS) is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Since the camera images are pretty large (e.g., 1242 ? 512), sending the original ones to our multi-modal pipeline is intractable. Therefore, we randomly sample a small patch (480 ? 320) from the original camera image as the 2D input <ref type="bibr" target="#b16">[17]</ref>, accelerating the training processing without performance drop. Then the cropped image patch and LiDAR point cloud independently pass through independent 2D and 3D encoders, where multi-scale features from the two backbones are extracted in parallel. Afterwards, multi-scale fusion-to-single knowledge distillation (MSFSKD) is conducted to enhance the 3D network using multi-modal features, i.e., fully utilizing texture and color-aware 2D priors as well as retaining the original 3D-specific knowledge. Finally, all the 2D and 3D features at each scale are used to generate semantic segmentation predictions, which are supervised by pure 3D labels. During inference, the 2D-related branch can be discarded, which effectively prevents extra computational burden in real application compared with fusion-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modal-Specific Architectures</head><p>Multi-Scale Feature Encoders. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we use two different networks to independently encode multi-scale features from 2D image and 3D point cloud. We apply ResNet34 <ref type="bibr" target="#b58">[59]</ref> encoder with 2D convolution as the 2D network. For the 3D network, we adopt sparse convolution <ref type="bibr" target="#b2">[3]</ref> to construct the 3D network. One merit of sparse convolution lies in the sparsity, with which the convolution operation only considers the non-empty voxels. Specifically, we design a hierarchical point-voxel encoder as that used in the decoder of <ref type="bibr" target="#b9">[10]</ref>, and adopt the ResNet bottleneck <ref type="bibr" target="#b58">[59]</ref> in each scale while replacing the ReLU with Leaky ReLU <ref type="bibr" target="#b59">[60]</ref>. In both network, we extract L feature maps from different scales, obtaining the 2D and 3D features, i.e., {F 2D l } L l=1 and {F 3D l } L l=1 . Prediction Decoders. After processing the features from images and point clouds at each scale, two modal-specific prediction decoders are independently applied to restore the down-sampled feature maps to their original sizes.</p><p>For the 2D network, we adopt FCN <ref type="bibr" target="#b18">[19]</ref> decoder to up-sample the features from each encoder layer. Specifically, the feature map D 2D l from the l-th decoder layer can be gained by up-sampling the feature map from the (L ? l + 1)-th encoder layer, where all the up-sampled feature maps will be merged through element-wise addition. Finally, the semantic segmentation of the 2D network is obtained by passing the fused feature map through a linear classifier.</p><p>For the 3D network, we do not adopt the U-Net decoder used in previous methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. In contrast, we up-sample the features from different scales to the original size and concatenate them together before feeding them into the classifier. We find out that such a structure can better learn hierarchical information while gaining the prediction in a more efficient way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Point-to-Pixel Correspondence</head><p>Since the 2D features and 3D features are generally represented as pixels and points, respectively, it is difficult to directly transfer information between two modalities. In this section, we aim to generate paired features of two modalities  for further knowledge distillation, using the point-to-pixel correspondence. The details of paired feature generation in two modalities are demonstrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. 2D Features. The process of 2D feature generation is illustrated in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>. By cropping a small patch I ? R H?W ?3 from the original image and passing it through a 2D network, multi-scale features can be extracted in the hidden layers with different resolution. Taking the feature map F 2D l ? R H l ?W l ?D l from l-th layer as an example, we first conduct a decovolution operation to upscale its resolution to the original oneF 2D l . Similar to the recent multi-sensor method <ref type="bibr" target="#b12">[13]</ref>, we adopt perspective projection and calculate a point-to-pixel mapping between point clouds and images. Specifically, given a LiDAR point cloud</p><formula xml:id="formula_0">P = {p i } N i=1 ? R N ?3 , the projection of each 3D point p i = (x i , y i , z i ) ? R 3 to a pointp i = (u i , v i ) ? R 2 in</formula><p>the image plane is given as:</p><formula xml:id="formula_1">[u i , v i , 1] T = 1 z i ? K ? T ? [x i , y i , z i , 1] T ,<label>(1)</label></formula><p>where K ? R 3?4 and T ? R 4?4 are the camera intrinsic and extrinsic matrices respectively. K and T are directly provided in KITTI <ref type="bibr" target="#b60">[61]</ref>. Since the lidar and cameras operate at different frequencies in NuScenes <ref type="bibr" target="#b17">[18]</ref>, we need to transform the LiDAR frame at timestamp t l to camera frame at timestamp t c via the global coordinate system. The extrinsic matrix T in NuScenes dataset <ref type="bibr" target="#b17">[18]</ref> is given as:</p><formula xml:id="formula_2">T = T camera ?ego tc ? T ego tc ?global ? T global?ego t l ? T ego t l ?lidar<label>(2)</label></formula><p>After the projection, the point-to-pixel mapping is represented as</p><formula xml:id="formula_3">M img = {( v i , u i )} N i=1 ? R N ?2 ,<label>(3)</label></formula><p>where ? is the floor operation. According to the point-to-pixel mapping, we extract a point-wise 2D featureF 2D ? R N img ?D l from the original feature map F 2D if any pixel on the feature map is included in M img . Here N img &lt; N represents the number of points that are included in M img . 3D Features. The process of 3D features is relatively straightforward (as shown in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>). Specifically, for the point cloud P</p><formula xml:id="formula_4">= {(x i , y i , z i )} N i=1</formula><p>, we obtain a point-to-voxel mapping in the l-th layer through</p><formula xml:id="formula_5">M voxel l = {( x i /r l , y i /r l , z i /r l )} N i=1 ? R N ?3 ,<label>(4)</label></formula><p>where r l is the voxelization resolution in the l-th layer. After that, given the 3D feature F 3D l ? R N l ?D l from a sparse convolution layer, we gain a point-wise 3D featureF 3D l ? R N ?D l through nearest interpolation on the original feature map F 3D l according to M voxel l . Finally, we filter the points by discarding points outside the image FOV:</p><formula xml:id="formula_6">F 3D l = {f i |f i ?F 3D l , M img i,1 ? H, M img i,2 ? W } N i=1 ? R N img ?D l ,<label>(5)</label></formula><p>2D Ground Truths. Considering only 2D images is provided, the 2D groundtruths are obtained by projecting the 3D point labels to the corresponding image plane using above point-to-pixel mapping. Afterwards, the projected 2D ground truths can work as the supervision for the 2D branch. Features Correspondence. Since both 2D and 3D feature use the same pointto-pixel mapping, 2D featuresF 2D l and 3D featuresF 3D l in arbitrary l-th layer have the same number of point N img and point-to-pixel correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-Scale Fusion-to-Single Knowledge Distillation (MSFSKD)</head><p>As the key of 2DPASS, MSFSKD aims at improving the 3D representation in each scale using auxiliary 2D priors through a fusion-then-distillation manner. The knowledge distillation (KD) design of MSFSKD is partially inspired by <ref type="bibr" target="#b16">[17]</ref>. However, <ref type="bibr" target="#b16">[17]</ref> conducts KD in a naive cross-modal manner, i.e., simply aligning the outputs from two sets of single modal features (i.e. either 2D or 3D), which inevitably pushes the features from two modals to their overlapped space. Therefore, such a manner actually discards the modal-specific information, which is crucial in multi-sensor segmentation. Although this issue can be relieved by introducing extra segmentation heads <ref type="bibr" target="#b16">[17]</ref>, it is inherent for the cross-modal distillation, resulting in biased predictions. To this end, we propose multi-scale fusion-to-single knowledge distillation (MSFSKD) module as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, which first fuses features of both images and point clouds and then conducts unidirectional alignment between the fused and the point cloud features. In our fusion-then-distillation manner, the fusion well retains the complete information from multi-modal data. Besides, the unidirectional alignment ensures boosted point cloud features from fusion without losing modal-specific information.</p><p>Modality Fusion. For each scale, considering the 2D and 3D feature gaps owing to different backbones, it is ineffective to directly fuse the raw 3D feature? F 3D l into their 2D counterpartsF 2D l . Thus, we firstly transformF 3D l toF learner l through a "2D learner" MLP, which struggles to narrow the feature gap. Afterwards, theF learner l not only flows into the subsequent concatenation with 2D featuresF 2D l to gain the fused featuresF 2D3D l through another MLP, but also goes back into the original 3D features via a skip connection to yield enhanced 3D featuresF 3De l . Besides, similar to attention mechanism, the final enhanced fused featuresF 2D3De l is obtained by:</p><formula xml:id="formula_7">F 2D3De l =F 2D l +?(MLP(F 2D3D l )) F 2D3D l ,<label>(6)</label></formula><p>where ? denotes Sigmoid activation function.</p><p>Modality-Preserving KD. Although theF learner l is generated from pure 3D features, it is influenced by the segmentation loss of the 2D decoder as well, which takes enhanced fused featureF 2D3De l as inputs. Acting like a residual between fused and point features, the 2D learner featureF learner l well prevents the distillation from contaminating the modal-specific information inF 3D l , achieving a Modality-Preserving KD. Finally, two independent classifiers (fully-connected layers) are respectively applied on top ofF 2D3De l andF 3De l to obtain the semantic scores S 2D3D l and S 3D l . We choose KL divergence as the distillation loss L xM as follows:</p><formula xml:id="formula_8">L xM = D KL (S 2D3D l ||S 3D l ).<label>(7)</label></formula><p>Through such an implementation, it enforces the uni-directional distillation by pushing S 3D l closer to S 2D3D l . By taking such a knowledge distillation scheme, there are several advantages in our framework: 1) The 2D learner and the fusion-to-single distillation provides rich texture information and structural regularization to enhance the 3D feature learning without losing any modal-specific information in 3D.</p><p>2) The fusion branch is only adopted in the training phase. Therefore, the enhanced model can almost run without extra computational cost during the inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setups</head><p>Datasets. We extensively evaluate 2DPASS on two large-scale outdoor benchmarks: SemanticKITTI <ref type="bibr" target="#b15">[16]</ref> and Nuscenes <ref type="bibr" target="#b17">[18]</ref>. SemanticKITTI provides dense semantic annotations for each individual scan of sequences 00-10 in KITTI dataset <ref type="bibr" target="#b60">[61]</ref>. According to the official setting, sequence 08 is the validation split, while the remaining are the train split. SemanticKITTI uses sequences 11-21 in KITTI as the test set, whose labels are held on for blind online testing 1 .</p><p>NuScenes contains 1000 scenes which show a great diversity in inner cities traffic and weather conditions. It officially divides the data into 700/150/150 scenes for train/val/test. Similar to SemanticKITTI, the test set of NuScenes is used for online benchmarking 2 . For 2D sensors, KITTI has only two front-view cameras, while NuScenes has six cameras covering the full 360 ? fields of view. Evaluation Metrics. We evaluate methods mainly using mean intersection over union (mIoU), which is defined as the average IoU over all classes. Additionally, we report the overall accuracy (Acc)/ frequency-weighted IOU (FwIOU) provided by the online leaderboard of two benchmarks. FwIoU is similar to mIoU except that each IoU is weighted by the point-level frequency of its class. Network Setup. We apply ResNet34 <ref type="bibr" target="#b58">[59]</ref> encoder with 2D convolution as the 2D network, where features after each down-sampling layers are extracted to generate 2D features. The 3D encoder is a modified SPVCNN <ref type="bibr" target="#b9">[10]</ref> (voxel size 0.1) with fewer parameters, whose hidden dimensions are 64 for SemanticKITTI and 128 for NuScenes to speed up the network. The number of layers L for MSFSKD  is set to 4 and 6 for SemanticKITTI and NuScenes, respectively. In each scale of knowledge distillation, 2D and 3D features are reduced to 64 dimensions through deconvolution or MLPs. Similarly, the hidden size of MLPs and 2D learner in MSFSKD are identically 64. Training and Inference Details. We employ the cross-entropy and Lovasz losses as <ref type="bibr" target="#b39">[40]</ref> for semantic segmentation. For the knowledge distillation, we set the proportion of segmentation loss and KL divergence as 1 : 0.05. Test-time augmentation <ref type="bibr" target="#b39">[40]</ref> is applied during the inference. Training details will be introduced in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark Results</head><p>SemanticKITTI. SemanticKITTI evaluates segmentation performance using two settings: single scan and multiple scans. For methods using a single scan as input, moving and non-moving are mapped to a single class. While methods using multiple scans as inputs should distinguish between moving and non-moving  objects, which is more challenging. All the reported results are from the official blind test competition website of SemanticKITTI.</p><p>Tab. 1 shows our performance under the single scan setting. Our baseline without 2DPASS already performs on par with a strong model Cylinder3D <ref type="bibr" target="#b39">[40]</ref> while runs at a faster speed. Even so, the application of 2DPASS still brings a significant improvement over the baseline. Thanks to the auxiliary knowledge distillation, 2DPASS does not put any extra burden on the original model and thus does not sacrifice the running speed of the baseline. Overall, 2DPASS achieves the best result in terms of mIoU and running speed, outperforming the state-of-the-art (i.e., (AF) 2 -S3Net [41]) by 2.1%. The visualization results on SemanticKITTI single scan are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>Tab. 2 reports the results under the multiple scans setting. The mIoU and overall accuracy are calculated over all 25 classes. Due to the limited space, we only report the per-class IOUs for dynamic objects with non-moving/moving properties. Under this challenge setting, 2DPASS surprisingly surpasses previous approaches with even larger margins, i.e., achieving better mIoU (5.5% improvement over (AF) 2 -S3Net <ref type="bibr" target="#b40">[41]</ref>) and overall accuracy.</p><p>NuScenes. The results on NuScenes are reported in Tab. 3, where 2DPASS achieves the 1st place as well. Note that we only include published works in Tab. 3 and the results are directly taken from the official leaderboard of NuScenes, where our model also ranks the 3rd place with slight disadvantage when considering unpublished works. Besides surpassing all single-modal methods, 2DPASS surprisingly outperforms those fusion-based approaches (the last two rows in Tab. 3). Note that NuScenes provides images covering the whole FOV of the LiDAR, and fusion-based approaches achieve such results by using both point clouds and image features during the inference. In contrast, our method only takes point clouds as input.   <ref type="bibr" target="#b67">[68]</ref> are pure knowledge distillation designs, where the former is the pioneer for the research field and the latter is newly proposed. As shown in the Tab. 4, pure knowledge distillation manners cannot be directly adopted on the LiDAR semantic segmentation, and their improvement upon the baseline model is limited. Recently, <ref type="bibr" target="#b16">[17]</ref> adopts cross-modal feature alignment technique in the task of domain adaptation on semantic segmentation. However, their improvement is still marginal. To the end, in the Tab. 4, 2DPASS significantly performs better, which illustrates the effectiveness of our multi-scale fusion-to-single knowledge distillation (MSFSKD). Design Analysis of MSFSKD. Tab. 5 demonstrates the ablation study on SemanticKITTI validation set. As shown in the table, our baseline only achieves a lower result of 65.58 mIoU. Note that simply using feature alignment between two modalities cannot effectively improve the result, where the metric of mIoU will be only increased to <ref type="bibr">66.34</ref>. After using 2D-3D fusion in each knowledge distillation scale, there is a significant improvement to 69.13. This improvement mainly comes from the knowledge provided by the stronger fusion prediction. Finally, we find out that 2D learner design can slightly improve the performance by about 0.2%. Note that the results on SemanticKITTI validation set is lower than that on benchmark since small object category (i.e., motocyclist) only occupies a small proportion. Distance-based Evaluation. We investigate how segmentation is affected by distance of the points to the ego-vehicle, and compare 2DPASS, current stateof-the-art and the baseline on the SemanticKITTI validation set. <ref type="figure" target="#fig_6">Fig. 6 (a)</ref> illustrates the mIoU of 2DPASS as opposed to the baseline and (AF) 2 -S3Net. The results of all the methods get worse by increasing the distance since points are relatively sparse in the long distance. 2DPASS improves the performance greatly within 10m, i.e., from 61.2 to 89.1, which is the best distance for the camera to capture objects' color and texture. There is also a significant improvement upon (AF) 2 -S3Net within this distance, i.e., 84.4 v.s. 89.1. Generality. We show our 2DPASS can be a "model-independent" training scheme that boosts the performance of other networks. We additionally trained two open-sourced baselines, i.e., MinkowskiNet and SPVCNN implemented in <ref type="bibr" target="#b9">[10]</ref> with 2DPASS. During the experiment, we keep all the setups the same except for the 2D-related components. As shown in <ref type="figure" target="#fig_6">Fig. 6 (b)</ref>, 2DPASS improves the former one from 63.1 to 66.2 and the latter from 63.8 to 66.9. These results sufficiently demonstrate the effectiveness and generality of 2DPASS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work proposes the 2D Priors Assisted Semantic Segmentation (2DPASS), a general training scheme, to boost the performance of LiDAR point cloud semantic segmentation via 2D prior-related knowledge distillation. By leveraging an auxiliary modal fusion and knowledge distillation in a multi-scale manner, 2DPASS acquires richer semantic and structural information from the multimodal data, effectively enhancing the performance of a pure 3D network. Eventually, it achieves the state-of-the-arts on two large-scale benchmarks (i.e., Se-manticKITTI and NuScenes). We believe that our work can be applied to a wider range of other scenarios in the future, such as 3D detection and tracking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Training and Inference Details</head><p>For the 3D input, we utilize the widely used data augmentation strategy for semantic segmentation, including global scaling with a random scaling factor sampled from [0.95, 1.05], and global rotation around the Z axis with a random angle. For the 2D input, we employ horizontal flipping and color jitter. Each 2D image is cropped to the size 480 ? 320 (width ? height) for faster training. The 2DPASS is trained in an end-to-end manner with the SGD optimizer. For the SemanticKITTI validation set, our model was trained with batch size 8 and learning rate 0.24 for 64 epochs, which is kept the same as SPVCNN <ref type="bibr" target="#b9">[10]</ref> for fair comparison. For the SemanticKITTI online benchmark, we conduct instance CutMix as <ref type="bibr" target="#b41">[42]</ref>, and fine-tune the last checkpoint with additional 48 epochs. As for the NuScenes dataset, we trained the model with batch size 16 for 80 epochs since the number of points per scene in NuScenes is generally smaller. During the inference, following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref>, we apply the voting test-time augmentation, i.e., rotating the input scene with 12 angles around the Z axis and averaging the prediction scores. All experiments are on Nvidia Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Comparing with Multi-Sensor Architecture</head><p>To further demonstrate the advantages of our 2DPASS upon multi-sensor methods, we set several multi-sensor baselines and compare against them.</p><p>-PointPainting: We follow the setup of previous work <ref type="bibr" target="#b14">[15]</ref>, which exploits the segmentation logits of images and projects them to the LiDAR space by bird's-eye projection <ref type="bibr" target="#b22">[23]</ref> or spherical projection <ref type="bibr" target="#b44">[45]</ref>. Here, we use several pre-trained backbones, i.e., FCN <ref type="bibr" target="#b18">[19]</ref> with ResNet34 <ref type="bibr" target="#b58">[59]</ref> and DeepLab v3 <ref type="bibr" target="#b4">[5]</ref>, to achieve the 2D semantic segmentation logits. After that, we use outputs of 2D backbones as the inputs of our 3D network. -Multi-branch Baseline: As shown in <ref type="figure" target="#fig_7">Fig. 1 (a)</ref>, we design an ensemble architecture through concatenating the output logits from the two modalities. -Multi-branch with Interaction: Instead of only concatenating the predictions, we also concatenate the 2D features from each layer into the corresponding layers in the 3D network, as illustrated in <ref type="figure" target="#fig_7">Fig. 1 (b)</ref>. -2DPASS (light): Since above multi-sensor manners are trained with the entire 2D image as input, they are time-consuming and GPU memory cost expensive. So we set all of hidden dimensions as 64 in the 3D network due to GPU memory limitation. This design is different from our manuscript with hidden dimensions 128 due to our light memory cost.</p><p>The experiment results are shown in <ref type="table" target="#tab_0">Table 1</ref>, where we illustrate the results on NuScenes validation set and inference time (speeds), respectively. As shown  in <ref type="table" target="#tab_0">Table 1</ref>, using naive combination such as PointPainting <ref type="bibr" target="#b14">[15]</ref> and concatenation (i.e., Multi-branch Baseline) of prediction cannot improve the segmentation results obviously while introducing huge computational burden (i.e., there are six 1600?900 camera images corresponding to each point cloud). Exploiting feature combination in each scale can slightly improve the performance, but leads to much slower network compared with the pure 3D network. On the contrary, 2DPASS (light) achieves the second-best performance in term of mIoU criterion while 60? speed faster than multi-sensor methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Concrete Results</head><p>In this section, we give our detailed results on the NuScenes dataset in <ref type="table" target="#tab_1">Table 2</ref> as a benchmark for future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>2D Priors Assisted Semantic Segmentation (2DPASS). It first crops a small patch from the original camera image as the 2D input. Then the cropped image patch and LiDAR point cloud independently pass through the 2D and 3D encoders to generate multi-scale features in parallel. Afterwards, for each scale, complementary 2D knowledge is effectively transferred to the 3D network via the multi-scale fusion-tosingle knowledge distillation (MSFSKD). The feature maps (in the form of either pixel grid or point set) are used to generate the final semantic scores using modal-specific decoders, which are supervised by pure 3D labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>l-th layer features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>2D and 3D feature generation. Part (a) demonstrates the 2D feature generation, where the point cloud will first be projected onto the image patch and generate the point-to-pixel (P2P) mapping. After that, it transfers the 2D feature map to the point-wise 2D features according to P2P mapping. Part (b) shows the 3D feature generation. The point-to-voxel (P2V) mapping is easy to obtain, and the voxel features will be interpolated onto the point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FFig. 4 .</head><label>4</label><figDesc>Internal structure of Multi-Scale Fusion-to-Single Knowledge Distillation (MSFSKD), which consists of the modality fusion and Modality-Preserving KD. For each scale, modality fusion is first ultilized to achieve an enhanced multi-modality featureF 2D3De l . Afterwards, the enhanced featureF 2D3De l promotes the 3D represen-tationF 3De l through the uni-directional Modality-Preserving KD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results of 2DPASS on the validation set of SemanticKITTI. Our baseline has a higher error recognizing small objects and region boundaries, while 2DPASS recognizes small objects better thanks to the prior of 2D modality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>3 77.3 72.0 83.9 73.8 97.1 66.5 77.5 74.0 87.7 86.8 270 PMF [13] L+C 77.0 89.0 82.0 40.0 81.0 88.0 64.0 79.0 80.0 76.0 81.0 67.0 97.0 68.0 78.0 74.0 90.0 88.0 125* 2D3DNet [66] L+C 80.0 90.1 83.0 59.4 88.0 85.1 63.7 84.4 82.0 76.0 84.8 71.9 96.9 67.4 79.8 76.0 92.1 89.2 -Baseline L 77.6 88.5 80.8 37.9 92.7 90.5 65.4 77.6 71.5 70.9 83.1 75.3 97.0 69.3 78.1 75.6 89.1 86.8 44 2DPASS(Ours) L 80.8 90.1 81.7 55.3 92.0 91.8 73.3 86.5 78.5 72.5 84.7 75.5 97.6 69.1 79.9 75.5 90.2 88.0 44</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Extensive experiment results. The part (a) shows the results on Se-manticKITTI validation set with different distance-range. Part (b) demonstrates the results before and after exploiting 2DPASS on MinkowskiNet<ref type="bibr" target="#b9">[10]</ref> and SPVCNN<ref type="bibr" target="#b9">[10]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 1 .</head><label>1</label><figDesc>The illustration of multi-sensor methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Semantic segmentation results on the SemanticKITTI test benchmark. Only approaches published before 03/08/2022 are compared. 89.7 74.7 67.4 40.0 93.5 97.0 61.1 63.6 63.4 61.5 86.2 73.9 71.0 77.9 81.3 74.1 72.9 65.0 70.4 62</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other-ground</cell><cell>building</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicle</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>fence</cell><cell>pole</cell><cell>traffic sign</cell><cell>speed (ms)</cell></row><row><cell cols="22">SqueezeSegV2 [38] 39.7 88.6 67.6 45.8 17.7 73.7 81.8 13.4 18.5 17.9 14.0 71.8 35.8 60.2 20.1 25.1 3.9 41.1 20.2 26.3 -</cell></row><row><cell cols="22">DarkNet53Seg [16] 49.9 91.8 74.6 64.8 27.9 84.1 86.4 25.5 24.5 32.7 22.6 78.3 50.1 64.0 36.2 33.6 4.7 55.0 38.9 52.2 -</cell></row><row><cell cols="22">RangeNet53++ [45] 52.2 91.8 75.2 65.0 27.8 87.4 91.4 25.7 25.7 34.4 23.0 80.5 55.1 64.6 38.3 38.8 4.8 58.6 47.9 55.9 83.3</cell></row><row><cell>3D-MiniNet [62]</cell><cell cols="21">55.8 91.6 74.5 64.2 25.4 89.4 90.5 28.5 42.3 42.1 29.4 82.8 60.8 66.7 47.8 44.1 14.5 60.8 48.0 56.6 -</cell></row><row><cell>SqueezeSegV3 [8]</cell><cell cols="21">55.9 91.7 74.8 63.4 26.4 89.0 92.5 29.6 38.7 36.5 33.0 82.0 58.7 65.4 45.6 46.2 20.1 59.4 49.6 58.9 238</cell></row><row><cell>PointNet++ [25]</cell><cell cols="21">20.1 72.0 41.8 18.7 5.6 62.3 53.7 0.9 1.9 0.2 0.2 46.5 13.8 30.0 0.9 1.0 0.0 16.9 6.0 8.9 5900</cell></row><row><cell>TangentConv [36]</cell><cell cols="21">40.9 83.9 63.9 33.4 15.4 83.4 90.8 15.2 2.7 16.5 12.1 79.5 49.3 58.1 23.0 28.4 8.1 49.0 35.8 28.5 3000</cell></row><row><cell>PointASNL [31]</cell><cell cols="21">46.8 87.4 74.3 24.3 1.8 83.1 87.9 39.0 0.0 25.1 29.2 84.1 52.2 70.6 34.2 57.6 0.0 43.9 57.8 36.9 -</cell></row><row><cell>RandLA-Net [1]</cell><cell cols="21">55.9 90.5 74.0 61.8 24.5 89.7 94.2 43.9 29.8 32.2 39.1 83.8 63.6 68.6 48.4 47.4 9.4 60.4 51.0 50.7 880</cell></row><row><cell>KPConv [29]</cell><cell cols="21">58.8 90.3 72.7 61.3 31.5 90.5 95.0 33.4 30.2 42.5 44.3 84.8 69.2 69.1 61.5 61.6 11.8 64.2 56.4 47.4 -</cell></row><row><cell>PolarNet [63]</cell><cell cols="21">54.3 90.8 74.4 61.7 21.7 90.0 93.8 22.9 40.3 30.1 28.5 84.0 65.5 67.8 43.2 40.2 5.6 61.3 51.8 57.5 62</cell></row><row><cell>JS3C-Net [2]</cell><cell cols="21">66.0 88.9 72.1 61.9 31.9 92.5 95.8 54.3 59.3 52.9 46.0 84.5 69.8 67.9 69.5 65.4 39.9 70.8 60.7 68.7 471</cell></row><row><cell>SPVNAS [10]</cell><cell cols="21">67.0 90.2 75.4 67.6 21.8 91.6 97.2 56.6 50.6 50.4 58.0 86.1 73.4 71.0 67.4 67.1 50.3 66.9 64.3 67.3 259</cell></row><row><cell>Cylinder3D [40]</cell><cell cols="21">68.9 92.2 77.0 65.0 32.3 90.7 97.1 50.8 67.6 63.8 58.5 85.6 72.5 69.8 73.7 69.2 48.0 66.5 62.4 66.2 131</cell></row><row><cell>RPVNet [42]</cell><cell cols="21">70.3 93.4 80.7 70.3 33.3 93.5 97.6 44.2 68.4 68.7 61.1 86.5 75.1 71.7 75.9 74.4 43.4 72.1 64.8 61.4 168</cell></row><row><cell>(AF) 2 -S3Net [41]</cell><cell cols="21">70.8 92.0 76.2 66.8 45.8 92.5 94.3 40.2 63.0 81.4 40.0 78.6 68.0 63.1 76.4 81.7 77.7 69.6 64.0 73.3 -</cell></row><row><cell>Baseline</cell><cell cols="21">67.4 89.8 73.8 62.1 33.5 91.9 96.3 54.9 51.1 55.8 51.6 86.5 72.3 71.3 76.8 79.8 30.3 68.7 63.7 70.2 62</cell></row><row><cell>2DPASS(Ours)</cell><cell>72.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison to the state-of-the-art methods on the test set of SemanticKITTI multiple scans challenge. -s indicates static and -m stands for moving.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>Acc</cell><cell>car-s</cell><cell>car-m</cell><cell>truck-s</cell><cell>truck-m</cell><cell>other-s</cell><cell>other-m</cell><cell>person-s</cell><cell>person-m</cell><cell>bicyclist-s</cell><cell>bicyclist-m</cell><cell>motorcyclist-s</cell><cell>motorcyclist-m</cell></row><row><cell>LatticeNet [64]</cell><cell cols="14">45.2 89.3 91.1 54.8 29.7 3.5 23.1 0.6 6.8 49.9 0.0 44.6 0.0 64.3</cell></row><row><cell cols="15">TemporalLidarSeg [65] 47.0 89.6 92.1 68.2 39.2 2.1 35.0 12.4 14.4 40.4 0.0 42.8 0.0 12.9</cell></row><row><cell>KPConv [29]</cell><cell cols="14">51.2 89.3 93.7 69.4 42.5 5.8 38.6 4.7 21.6 67.5 0.0 67.4 0.0 47.2</cell></row><row><cell>Cylinder3D [40]</cell><cell cols="14">52.5 91.0 94.6 74.9 41.3 0.0 38.8 0.1 12.5 65.7 1.7 68.3 0.2 11.9</cell></row><row><cell>(AF) 2 -S3Net [41]</cell><cell cols="14">56.9 88.1 91.8 65.3 15.7 5.6 27.5 3.9 16.4 67.6 15.1 66.4 67.1 59.6</cell></row><row><cell>2DPASS(Ours)</cell><cell cols="14">62.4 91.4 96.2 82.1 48.2 16.1 52.7 3.8 35.4 80.3 7.9 71.2 62.0 73.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Semantic segmentation results on the Nuscenes test benchmark. Only approaches published before 03/08/2022 are compared. L and C stand for LiDAR and camera, respectively. (*) The speed reported in PMF<ref type="bibr" target="#b12">[13]</ref> is accelerated by TensorRT, and we test their model without such technique in the same environment.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>mIoU</cell><cell>FW mIoU</cell><cell>barrier</cell><cell>bicycle</cell><cell>bus</cell><cell>car</cell><cell>construction</cell><cell>motorcycle</cell><cell>pedestrian</cell><cell>traffic cone</cell><cell>trailer</cell><cell>truck</cell><cell>driveable</cell><cell>other flat</cell><cell>sidewalk</cell><cell>terrain</cell><cell>manmade</cell><cell>vegetation</cell><cell>speed (ms)</cell></row><row><cell>PolarNet [63]</cell><cell cols="20">L 69.4 87.4 72.2 16.8 77.0 86.5 51.1 69.7 64.8 54.1 69.7 63.5 96.6 67.1 77.7 72.1 87.1 84.5 -</cell></row><row><cell>JS3C-Net [2]</cell><cell cols="20">L 73.6 88.1 80.1 26.2 87.8 84.5 55.2 72.6 71.3 66.3 76.8 71.2 96.8 64.5 76.9 74.1 87.5 86.1 -</cell></row><row><cell>Cylinder3D [40]</cell><cell cols="20">L 77.2 89.9 82.8 29.8 84.3 89.4 63.0 79.3 77.2 73.4 84.6 69.1 97.7 70.2 80.3 75.5 90.4 87.6 63</cell></row><row><cell>AMVNet [39]</cell><cell cols="20">L 77.3 90.1 80.6 32.0 81.7 88.9 67.1 84.3 76.1 73.5 84.9 67.3 97.5 67.4 79.4 75.5 91.5 88.7 85</cell></row><row><cell>SPVCNN [10]</cell><cell cols="20">L 77.4 89.7 80.0 30.0 91.9 90.8 64.7 79.0 75.6 70.9 81.0 74.6 97.4 69.2 80.0 76.1 89.3 87.1 63</cell></row><row><cell cols="10">(AF) 2 -S3Net [41] L 78.3 88.5 78.9 52.2 89.9 84.2 77.4 74.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison with different knowledge distillation.</figDesc><table><row><cell>Method</cell><cell>SemanticKITTI</cell></row><row><cell>Hinton et.al. [46]</cell><cell>66.34</cell></row><row><cell>Huang et.al. [67]</cell><cell>66.46</cell></row><row><cell>Yang et.al. [68]</cell><cell>66.75</cell></row><row><cell>xMUDA [17]</cell><cell>67.88</cell></row><row><cell>2DPASS</cell><cell>69.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on the SemanticKITTI validation set. Hinton et.al. [46], Huang et.al. [67] and Yang et.al.</figDesc><table><row><cell>baseline</cell><cell>MSFSKD KL Div Modality Fusion 2D Learner</cell><cell>SemanticKITTI</cell></row><row><cell></cell><cell></cell><cell>65.58</cell></row><row><cell></cell><cell></cell><cell>66.34</cell></row><row><cell></cell><cell></cell><cell>69.13</cell></row><row><cell></cell><cell></cell><cell>69.32</cell></row><row><cell>4.3 Comprehensive Analysis</cell><cell></cell><cell></cell></row><row><cell cols="3">Comparing with Other Knowledge Distillation. To further verify the ef-</cell></row><row><cell cols="3">fectiveness of our fusion-to-single knowledge distillation paradigm upon com-</cell></row><row><cell cols="3">mon teach-student architecture and other cross-modal manners, we compare</cell></row><row><cell cols="3">2DPASS with typical approaches of knowledge transfer in Tab. 4, where we uti-</cell></row><row><cell cols="3">lize these methods in each scale for fair comparison. Among all the methods,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Acknowledgment. This work was supported in part by NSFC-Youth 61902335, by the Basic Research Project No. HZQB-KCZYZ-2021067 of Hetao Shenzhen HK S&amp;T Cooperation Zone, by the National Key R&amp;D Program of China with grant No.2018YFB1800800, by Shenzhen Outstanding Talents Training Fund, by Guangdong Research Project No. 2017ZT07X152 and No. 2019CX01X104, by the Guangdong Provincial Key Laboratory of Future Networks of Intelligence</figDesc><table /><note>(Grant No. 2022B1212010001), by the NSFC 61931024&amp;8192 2046, by NSFC- Youth 62106154, by zelixir biotechnology company Fund, by Tencent Open Fund, and by ITSO at CUHKSZ.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>Comparison with different multi-sensor manners.</figDesc><table><row><cell>Method</cell><cell cols="2">mIoU (%) Speed (ms)</cell></row><row><cell>PointPainting-FCN-ResNet34</cell><cell>76.54</cell><cell>2330</cell></row><row><cell>PointPainting-DeepLabV3</cell><cell>76.56</cell><cell>3347</cell></row><row><cell>Multi-branch Baseline</cell><cell>77.25</cell><cell>2353</cell></row><row><cell cols="2">Multi-branch with Interaction 79.12</cell><cell>2374</cell></row><row><cell>Basline (light)</cell><cell>76.04</cell><cell>40</cell></row><row><cell>2DPASS (light)</cell><cell>78.87</cell><cell>40</cell></row><row><cell>2D Network</cell><cell></cell><cell></cell></row><row><cell>3D</cell><cell></cell><cell></cell></row><row><cell>Decoder</cell><cell></cell><cell></cell></row><row><cell>3D Network</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>Semantic segmentation results on the NuScenes valid set. -S3Net [41] L 62.2 60.3 12.6 82.3 80.0 20.1 62.0 59.0 49.0 42.2 67.4 94.2 68.0 64.1 68.6 82.9 82.4 AMVNet [39] L 76.1 79.8 32.4 82.2 86.4 62.5 81.9 75.3 72.3 83.5 65.1 97.4 67.0 78.8 74.6 90.8 87.9 Cylinder3D [40] L 76.1 76.4 40.3 91.2 93.8 51.3 78.0 78.9 64.9 62.1 84.4 96.8 71.6 76.4 75.4 90.5 87.4 RPVNet [42] L 77.6 78.2 43.4 92.7 93.2 49.0 85.7 80.5 66.0 66.9 84.0 96.9 73.5 75.9 76.0 90.6 88.9 PMF [13] L+C 76.9 74.1 46.6 89.8 92.1 57.0 77.7 80.9 70.9 64.6 82.9 95.5 73.3 73.6 74.8 89.4 87.7 2D3DNet [66] L+C 79.0 78.3 55.1 95.4 87.7 59.4 79.3 80.7 70.2 68.2 86.6 96.1 74.9 75.7 75.1 91.4 89.9 Baseline L 76.2 75.3 43.5 95.3 91.2 54.5 78.9 72.8 62.1 70.0 83.2 96.3 73.2 74.2 74.9 88.1 85.9 2DPASS(Ours) L 79.4 78.8 49.6 95.6 93.6 60.0 84.1 82.2 67.5 72.6 88.1 96.8 72.8 76.2 76.5 89.4 87.2</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>mIoU</cell><cell>barrier</cell><cell>bicycle</cell><cell>bus</cell><cell>car</cell><cell>construction</cell><cell>motorcycle</cell><cell>pedestrian</cell><cell>traffic cone</cell><cell>trailer</cell><cell>truck</cell><cell>driveable</cell><cell>other flat</cell><cell>sidewalk</cell><cell>terrain</cell><cell>manmade</cell><cell>vegetation</cell></row><row><cell>(AF) 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/competitions/20331 2 https://eval.ai/web/challenges/challenge-page/720/leaderboard/1967</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3101" to="3109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ccnet: Crisscross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9939" to="9948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="685" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Beyond 3d siamese tracking: A motion-centric paradigm for 3d single object tracking in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01730</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Box-aware feature enhancement for single object tracking on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13199" to="13208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perception-aware multisensor fusion for 3d lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16280" to="16290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rgb and lidar fusion based 3d semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>El Madawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4604" to="4612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">xmuda: Cross-modal unsupervised domain adaptation for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12605" to="12614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<title level="m">Ocnet: Object context network for scene parsing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="134826" to="134840" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep projective 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">3DOR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page" from="1887" to="1893" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Widjaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Chong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cylinder3d: An effective 3d framework for driving-scene lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01550</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Af2-s3net: Attentive feature fusion with adaptive feature selection for sparse semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12547" to="12556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rpvnet: A deep and efficient range-point-voxel fusion network for lidar point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16024" to="16033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fuseseg: Lidar point cloud segmentation fusing multi-modal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krispel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sensor fusion for joint 3d object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2654" to="2662" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Knowledge transfer with jacobian matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4723" to="4731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An efficient approach to informative feature extraction from multimodal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5281" to="5288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Rgb-based 3d hand pose estimation via privileged learning with depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07376</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">3d-to-2d distillation for indoor scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Knowledge as priors: Cross-modal knowledge generalization for datasets without superior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6528" to="6537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04687</idno>
		<title level="m">Learning from 2d: Pixel-to-point knowledge transfer for 3d pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04180</idno>
		<title level="m">Image2point: 3d point-cloud understanding with pretrained 2d convnets</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">X-trans2cap: Crossmodal knowledge transfer using transformer for 3d dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="8563" to="8573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning a 2d representation from point clouds for fast and efficient 3d lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10893</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Latticenet: Fast point cloud segmentation using permutohedral lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05905</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Lidar-based recurrent 3d semantic segmentation with temporal memory alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Duerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfaller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Weigel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="781" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning 3d semantic segmentation with only 2d image supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="361" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation: An inheritance and exploration framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3579" to="3588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Knowledge distillation via softmax regression representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B G T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
