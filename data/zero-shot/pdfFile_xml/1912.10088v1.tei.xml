<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of Picture Quality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqiang</forename><surname>Ying</surname></persName>
							<email>zqying@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Niu</surname></persName>
							<email>haoranniu@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praful</forename><surname>Gupta</surname></persName>
							<email>prafulgupta@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
							<email>deeptigp@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
							<email>bovik@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of Picture Quality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Blind or no-reference (NR) perceptual picture quality prediction is a difficult, unsolved problem of great consequence to the social and streaming media industries that impacts billions of viewers daily. Unfortunately, popular NR prediction models perform poorly on real-world distorted pictures. To advance progress on this problem, we introduce the largest (by far) subjective picture quality database, containing about 40000 real-world distorted pictures and 120000 patches, on which we collected about 4M human judgments of picture quality. Using these picture and patch quality labels, we built deep region-based architectures that learn to produce state-of-the-art global picture quality predictions as well as useful local picture quality maps. Our innovations include picture quality prediction architectures that produce global-to-local inferences as well as local-toglobal inferences (via feedback).</p><p>It is important to distinguish between the concepts of picture quality [2] and picture aesthetics <ref type="bibr" target="#b6">[7]</ref>. Picture quality is specific to perceptual distortion, while aesthetics also relates to aspects like subject placement, mood, artistic value, and so on. For instance, <ref type="figure">Fig. 2(a)</ref> is noticeably blurred and of lower perceptual quality than <ref type="figure">Fig. 2(b)</ref>, which is less distorted. Yet, <ref type="figure">Fig. 2(a)</ref> is more aesthetically pleasing than the unsettling <ref type="figure">Fig. 2(b)</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Digital pictures, often of questionable quality, have become ubiquitous. Several hundred billion photos are uploaded and shared annually on social media sites like Facebook, Instagram, and Tumblr. Streaming services like Netflix, Amazon Prime Video, and YouTube account for 60% of all downstream internet traffic <ref type="bibr" target="#b0">[1]</ref>. Being able to understand and predict the perceptual quality of digital pictures, given resource constraints and increasing display sizes, is a high-stakes problem.</p><p>It is a common misconception that if two pictures are impaired by the same amount of a distortion (e.g., blur), they will have similar perceived qualities. However, this is far from true because of the way the vision system processes picture impairments. For example, Figs. 1(a) and (b) have identical amounts of JPEG compression applied, but * ? Equal contribution (a) (b) (c) <ref type="figure" target="#fig_6">Fig. 1</ref>: Challenges in distortion perception: Quality of a (distorted) image as perceived by human observers is perceptual quality. Distortion perception is highly content-dependent. Pictures (a) and (b) were JPEG compressed using identical encode parameters, but present very different degrees of perceptual distortion. The spatially uniform noise in (c) varies in visibility over the picture content, because of contrast masking <ref type="bibr" target="#b1">[2]</ref>. <ref type="figure" target="#fig_6">Fig. 1</ref>(a) appears relatively unimpaired perceptually, while <ref type="figure" target="#fig_6">Fig. 1(b)</ref> is unacceptable. On the other hand, <ref type="figure" target="#fig_6">Fig. 1</ref>(c) has had spatially uniform white noise applied to it, but its perceived distortion severity varies across the picture. The complex interplay between picture content and distortions (largely determined by masking phenomena <ref type="bibr" target="#b1">[2]</ref>), and the way distortion artifacts are visually processed, play an important role in how visible or annoying visual distortions may present themselves. Moreover, perceived quality correlates poorly with simple quantities like resolution and bit rate <ref type="bibr" target="#b2">[3]</ref>. Generally, predicting perceptual picture quality is a hard, long-standing research problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, despite its deceptive simplicity (we sense distortion easily with little, if any, thought).</p><p>(a) (b) <ref type="figure">Fig. 2</ref>: Aesthetics vs. perceptual quality (a) is blurrier than (b), but likely more aesthetically pleasing to most viewers.</p><p>adding film grain <ref type="bibr" target="#b7">[8]</ref> or blur (bokeh) <ref type="bibr" target="#b8">[9]</ref> to achieve photographic effects. While both concepts are important, picture quality prediction is a critical, high-impact problem affecting several high-volume industries, and is the focus of this work. Robust picture quality predictors can significantly improve the visual experiences of social media, streaming TV and home cinema, video surveillance, medical visualization, scientific imaging, and more. In many such applications, it is greatly desired to be able to assess picture quality at the point of ingestion, to better guide decisions regarding retention, inspection, culling, and all further processing and display steps. Unfortunately, measuring picture quality without a pristine reference picture is very hard. This is the case at the output of any camera, and at the point of content ingestion by any social media platform that accepts user-generated content (UGC). No-reference (NR) or blind picture quality prediction is largely unsolved, though popular models exist <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. While these are often predicated on solid principles of visual neuroscience, they are also simple and computationally shallow, and fall short when tested on recent databases containing difficult, complex mixtures of real-world picture distortions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Solving this problem could affect the way billions of pictures uploaded daily are culled, processed, compressed, and displayed.</p><p>Towards advancing progress on this high-impact unsolved problem, we make several new contributions. ? We built the largest picture quality database in existence.  <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. This data will help us to learn these relationships and to better model global picture quality. ? We created a series of state-of-the-art deep blind picture quality predictors, that builds on existing deep neural network architectures. Using a modified ResNet <ref type="bibr" target="#b20">[21]</ref> as a baseline, we (a) use patch and picture quality labels to train a region proposal network <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> to predict both global picture quality and local patch quality. This model is able to produce better global picture quality predictions by learning relationships between global and local picture quality (Sec. 4.2). We then further modify this model to (b) predict spatial maps of picture quality, useful for localizing picture distortions (Sec. 4.3). Finally, we (c) innovate a local-to-global feedback architecture that produces further improved whole picture quality predictions using local patch predictions (Sec. <ref type="bibr">4.4)</ref>. This series of models obtains state-of-the art picture quality performance on the new database, and transfer well -without finetuning -on smaller "in-the-wild databases such as LIVE Challenge (CLIVE) <ref type="bibr" target="#b16">[17]</ref> and KonIQ-10K <ref type="bibr" target="#b17">[18]</ref> (Sec. 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Image Quality Datasets: Most picture quality models have been designed and evaluated on three "legacy" databases: LIVE IQA <ref type="bibr" target="#b23">[24]</ref>, TID-2008 <ref type="bibr" target="#b24">[25]</ref>, and TID-2013 <ref type="bibr" target="#b25">[26]</ref>. These datasets contain small numbers of unique, pristine images (? 30) synthetically distorted by diverse types and amounts of single distortions (JPEG, Gaussian blur, etc.). They contain limited content and distortion diversity, and do not capture complex mixtures of distortions that often occur in real-world images. Recently, "in-the-wild" datasets such as CLIVE <ref type="bibr" target="#b16">[17]</ref> and KonIQ-10K <ref type="bibr" target="#b17">[18]</ref>, have been introduced to attempt to address these shortcomings <ref type="table" target="#tab_1">(Table 1)</ref>. Full-Reference models: Many full-reference (FR) perceptual picture quality predictors, which make comparisons against high-quality reference pictures, are available <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Although some FR algorithms (e.g. SSIM <ref type="bibr" target="#b4">[5]</ref>, [34], VIF <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr">[35,</ref><ref type="bibr">36]</ref>) have achieved remarkable commercial success (e.g. for monitoring streaming content), they are limited by their requirement of pristine reference pictures. Current NR models arent general enough: No-reference or blind algorithms predict picture content without the benefit of a reference signal. Popular blind picture quality algorithms usually measure distortion-induced deviations from perceptually relevant, highly regular bandpass models of picture statistics <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr">[37,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40]</ref>. Examples include BRISQUE <ref type="bibr" target="#b9">[10]</ref>, NIQE <ref type="bibr" target="#b10">[11]</ref>, CORNIA <ref type="bibr" target="#b12">[13]</ref>, FRIQUEE <ref type="bibr" target="#b11">[12]</ref>, which use "handcrafted" statistical features to drive  shallow learners (SVM, etc.). These models produce accurate quality predictions on legacy datasets having single, synthetic distortions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">41]</ref>, but struggle on recent in-the-wild <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> databases. Several deep NR models <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b44">46]</ref> have also been created that yield state-of-the-art performance on legacy synthetic distortion databases <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">41]</ref>, e.g., by pretraining deep nets <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b47">49]</ref> on ImageNet <ref type="bibr" target="#b48">[50]</ref>, then fine tuning, or by training on proxy labels generated by an FR model <ref type="bibr" target="#b43">[45]</ref>. However, most deep models also struggle on CLIVE <ref type="bibr" target="#b16">[17]</ref>, because it is too difficult, yet too small to sufficiently span the perceptual space of picture quality to allow very deep models to map it. The authors of <ref type="bibr" target="#b49">[51]</ref>, the code of which is not made available, reported high results, but we have been unable to reproduce their numbers, even with more efficient networks. The authors of <ref type="bibr" target="#b50">[52]</ref> use a pre-trained ResNet-101 and report high performance on <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, but later disclosed <ref type="bibr" target="#b51">[53]</ref> that they are unable to reproduce their own results in <ref type="bibr" target="#b50">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Large-Scale Dataset and Human Study</head><p>Next we explain the details of the new picture quality dataset we constructed, and the crowd-sourced subjective quality study we conducted on it. The database has about 40, 000 pictures and 120, 000 patches, on which we collected 4M human judgments from nearly 8, 000 unique subjects (after subject rejection). It is significantly larger than commonly used "legacy databases <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">41]</ref> and more recent "in-the-wild" crowd-sourced datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">UGC-like picture sampling</head><p>Data collection began by sampling about 40K highly diverse contents of diverse sizes and aspect ratios from hundreds of thousands of pictures drawn from public databases, including AVA <ref type="bibr" target="#b6">[7]</ref>, VOC <ref type="bibr" target="#b52">[54]</ref>, EMOTIC <ref type="bibr" target="#b53">[55]</ref>, and CERTH Blur <ref type="bibr" target="#b54">[56]</ref>. Because we were interested in the role of local quality perception as it relates to global quality, we also cropped three patches from each picture, yielding about 120K patches. While internally debating the concept of "representative, we settled on a method of sampling a large image collection so that it would be substantially "UGClike. We did this because billions of pictures are uploaded, shared, displayed, and viewed on social media, far more than anywhere else. We sampled picture contents using a mixed integer programming method <ref type="bibr" target="#b55">[57]</ref> similar to <ref type="bibr" target="#b17">[18]</ref>, to match a specific set of UGC feature histograms. Our sampling strategy was different in several ways: firstly, unlike KonIQ <ref type="bibr" target="#b17">[18]</ref>, no pictures were down sampled, since this intervention can substantially modify picture quality. Moreover, including pictures of diverse sizes better reflects actual practice. Second, instead of uniformly sampling feature values, we designed a picture collection whose feature histograms match those of 15M randomly selected pictures from a social media website. This in turn resulted in a much more realistic and difficult database to predict features on, as we will describe later. Lastly, we did not use a pre-trained IQA algorithm to aid the picture sampling, as that could introduce algorithmic bias into the data collection process.</p><p>To sample and match feature histograms, we computed the following diverse, objective features on both our picture collection and the 15M UGC pictures:</p><formula xml:id="formula_0">? absolute brightness L = R + G + B.</formula><p>? colorfulness using the popular model in <ref type="bibr" target="#b56">[58]</ref>.</p><p>? RMS brightness contrast <ref type="bibr" target="#b57">[59]</ref>. ? Spatial Information(SI), the global standard deviation of Sobel gradients <ref type="bibr" target="#b58">[60]</ref>, a measure of complexity. ? pixel count, a measure of picture size. ? number of detected faces using <ref type="bibr" target="#b59">[61]</ref>.</p><p>In the end, we arrived at about 40K pictures. <ref type="figure" target="#fig_0">Fig. 3</ref> shows 16 randomly selected pictures and <ref type="figure" target="#fig_1">Fig. 4</ref> highlights the diverse sizes and aspect ratios of pictures in the new database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Patch cropping</head><p>We applied the following criteria when randomly cropping out patches: (a) aspect ratio: patches have the same aspect ratios as the pictures they were drawn from. (b) dimension: the linear dimensions of the patches are 40%, 30%, and 20% of the picture dimensions. (c) location: every patch is entirely contained within the picture, but no patch overlaps the area of another patch cropped from the same image by more than 25%. <ref type="figure" target="#fig_2">Fig. 5</ref> shows two exemplar pictures, and three patches obtained from each. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Crowdsourcing pipeline for subjective study</head><p>Subjective picture quality ratings are true psychometric measurements on human subjects, requiring 10-20 times as much time for scrutiny (per photo) as for example, object labelling <ref type="bibr" target="#b48">[50]</ref>. We used the Amazon Mechanical Turk (AMT) crowdsourcing system, well-documented for this purpose <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b61">63]</ref>, to gather human picture quality labels.</p><p>We divided the study into two separate tasks: picture quality evaluation and patch quality evaluation. Most subjects (7141 out of 7865 workers) only participated in one of these, to avoid biases incurred by viewing both, even on different dates. Either way, the crowdsource workflow was the same, as depicted in <ref type="figure" target="#fig_3">Fig. 6</ref>. Each worker was given instructions, followed by a training phase, where they were shown several contents to learn the rating task. They then viewed and quality-rated N contents to complete their human intelligent task (HIT), concluding with a survey regarding their experience. At first, we set N = 60, but as the study accelerated and we found the workers to be delivering consistent scores, we set N = 210. We found that the workers performed as well when viewing the increased number of pictures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Processing subjective scores</head><p>Subject rejection: We took the recommended steps <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b61">63]</ref> to ensure the quality of the collected human data. ? We only accepted workers with acceptance rates &gt; 75%. ? Repeated images: 5 of the N contents were repeated randomly per session to determine whether the subjects were giving consistent ratings. ? "Gold" images: 5 out of N contents were "gold ones sampled from a collection of 15 pictures and 76 patches that were separately rated in a controlled lab study by 18 reliable subjects. The "gold" images are not part of the new database. We accepted or rejected each raters scores within a HIT based on two factors: the difference of the repeated content scores compared with overall standard deviation, and whether more than 50% of their scores were identical. Since we desired to capture many ratings, workers could participate in multiple HITs. Each content received at least 35 quality ratings, with some receiving as many as 50.</p><p>The labels supplied by each subject were converted into normalized Z scores <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b16">[17]</ref>, averaged (by content), then scaled to [0, 100] yielding Mean Opinion Scores (MOS). The total number of human subjective labels collected after subject rejection was 3, 931, 710 (950, 574 on images, and 2, 981, 136 on patches). Inter-subject consistency: A standard way to test the consistency of subjective data <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b16">[17]</ref>, is to randomly divide subjects into two disjoint equal sets, compute two MOS on each picture (one from each group), then compute the Pearson linear correlation (LCC) between the MOS values of the two groups. When repeated over 25 random splits, the average LCC between the two groups MOS was 0.48, indicating the difficulty of the quality prediction problem on this realistic picture dataset. <ref type="figure" target="#fig_6">Fig. 12</ref> (left) shows a scatter plot of the two halves of human labels for one split, showing a linear relationship and fairly broad spread. We applied the same process to the patch scores, obtaining a higher LCC of 0.65. This is understandable: smaller patches contain less spatial diversity; hence they receive more consistent scores. We also found that nearly all the non-rejected subjects had a positive Spearman rank ordered correlation (SRCC) with the golden pictures, validating the data collection process. Relationships between picture and patch quality: <ref type="figure" target="#fig_6">Fig.  12</ref> (right) is a scatter plot of the entire database of picture MOS against the MOS of the largest patches cropped from them. The linear correlation coefficient (LCC) between them is 0.43, which is strong, given that each patch represents only 16% of the picture area. The scatter plots of the picture MOS against that of the smaller (30% and 20%) patches are quite similar, with somewhat reduced LCC of 0.36 and 0.28, respectively (supplementary material).</p><p>An outcome of creating highly realistic "in the wild data is that it is much more difficult to train successful models on. Most pictures uploaded to social media are of reasonably good quality, largely owing to improved mobile cameras. Hence, the distribution of MOS in the new database is narrower and peakier as compared to those of the two previous "in the wild picture quality databases <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. This is important, since it is desirable to be able to predict small changes in MOS, which can be significant regarding, for example, compression parameter selection <ref type="bibr" target="#b62">[64]</ref>. As we show in Sec. 4, the new database is very challenging, even for deep models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning Blind Picture Quality Predictors</head><p>With the availability of the new dataset comprising pictures and patches associated with human labels (Sec. 3), we created a series of deep quality prediction models that exploit its unique characteristics. We conducted four picture quality learning experiments, evolving from a simple network into models of increasing sophistication and perceptual relevance which we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">A baseline picture-only model</head><p>To start with, we created a simple model that only processes pictures and the associated human quality labels. We will refer to this hereafter as the Baseline Model. The basic network that we used is the well-documented pre-trained ResNet-18 <ref type="bibr" target="#b20">[21]</ref>, which we modified (described next) and fine-tuned to conduct the quality prediction task. Input image pre-processing: Because picture quality prediction (whether by human or machine) is a psychometric prediction, it is crucial to not modify the pictures being fed into the network. While most visual recognition learners augment input images by cropping, resizing, flipping, etc., doing the same when training a perceptual quality predictor would be a psychometric error. Such input pre-processing would result in perceptual quality scores being associated with different pictures than they were recorded on.</p><p>The new dataset contains thousands of unique combinations of picture sizes and aspect ratios (see <ref type="figure" target="#fig_1">Fig. 4</ref>). While this is a core strength of the dataset and reflects its realism, it also poses additional challenges when training deep networks. We attempted several ways of training the ResNet on raw multi-sized pictures, but the training and validation losses were not stable, because of the fixed sized pooling and fully connected layers.</p><p>In order to tackle this aspect, we white padded each training picture to size 640 ? 640, centering the content in each instance. Pictures having one or both dimensions larger than 640 were moved to the test set. This approach has the following advantages: (a) it allows supplying constantsized pictures to the network, causing it to stably converge well, (b) it allows large batch sizes which improves training, (c) it agrees with the experiences of the picture raters, since AMT renders white borders around pictures that do not occupy the full webpage's width. Training setup: We divided the picture dataset (and associated patches and scores) into training, validation and testing sets. Of the collected 39, 810 pictures (and 119, 430 patches), we used about 75% for training (30K pictures, along with their 90K patches), 19% for validation (7.7K pictures, 23.1K patches), and the remaining for testing (1.8K pictures, 5.4K patches). When testing on the validation set, the pictures fed to the trained networks were also white bordered to size 640 ? 640. As mentioned earlier, the test set is entirely composed of pictures having at least one linear dimension exceeding 640. Being able to perform well on larger pictures of diverse aspect ratios was deemed as an additional challenge to the models. Implementation Details: We used the PyTorch implementation of ResNet-18 <ref type="bibr" target="#b63">[65]</ref> pre-trained on ImageNet and retained only the CNN backbone during fine-tuning. To this, we added two pooling layers (adaptive average pooling and adaptive max pooling), followed by two fully-connected (FC) layers, such that the final FC layer outputs a single score. We used a batch size of 120 and employed the MSE loss when regressing the single output quality score. We employed the Adam optimizer with ? 1 = 0.9 and ? 2 = 0.99, a weight decay of 0.01, and do a full finetuning for 10 epochs. We followed a discriminative learning approach <ref type="bibr" target="#b64">[66]</ref>, using a lower learning rate of 3e ?4 , but a higher learning rate of 3e ?3 for the head layers. These settings apply to all the models we describe in the following.</p><p>Evaluation setup: Although the baseline model was trained on whole pictures, we tested it on both pictures and patches. For comparison with popular shallow methods, we also trained and tested BRISQUE <ref type="bibr" target="#b9">[10]</ref> and the "completely blind NIQE <ref type="bibr" target="#b10">[11]</ref>, which does not involve any training. We reimplemented two deep picture quality methods -NIMA <ref type="bibr" target="#b44">[46]</ref> which uses a Mobilenet-v2 <ref type="bibr" target="#b65">[67]</ref> (except we replaced the output layer to regress a single quality score), and CN-NIQA <ref type="bibr" target="#b66">[68]</ref>, following the details provided by the authors.</p><p>As is the common practice in the field of picture quality assessment, we report two metrics: (a) Spearman Rank Correlation Coefficient (SRCC) and (b) Linear Correlation Coefficient (LCC). <ref type="table" target="#tab_5">Table 5</ref>, the first thing to notice is the level of performance attained by popular shallow models (NIQE <ref type="bibr" target="#b10">[11]</ref> and BRISQUE <ref type="bibr" target="#b9">[10]</ref>), which have the same feature sets. The unsupervised NIQE algorithm performed poorly, while BRISQUE did better, yet the reported correlations are far below desired levels. Despite being CNN-based, CNNIQA <ref type="bibr" target="#b66">[68]</ref> performed worse than BRISQUE <ref type="bibr" target="#b9">[10]</ref>. Our Baseline Model outperformed most methods and competed very well with NIMA <ref type="bibr" target="#b44">[46]</ref>. The other entries in the table (the ROIPool and Feedback Models) are described later. <ref type="table" target="#tab_6">Table 6</ref> shows the performances of the same trained, unmodified models on the associated picture patches of three reduced sizes (40%, 30% and 20% of linear image dimensions). The Baseline Model maintained or slightly improved performance across patch sizes, while NIQE continued to lag, despite the greater subject agreement on reduced-size patches (Sec. 3.4). The performance of NIMA suffered as the patch sizes decreased. Conversely, BRISQUE and CNNIQA improved as the patch sizes decreased, although they were trained on whole pictures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results: From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">RoIPool : a picture + patches model</head><p>Next, we developed a new type of picture quality model that leverages both picture and patch quality information. Our "RoIPool Model" is designed in the same spirit as Fast/Faster R-CNN <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, which was originally designed for object detection. As in Fast-RCNN, our model has an RoIPool layer which allows the flexibility to aggregate at both patch and picture-sized scales. However, it differs from Fast-RCNN <ref type="bibr" target="#b21">[22]</ref> in three important ways. First, instead of regressing for detecting bounding boxes, we predict full-picture and patch quality. Second, Fast-RCNN performs multi-task learning with two separate heads, one for image classification and another for detection. Our model instead shares a single head between patches and images. This was done to allow sharing of the "quality-aware weights between pictures and patches. Third, while both heads of Fast-RCNN operate solely on features from ROIpooled region proposals, our model pools over the entire picture to conduct global picture quality prediction. Implementation details: As in Sec. 4.1, we added an ROIPool layer followed by two fully-connected layers to the pre-trained CNN backbone of ResNet-18. The output size of the RoIPool unit was fixed at 2 ? 2. All of the hyperparameters are the same as detailed in Sec  <ref type="table" target="#tab_5">Table 5</ref>, the RoIPool Model yields better results than the Baseline Model and NIMA on whole pictures on both validation and test datasets. When the same trained RoIPool Model was evaluated on patches, the performance improvement was more significant. Unlike the Baseline Model, the performance of the ROIPool model increased as the patch sizes were reduced. This suggests that: (i) the RoIPool Model is more scalable than the Baseline Model, hence better able to predict the qualities of pictures of varying sizes, (ii) accurate patch predictions can help guide global picture prediction, as we show in Sec. 4.4, (iii) this novel picture quality prediction architecture allows computing local quality maps, which we explore next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Predicting perceptual quality maps</head><p>Next, we used the ROIPool model to produce patch-wise quality maps on each image, since it is flexible enough to make predictions on any specified number of patches. This unique picture quality map predictor is the first deep  model that is learned from true human-generated picture and patch labels, rather than from proxy labels delivered by an algorithm, as in <ref type="bibr" target="#b43">[45]</ref>. We generated picture quality maps in the following manner: (a) we partitioned each picture into a grid of 32 ? 32 non-overlapping blocks, thus preserving aspect ratio (this step can be easily extended to process denser, overlapping, or smaller blocks) (b) Each block's boundary coordinates (left, top, right, bottom) were provided as input to the RoIPool to guide learning of patch quality scores (c) For visualization, we applied bi-linear interpolation to the block predictions, and represented the results as magma color maps. We then ?-blended the quality maps with the original pictures (? = 0.8). From <ref type="figure" target="#fig_6">Fig. 10</ref>, we may observe that the ROIPool Model is able to accurately distinguish regions that are blurred, washed-out, or poorly exposed, from highquality regions. Such spatially localized quality maps have great potential to support applications like image compression, image retargeting, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">A local-to-global feedback model</head><p>As noted in Sec. 4.3, local patch quality has a significant influence on global picture quality. Given this, how do we effectively leverage local quality predictions to further improve global picture quality? To address this question, we developed a novel architecture referred to as the Feedback Model <ref type="figure" target="#fig_7">(Fig. 9(c)</ref>). In this framework, the pre-trained backbone has two branches: (i) an RoIPool layer followed by an FC-layer for local patch and image quality prediction (Head0) and (ii) a global image pooling layer. The predictions from Head0 are concatenated with the pooled image features from the second branch and fed to a new FC layer (Head1), which makes whole-picture predictions. From Tables 5 and 6, we observe that the performance of the Feedback Model on both pictures and patches is improved even further by the unique local-to-global feedback architecture. This model consistently outperformed all shallow and deep quality models. The largest improvement is made on the whole-picture predictions, which was the main goal. The improvement afforded by the Feedback Model is understandable from a perceptual perspective, since, while quality perception by a human is a low-level task involving low-level processes, it also involves a viewer casting their foveal gaze at discrete localized patches of the picture being viewed. The overall picture quality is likely an integrated combination of quality information gathered around each fixation point, similar to the Feedback Model. Failure cases: While our model attains good performance on the new database, it does make errors in prediction. <ref type="figure" target="#fig_6">Fig 11(a)</ref> shows a picture that was considered of a very poor quality by the human raters (MOS=18), while the Feedback model predicted an overrated score of 57, which is moderate. This may have been because the subjects were less forgiving of the blurred moving object, which may have drawn their attention. Conversely, <ref type="figure" target="#fig_6">Fig 11(b)</ref> is a picture that was underrated by our model, receiving a predicted score of 68 against the subject rating of 82. It may have been that the subjects discounted the haze in the background in favor of the clearly visible waterplane. These cases further reinforce the difficulty of perceptual picture quality prediction and highlight the strength of our new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Cross-database comparisons</head><p>Finally, we evaluated the Baseline (Sec. 4.1), RoIPool (Sec. 4.2), Feedback (Sec. <ref type="bibr">4.4)</ref> , and other baselines -all trained on the proposed dataset -on two other smaller "in-the-wild databases CLIVE <ref type="bibr" target="#b16">[17]</ref> and KonIQ-10k <ref type="bibr" target="#b17">[18]</ref> without any fine-tuning. From <ref type="table" target="#tab_7">Table 7</ref>, we may observe that all our three models, trained on the proposed dataset, transfer well to other databases. The Baseline, RoIPool, and Feedback Models all outperformed the shallow and other deep models <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b66">68]</ref> on both datasets. This is a powerful result that highlights the representativeness of our new dataset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>Problems involving perceptual picture quality prediction are long-standing and fundamental to perception, optics, image processing, and computational vision. Once viewed as a basic vision science modelling problem to improve on weak Mean Squared Error (MSE) based ways of assessing television systems and cameras, the picture quality problem has evolved into one that demands the large-scale tools of data science and computational vision. Towards this end we have created a database that is not only substantially larger and harder than previous ones, but contains data that enables global-to-local and local-to-global quality inferences. We also developed a model that produces local quality inferences, uses them to compute picture quality maps, and global image quality. We believe that the proposed new dataset and models have the potential to enable quality-based monitoring, ingestion, and control of billions of social-media pictures and videos.</p><p>Finally, examples in <ref type="figure" target="#fig_6">Fig. 11</ref> of competing local vs. global quality percepts highlight the fundamental difficulties of the problem of no-reference perceptual picture quality assessment: its subjective nature, the complicated interactions between content and myriad possible combinations of distortions, and the effects of perceptual phenomena like masking.</p><p>More complex architectures might mitigate some of these issues. Additionally, mid-level semantic side-information about objects in a picture (e.g., faces, animals, babies) or scenes (e.g., outdoor vs. indoor) may also help capture the role of higher-level processes in picture quality assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material -From Patches to Pictures (PaQ-2-PiQ):</head><p>Mapping the Perceptual Space of Picture Quality</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance Summary</head><p>The performance of NIMA <ref type="bibr" target="#b44">[46]</ref> reported in the paper used a default MobileNet <ref type="bibr" target="#b65">[67]</ref> backbone. For a fair comparison against the proposed family of models which used ResNet-18 backbone, we reported the performance of NIMA (ResNet-18) on images <ref type="table" target="#tab_5">(Table 5</ref>) and patches <ref type="table" target="#tab_6">(Table 6</ref>) of the new datatbase, and also cross-database performance on CLIVE <ref type="bibr" target="#b16">[17]</ref> and KonIQ-10K <ref type="bibr" target="#b17">[18]</ref>  <ref type="table" target="#tab_7">(Table 7)</ref>. Given that the proposed models either compete well or outperform other models in all categories further demonstrates their quality prediction strength across multiple databases containing diverse image distortions.    <ref type="table" target="#tab_8">Table 8</ref> summarizes the number of learnable parameters used by each of the compared models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Information on Model Parameters</head><p>? CNNIQA's <ref type="bibr" target="#b66">[68]</ref> poor performance can be attributed to its shallow CNN-based architecture with less than 1M parameters indicating its inability to model the complex problem. ? It is interesting to note that NIMA (MobileNet-v2) performed consistently at par with NIMA (ResNet-18) even though it used only 20% of the total parameters.</p><p>? Although RoIPool Model used the same number of parameters as the Baseline Model, it achieved significantly better performance suggesting the importance of accurate local quality predictions for global quality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Amazon Mechanical Turk Interface</head><p>We allowed the workers on Amazon Mechanical Turk (AMT) to preview the "Instructions" page (as shown in <ref type="figure" target="#fig_0">Fig 13)</ref> before they accept to participate in the study. Once accepted, they were tasked with rating the quality of images on a Likert scale marked with "Bad", "Poor", "Fair", "Good" and "Excellent" as demonstrated in <ref type="figure" target="#fig_1">Fig. 14 and 15</ref>. A similar user interface was used for patch quality rating task.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Exemplar pictures from the new database, each resized to fit. Actual pictures are of highly diverse sizes and shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Scatter plot of picture width versus picture height with marker size indicating the number of pictures for a given dimension in the new database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Sample pictures and 3 randomly positioned crops (20%, 30%, 40%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>AMT task: Workflow experienced by crowd-sourced workers when rating either pictures or patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Scatter plots descriptive of the new subjective quality database. Left: Inter-subject scatter plot of a random 50% divisions of the human labels of all 40K+ pictures into disjoint subject sets. Right: Scatter plot of picture MOS vs MOS of largest patch (40% of linear dimension) cropped from each same picture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>MOS (Z-score) histograms of three "in-the-wild databases. Left: CLIVE<ref type="bibr" target="#b16">[17]</ref>. Middle: KoniIQ-10K<ref type="bibr" target="#b17">[18]</ref>. Right: The new database introduced here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>. 4. 1 .</head><label>1</label><figDesc>Train and test setup: Recall that we sampled 3 patches per image and obtained picture and patch subjective scores (Sec. 3). During training, the model receives the following input: (a) image, (b) location coordinates (left, top, right, bottom) of all 3 patches and, (c) ground truth quality scores of the image and patches. At test time, the RoIPool Model can process both pictures and patches of any size. Thus, it offers the advantage of predicting the qualities of patches of any number and specified locations, in parallel with the picture predictions. Results: As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Illustrating the different deep quality prediction models we studied. (a) Baseline Model: ResNet-18 with a modified head trained on pictures (Sec. 4.1). (b) RoIPool Model: trained on both picture and patch qualities (Sec. 4.2). (c) Feedback Model: where the local quality predictions are fed back to improve global quality predictions (Sec. 4.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Spatial quality maps generated using the RoIPool Model (Sec. 4.2). Left: Original Images. Right: Quality maps blended with the originals using magma color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Failure cases: Examples where the Feedback Model's predictions differed the most from the ground truth predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :</head><label>12</label><figDesc>Scatter plots of picture MOS vs patch MOS. Left: Scatter plot of picture MOS vs MOS of second largest patch (30% of linear dimension) cropped from each same picture. Right: Scatter plot of picture MOS vs MOS of smallest patch (20% of linear dimension) cropped from each same picture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 :</head><label>13</label><figDesc>AMT task: The "Instructions" page shown to workers at the beginning of each HIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 :</head><label>14</label><figDesc>AMT task: Training session interface of AMT task experienced by crowd-sourced workers when rating pictures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 :</head><label>15</label><figDesc>AMT task: Testing session interface of AMT task experienced by crowd-sourced workers when rating pictures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We sampled hundreds of thousands of open source digital pictures to match the feature distributions of the largest use-case: pictures shared on social media. The final collection includes about 40, 000 real-world, unprocessed (by us) pictures of diverse sizes, contents, and distortions, and about 120, 000 cropped image patches of various scales and aspect ratios (Sec. 3.1, 3.2). ? We conducted the largest subjective picture quality study to date. We used Amazon Mechanical Turk to collect about 4M human perceptual quality judgments from almost 8, 000 subjects on the collected content, about four 3M human quality labels on patches drawn from the same pictures. Local picture quality is deeply related to global quality, although this relationship is not well understood</figDesc><table /><note>times more than any prior image quality study (Sec. 3.3).? We collected both picture and patch quality labels to relate local and global picture quality. The new database includes about 1M human picture quality judg- ments and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of popular IQA datasets. In the legacy datasets, pictures were synthetically distorted with different types of single distortions. "In-the-wild" databases contain pictures impaired by complex mixtures of highly diverse distortions, each as unique as the pictures they afflict.</figDesc><table><row><cell>Database</cell><cell># Unique contents</cell><cell># Distortions</cell><cell># Picture contents</cell><cell># Patch contents</cell><cell>Distortion type</cell><cell>Subjective study framework</cell><cell># Annotators</cell><cell># Annotations</cell></row><row><cell>LIVE IQA (2003) [24]</cell><cell>29</cell><cell>5</cell><cell>780</cell><cell>0</cell><cell>single, synthetic</cell><cell>in-lab</cell><cell></cell><cell></cell></row><row><cell>TID-2008 [25]</cell><cell>25</cell><cell>17</cell><cell>1700</cell><cell>0</cell><cell>single, synthetic</cell><cell>in-lab</cell><cell></cell><cell></cell></row><row><cell>TID-2013 [25]</cell><cell>25</cell><cell>24</cell><cell>3000</cell><cell>0</cell><cell>single, synthetic</cell><cell>in-lab</cell><cell></cell><cell></cell></row><row><cell>CLIVE (2016) [17]</cell><cell>1200</cell><cell>-</cell><cell>1200</cell><cell>0</cell><cell>in-the-wild</cell><cell>crowdsourced</cell><cell>8000</cell><cell>350K</cell></row><row><cell>KonIQ (2018) [18]</cell><cell>10K</cell><cell>-</cell><cell>10K</cell><cell>0</cell><cell>in-the-wild</cell><cell>crowdsourced</cell><cell>1400</cell><cell>1.2M</cell></row><row><cell>Proposed database</cell><cell>39, 810</cell><cell>-</cell><cell>39, 810</cell><cell>119, 430</cell><cell>in-the-wild</cell><cell>crowdsourced</cell><cell>7865</cell><cell>3, 931, 710</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Picture quality predictions: Performance of picture quality models on the full-size validation and test pictures in the new database. A higher value indicates superior performance. NIQE is not trained.</figDesc><table><row><cell></cell><cell cols="2">Validation Set</cell><cell cols="2">Testing Set</cell></row><row><cell>Model</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell></row><row><cell>NIQE [11]</cell><cell>0.094</cell><cell>0.131</cell><cell>0.211</cell><cell>0.288</cell></row><row><cell>BRISQUE [10]</cell><cell>0.303</cell><cell>0.341</cell><cell>0.288</cell><cell>0.373</cell></row><row><cell>CNNIQA [68]</cell><cell>0.259</cell><cell>0.242</cell><cell>0.266</cell><cell>0.223</cell></row><row><cell>NIMA [46]</cell><cell>0.521</cell><cell>0.609</cell><cell>0.583</cell><cell>0.639</cell></row><row><cell>Baseline Model (Sec. 4.1)</cell><cell>0.525</cell><cell>0.599</cell><cell>0.571</cell><cell>0.623</cell></row><row><cell>RoIPool Model (Sec. 4.2)</cell><cell>0.541</cell><cell>0.618</cell><cell>0.576</cell><cell>0.655</cell></row><row><cell>Feedback Model (Sec. 4.4)</cell><cell>0.562</cell><cell>0.649</cell><cell>0.601</cell><cell>0.685</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Patch quality predictions: Results on (a) the largest patches (40% of linear dimensions), (b) middle-size patches (30% of linear dimensions) and (c) smallest patches (20% of linear dimensions) in the validation and test sets. Same protocol as used inTable 5.</figDesc><table><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Validation</cell><cell>Test</cell><cell></cell><cell cols="2">Validation</cell><cell>Test</cell><cell></cell><cell cols="2">Validation</cell><cell>Test</cell><cell></cell></row><row><cell>Model</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell></row><row><cell>NIQE [11]</cell><cell>0.109</cell><cell>0.106</cell><cell>0.251</cell><cell>0.271</cell><cell>0.029</cell><cell>0.011</cell><cell>0.217</cell><cell>0.109</cell><cell>0.052</cell><cell>0.027</cell><cell>0.154</cell><cell>0.031</cell></row><row><cell>BRISQUE [10]</cell><cell>0.384</cell><cell>0.467</cell><cell>0.433</cell><cell>0.498</cell><cell>0.442</cell><cell>0.503</cell><cell>0.524</cell><cell>0.556</cell><cell>0.495</cell><cell>0.494</cell><cell>0.532</cell><cell>0.526</cell></row><row><cell>CNNIQA [68]</cell><cell>0.438</cell><cell>0.400</cell><cell>0.445</cell><cell>0.373</cell><cell>0.522</cell><cell>0.449</cell><cell>0.562</cell><cell>0.440</cell><cell>0.580</cell><cell>0.481</cell><cell>0.592</cell><cell>0.475</cell></row><row><cell>NIMA [46]</cell><cell>0.587</cell><cell>0.637</cell><cell>0.688</cell><cell>0.691</cell><cell>0.547</cell><cell>0.560</cell><cell>0.681</cell><cell>0.670</cell><cell>0.395</cell><cell>0.411</cell><cell>0.526</cell><cell>0.524</cell></row><row><cell>Baseline Model (Sec. 4.1)</cell><cell>0.561</cell><cell>0.617</cell><cell>0.662</cell><cell>0.701</cell><cell>0.577</cell><cell>0.603</cell><cell>0.685</cell><cell>0.704</cell><cell>0.563</cell><cell>0.541</cell><cell>0.633</cell><cell>0.630</cell></row><row><cell>RoIPool Model (Sec. 4.2)</cell><cell>0.641</cell><cell>0.731</cell><cell>0.724</cell><cell>0.782</cell><cell>0.686</cell><cell>0.752</cell><cell>0.759</cell><cell>0.808</cell><cell>0.733</cell><cell>0.760</cell><cell>0.769</cell><cell>0.792</cell></row><row><cell>Feedback Model (Sec. 4.4)</cell><cell>0.658</cell><cell>0.744</cell><cell>0.726</cell><cell>0.783</cell><cell>0.698</cell><cell>0.762</cell><cell>0.770</cell><cell>0.819</cell><cell>0.756</cell><cell>0.783</cell><cell>0.786</cell><cell>0.808</cell></row><row><cell>Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Head</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Image score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ImagePatchRoIPool</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Head</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Image &amp; Patch scores</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Cross-database comparisons: Results when models trained on the new database are applied on CLIVE<ref type="bibr" target="#b16">[17]</ref> and KonIQ<ref type="bibr" target="#b17">[18]</ref> without finetuning.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Validation Set</cell><cell></cell></row><row><cell></cell><cell cols="2">CLIVE [17]</cell><cell cols="2">KonIQ [18]</cell></row><row><cell>Model</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell></row><row><cell>NIQE [11]</cell><cell>0.503</cell><cell>0.528</cell><cell>0.534</cell><cell>0.509</cell></row><row><cell>BRISQUE [10]</cell><cell>0.660</cell><cell>0.621</cell><cell>0.641</cell><cell>0.596</cell></row><row><cell>CNNIQA [68]</cell><cell>0.559</cell><cell>0.459</cell><cell>0.596</cell><cell>0.403</cell></row><row><cell>NIMA [46]</cell><cell>0.712</cell><cell>0.705</cell><cell>0.666</cell><cell>0.721</cell></row><row><cell>Baseline Model (Sec. 4.1)</cell><cell>0.740</cell><cell>0.725</cell><cell>0.753</cell><cell>0.764</cell></row><row><cell>RoIPool Model (Sec. 4.2)</cell><cell>0.762</cell><cell>0.775</cell><cell>0.776</cell><cell>0.794</cell></row><row><cell>Feedback Model (Sec. 4.4)</cell><cell>0.784</cell><cell>0.754</cell><cell>0.788</cell><cell>0.808</cell></row><row><cell cols="5">and the efficacy of our models. The best reported numbers</cell></row><row><cell cols="5">on both databases [69] uses a Siamese ResNet-34 backbone</cell></row><row><cell cols="5">by training and testing on the same datasets (along with 5</cell></row><row><cell cols="5">other datasets). While this model reportedly attains 0.851</cell></row><row><cell cols="5">SRCC on CLIVE and 0.894 on KonIQ-10K, we achieved</cell></row><row><cell cols="5">the above results by directly applying pre-trained models,</cell></row><row><cell cols="5">thereby not allowing them to adapt to the distortions of the</cell></row><row><cell cols="5">test data. When we also trained and tested on these datasets,</cell></row><row><cell cols="5">our picture-based Baseline Model also performed at a simi-</cell></row><row><cell cols="5">lar level, obtaining an SRCC of 0.844 on CLIVE and 0.890</cell></row><row><cell>on KonIQ-10K.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Picture quality predictions: Performance of picture quality models on the full-size validation and test pictures in the new database. A higher value indicates superior performance. NIQE is not trained.</figDesc><table><row><cell></cell><cell cols="2">Validation Set</cell><cell cols="2">Testing Set</cell></row><row><cell>Model</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell></row><row><cell>NIMA(MobileNet v2) [46]</cell><cell>0.521</cell><cell>0.609</cell><cell>0.583</cell><cell>0.639</cell></row><row><cell>NIMA(ResNet 18) [46]</cell><cell>0.503</cell><cell>0.577</cell><cell>0.580</cell><cell>0.611</cell></row><row><cell>Baseline Model (Sec. 4.1)</cell><cell>0.525</cell><cell>0.599</cell><cell>0.571</cell><cell>0.623</cell></row><row><cell>RoIPool Model (Sec. 4.2)</cell><cell>0.541</cell><cell>0.618</cell><cell>0.576</cell><cell>0.655</cell></row><row><cell>Feedback Model (Sec. 4.4)</cell><cell>0.562</cell><cell>0.649</cell><cell>0.601</cell><cell>0.685</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Patch quality predictions: Results on (a) the largest patches (40% of linear dimensions), (b) middle-size patches (30% of linear dimensions) and (c) smallest patches (20% of linear dimensions) in the validation and test sets. Same protocol as used inTable 5.</figDesc><table><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Validation</cell><cell>Test</cell><cell></cell><cell cols="2">Validation</cell><cell>Test</cell><cell></cell><cell cols="2">Validation</cell><cell>Test</cell><cell></cell></row><row><cell>Model</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell></row><row><cell>NIMA(MobileNet v2) [46]</cell><cell>0.587</cell><cell>0.637</cell><cell>0.688</cell><cell>0.691</cell><cell>0.547</cell><cell>0.560</cell><cell>0.681</cell><cell>0.670</cell><cell>0.395</cell><cell>0.411</cell><cell>0.526</cell><cell>0.524</cell></row><row><cell>NIMA(ResNet 18) [46]</cell><cell>0.578</cell><cell>0.600</cell><cell>0.676</cell><cell>0.696</cell><cell>0.516</cell><cell>0.505</cell><cell>0.672</cell><cell>0.657</cell><cell>0.324</cell><cell>0.316</cell><cell>0.504</cell><cell>0.483</cell></row><row><cell>Baseline Model (Sec. 4.1)</cell><cell>0.561</cell><cell>0.617</cell><cell>0.662</cell><cell>0.701</cell><cell>0.577</cell><cell>0.603</cell><cell>0.685</cell><cell>0.704</cell><cell>0.563</cell><cell>0.541</cell><cell>0.633</cell><cell>0.630</cell></row><row><cell>RoIPool Model (Sec. 4.2)</cell><cell>0.641</cell><cell>0.731</cell><cell>0.724</cell><cell>0.782</cell><cell>0.686</cell><cell>0.752</cell><cell>0.759</cell><cell>0.808</cell><cell>0.733</cell><cell>0.760</cell><cell>0.769</cell><cell>0.792</cell></row><row><cell>Feedback Model (Sec. 4.4)</cell><cell>0.658</cell><cell>0.744</cell><cell>0.726</cell><cell>0.783</cell><cell>0.698</cell><cell>0.762</cell><cell>0.770</cell><cell>0.819</cell><cell>0.756</cell><cell>0.783</cell><cell>0.786</cell><cell>0.808</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Cross-database comparisons: Results when models trained on the new database are applied on CLIVE<ref type="bibr" target="#b16">[17]</ref> and KonIQ<ref type="bibr" target="#b17">[18]</ref> without fine-tuning.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Validation Set</cell><cell></cell></row><row><cell></cell><cell cols="2">CLIVE [17]</cell><cell cols="2">KonIQ [18]</cell></row><row><cell>Model</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell></row><row><cell>NIMA(MobileNet v2) [46]</cell><cell>0.712</cell><cell>0.705</cell><cell>0.666</cell><cell>0.721</cell></row><row><cell>NIMA(ResNet 18) [46]</cell><cell>0.707</cell><cell>0.645</cell><cell>0.707</cell><cell>0.679</cell></row><row><cell>Baseline Model (Sec. 4.1)</cell><cell>0.740</cell><cell>0.725</cell><cell>0.753</cell><cell>0.764</cell></row><row><cell>RoIPool Model (Sec. 4.2)</cell><cell>0.762</cell><cell>0.775</cell><cell>0.776</cell><cell>0.794</cell></row><row><cell>Feedback Model (Sec. 4.4)</cell><cell>0.784</cell><cell>0.754</cell><cell>0.788</cell><cell>0.808</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Number of model parameters.</figDesc><table><row><cell>Model</cell><cell>Backbone params</cell><cell>Head params</cell><cell>Total params</cell></row><row><cell>CNNIQA [68]</cell><cell>-</cell><cell>-</cell><cell>724.90 K</cell></row><row><cell>NIMA (MobileNet v2) [46]</cell><cell>2.22 M</cell><cell>10.11 K</cell><cell>2.23 M</cell></row><row><cell>NIMA (ResNet-18) [46]</cell><cell>11.17 M</cell><cell>10.11 K</cell><cell>11.18 M</cell></row><row><cell>Baseline (Sec. 4.1)</cell><cell>11.17 M</cell><cell>537.99 K</cell><cell>11.70 M</cell></row><row><cell>RoIPool Model (Sec. 4.2)</cell><cell>11.17 M</cell><cell>537.99 K</cell><cell>11.70 M</cell></row><row><cell>Feedback Model (Sec. 4.4)</cell><cell>11.17 M</cell><cell>1.07 M</cell><cell>12.24 M</cell></row><row><cell cols="2">C. Picture MOS vs Patch MOS scatter plots</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Global Internet Phenomena Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sandvine</surname></persName>
		</author>
		<ptr target="https://www.sandvine.com/global-internet-phenomena-report-2019.1" />
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic prediction of perceptual image and video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mean squared error: Love it or leave it? A new look at signal fidelity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The effects of a visual fidelity criterion of the encoding of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mannos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sakrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theor</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="1974-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AVA: A largescale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Comput. Vision and Pattern Recogn. (CVPR)</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Film grain synthesis for AV1 video codec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Norkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conf. (DCC)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simulating bokeh effect with kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim Conf. Multimedia, Sept</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making a &quot;Completely blind image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual quality prediction on authentically distorted images using a bag of features approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Comput. Vision and Pattern Recogn. (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Blind image quality assessment based on high order statistics aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4444" to="4457" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using Free Energy Principle For Blind Image Quality Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="63" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning without human scores for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Comput. Vision and Pattern Recogn. (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="995" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Massive online crowdsourced study of subjective and objective picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Koniq-10K: Towards an ecologically valid and large-scale IQA database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08489</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual importance pooling for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selected Topics in Signal Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VQpooling: Video quality pooling adaptive to perceptual distortion severity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="610" to="620" />
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vision and Pattern Recogn</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Comput. Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Info Process Syst (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TID2008-a database for evaluation of full-reference visual quality assessment metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zelensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances of Modern Radioelectronics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Color image database TID2013: Peculiarities and preliminary results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vozel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">. J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Workshop on Visual Information Processing</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conf. Signals, Systems Comput</title>
		<meeting><address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Most apparent distortion: Full-reference image quality assessment and the role of strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<idno>011006:1011006:21</idno>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FSIM: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">VSNR: A wavelet-based visual signal-to-noise ratio for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2284" to="2298" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Haar wavelet-based perceptual similarity index for image quality assessment</title>
	</analytic>
	<monogr>
		<title level="j">Image Comm</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="33" to="43" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Signal Process</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">VSI: A visual saliency-induced index for perceptual image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4270" to="4281" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">VMAF: The Journey Continues, The Netflix Tech Blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cock</surname></persName>
		</author>
		<ptr target="https://medium.com/netflix-techblog/vmaf-the-journey-continues-44b51ee9ed12" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Optimized shot-based encodes: Now streaming!, The Netflix Tech Blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aaron</surname></persName>
		</author>
		<ptr target="https://medium.com/netflix-techblog/optimized-shot-based-encodes-now-streaming-4b9464204830" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relations between the statistics of natural images and the response properties of cortical cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2379" to="2394" />
			<date type="published" when="1987-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The statistics of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: computation in neural systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="517" to="548" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Natural image statistics and neural representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1193" to="1216" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multichannel texture analysis using localized spatial filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5573</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Categorical image quality (CSIQ) database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<ptr target="http://vision.eng.shizuoka.ac.jp/mod/page/view.php?id=23.3" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Blind image quality assessment on real distorted images using deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Global Conference on Signal and Information processing</title>
		<imprint>
			<biblScope unit="page" from="946" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep convolutional neural models for picture-quality prediction: Challenges and solutions to data-driven image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="130" to="141" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A deep neural network for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maniry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Int&apos;l Conf. Image Process. (ICIP)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="3773" to="3777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fully deep blind image quality predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selected Topics in Signal Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">NIMA: Neural image assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018-08-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">RankIQA: Learning from rankings for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Comput. Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10401049</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end blind image quality assessment using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1202" to="1213" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vision and Pattern Recogn</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">On the use of deep learning for blind image quality assessment. Signal, Image and Video Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Celona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DeepRN: A content preserving deep architecture for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<ptr target="http://www.inf.uni-konstanz.de/saupe.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (VOC) challenge. Int&apos;l J. of Comput. Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">EMOTIC: Emotions in context dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vision and Pattern Recogn. Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">No-reference blur assessment in natural images using fourier transform and spatial pyramids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mavridaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Image Process. (ICIP)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A probabilistic approach to people-centric photo selection and sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arnfred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2609" to="2624" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Measuring colorfulness in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Suesstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Conf. on Human Vision and Electronic Imaging VIII</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Contrast in complex images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Peli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2032" to="2040" />
			<date type="published" when="1990-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Image complexity and spatial information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Workshop on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Face detection using haar cascades. OpenCV-Python Tutorials</title>
		<idno>Online] Available: https:</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Evaluating amazon&apos;s mechanical turk as a tool for experimental behavioral research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J C</forename><surname>Crump</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Gureckis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Large-scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Predicting the quality of images compressed after distortion in two steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5757" to="5770" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Models</forename><surname>Torchvision</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/docs/stable/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Comput. Vision and Pattern Recogn. (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Comput. Vision and Pattern Recogn. (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00516</idno>
		<title level="m">Learning to blindly assess image quality in the laboratory and wild</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
