<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADAPTIVE DIMENSION REDUCTION AND VARIATIONAL INFERENCE FOR TRANSDUCTIVE FEW-SHOT CLASSIFICATION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-20">September 20, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Hu</surname></persName>
							<email>yuqing.hu@imt-atlantique.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IMT-Atlantique</orgName>
								<address>
									<settlement>Orange Labs, Cesson-S?vign?, Brest</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
							<email>stephane.pateux@orange.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Cesson-S?vign?</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
							<email>vincent.gripon@imt-atlantique.fr</email>
							<affiliation key="aff2">
								<orgName type="department">IMT-Atlantique</orgName>
								<address>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADAPTIVE DIMENSION REDUCTION AND VARIATIONAL INFERENCE FOR TRANSDUCTIVE FEW-SHOT CLASSIFICATION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-20">September 20, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transductive Few-Shot learning has gained increased attention nowadays considering the cost of data annotations along with the increased accuracy provided by unlabelled samples in the domain of few shot. Especially in Few-Shot Classification (FSC), recent works explore the feature distributions aiming at maximizing likelihoods or posteriors with respect to the unknown parameters. Following this vein, and considering the parallel between FSC and clustering, we seek for better taking into account the uncertainty in estimation due to lack of data, as well as better statistical properties of the clusters associated with each class. Therefore in this paper we propose a new clustering method based on Variational Bayesian inference, further improved by Adaptive Dimension Reduction based on Probabilistic Linear Discriminant Analysis. Our proposed method significantly improves accuracy in the realistic unbalanced transductive setting on various Few-Shot benchmarks when applied to features used in previous studies, with a gain of up to 6% in accuracy. In addition, when applied to balanced setting, we obtain very competitive results without making use of the class-balance artefact which is disputable for practical use cases. We also provide the performance of our method on a high performing pretrained backbone, with the reported results further surpassing the current state-of-the-art accuracy, suggesting the genericity of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Few-shot learning, and in particular Few-Shot Classification, has become a subject of paramount importance in the last years with a large number of methodologies and discussions. Where large datasets continuously benefit from improved machine learning architectures, the ability to transfer this performance to the low-data regime is still a challenge due to the high uncertainty posed using few labels. In more details, there are two main types of FSC tasks. In inductive FSC <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b32">33]</ref>, the situation comes to its extremes with only a few data samples available for each class, leading sometimes to completely intractable settings, such as when facing a black dog on the one hand and a white cat on the other hand. In transductive FSC, additional unlabelled samples are available for prediction, leading to improved reliability and more elaborate solutions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Inductive FSC is likely to occur when data acquisition is difficult or expensive, or when categories of interest correspond to rare events. Transductive FSC is more likely encountered when data labeling is expensive, for fast prototyping of solutions, or when the categories of interest are rare and hard to detect. Since the latter correspond to situations where it is possible to exploit, at least partially, the distribution of unlabelled samples, the trend evolved to using potentially varying parts of this additional source of information. With most standardized benchmarks using very limited scope of variability in the generated Few-Shot tasks, this even came to the point the best performing methods are often relying arXiv:2209.08527v1 <ref type="bibr">[cs.</ref>LG] <ref type="bibr" target="#b17">18</ref> Sep 2022 (a) Few-Shot task (b) Initialization (c) PLDA and VB inference <ref type="figure" target="#fig_1">Figure 1</ref>: Summary of the proposed method. Here we illustrate a 3-way classification task in a standard 2-simplex using soft-classification probabilities. Trajectories show the evolution across iterations. For a given Few-Shot task which nearest-class-mean probabilities are depicted in (a), a Soft-KMEANS clustering method is performed in (b) to initialize o nk (see Alg. 1). Then in (c) an iteratively refined Variational Bayesian (VB) model with Adaptive Dimension Reduction using Probabilistic Linear Discriminant Analysis (PLDA) is applied to obtain the final class predictions.</p><p>on questionable information, such as equidistribution between the various classes among the unlabelled samples, that is unlikely realistic in applications.</p><p>This limitation of benchmarking for transductive FSC has recently been discussed in <ref type="bibr" target="#b38">[39]</ref>. In this paper, the authors propose a new way of generating transductive FSC benchmarks where the distribution of samples among classes can drastically change from a Few-Shot generated task to the next one. Interestingly, they showed the impact of generating class imbalance on the performance on various popular methods, resulting in some cases in drops in average accuracy of more than 10%.</p><p>A simple way to reach state-of-the-art performance in transductive FSC consists in extracting features from the available samples using a pretrained backbone deep learning architecture, and then using semi-supervised clustering routines to estimate samples distribution among classes. Due to the very limited number of available samples, distribution-agnostic clustering algorithms are often preferred, such as K-MEANS or its variants <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref> or mean-shift <ref type="bibr" target="#b8">[9]</ref> for instance.</p><p>In this paper, we are interested in showing it is possible to combine data reduction with statistical inference through a Variational Bayesian (VB) <ref type="bibr" target="#b12">[13]</ref> approach. Here, data reduction helps considerably reduce the number of parameters to infer, while VB provides more flexibility than the usual K-Means methods. Interestingly, the proposed approach can easily cope with standard equidistributed Few-Shot tasks or the unbalanced ones proposed in <ref type="bibr" target="#b38">[39]</ref>, defining a new state-of-the-art for five popular transductive Few-Shot vision classification benchmarks.</p><p>Our claims are the following:</p><p>? We introduce a novel semi-supervised clustering algorithm based on VB inference and Probabilistic Linear Discriminant Analysis (PLDA),</p><p>? We demonstrate the general utility of our proposed method being able to improve accuracy in a variety of deep learning models and settings,</p><p>? We show the ability of the proposed method to reach state-of-the-art transductive FSC performance on multiple vision benchmarks (balanced and unbalanced).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>There are two main frameworks in the field of FSC: 1) only one unlabelled sample is processed at a time for class predictions, which is called inductive FSC, and 2) the entire unlabelled samples are available for further estimations, which is called transductive FSC. Inductive methods focus on training a feature extractor that generalizes well the embedding in a feature sub-space, they include meta learning methods such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37</ref>] that train a model in an episodic manner, and transfer learning methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33</ref>] that train a model with a set of mini-batches.</p><p>Recent state-of-the-art works on inductive FSC <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19]</ref> combine the above two strategies and propose a transfer based training used as model initialization, followed by an episodic training that adapts the model to better fit the Few-Shot tasks.</p><p>Transductive methods are becoming more and more popular thanks to their better performance due to the use of unlabelled data, as well as their utility in situations where data annotation is costly. Early literature of this branch operates on a class-balanced setting where unlabelled instances are evenly distributed among targeted classes. Graphbased methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b20">21]</ref> make use of the affinity among features and propose to group those that belong to the same class. More recent works such as <ref type="bibr" target="#b15">[16]</ref> propose methods based on Optimal Transport that realizes sample-class allocation with a minimum cost. While effective, these methods often require class-balanced priors to work well, which is not realistic due to the arbitrary unknown query set. In <ref type="bibr" target="#b38">[39]</ref> the authors put forward a novel unbalanced setting that composes a query set with unlabelled instances sampled following a Dirichlet distribution, injecting more imbalance for predictions.</p><p>In this paper we propose a clustering method to solve transductive FSC, where the aim is to estimate cluster parameters giving high predictions for unlabelled samples. Under Gaussian assumptions, previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref> have utilised algorithms such as Expectation Maximization <ref type="bibr" target="#b9">[10]</ref> (EM), with the goal of maximizing likelihoods or posteriors with respect to the parameters for a cluster, with the hidden variables marginalized. However, this may not be the most suitable way due to the scarcity of available data in a given Few-Shot task, which increases the level of uncertainty for cluster estimations. Therefore, in this paper we propose a Variational Bayesian (VB) approach, in which we regard some unknown parameters as hidden variables in order to inject more flexibility into the model, and we try to approximate the posterior of the hidden variables by a variational distribution.</p><p>As models with too few labelled samples often give too much randomness for a cluster to be stably reckoned, they often require the use of feature dimension reduction techniques to stabilize cluster estimations. Previous literature such as <ref type="bibr" target="#b24">[25]</ref> applies a PCA method that reduces dimension in a non-supervised manner, and <ref type="bibr" target="#b5">[6]</ref> proposes a modified LDA during backbone training that maximizes the ratio of inter/intra-class distance. In this paper we propose to use Probabilistic Linear Discriminant Analysis <ref type="bibr" target="#b16">[17]</ref> (PLDA) that 1) is applied on extracted features, 2) fits data more desirably into distribution assumptions, and 3) is semi-supervised in combination of a VB model. We integrate PLDA into the VB model in order to refine the reduced space through iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we firstly present the standard setting in transductive FSC, including the latest unbalanced setting proposed by <ref type="bibr" target="#b38">[39]</ref> where unlabelled samples are non-uniformly distributed among classes. Then we present our proposed method combining PLDA and VB inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem formulation</head><p>Following other works in the domain, our proposed method is operated on a feature space obtained from a pre-trained backbone. Namely, we are given the extracted features of 1) a generic base class dataset</p><formula xml:id="formula_0">D base = {x base i } N base i=1</formula><p>? C base that contains N base labelled samples where each sample x base i is a column vector of length D, and C base is the set of base classes to which these samples belong. These base classes have been used to train the backbone. And similarly, 2) a novel class dataset D novel = {x novel n } N n=1 containing N samples belonging to a set of K novel classes C novel (C base ? C novel = ?). On this novel dataset, only a few elements are labelled, and the aim is to predict the missing labels. Denote X the matrix obtained by aggregating elements in D novel row-wise.</p><p>When benchmarking transductive FSC methods, it is common to randomly generate Few-Shot tasks by sampling D novel from a larger dataset. These tasks are generated by sampling K distinct classes, L distinct labelled elements for each class (called support set) and Q total unlabelled elements without repetition and distinct from the labelled ones (called query set). All these unlabelled elements belong to one of the selected classes. We obtain a total of N = KL + Q elements in the task, and compute the accuracy on the Q unlabelled ones. Depending on how unlabelled instances are distributed among selected classes within a task, we further distinguish a balanced setting where the query set is evenly distributed among the K classes, from an unbalanced setting where it can vary from class to class. An automatic way to generate such unbalanced Few-Shot tasks has been proposed in <ref type="bibr" target="#b38">[39]</ref> where the number of elements to draw from each class is determined using a Dirichlet distribution parameterized by ? * o 1, where 1 is the all-one vector. To solve a transductive FSC task, our method is composed of PLDA and VB inference, that we introduce in the next paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probabilistic Linear Discriminant Analysis (PLDA)</head><p>In our work, PLDA <ref type="bibr" target="#b16">[17]</ref> is mainly used to reduce feature dimensions. For a Few-Shot task X, let ? w be a positive definite matrix representing the estimated shared within-class covariance of a given class, and ? b be a positive semidefinite matrix representing the estimated between-class covariance that generates class variables. The goal of PLDA is to project data onto a subspace while maximizing the signal-to-noise ratio for class labelling. In details, we obtain a</p><formula xml:id="formula_1">W T ? w W = I, W T ? b W = ? (1)</formula><p>where I is an identity matrix and ? is a diagonal matrix. In this paper, we assume a similar distribution between the pre-trained base classes and the transferred novel classes <ref type="bibr" target="#b44">[45]</ref>. Therefore we propose to estimate ? w to be the within-class scatter matrix of D base , denoted as S bass w . In practice we implement PLDA by firstly transforming X using a rotation matrix R ? R D?D and a set of scaling values s ? R D obtained from S base w . Note that we clamp the scaling values to be no larger than an upper-bound s max in order to prevent too large values, s max is a hyper-parameter. Then we project the transformed data onto their estimated class centroids space, in accordance with the d largest eigenvalues of ?, and obtain dimension-</p><formula xml:id="formula_2">reduced data U = [u 1 , ..., u n , ..., u N ] T ? R N ?d where u n = W T x n and d = K ? 1.</formula><p>More detailed implementation can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variational Bayesian (VB) Inference</head><p>During VB inference, we operate on a reduced d-dimensional space obtained after applying PLDA. Considering a Gaussian mixture model for a given task U ? R N ?d in reduced space, let ? be the unknown variables of the model. In VB we attempt to find a probability distribution q(?) that approximates the true posterior p(?|U), i.e. maximizes the ELBO (see Appendix for more details). In our case, we define ? = {Z, ?, ?} where Z = {z n } N n=1 is a set of latent variables used as class indicators, each latent variable z n has an one-of-K representation, ? is a K-dimensional vector representing mixing ratios between the classes, and ? = {? k } K k=1 where ? k is the centroid for class k. Note that 1) contrary to EM where ?, ? are seen as parameters that can be estimated directly, in VB they are deemed as hidden variables following certain distribution laws. 2) This is not a full VB model due to the lack of precision matrix (i.e. the inverse of covariance matrix) as a variable in ?. Although a VB model frees up more parameters for the unknown variables, it also increases the instability in estimations so that the model becomes too sensible. Therefore, in this paper we impose an assumption that all classes in U share the same precision matrix and it is fixed during VB iterations. Namely we define ? k = ? = T vb I for k = 1, ..., K, where T vb is a hyper-parameter in order to compensate the variation between base and estimated novel class distributions.</p><p>In order for a model to be in a variational bayesian setting, we define priors and likelihoods on the unknown variables, with several initialization parameters attached:</p><formula xml:id="formula_3">priors : p(?) = Dir(?|? o ), p(?) = K k=1 N (? k |m o , (? o ?) ?1 ), likelihoods : p(Z|?) = N n=1 Categorical(z n |?), p(U |Z, ?) = N n=1 K k=1 N (u n |? k , ? ?1 ) z nk (2)</formula><p>where ? follows a K-dimensional symmetric Dirichlet distribution, with ? o being the prior of component weight for each class, which we set to 2.0 in accordance with <ref type="bibr" target="#b38">[39]</ref>, i.e. the same value as the Dirichlet distribution parameter ? * o that is used to generate Few-Shot tasks. The vector m o is the prior about the class centroid variables, we let it to be 0. And ? o stands for the prior about the moving range of class centroid variables: the larger it is, the closer the centroids are to m o . We empirically found that ? o = 10.0 gives consistent good results across datasets and FSC problems.</p><p>As previously stated, we approximate a variable distribution to the true posterior. To further simplify, we follow the Mean-Field assumption <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b17">18]</ref> and assume that the unknown variables are independent from one another. Therefore we let q(?) = q(Z, ?, ?) = q(Z)q(?)q(?) ? p(Z, ?, ?/U) and solve for each term. The explicit formulation for these marginals is provided in Eq. 3, 4 (see Appendix for more details). The estimation of the various parameters is then classically performed through an iterative EM framework as presented further.</p><p>Denote o n = [o n1 , ..., o nk , ..., o nK ] as the soft class assignment for u n (o nk ? 0, K k=1 o nk = 1), and o nk represents the portion of nth sample allocated to kth class.</p><p>M step: In this step we estimate q(?) and q(?) in use of the class assignments o nk :</p><formula xml:id="formula_4">p(?) = Dir(?|? o ) =? q * (?) = Dir(?|?) with ? k = ? o + N k , p(?) = K k=1 N (? k |m o , (? o ?) ?1 ) =? q * (?) = K k=1 N (? k |m k , (? k ?) ?1 ) with ? k = ? o + N k , m k = 1 ? k (? o m o + N n=1 o nk u n ),<label>(3)</label></formula><p>where ? = [? 1 , ..., ? k , ..., ? K ] are the estimated component weights for classes, and N k = N n=1 o nk is the sum of the soft assignments for all samples in class k. We also estimate the moving range parameter ? k and the centroid m k for each class centroid variable. We observe that the posteriors take the same forms as the priors. Demonstration of these results is presented in Appendix. </p><formula xml:id="formula_5">p(Z|?) = N n=1 Categorical(z n |?) =? q * (Z) = N n=1 Categorical(z n |o n )<label>(4)</label></formula><p>where each element o nk can be computed as o nk = ? nk K j=1 ?nj in which:</p><formula xml:id="formula_6">log ? nk = ?(? k ) ? ?( K j=1 ? j ) + 1 2 log |?| ? d 2 log 2? ? 1 2 [d? ?1 k + (u n ? m k ) T ?(u n ? m k )],<label>(5)</label></formula><p>with ?(?) being the logarithmic derivative of the gamma function (also known as the digamma function). We observe that q * (Z) follows the same categorical distribution as the likelihood, and it is parameterized by o nk . More details can be found in Appendix.</p><p>Proposed algorithm The proposed method combines PLDA and VB inference which leads to an Efficiency Guided Adaptive Dimension Reduction for VAriational BAyesian inference. We thus name our proposed method "BAVARDAGE", and the detailed description is presented in Algorithm 1. Given a Few-Shot task X and a within-class scatter matrix S base w , we initialize o nk using EM algorithm with an assumed covariance matrix, adjusted by a temperature hyper-parameter T km , for all classes. Note that this is equivalent to Soft-KMEANS <ref type="bibr" target="#b19">[20]</ref> algorithm. And for each iteration we update parameters: in M step we update ? k , ? k and centroids m k , in E step we only update o nk , and we apply PLDA with the updated o nk to reduce feature dimensions. Finally, predicted labels are obtained by selecting the class that corresponds to the largest value in o nk .</p><p>The illustration of our proposed method is presented in <ref type="figure" target="#fig_1">Figure 1</ref>. For a Few-Shot task that has three classes (red, blue and green) with unlabelled samples depicted on the probability simplex, we firstly initialize o nk with Soft-KMEANS which directs some data points to their belonging classes while further distancing some points from their targeted classes. Then we apply the proposed VB inference integrated with PLDA, resulting in additional points moving towards their corresponding classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we provide details on the standard transductive Few-Shot classification settings, and we evaluate the performance of our proposed method.</p><p>Benchmarks We test our method on standard Few-Shot benchmarks: mini-Imagenet <ref type="bibr" target="#b34">[35]</ref>, tiered-Imagenet <ref type="bibr" target="#b31">[32]</ref> and caltech-ucsd birds-200-2011 (CUB) <ref type="bibr" target="#b40">[41]</ref>. mini-Imagenet is a subset of ILSVRC-12 <ref type="bibr" target="#b34">[35]</ref> dataset, it contains a total of 60, 000 images of size 84 ? 84 belonging to 100 classes (600 images per class) and follows a 64-16-20 split for base, validation and novel classes. tiered-Imagenet is a larger subset of ILSVRC-12 containing 608 classes with 779, 165 images of size 84 ? 84 in total, and we use the standard 351-97-160 split, and CUB is composed of 200 classes following a 100-50-50 split (Image size: 84 ? 84). In Appendix we also show the performance of our proposed method on other well-known Few-Shot benchmarks such as FC100 <ref type="bibr" target="#b29">[30]</ref> and CIFAR-FS <ref type="bibr" target="#b3">[4]</ref>. </p><formula xml:id="formula_7">? k = ? o + N n=1 o nk ? k = ? o + N n=1 o nk m k = 1 ? k (? o m o + N n=1 o nk u n ) VB (E step): log ? nk = ?(? k ) ? ?( K j=1 ? j ) + 1 2 log |?| ? d 2 log 2? ? 1 2 [d? ?1 k + (u n ? m k ) T ?(u n ? m k )] o nk = ? nk K j=1 ?nj end for return? (x n ) = arg max k (o nk )</formula><p>Settings Following previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>, our proposed method is evaluated on 1-shot 5-way (K = 5, L = 1), and 5-shot 5-way (K = 5, L = 5) scenarios. As for the query set, we set a total number of Q = 75 unlabelled samples, from which we further define two settings: 1) a balanced setting where unlabelled instances are evenly distributed among K classes, and 2) an unbalanced setting where the query set is randomly distributed, following a Dirichlet distribution parameterized by ? * o . In our paper we follow the same setting as <ref type="bibr" target="#b38">[39]</ref> and set ? * o = 2.0, further experiments with different values are conducted in the next sections. The performance of our proposed method is evaluated by computing the averaged accuracy over 10, 000 Few-Shot tasks.</p><p>Implementation details In this paper we firstly compare our proposed algorithm with the other state-of-the-art methods using the same pretrained backbones and benchmarks provided in <ref type="bibr" target="#b38">[39]</ref>. Namely we extract the features using the same ResNet-18 (RN18) and WideResNet28_10 (WRN) neural models, and present the performance on mini-Imagenet, tiered-Imagenet and CUB datasets. In our proposed method, the raw features are preprocessed following <ref type="bibr" target="#b41">[42]</ref>. As for the hyper-parameters, we set T km = 10, T vb = 50, s max = 2 for the balanced setting; T km = 50, T vb = 50, s max = 1 for the unbalanced setting, and we use the same VB priors for all settings. To further show the functionality of our proposed method on different backbones and other benchmarks, we tested BAVARDAGE on a recent high performing feature extractor trained on a ResNet-12 (RN12) neural model <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref>, and we report the accuracy in <ref type="table">Table 1</ref> and in Appendix with various settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main results</head><p>The main results on the relevant settings are presented in <ref type="table">Table 1</ref>. Note that we report the accuracy of other methods following <ref type="bibr" target="#b38">[39]</ref>, and add the performance of our proposed method in comparison, using the same pretrained RN18 and WRN feature extractors, and we also report the result of a RN12 backbone pretrained following <ref type="bibr" target="#b2">[3]</ref>. We observe that our proposed algorithm reaches state-of-the-art performance for nearly all referenced datasets in the unbalanced setting, surpassing previous methods by a noticeable margin especially on 1-shot. In the balanced setting we also reach competitive accuracy compared with <ref type="bibr" target="#b15">[16]</ref> along with other works that make use of a perfectly balanced prior on unlabelled samples, while our proposed method suggests no such prior. In addition, we provide results on the other Few-Shot benchmarks with different settings in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation studies</head><p>Analysis on the elements of BAVARDAGE In this experiment we dive into our proposed method and conduct an ablation study on the impact of each element. Namely, we report the performance in the following 3 scenarios: 1) only run Soft-KMEANS on the extracted features to obtain a baseline accuracy; 2) run the VB model with o nk initialized by Soft-KMEANS, without reducing the feature space; and 3) integrate PLDA into VB iterations. From <ref type="table" target="#tab_1">Table 2</ref> we observe only a slight increase of accuracy compared with baseline when no dimensionality reduction is applied. This is due to the fact that high feature dimensions increase uncertainty in the estimations, making the model sensitive to parameters. With our implementation of PLDA iteratively applied in the VB model, we can see from the table that the <ref type="table">Table 1</ref>: Comparisons of the state-of-the-art methods on mini-Imagenet, tiered-Imagenet and CUB datasets using the same pretrained backbones as <ref type="bibr" target="#b38">[39]</ref>, along with the accuracy of our proposed method on a ResNet-12 backbone pretrained following <ref type="bibr" target="#b2">[3]</ref>. performance increases by a relatively large margin, suggesting the effectiveness of our proposed adaptive dimension reduction method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of features for different projections</head><p>To further showcase the effect of proposed PLDA, in <ref type="figure">Fig. 2</ref> we visualize the extracted features of a 3-way Few-Shot task in the following 3 scenarios: (a) features in the original space, using T-SNE <ref type="bibr" target="#b37">[38]</ref> for visualization purpose; (b) features that are projected directly onto their centroids space, and finally (c) features projected using PLDA. The ellipses drawn in (b) and (c) are the cluster estimations computed using the real labels of data samples, and we can thus observe a larger separation of different clusters with PLDA projection for the task in which the original features overlap heavily between clusters in blue and green.  <ref type="figure">Figure 2</ref>: Visualization of extracted features of a Few-Shot task using different projection methods (dataset: mini-Imagenet, backbone: WRN), we report a 86.7%, 90.0% and 95.0% prediction accuracy corresponding to each projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness against imbalance</head><p>In <ref type="table">Table 1</ref> we show the accuracy of our proposed method using VB priors introduced in Section 3.3, in which ? o is set to be equal to the Dirichlet's parameter ? * o for the level of imbalance in the query set. Therefore, in this experiment we test the robustness of BAVARDAGE, namely in <ref type="figure">Fig. 3</ref> we alter ? o and report the accuracy on different imbalance levels (varying ? * o ) in both 1-shot and 5-shot settings. Note that the proposed model becomes slightly more sensitive to ? o when the level of imbalance increases (smaller ? * o ), with an approximate 1% drop of accuracy when increasing ? o in the case of ? * o = 1.  Varying Few-Shot settings In this experiment we observe the performance of BAVARDAGE on different Few-Shot settings, namely we vary the number of labelled samples per class L as well as the total number of unlabelled samples Q in a task, for further comparison we also report the accuracy using only Soft-KMEANS algorithm. In <ref type="figure" target="#fig_4">Fig. 4</ref> we can observe constant higher accuracy of our proposed method, and a slightly larger difference gap when Q increases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we proposed a clustering method based on Variational Bayesian Inference and Probabilistic Linear Discriminant Analysis for transductive Few-Shot Classification. BAVARDAGE has reached state-of-the-art accuracy on nearly all Few-Shot benchmarks in the realistic unbalanced setting, as well as competitive performance in the balanced setting without using a perfectly class-balanced prior. As our proposed method assumes a shared isotropic covariance matrix for all clusters, the estimations in VB models could be limited. Therefore the future work could study a better estimation of covariance matrices associated with each cluster. An interesting asset of the proposed method is that it performs most of its processing in a reduced (K ? 1)-dimensional space, where K is the number of classes, suggesting interests for visualization and suitability for more elaborate statistical machine learning methods. As in <ref type="bibr" target="#b38">[39]</ref>, we encourage the community to rethink the works in transductive settings to provide fairer grounds of comparison between the various proposed approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation details on the proposed PLDA</head><p>In this section we present more details on our implementation of PLDA proposed in section 3.2 in the paper. Given X ? R N ?D , we estimate its within-class covariance matrix to be S base w calculated from D base . Denote I base c as the set of samples belonging to base class c where c ? 1, ..., |C base |, therefore ? w is approximated as follows:</p><formula xml:id="formula_8">? w ? S base w = c i?I base c (x base i ? m base c )(x base i ? m base c ) T N base ,<label>(6)</label></formula><p>where m base</p><formula xml:id="formula_9">c = 1 |I base c | i?I base c x base i</formula><p>is the mean of c-th base class. Let ? = [? 1 , ..., ? i , ..., ? D ] ? R D be the eigenvalues of S base w in descending order, and we set R = [r 1 , ..., r i , ..., r D ] ? R D?D to be the corresponding eigenvectors. In this paper we define a transformation matrix T = SR where S is a diagonal matrix with diagonal values being the square root of multiplicative inverse of ?, clamped to an upper bound s max . Namely, s = diag(S) where s = [s 1 , ...s i , ...s D ] ? R D is a D-length vector containing the scaling value for each dimension, and we set s i to be as follows:</p><formula xml:id="formula_10">s i = ? ?0.5 i if ? ?0.5 i ? s max s max otherwise .<label>(7)</label></formula><p>We can see from Eq. 7 that T is composed of a rotation matrix and scaling values on feature dimensions that help morph the within-class distribution into an identity covariance matrix. This corresponds to a data sphering/whitening process in which we decorrelate samples in each of the dimensions. In our implementation we transform X by multiplying it with T. Therefore the sphered data samples, denoted as X = [x 1 , ...x n , ...x N ] T ? R N ?D , are obtained from x n = Tx n .</p><p>Next, we project X onto a subspace that corresponds to the K ? 1 largest eigenvalues of its between-scatter matrix. Denote m k as the estimated centroid for class k, given soft class assignments o nk (1 ? n ? N, 1 ? k ? K), m k is computed as:</p><formula xml:id="formula_11">m k = N n=1 o nk x n ? + N k , N k = N n=1 o nk ,<label>(8)</label></formula><p>Algorithm 2 Proposed PLDA Fonction PLDA (X, S base w , s max , o nk ) Sphere X using T (Eq. 7), obtain X . Estimate centroids m k using o nk (Eq. 8). Compute ? using m k (Eq. 9). Project X onto the centroids space, obtain U. Return U where ? is used as an offset indicating how close the centroids are to 0, in this paper we set it to 10.0, same as ? o in the VB model in reduced space. Therefore, the between-class scatter matrix ? of sphered samples can be calculated as:</p><formula xml:id="formula_12">? = K k=1 (m k ? m )(m k ? m ) T ,<label>(9)</label></formula><p>where m = 1 K K k=1 m k is the mean of estimated class centroids. Then we project X onto a d-length subspace, where d = K ? 1. In details, denote V = [v 1 , ..., v i , ..., v d ] ? R D?d to be the eigenvectors corresponding to the d largest eigenvalues of ?, the projected data U are obtained as u n = V T x n for each sample. Note that the formulation of ? in Eq. 9 allows at most K ? 1 non-zero eigenvalues, therefore the resulting subspace projection using these eigenvectors is equivalent to a projection onto the affine subspace containing the centroids m k . Furthermore, according to Eq. 1 in the paper, we can further deduce the projection matrix W to be as follows:</p><formula xml:id="formula_13">u n = W T x n = V T x n = V T Tx n = V T SRx n , =? W = (V T SR) T = R T SV.<label>(10)</label></formula><p>The entire process is described in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation details on the proposed VB model</head><p>In this section we provide more detailed explanation of our proposed VB model. Given a posterior p(?|U), we approximate it with a function variational distribution q(?) by minimizing the Kullback-Leibler divergence: </p><p>where the evidence log p(U) is considered fixed, and L(q) = q(?) log p(?,U) q(?) d? stands for Evidence Lower BOund (ELBO) providing "evidence" that we have chosen the right model. We can see that minimizing the Kullback-Leibler divergence is equivalent to maximizing the ELBO. Suppose ? = {? 1 , ..., ? m , ..., ? M }, we firstly factorize q(?) = M m=1 q(? m ) according to the Mean-Field assumption, then we solve each term individually:</p><formula xml:id="formula_15">L(q) = q(?) log p(?, U) q(?) d? = M m=1 q(? m ) log p(?, U) ? M m=1 log q(? m ) d? 1 d? 2 ...d? M = M m=1 q(? m ) q(? ?m ) log p(?, U)d? ?m d? m ? q(? m ) log q(? m )d? m ,<label>(12)</label></formula><p>and the ELBO is maximized when:</p><formula xml:id="formula_16">log q * (? m ) = E ??m [log p(?, U)] + const,<label>(13)</label></formula><p>where E ??m [?] stands for the expectation with respect to all variables in ? except ? m . In our method we define ? = {Z, ?, ?}, the detailed formula of some variables are presented as follows:</p><formula xml:id="formula_17">z n = [z n1 , ..., z nk , ..., z nK ] ? {0, 1} K , K k=1 z nk = 1, ? = [? 1 , ..., ? k , ..., ? K ], ? k ? 0, K k=1 ? k = 1.<label>(14)</label></formula><p>According to Bayes' theorem, we rewrite the posterior to be:</p><formula xml:id="formula_18">p(?|U) = p(Z, ?, ?|U) = p(Z, ?, ?, U) p(U) = p(U|Z, ?)p(Z|?)p(?)p(?) p(U) ,<label>(15)</label></formula><p>in which:</p><formula xml:id="formula_19">p(U|Z, ?) = N n=1 K k=1 N (u n |? k , ? ?1 ) z nk , p(Z|?) = N n=1 Categorical(z n |?) = N n=1 K k=1 ? z nk k , p(?) = Dir(?|? o ) = ?( K k=1 K? o ) K k=1 ?(? o ) K k=1 ? ?o?1 k = C(? o ) K k=1 ? ?o?1 k , p(?) = K k=1 N (? k |m o , (? o ?) ?1 ).<label>(16)</label></formula><p>According to Eq. 13, q * (?) can be computed as follows:</p><formula xml:id="formula_20">log q * (?) = E Z,? [log p(Z, ?, ?, U)] + const = E Z [log p(Z|?)] + log p(?) + const = N n=1 K k=1 E Z [z nk ] log ? k + K k=1 (? o ? 1) log ? k + const = K k=1 N n=1 o nk log ? k + K k=1 (? o ? 1) log ? k + const = K k=1 (N k + ? o ? 1) log ? k + const, =? q * (?) = K k=1 ? N k +?o?1 k + const = K k=1 ? ? k ?1 k + const = Dir(?|?).<label>(17)</label></formula><p>Similarly for q * (?) we can compute it as shown below:</p><formula xml:id="formula_21">log q * (?) = E Z,? [log p(Z, ?, ?, U)] + const = E Z [log p(U|Z, ?)] + log p(?) + const = N n=1 K k=1 E Z [z nk ] log N (u n |? k , ? ?1 ) + K k=1 log N (? k |m o , (? o ? ?1 ) + const = 1 2 N n=1 K k=1 o nk log |?| ? 1 2 N n=1 K k=1 o nk (u n ? ? k ) T ?(u n ? ? k ) + 1 2 K k=1 log |? o ?| ? 1 2 K k=1 (? k ? m o ) T ? o ?(? k ? m o ).<label>(18)</label></formula><p>To compute ? k , we gather the quadratic terms that contain ? k in Eq. 18:</p><formula xml:id="formula_22">(quad) = ? 1 2 N n=1 K k=1 o nk ? T k ?? k ? 1 2 K k=1 ? T k ? o ?? k = ? 1 2 K k=1 ? T k (N k ? + ? o ?)? k = ? 1 2 K k=1 ? T k (? o + N k )? k ? k , =? ? k = ? o + N k .<label>(19)</label></formula><p>As for m k , we gather the linear terms that contain ? k in Eq. 18:</p><formula xml:id="formula_23">(linear) = 1 2 N n=1 K k=1 o nk ? T k ?u n + 1 2 K k=1 ? T k ? o ?m o = 1 2 K k=1 ? T k ?(? o m o + N n=1 o nk u n ) = 1 2 K k=1 ? T k ? k ?m k , =? m k = 1 ? k (? o m o + N n=1 o nk u n ).<label>(20)</label></formula><p>Therefore q * (?) can be reformulated as:</p><formula xml:id="formula_24">q * (?) = K k=1 q * (? k ) = K k=1 N (? k |m k , (? k ?) ?1 ).<label>(21)</label></formula><p>We also provide a more detailed calculation of q * (Z):</p><formula xml:id="formula_25">log q * (Z) = E ?,? [log p(Z, ?, ?, U)] + const = E ? [log p(Z|?)] + E ? [log p(U|Z, ?)] + const = N n=1 K k=1 z nk E ? [log ? k ] + E ? [log N (u n |? k , ? ?1 )] + const = N n=1 K k=1 z nk log ? nk + const,<label>(22)</label></formula><p>where</p><formula xml:id="formula_26">log ? nk = E ? [log ? k ] + E ? [log N (u n |? k , ? ?1 )] = E ? [log ? k ] + 1 2 log |?| ? d 2 log 2? ? 1 2 E ? [(u n ? ? k ) T ?(u n ? ? k )].<label>(23)</label></formula><p>Therefore q * (Z) can be expressed as:</p><formula xml:id="formula_27">q * (Z) = N n=1 K k=1 o z nk nk = N n=1 Categorical(z n |o n ), o nk = ? nk K j=1 ? nj ,<label>(24)</label></formula><p>we can see that the variable follows a categorical distribution, parameterized by o nk , and o nk = E Z [z nk ]. As for Eq. 23, more details are shown as follows:</p><formula xml:id="formula_28">E ? [log ? k ] = ?(? k ) ? ?( K j=1 ? j ), E ? [(u n ? ? k ) T ?(u n ? ? k )] = (u n ? ? k ) T ?(u n ? ? k )q * (? k )d? k = (u n ? m k ) T ? k (u n ? m k ) + Tr[? ? (? k ?) ?1 ] = d? ?1 k + (u n ? m k ) T ?(u n ? m k ),<label>(25)</label></formula><p>?(?) is the logarithmic derivative of the gamma function, and the distribution for ? k and ? k follows Eq. 17 and 21. Therefore:</p><formula xml:id="formula_29">log ? nk = ?(? k ) ? ?( K j=1 ? j ) + 1 2 log |?| ? d 2 log 2? ? 1 2 [d? ?1 k + (u n ? m k ) T ?(u n ? m k )].<label>(26)</label></formula><p>From the above equations we observe a dependency between priors and posteriors, which can be estimated iteratively depending on the class allocations. Therefore in this paper we propose to solve it under a basic Expectation Maximization framework where we estimate o nk in the E-step, while updating ? k , ? k and m k in the M-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hyperparameter tuning</head><p>In this section we detail about how the hyperparameters in our proposed method are obtained. Namely, for a standard Few-Shot benchmark that has been split into base-validation-novel class set, we firstly tune our model using validation set and choose the hyperparameters accordingly before applying to the novel set. For example in <ref type="figure" target="#fig_6">Figure 5</ref> we tune two temperature parameters T km , T vb , the scaling up-bound parameter s max and the VB prior ? o that are used in our proposed BAVARDAGE. The blue curves show the performance on validation set while the red curves show the accuracy on the novel set (benchmark: mini-Imagenet). From the figure we see a similar behavior between two sets in terms of performance, T km has little impact on the accuracy, same for T vb when it is large. For s max we observe an uptick when it is around 1, followed by a slowing decrease and finally stabilizing to the same accuracy when it becomes larger. In this paper we tune hyperparameters for each benchmark in the same way. For tiered-Imagenet we set T km , T vb and s max to be 10, 100, 2 in the balanced setting, 100, 100, 1 in the unbalanced setting; for CUB we set them to be 10, 4, 5 in both balanced and unbalanced settings; and for FC100 and CIFAR-FS we set the hyperparameters to be the same as mini-Imagenet. As for ? o we set it to be 10 across datasets since it gives the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Additional experiments on other Few-Shot benchmarks</head><p>In Section 4 in the paper we tested our proposed method on three standard Few-Shot benchmarks:  600 images of size 32x32 pixels; 2) CIFAR-FS (https://github.com/bertinetto/r2d2) is also sampled from CIFAR-100 and shares the same quantity of classes in the base-validation-novel splits as for mini-Imagenet. Each class contains 600 images of size 32x32 pixels. In <ref type="table" target="#tab_3">Table 3</ref> below we report the accuracy of our proposed method on all benchmarks, note that for FC100 and CIFAR-FS we believe to be among the first to conduct experiments in the unbalanced setting.</p><p>In <ref type="table" target="#tab_3">Table 3</ref> we also show the results using WRN and RN18 pretrained from <ref type="bibr" target="#b38">[39]</ref> and RN12 pretrained from <ref type="bibr" target="#b2">[3]</ref>, same as <ref type="table">Table 1</ref> in the paper, with a confidence interval of 95% added next to the accuracy. In addition, given that some works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref> in the field utilize data augmentation techniques to extract features based on images in original dimensions instead of reduced ones, here we apply our BAVARDAGE following the same setting and report the accuracy on a pretrained RN12 feature extractor <ref type="bibr" target="#b2">[3]</ref> with data augmentation (denote RN12*). For comparison purpose we also provide a baseline accuracy on each Few-Shot benchmark using Soft-KMEANS algorithm.</p><p>With BAVARDAGE, we observe a clear increase of accuracy for all datasets compared with Soft-KMEANS in both balanced and unbalanced settings, suggesting the genericity of the proposed method. As for the computational time, we evaluate an average of 1.72 seconds per accuracy (on 10,000 Few-Shot tasks) using a GeForce RTX 3090 GPU. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>E step: In this step we estimate q(Z) by updating o nk , using the current values of all other parameters computed in the M-step, i.e. ? k , ? k and m k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 BAVARDAGE</head><label>1</label><figDesc>Inputs: X ? R N ?D , S base w ? R D?D Hyper-parameters: T km , T vb , s max Priors for VB: ? o = 2.0, ? o = 10.0, m o = 0, ? = T vb ? I Initializations: o nk = EM (X, T km ) for i = 1 to n step do U = PLDA (X, S base w , s max , o nk ) # See more details in Appendix. VB (M step):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 : 1 -</head><label>31</label><figDesc>shot and 5-shot accuracy on different imbalance levels (varying ? * o ) as a function of VB priors ? o (dataset: mini-Imagenet, backbone: WRN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Accuracy as a funtion of L and Q in comparison with Soft-KMEANS (dataset: mini-Imagenet, backbone: WRN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Hyperparameter tuning of our proposed method. Here we tune 4 hyperparameters of BAVARDAGE on mini-Imagenet (backbone: WRN) in the unbalanced setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the elements of our proposed method, with results tested on mini-Imagenet (backbone: WRN) and CUB (backbone: RN18) in the unbalanced setting.</figDesc><table><row><cell></cell><cell cols="2">mini-Imagenet</cell><cell cols="2">CUB</cell></row><row><cell cols="5">Soft-KMEANS VB PLDA 1-shot 5-shot 1-shot 5-shot</cell></row><row><cell></cell><cell>71.4</cell><cell>82.4</cell><cell>77.5</cell><cell>86.7</cell></row><row><cell></cell><cell>71.8</cell><cell>82.5</cell><cell>77.8</cell><cell>87.2</cell></row><row><cell></cell><cell>74.1</cell><cell>85.5</cell><cell>82.0</cell><cell>90.7</cell></row><row><cell>(a) T-SNE</cell><cell cols="2">(b) Centroids projection</cell><cell></cell><cell>(c) PLDA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>mini-Imagenet 1 , tiered-Imagenet 2 and CUB 3 , following the same setting as presented in https://github.com/oveilleux/Realistic_ Transductive_Few_Shot. In this section we further conduct experiments on two other well-known Few-Shot datasets: 1) FC100 (https://github.com/ElementAI/TADAM) is a recent split dataset based on CIFAR-100<ref type="bibr" target="#b21">[22]</ref> that contains 60 base classes for training, 20 classes for validation and 20 novel classes for evaluation, each class is composed of</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">1-shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">5-shot</cell><cell></cell></row><row><cell>Accuracy</cell><cell>74 76 78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86 88 87</cell><cell></cell><cell></cell><cell>val test</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">T km</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">T km</cell><cell></cell></row><row><cell>Accuracy</cell><cell>72 74 76 78</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell></cell><cell>100</cell><cell>86 88 82 84</cell><cell>40</cell><cell>60</cell><cell>80 val test</cell><cell></cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell>T vb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>T vb</cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>74 76 78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86 87 88 85</cell><cell></cell><cell></cell><cell>val test</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell></cell><cell>4</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell cols="2">4</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s max</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s max</cell><cell></cell></row><row><cell>Accuracy</cell><cell>72 74 76 78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86 88 84</cell><cell></cell><cell></cell><cell>val test</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell cols="2">15</cell><cell>20</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell cols="2">15</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell></cell><cell>? o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? o</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Detailed results of BAVARDAGE with confidence interval of 95% on the Few-Shot benchmarks, along with a baseline accuracy using Soft-KMEANS. We use RN18 and WRN pretrained from<ref type="bibr" target="#b38">[39]</ref>, RN12 and RN12* pretrained from<ref type="bibr" target="#b2">[3]</ref>. 68.82 ? 0.27% 81.<ref type="bibr" target="#b26">27</ref> ? 0.17% 73.47 ? 0.26% 83.04 ? 0.15% WRN [39] 71.35 ? 0.27% 82.41 ? 0.16% 75.70 ? 0.25% 84.42 ? 0.14% RN12 [3] 75.65 ? 0.25% 86.35 ? 0.14% 80.81 ? 0.24% 87.92 ? 0.12% RN12* [3] 77.51 ? 0.26% 87.78 ? 0.14% 82.14 ? 0.24% 89.08 ? 0.12% BAVARDAGE RN18 [39] 71.01 ? 0.31% 83.60 ? 0.17% 75.07 ? 0.28% 84.49 ? 0.14% WRN [39] 74.10 ? 0.30% 85.52 ? 0.16% 78.51 ? 0.27% 87.41 ? 0.13% RN12 [3] 77.85 ? 0.28% 88.02 ? 0.14% 82.67 ? 0.25% 89.50 ? 0.11% RN12* [3] 79.76 ? 0.29% 89.85 ? 0.13% 84.80 ? 0.25% 91.65 ? 0.10% 73.92 ? 0.28% 85.02 ? 0.18% 78.59 ? 0.27% 85.76 ? 0.16% RN18 [39] 73.79 ? 0.28% 84.65 ? 0.18% 78.34 ? 0.27% 85.52 ? 0.17% RN12 [3] 78.15 ? 0.27% 87.65 ? 0.17% 83.11 ? 0.25% 88.80 ? 0.15% RN12* [3] 79.62 ? 0.27% 88.61 ? 0.16% 84.08 ? 0.24% 89.56 ? 0.14% BAVARDAGE WRN [39] 77.45 ? 0.31% 87.48 ? 0.18% 81.47 ? 0.28% 88.27 ? 0.16% RN18 [39] 76.55 ? 0.31% 86.46 ? 0.19% 80.32 ? 0.28% 87.14 ? 0.16% RN12 [3] 79.38 ? 0.29% 88.04 ? 0.18% 83.52 ? 0.26% 89.03 ? 0.15% RN12* [3] 81.17 ? 0.29% 89.63 ? 0.17% 85.20 ? 0.25% 90.41 ? 0.14% CUB unbalanced balanced 77.54 ? 0.26% 86.70 ? 0.14% 82.67 ? 0.24% 89.04 ? 0.11% RN12 [3] 81.24 ? 0.25% 87.27 ? 0.14% 84.87 ? 0.22% 89.64 ? 0.11% RN12* [3] 82.40 ? 0.24% 89.40 ? 0.13% 87.38 ? 0.20% 91.29 ? 0.10% BAVARDAGE RN18 [39] 82.00 ? 0.28% 90.67 ? 0.12% 85.64 ? 0.25% 91.42 ? 0.10% RN12 [3] 83.12 ? 0.26% 90.81 ? 0.12% 87.41 ? 0.22% 92.03 ? 0.09% RN12* [3] 86.96 ? 0.24% 92.84 ? 0.10% 90.42 ? 0.20% 93.50 ? 0.08% 51.24 ? 0.27% 64.70 ? 0.22% 54.59 ? 0.26% 66.37 ? 0.20% RN12* [3] 51.64 ? 0.27% 65.26 ? 0.22% 54.87 ? 0.26% 66.89 ? 0.20% BAVARDAGE RN12 [3] 52.60 ? 0.32% 65.35 ? 0.25% 56.66 ? 0.28% 69.69 ? 0.21% RN12* [3] 53.78 ? 0.30% 68.75 ? 0.24% 57.27 ? 0.29% 70.60 ? 0.21% 80.72 ? 0.25% 88.31 ? 0.17% 85.47 ? 0.22% 89.36 ? 0.15% RN12* [3] 81.75 ? 0.25% 88.92 ? 0.17% 86.07 ? 0.22% 89.85 ? 0.15% BAVARDAGE RN12 [3] 82.68 ? 0.27% 88.97 ? 0.18% 86.20 ? 0.23% 89.58 ? 0.15% RN12* [3] 83.82 ? 0.27% 89.84 ? 0.18% 87.35 ? 0.23% 90.63 ? 0.16%</figDesc><table><row><cell>mini-Imagenet</cell><cell></cell><cell cols="2">unbalanced</cell><cell>balanced</cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell cols="2">Soft-KMEANS RN18 [39] tiered-Imagenet</cell><cell cols="2">unbalanced</cell><cell>balanced</cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell cols="2">Soft-KMEANS WRN [39] Method Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell cols="2">Soft-KMEANS RN18 [39] FC100</cell><cell cols="2">unbalanced</cell><cell>balanced</cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell cols="2">Soft-KMEANS RN12 [3] CIFAR-FS</cell><cell cols="2">unbalanced</cell><cell>balanced</cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Soft-KMEANS</cell><cell>RN12 [3]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/yaoyao-liu/mini-imagenet-tools 2 https://github.com/yaoyao-liu/tiered-imagenet-tools 3 http://www.vision.caltech.edu/datasets/cub_200_2011</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to train your MAML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meta-learning with task-adaptive loss function for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9465" to="9474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Easy: Ensemble augmented-shot y-shaped learning: State-of-the-art few-shot classification with simple ingredients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bendou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lafargue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pasdeloup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09699</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information maximization for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2445" to="2457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A theoretical analysis of the number of shots in few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical graph neural networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="240" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A tutorial on variational bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Leveraging the feature distribution in transfer-based few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic linear discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="531" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving the mean field approximation via the use of mixture distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="163" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relational embedding for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8822" to="8833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An information-theoretic analysis of hard and soft assignment methods for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="495" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Iterative label cleaning for transductive and semi-supervised few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lazarou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8751" to="8760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Few-shot and continual learning with attentive independent mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9455" to="9464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tafssl: Task-adaptive feature sub-space learning for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichtenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="522" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Prototype rectification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="741" to="756" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 16</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectifying the shortcut learning of background for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The expectation-maximization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Rodr?guez</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mean field approximation for the stochastic schr?dinger equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V</forename><surname>Prezhdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8366" to="8377" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring complementary strengths of invariant and equivariant representations for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10836" to="10846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Embedding propagation: Smoother manifold for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drouin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Realistic evaluation of transductive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veilleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Few-shot classification with feature map reconstruction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8012" to="8021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dpgn: Distribution propagation graph network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13390" to="13399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Free lunch for few-shot learning: Distribution calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Few-shot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8808" to="8817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12203" to="12213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Laplacian regularized few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11660" to="11670" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
