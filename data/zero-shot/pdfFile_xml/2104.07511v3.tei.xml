<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ensemble of MRR and NDCG models for Visual Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
							<email>idansc@cs.technion.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Technion NetApp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ensemble of MRR and NDCG models for Visual Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single humanderived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., 'yeah' and 'yes'). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as 'I don't know.' Crafting a model that excels on both MRR and NDCG metrics is challenging (Murahari et al., 2020). Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a twostep non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https: //github.com/idansc/mrr-ndcg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction <ref type="bibr" target="#b1">Das et al. (2017)</ref> introduced the task of Visual Dialog, which requires an agent to converse about visual input. Evaluating visually aware conversation should examine both linguistic properties and visual reasoning. Analysis of generative metrics for dialog often shows no correlation with human judgments <ref type="bibr">(Liu et al., 2016)</ref>. Hence, to evaluate the correctness of the candidate answers, a retrieval approach is preferred. Two metrics are standard, Question: what is the nightstand made of ? 1. can't tell it's covered in cloth 2. it appears to be a large red pillow that may be leather 5. not sure 6. can't tell 7. some kind of metal , it's out of focus 8. Wood ... 99. 0 100.I can't see a baggage cart MRR NDCG <ref type="figure">Figure 1</ref>: A visual dialog interaction. The question asks, "what is the nightstand made of ?". We show our final ranking, created by the ensemble of an MRR/NDCG models' rankings. The MRR/NDCG models are trained to optimize the MRR/NDCG metric. The MRR metric measures the number of retrievals to retrieve the human-derived answer. Hence, the MRR model favors human-like and detailed answers. On the other hand, the NDCG metric measures the rank of all the correct candidates based on dense annotation, which are often general and uncertain. Our ensemble approach seeks a minimal candidate set that is likely to contain the human-derived answer. The remaining candidates are ranked according to the NDCG model. MRR and NDCG. The MRR metric focuses on a single human-derived ground-truth answer. Despite preferring the more human-like answer, the metric ignores many correct candidate answers. Differently, the NDCG considers the rank of all the correct answers. The metric relies on dense annotation, where three annotators were asked to mark all the correct candidate answers. However, the candidate answers are generated plausible answers. The analysis shows that the NDCG metric favors uncertain, generally correct answers, such as "not sure" <ref type="bibr">(Murahari et al., 2020;</ref><ref type="bibr">Qi et al., 2020)</ref>.</p><p>Prior work in visual dialog focused on a single metric. Ideally, an AI agent should answer humanlike and detailed reply (the MRR metric) and be able to validate the correctness of any answer (the NDCG metric). However, crafting a model that excels in both metrics is challenging <ref type="bibr">(Murahari et al., 2020)</ref>. To this end, we propose principals to ensemble the rankings of strong MRR and NDCG models. Our approach is to find a minimal set that is likely arXiv:2104.07511v3 [cs.AI] 4 Aug 2021 to hold the human-derived answer. This permits ranking the rest of the candidates according to the NDCG model. Our approach won the recent Visual Dialog 2020 challenge and achieved strong performance on both the MRR and the NDCG metrics simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual conversation evaluation: Early attempts to marry conversation with vision used street scene images, and binary questions <ref type="bibr">(Geman et al., 2015)</ref>. While binary answers are easy to verify, such an approach is limiting for an AI agent. On the other hand, analysis of generative metrics for dialog often show no correlation with human judgements <ref type="bibr">(Liu et al., 2016)</ref>. Intuitively, metrics like BLEU-scores rely on corresponding words with the ground-truth answer and often miss synonyms or the subjective nature. More importantly, generative metrics are geared toward textual assessment rather than visual reasoning, which results in models mainly relying on textual cues <ref type="bibr">(Schwartz et al., 2019a)</ref>. <ref type="bibr">Malinowski and Fritz (2014)</ref> suggest Wu-Palmer similarity metric that calculates similarity based on the depth of two words based on the WordNet taxonomy <ref type="bibr">(Miller, 1995)</ref>. A different approach suggested in the VQA dataset focus only on brief, mostly 1-word answers <ref type="bibr" target="#b0">(Antol et al., 2015)</ref>. In this setup, the task turns into popular answers classification, alleviating many text-generation challenges. Notably, VQA requires 3 out of 10 annotators to agree on the answer, which is robust to inter-person variation. Still, accuracy ignores the reasoning process. <ref type="bibr">Hudson and Manning (2019)</ref> propose GQA, which extends the accuracy metric and uses a scene graph for both question generation and evaluation. Following, <ref type="bibr" target="#b1">Das et al. (2017)</ref> propose the VisDial dataset for the visual dialog task, which formulates multiple image-language interactions via a dialog. <ref type="bibr">Concurrently, de Vries et al. (2017)</ref> propose Guess-What, a goal-driven dialog dataset for object identification. Different from VQA and goal-driven dialogs, the VisDial answers are detailed and more human-like. For instance, in <ref type="figure">Fig. 1</ref>, the answer is "Can't tell...cloth", while a VQA answer would be "cloth". Therefore, metrics that require exact matching are no longer suitable. Instead, each question is accompanied with 100 candidate answers. Consequently, the metric has been shifted from accuracy to retrieval-based metrics, e.g., MRR and NDCG. Prior works focus on optimizing a single metric <ref type="bibr">(Guo et al., 2019;</ref><ref type="bibr">Jiang et al., 2020;</ref><ref type="bibr">Hu et al., 2017;</ref><ref type="bibr">Gan et al., 2019)</ref>. Differently, <ref type="bibr">Murahari et al. (2020)</ref> attempt to optimize both metrics with a joint loss. Still, a dedicated single metric model is superior. Instead, we propose principals to ensemble two dedicated models, one for NDCG and one for MRR. Our approach allows most of the MRR and NDCG to be preserved simultaneously. Visual dialog models: Various approaches were proposed to solve the Visual Dialog task. Most of them focus on dialog history reasoning per interaction. <ref type="bibr">Serban et al. (2017)</ref>   <ref type="bibr">(FGA)</ref>, that lets all entities (e.g., question-words, image-regions, answer-candidate, and caption-words) interact to infer an attention map for each modality. An ensemble of five FGA models achieves the state-of-the-art MRR performance. However, FGA optimizes using the sparse annotations, i.e., the human-derived answer. <ref type="bibr">Murahari et al. (2020)</ref> recently propose Large-Scale(LS) model, which pre-trains on related vision-language datasets, e.g., Conceptual Captions and Visual Question Answering <ref type="bibr">(Sharma et al., 2018;</ref><ref type="bibr" target="#b0">Antol et al., 2015)</ref>. <ref type="bibr">Concurrently, Wang et al. (2020)</ref> leverage the pretrained BERT language models, and <ref type="bibr">Nguyen et al. (2020)</ref> propose a lightweight Transformer that handles the interplay between many modalities. The three methods mentioned above finetune using the dense annotation (i.e., human assessment of all the candidates), resulting in a substantial improvement on the NDCG metric. Importantly, Murahari et al. find that finetuning a model for NDCG hurts MRR performance. This work demonstrates that re-ranking MRR model (e.g., FGA) and NDCG model (e.g., LS) with simple principles keeps most MRR and NDCG performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Two-step Rank Ensemble</head><p>The MRR metric depends on a single humanderived answer. Hence, given that this answer is ranked highly, the remaining candidates can be ranked according to the NDCG model. In the following, we describe two steps: (i) the MRR step responsible for preserving the human-derived rank high, and (ii) the NDCG step responsible for ranking the remaining candidates based on the NDCG model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We are given a set of dialog questions {(q, C q ) i } d i=1 , where d is the dataset size, q is a dialog question, and C q = {c q,j } 100 j=1 are the corresponding candidates. The MRR metric, i.e., the inverse harmonic mean of rank, is defined as:</p><formula xml:id="formula_0">MRR = 1 d d i=1 1 r i ,<label>(1)</label></formula><p>where r i is the rank of the human response for the i-th dialog question. The DCG, i.e., discounted cumulative gain over the K correct answers, is defined as:</p><formula xml:id="formula_1">DCG K = K i=1 s i log 2 (i + 1) ,<label>(2)</label></formula><p>where s i is a binary score, representing the fraction of annotators that marked the candidate at position as correct. We normalize by the ideal DCG K score (IDCG K ), i.e., NDCG K = DCG K IDCG K . We denote the set of MRR models as M = {M 1 , . . . , M nm } where n m is the number of MRR models. Each MRR model is built by altering the initial conditions. We denote the NDCG model as N . We define an operator T(M, n, q) that returns the model M 's top n responses given a question q. Next, we describe the MRR step that aims to keep the MRR score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MRR step</head><p>The purpose of the MRR step is to find a minimal candidate set C MRR,q that is likely to contain the human-derived answer given a question q. We build this set as a union of three sets, as follows:</p><formula xml:id="formula_2">C MRR,q = T q ? N q ? H q ,<label>(3)</label></formula><p>where T q is a set of first ranked candidates according to MRR models, N q is a set of high ranked candidates by both MRR and NDCG models, H q is a set of high-certainty candidates agreed by all the MRR models. All sets are conditioned by the question q. In the following, we formally define those sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">High-certainty answers</head><p>One of the most significant signals to be the humanderived answer is being a top MRR-model's answer. However, in many subjective questions, the MRR model is not certain. We found that in those cases, the top answers often varies between different MRR models. Thus, to verify the top candidate's certainty, we require an agreement of MRRmodels. Let q be a dialog question, we define the high-certainty set as follows:</p><formula xml:id="formula_3">H q = {c| (?M ? M; c ? T(M, ? h , q))},<label>(4)</label></formula><p>where ? h ? R is an hyperparameter. Intuitively, a low ? h results in higher certainty. We Next, we add the MRR-models' answer at first retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Top answers</head><p>The MRR metric prioritizes the first-ranked answer (see Eq. <ref type="formula" target="#formula_0">(1)</ref>). This property suits the nature of dialog models that reply with a single response. Consequently, we keep the first responses of the MRR models. Let q be a dialog question, the top-answers set is defined as:</p><formula xml:id="formula_4">T q = {c| (?M ? M; c ? T(M, ? t , q))},<label>(5)</label></formula><p>where ? t ? R is an hyperparameter. We note that ? t should be low to maintain candidates' certainty.</p><p>In the next step, we consider top NDCG candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">NDCG-agreement answers</head><p>When the NDCG model and the MRR model agree that a candidate is likely to be correct, it implies that both the NDCG and MRR metrics gain by ranking this candidate high. Thus, we want to rank it high. We note that the MRR set is ranked first, so we include these candidates in the MRR set. Let q be a dialog question, the ndcg-agreement set is defined as:</p><formula xml:id="formula_5">N q = {c|?M ? M; c ? T(N, ? n n , q) ? T(M, ? n m , q)},<label>(6)</label></formula><p>where ? n n , ? n m ? R are hyperparameters that indicate relevancy to NDCG and MRR, respectively. I.e., as ? nn increases, we may include more relevant candidates according to the NDCG model.</p><p>Up until this stage we have built a minimal set C MRR,q that is likely to hold the human-derived answer. In the following we describe how we rank this set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">MRR ranking</head><p>Let r M i ,c,q denote the rank according to M i ? M of candidate c for a question q. We compute the MRR rank of candidate c ? C MRR,q via geometric mean: r MRR,c,q = nm i=1 r M i ,c,q .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NDCG step</head><p>In this step, we rank the remaining candidates C NDCG,q = C q \ C MRR,q . We assume the correct MRR answer is in C MRR . Thus, we rank the remaining candidates, according to the NDCG model via geometric mean: r NDCG,c,q = (r N,c,q ) p ? r M,c,q , where M ? M is the most accurate MRR model, and p ? R is a calibration hyperparameter which controls the trade-off between MRR and NDCG.</p><p>To conclude, let q be a dialog question and C q the corresponding candidates. We first find C MRR,q , and rank the set according to r MRR,c,q . We then rank the remaining candidates, according to r NDCG,c,q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We show our results on the VisDial v1.0 dataset, where 123,287 images are used for training, 2,000 images for validation, and 8,000 images for testing <ref type="bibr" target="#b1">(Das et al., 2017)</ref>. Each image is associated with ten questions, and each question has 100 cor- responding answer candidates. We use two MRR models (i.e., n m = 2), FGA (Schwartz et al., 2019b) and an ensemble of LS (Murahari et al., 2020) with FGA. We use LS(CE) as the NDCG model. We set ? h = 3, ? t = 1, ? n n = 5, ? n m = 10, and p = 3. We tune these parameters using the validation set. Comparison to state-of-the-art: In Tab. 1 we compare our method to na?ve ensembles and previous baselines. We first ensemble the LS's output with the FGA's output. By combining them, we achieve the new MRR state-of-the-art (71.24% vs. 69.37%). Next, we build a na?ve ensemble of the MRR model and the NDCG model. We do so by adding the MRR ensemble scores (denoted by S M ) and LS(CE) scores (denoted by S N ), as follows: ? ? S M + (1 ? ?)S N , where ? ? R calibrates the trade-off between MRR and NDCG performance. We show in <ref type="figure" target="#fig_0">Fig. 2</ref> an analysis of different ? values on the validation set. In Tab. 1, we report results for ? = 0.8. Our two-step method outperforms the MRR (70.41% vs. 68.78%) and NDCG (72.16% vs. 69.22%) metrics, despite lacking the output scores and only requiring rankings.</p><p>We also compare our approach to previous baselines. Most methods use the sparse annotations, i.e., the human-derived answer, while MReal-BDAI, VD-BERT, and LS(CE) finetune using the dense annotations. Finetuning with the dense annotations tremendously boosts the NDCG performance but loses MRR performance. The MRR performance decline can be attributed to NDCG being biased toward uncertain answers. We also note that LS leverages large-scale image-text corpora. LS(CE+NSP) optimizes both the dense and sparse annotations but still suffers from a performance drop compared to metric-dedicated LS models, i.e., MRR (63.92% vs. 67.50%) and NDCG (68.08% vs. 74.47%). Unlike the method mentioned above, our method re-rank the candidates based on two distinct models, with two distinct steps, to keep the human-derived answer high. In doing so, we achieve a good MRR performance (70.41% vs. 71.24%), yet notably with limited NDCG drop (72.25% vs. 75.35%). This property comes in handy in the recent Visual Dialog challenge, where the winners were picked based on both the NDCG and MRR evaluation metrics. Our method performs well on both metrics simultaneously and won the challenge. Ablation analysis: The MRR candidate set consists of different subsets. In Tab. 2 we show the influence of each of subset independently on the retrieval metrics. Further, omitting a subset harms the performance, i.e. each component is essential to preserve both the MRR and NDCG metrics. We also report the average size of the MRR-candidate set, and the validation performance of the MRR model (i.e., 5xFGA) and the NDCG model (i.e., LS(CE)). In addition we provide the results of the MRR ensemble, and the na?ve NDCG and MRR ensemble for ? = 0.8. In <ref type="figure" target="#fig_1">Fig. 3</ref>, we examine how the NDCG and MRR metrics are affected by modifying one hyperparameter while maintaining the others. On the first figure from the left, we alter ? c . The higher ? c , we require higher agreement between the MRR models, resulting in higher certainty for elements in the MRR set. Because the MRR models are responsible for the MRR set ranking, an MRR set that is too large hurts the NDCG metric. For the same reason, in the second image from the left, increasing ? t , significantly harms the NDCG performance. In the third figure from the left, we show that considering more candidates that both NDCG and MRR models agree upon (i.e., increasing ? n n ) helps both metrics' performance. However, adding too many candidates harms the NDCG metric. In the fourth image from the left, we show that the performance remains stable when ? n m is larger than three. Last, on the fifth image from the left, we show the effect of changing p, which calibrates the trade-off between MRR and NDCG during the NDCG ranking step. Qualitative analysis: In <ref type="figure">Fig. 4</ref>  <ref type="figure">Figure 4</ref>: An illustration of two visual dialog samples. Each sample includes the MRR candidate set and four answers from the remaining NDCG candidates. We find that the MRR candidate set has more certain answers. We colorize the highcertainty candidates (H) with orange, the NDCG-agreement candidates (N ) with purple, and the top-answers subset (T ) with red. Note, if a candidate belongs to more than one set, we sketch the colors in the following order: orange?red?purple.</p><p>we provide the ranked MRR candidate set and the next 4 NDCG candidates. The analysis reveals the answers' ambiguity and that the MRR candidate set mostly consists of certain responses. In addition, we highlight the candidates within each MRR candidate subset with different colors. Additional samples can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We describe a non-parametric method to ensemble the candidate ranks of two strong MRR and NDCG models into a single ranking that excels on both NDCG and MRR. Intuitively, we use the MRRmodel for non-ambiguous questions with certain answers. The dense-annotations cue is more applicable in ambiguous questions than the sparse annotations. Thus, in the case of low certainty, our method relies almost entirely on the NDCG model. We hope the proposed principles can guide the community towards a parametric model that can employ answers' semantics to measure certainty. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Performance of a na?ve score ensemble of the MRR model and the NDCG model on the VisDialv1.0 val set. We calibrate the importance of each model with a scalar ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>MRR and NDCG scores for different hyperparameter values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>MRR candidates set ablation analysis. Performance reported on the VisDialv1.0 val set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, we show two sample visual dialogs from test-std. For each sample,</figDesc><table><row><cell></cell><cell>MRR candidate set</cell><cell>Top 4 from the</cell></row><row><cell></cell><cell>1. she 's probably 60</cell><cell>remaining NDCG</cell></row><row><cell></cell><cell>2. middle aged</cell><cell>candidates:</cell></row><row><cell></cell><cell>3. late UNK 's</cell><cell>1. ca n't tell</cell></row><row><cell></cell><cell>4. i can not tell</cell><cell>2. looks middle aged</cell></row><row><cell>how old is the woman?</cell><cell>5. unable to tell</cell><cell>3. mid sixties 4. i ca n't tell</cell></row><row><cell></cell><cell>MRR candidate set</cell><cell>Top 4 from the</cell></row><row><cell></cell><cell>1. yes</cell><cell>remaining NDCG</cell></row><row><cell></cell><cell>2. metal and wood -looks</cell><cell>candidates:</cell></row><row><cell></cell><cell>like a zoo enclosure</cell><cell>1. yes it is ,</cell></row><row><cell></cell><cell>3. yes it is</cell><cell>2. i think so</cell></row><row><cell>is the cage made of metal ?</cell><cell>4. it appears to be 5. yes , it is</cell><cell>3. yep 4. wood</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Vishvak Murahari, Dhruv Batra, Devi Parikh, and Abhishek Das. 2020. Large-scale pretraining for visual dialog: A simple state-of-the-art baseline. ECCV. Van-Quang Nguyen, Masanori Suganuma, and Takayuki Okatani. 2020. Efficient attention mechanism for handling all the interactions between many inputs with application to visual dialog. ECCV. Zhiwu Lu, and Ji-Rong Wen. 2019. Recursive visual attention in visual dialog. In CVPR. the MRR candidate set mostly consists of the human-derived response. To further detail our approach, we illustrate the candidates within each MRR candidate subset by highlighting the candidates with different colors. We colorize the high-certainty candidates</figDesc><table><row><cell>9. i do n't think so 1. yes 1. no 1. not really 6. 1 looks old while the other looks mid 30s Top 10 from the remaining NDCG candidates 1. the carrots and what looks like maybe corn 1. white 3. yes but it is not on 3. at least 4 Question:what is made out of wood ? 3. yes there is</cell><cell>4. it is in color 1. yes 2. lots 10. early 30s maybe 1. looks like it Question:where are the onions ? 5. no there 's not 9. yes there are 1. no Question:does the field have a lot of grass ? 2. 3 3. no people at all 7. i think so Question:what does the sign say ? 1. no 5. kinda</cell></row><row><cell>Zhe Gan, Yu Cheng, Ahmed EI Kholy, Linjie Li, Jingjing Liu, and Jianfeng Gao. 2019. Multi-step reasoning via recurrent dual attention for visual dia-log. ACL. Donald Geman, Stuart Geman, Neil Hallonquist, and Laurent Younes. 2015. Visual turing test for com-puter vision systems. NAS. Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation met-rics for dialogue response generation. arXiv preprint arXiv:1603.08023. (H) with orange, the NDCG-agreement candidates (N ) with purple, and the top-answers subset (T ) with red. Note, if a candidate belongs to more than one set, we sketch the colors in the following order: orange?red?purple. Question:what color is the light ? MRR candidate set 10. nope Question:are they all facing the camera ? MRR candidate set 1. yes 2. no 3. slightly 4. yes they are 5. nope Top 10 from the remaining NDCG candidates 1. not really 2. i don ' t think so 3. yes they all are 4. yes i believe so 5. no side view 6. i do n't think so 7. i think so 8. all but 1 9. not all of them Question:how old is the man ? MRR candidate set 1. i can not tell 2. can not tell 3. unable to tell 4. ca n't see his face 5. i ca n't tell 6. ca n't tell Top 10 from the remaining NDCG candidates 1. i ca n't tell i ca n't see their face 2. twenties maybe , hard to tell 3. hard to tell but i would say mid 20 's 4. hard to say but i guess around thirty 5. not sure 6. i can not see their face 7. maybe 25ish 8. he looks to be about 30ish 9. it 's hard to tell , it 's from a distance Question:is there anything on the wallpaper of the laptop ? MRR candidate set 1. no 2. nope 3. not that i can see Top 10 from the remaining NDCG candidates 1. no there is not 2. i can not tell 3. i do n't see any 4. ca n't tell 5. i ca n't tell 6. 0 7. i do n't think so 8. just a screen over it 9. not that i can UNK for sure Question:is this inside ? MRR candidate set 1. yes 2. yes it is 3. yes , it is 4. yes i think so Top 10 from the remaining NDCG candidates 1. yes , it seems to be 2. i think so , yes 3. this is inside 4. it seems to be 5. it is indoors 6. i believe so 7. i think so 8. looks like it 9. no Question:is the image in color ? MRR candidate set 1. yes 2. yes it is 3. yes , it is in color 4. yes it is in color 5. yes , it 's in color 6. yes , the photo is in color Top 10 from the remaining NDCG candidates 1. yes , it is 2. yes the photo is in color 3. yes , full color photo 4. yes it 's color 5. it is 6. yes in color 7. it is yes 8. yess 9. yea 10. yyes Question:is it daytime ? 2. yes it is 3. yes , it is in color 4. yes it is in color 5. yes it 's in color Top 10 from the remaining NDCG candidates 1. yes , it is 2. yes the picture is in color 3. yeah , this photo is in color 4. yes in color 5. it is 6. it is in color 7. yup 8. it is a beautiful color picture 9. ye 10. y Question:any buildings visible ? MRR candidate set 1. yes a few 2. yes 3. yes , a few buildings 4. yess 5. i think so Top 10 from the remaining NDCG candidates Question:how old is the woman ? MRR candidate set 1. she looks like in her 30 's 2. in her twenties i believe 3. not sure Top 10 from the remaining NDCG candidates 1. 20 's 2. early 20 's 3. i ca n't tell 4. ca n't tell 5. 20 's maybe 6. looks to be late 20s 7. i can not tell 8. young adult Question:is anyone in the bathroom ? MRR candidate set 1. no 2. no the bathroom 3. not that i can see 4. no people 5. nope 6. i do n't think so Top 10 from the remaining NDCG candidates 1. no it appears empty 2. not it 's not visible 3. no he 's not 4. i would say no 5. no , there is no 1 in the kitchen 6. UNK 7. don ' t know 8. 0 9. not really 2. yes 3. not that i can see 4. i do n't see any 5. nope Top 10 from the remaining NDCG candidates 1. i ca n't see any , no 2. no people 3. no people are visible 4. no , there are n't any people in the image 5. no there are n't 6. no people in the photo 7. 0 i can see 8. not that you can see 9. i ca n't see 10. on the screens there are Question:do they have shoes on ? MRR candidate set 1. yes 2. i can not tell 3. i see 1 shoe the other person is to blurry to tell 4. it looks like it 5. it 's hard to tell 6. i ca n't tell 2. i ca n't tell 3. no i ca n't 4. i do n't think so but it 's hard to tell 5. not that i can see 6. not sure 7. no , there is n't 8. maybe , i ca n't really tell 9. possibly 10. not in the picture Question:is the picture in color ? MRR candidate set 7. very old 8. they look fairly big 9. no 10. i ca n't tell , but judging by the UNK , proba-bly Question:is it a metal fence ? MRR candidate set 1. yes 2. wire 3. yes it is 4. it appears to be 5. yes , it is Top 10 from the remaining NDCG candidates 1. it is 2. yes , it looks like it is 3. it looks like it 4. looks like it is 5. i think so it 6. it has some lines , but yes 7. i think so 8. yes they are 9. yes , i can see a fence Question:can you see clouds ? MRR candidate set 1. i see no clouds the sky is fairly grey though 2. no clouds are visible 3. yes i can see the sky cloudy 4. no 5. no , it looks cloudy Top 10 from the remaining NDCG candidates 1. no , 2. there seems to be a few clouds 3. nope 4. yes they are clouds 5. very few 6. not that i can see 7. not really 8. i do n't think so 9. yes i can 10. 0 1. black 2. yes and black 3. no 4. no , her eyes are brown 5. nope 6. it looks to be maybe of shoulder length 7. ca n't tell 8. i ca n't tell 9. yes i do 10. i can not tell Question:are there other people ? MRR candidate set 1. there is a gentleman beside her 2. no 3. yes , there are 2 other people in the back-ground 4. no , there is not 5. not that i can see 6. no there are not 7. nope Top 10 from the remaining NDCG candidates 1. no other people in the photo 2. 0 that i can see in the picture 3. no , there are no other people com Question:is the yard large ? MRR candidate set 1. i can not tell 2. yes 3. i ca n't tell 4. ca n't tell 5. can not tell too close up 6. not sure Top 10 from the remaining NDCG candidates 1. from what i can see 2. seems like it very close up photo 3. i think so 4. maybe 5. no 6. no , but hard to tell from the close up 7. yes it is 8. yes , it is 9. not really Question:are there any lights ? MRR candidate set 1. no 2. i think it 's light coming from a camera he is holding 3. yes 4. not that i can see 5. 0 that i can see 6. nope Top 10 from the remaining NDCG candidates 1. not seen 2. i can not see them 3. i do n't think so 4. yes , well lit 5. i ca n't tell 6. 0 at all 7. i think so Question:are there any bath mats ? MRR candidate set 1. no 2. yes there is a bath mat 3. i can not tell 4. nope 5. no , it does n't appear so Top 10 from the remaining NDCG candidates 1. yes 2. yes there are 3. no , 4. yes there is 1 5. there are 6. not that i can see 7. not that you can see 8. just 1 9. i do n't think so 10. 0 that i can see Question:can you see the sky ? are about equal 2. i can not tell 3. i ca n't tell 4. i ca n't see the image 5. not sure Top 10 from the remaining NDCG candidates 1. meat 2. ca n't tell 3. it looks quite fresh 4. it looks like it might be apple 5. it appears to be people but i do n't have the best view 6. looks brown or black spots on cream 7. light colored 8. a little bit 9. hard to tell , tomato sandwich 10. just the ocean 2. there is a blue sign and a white with red writ-4. yes , 1 4. ca n't see the bottom part MRR candidate set 4. yes , there is 1. wood ing sign 5. yeds 5. 2 2. wood maybe 5. yep 3. blue 4. black and white 6. nope 7. not seen 6. at least 5 7. 4 3. i can not tell Top 10 from the remaining NDCG candidates 4. cabinets , stove 1. UNK 5. white and black Top 10 from the remaining NDCG candidates 8. no , there is n't 9. the top of the desk is not visible 8. 5 that i can see 9. i see 4 5. ca n't tell 2. i think so 6. hard to tell 3. yes , it is 1. white with writing 2. it is white 3. white , black and red 4. the sign is black 10. no there is n't 10. there are only 2 Question:are there any farmers near them ? MRR candidate set 7. i ca n't tell 4. yes it is 8. not sure 5. yes behind 1. there are no people visible 2. no , there are n't any people in the image Top 10 from the remaining NDCG candidates 6. yes there are 2 1. looks like metal with wood slats 7. yes , several of them Question:if you were to guess , what building is Question:is there any cream or sugar near the mug 5. yes , it is 6. red and white 7. they are white with black around the edges 8. 1 yellow and 1 white 9. yes it is 10. UNK 3. no , there is not 2. looks like a table 8. yes it 's black ? the image set in ? MRR candidate set MRR candidate set 4. no there are not 5. no there are n't 3. table 4. the walls 9. yes there is a rug 1. i can not tell 1. no , there is a spoon next to the mug on the plate however 10. yes on a hamper 6. nope 5. ca n't really tell but i would assume so 2. i can not see it from here 2. yes , a small scoop of yellowish ice cream on Top 10 from the remaining NDCG candidates 6. yes 3. i ca n't tell top of the pastry 1. no 2. not that i can see 7. yes , it is 4. ca n't tell 3. no there is not MRR candidate set 1. no people are visible 5. light greenish yellow Question:are there laying on the floor ? 3. yes a few of them 9. no , you can not 9. mug Question:how old does she look ? Question:are there people ? MRR candidate set 1. no 2. no people 3. nope 4. no there are no people 5. no there are n't Top 10 from the remaining NDCG candidates Question:is the grass green ? MRR candidate set 1. yes 2. yes , some is 3. some of it yes 4. yes it is 5. yes , it is 1. there is , yes 2. some 3. yes , i think so 4. some parts of it 2. some have leaves 8. it is hard to tell , i can only see the table cloth 8. 0 4. yes they do 1. yes , they are 7. i can not tell rant is Top 10 from the remaining NDCG candidates 7. i can not tell by the picture how nice the restau-6. yes there is 5. 'yes 6. in a parking lot somewhere 5. yes Top 10 from the remaining NDCG candidates 3. yes , very 5. this was not taken by a professional 4. i do n't think so 3. there are no people in the image 4. no there isn ' t 5. no , there are no people 6. i do n't see any 7. no there 's not 8. i think so 4. no that i can see 5. not sure 9. i ca n't tell but i am guessing a large bin 10. it looks to be scissors and a beer can box Question:do the trees have leaves ? MRR candidate set 5. not that i can see Top 10 from the remaining NDCG candidates 1. it is in a home Top 10 from the remaining NDCG candidates 2. maybe an office or a school 1. no 8. there are no people 9. no people 10. 0 i can see 1. yes 2. yes a lot 2. nope 3. can not tell since it is outside 4. it looks like a restaurant 3. not visible in the picture , no</cell><cell>Yulei Niu, Hanwang Zhang, Manli Zhang, Jianhong Zhang, Jiaxin Qi, Yulei Niu, Jianqiang Huang, and Hanwang Zhang. 2020. Two causal principles for improving visual dialog. CVPR. Idan Schwartz, Alexander G Schwing, and Tamir Hazan. 2019a. A simple baseline for audio-visual scene-aware dialog. In CVPR. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for au-tomatic image captioning. In ACL. Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron C Courville. 2017. Guesswhat?! visual object discovery through multi-modal dialogue. In CVPR. Yue Wang, Shafiq Joty, Michael R Lyu, Irwin King, Caiming Xiong, and Steven CH Hoi. 2020. Vd-bert: A unified vision and dialog transformer with bert. EMNLP. Tianhao Yang, Zheng-Jun Zha, and Hanwang Zhang. 2019. Making history matter: History-advantage se-quence training for visual dialog. In ICCV. Zilong Zheng, Wenguan Wang, Siyuan Qi, and Song-Chun Zhu. 2019. Reasoning visual dialogs with structural and partial observations. In CVPR. 5. it 's in color 6. yes , it 's very colorful ! 7. i think so 8. looks like it 9. it is , but there is not much color in the photo due to the objects 10. yes it is well lit Question:what are the vegetables he has ? MRR candidate set 1. salmon and greens , maybe spinach 2. it looks like greens , hard to tell 3. ca n't really tell 4. i can not tell 5. i ca n't tell 6. ca n't tell Top 10 from the remaining NDCG candidates 1. not sure 2. i 'm not a 100 % sure , look like it 3. yes 4. not that i can see 5. can not see grass 6. green 7. plate Question:is it a poodle ? MRR candidate set 1. no 2. i do n't think so 3. nope Top 10 from the remaining NDCG candidates 1. does not look like it 2. i ca n't tell 3. i can not tell 4. not sure 5. i can not say 6. ca n't tell 7. not that i can see 8. i do n't think so , but it 's hard to tell 9. i 'm not sure what that is Question:is there anything near it ? MRR candidate set 1. no 2. grass and tree 's 3. nothing 4. not that i can see 5. nope 6. ca n't see anything Top 10 from the remaining NDCG candidates 1. yes 2. i do n't think so 3. i don ' t see any 4. not really 5. you can see a pitcher and maybe a wall 6. a few things 7. no , it 's just dirt 8. in the distance 9. no but it has a lot of stuff on it 10. not sure there may be , but it does n't show Question:is it date stamped ? MRR candidate set 1. no 2. no it 's not 3. no it is n't 4. nope 5. no , it 's not Top 10 from the remaining NDCG candidates 1. not that i can see 2. i do n't think so 3. i do n't know 4. do n't know 5. i ca n't tell 6. ca n't tell 7. not sure 8. i can not tell 9. not readable Question:does the food look appetizing ? MRR candidate set 1. yes 2. no 3. not really 4. not at all 5. nope Top 10 from the remaining NDCG candidates 1. yes it does 2. no , he does n't 3. yes , it does 4. i can not tell 5. ca n't tell 6. a little 7. not that i can see 8. sort of 9. kinda 2. yes it is 3. yes , it is daytime 4. yes it is daytime 5. yes , it is Top 10 from the remaining NDCG candidates 1. it is 2. yes , it looks to be 3. it is daytime 4. yup 5. it is day time 6. yes , i think so 7. i believe so 8. yes it is , it looks sunny 9. daytime 10. i think so Question:is the photo in color ? 3. in the distance 4. yes there are some small ones 5. yes , 1 across the street 6. yes in the far background 7. yes , but i ca n't tell what kind 8. yes , it is 9. very far in the background 10. no Question:is there anything made out of plastic ? MRR candidate set 1. no 2. yes 3. nope 4. not that i can see 5. i do n't think so Top 10 from the remaining NDCG candidates 1. ca n't tell 2. no that i can see 3. not sure 4. ca n't see 5. i ca n't tell 6. i can not tell 7. maybe 8. i think so 9. not really Question:is the image in color ? MRR candidate set 1. yes 2. yes it is 3. yes , it is 4. yes , it is in color 5. yes it is in color Top 10 from the remaining NDCG candidates 1. yes this is a color image 2. yes , it 's in color 3. it is yes 4. yes this picture is in color 5. yes the photo is in color 6. yes in color 7. yep 8. it is in color 9. yes it 's color Question:is he wearing a hat ? MRR candidate set 1. no 2. no he is not 3. nope 4. i do n't see 1 Top 10 from the remaining NDCG candidates 1. i do n't think so 2. no , no hats 3. no baseball hat 4. not that i can see 5. no ca n't see it 6. no he is not wearing a helmet 7. ca n't tell 8. i ca n't tell 9. can not tell 10. not really Question:are there any people ? 2. i guess so , it is hard to tell 3. yes i think so , you ca n't really see 4. ca n't tell 5. i think so 6. they are 7. not sure 8. no , they do not 9. ca n't see 10. yes they are green Question:does it look like a ski resort ? MRR candidate set 1. no 2. i can not tell 3. i do n't think so 4. no it does n't 5. probably not 6. nope Question:are there pictures on the walls ? MRR candidate set 1. yes 2. no 3. yes there are 4. yes for sure Top 10 from the remaining NDCG candidates 1. i think so 2. yes , 1 that i can see 3. not that i can see 4. nope 5. no pictures on the wall 6. yeah , 3 7. 1 8. might be but it is hard to see 9. no , i do n't see any MRR candidate set 1. on the right of the plate 2. you can not really tell 3. i can not tell 4. not sure 5. i ca n't tell Top 10 from the remaining NDCG candidates 1. yes 2. ca n't tell 3. UNK 4. sitting at the table 5. it looks like a box of them and 2 sitting out 6. there is squash , shredded carrots , and shred-ded cabbage 7. not that i can see 8. the UNK of heaven 9. yes , it is 10. i think os Question:does she have brown hair ? MRR candidate set 1. yes 2. yes , she does 3. yes , it is 4. i believe so 5. i think so 6. not from what i can see 7. no , i ca n't see anyone 8. not within view 9. yes 2 people can be seen in the background 10. i do n't think so Question:how many zebras are there ? MRR candidate set 1. 9 2. i count twelve 3. 10 4. 7 5. 8 or so 6. too many to count Top 10 from the remaining NDCG candidates 1. about ten , of different species 2. eleven 3. at least 20 4. 11 or 12 5. looks like around a dozen 6. a lot 7. over 20 8. about 12-14 9. about 25 Question:does the picture match the caption ? MRR candidate set 1. yes 2. no 3. yes , it does 4. yes it does match the caption 5. yes it does 6. no it is not Top 10 from the remaining NDCG candidates 1. nope 2. no , 3. not really 4. it does 5. yes , the picture is in color 6. yes , it 's in color 7. it is 8. yes , it is 9. yes it is 10. 0 Question:how many kids are there ? MRR candidate set 1. 2 2. there are 2 people but they are n't kids 3. i think 2 4. only 2 5. there is only 1 Top 10 from the remaining NDCG candidates 1. 2 , possibly 3 2. 1 3. 3 4. looks like 3 5. i can see 2 clearly and there may be part of a third but i 'm not sure 6. there is 1 7. only see 1 , too close up to see much 8. 3 total 1 looks like the guide 9. there are tow 2. no i can not 3. no i ca n't 4. no , i can not 5. nope Top 10 from the remaining NDCG candidates 1. no you ca n't 2. no i ca n't see the sky 3. no i do not 4. no , the sky is not visible 5. not really 6. no i can UNK 7. just a little bit 8. n 9. i do n't think so 10. a little bit Question:is there a chair ? MRR candidate set 1. no , i can only see the close up of what ' s on the desk 2. no 3. not that i can see MRR candidate set 1. no 2. it is in a parking lot 3. no , it does n't 4. nope 5. not that i can see Top 10 from the remaining NDCG candidates 1. that part is not in the photo 2. no not in view 3. not really 4. not especially 5. 0 6. ca n't tell 7. i do n't think so 8. i can not tell 9. i ca n't tell 3. about 9 , i am not sure 4. i do n't see any 8. green MRR candidate set 1. welcome to UNK 2. yes 6. yes , they look fine 4. maybe 2 at oldest 5. 0 that i can see 9. yes , they are 2. UNK no UNK 3. nope 7. yes , it is 5. maybe under 1 year old 6. maybe 5 6. 0 7. not that can be seen 3. it just says UNK dame with an arrow pointing 4. no there are n't 8. yes , a few 10. yes light brown left Top 10 from the remaining NDCG candidates 9. green Top 10 from the remaining NDCG candidates 1. maybe 7 8. nobody in the photo 9. no , this is a looking up picture 4. words 1. 1 is 10. yes it is 5. UNK UNK 2. no there is not 2. 4 3. 1 4. maybe 7-10 5. looks like a preteen girl 10. i do n't think so 6. s gay st 3. i do n't think so Question:what is on the sign ? Question:can you see his opponent ? Top 10 from the remaining NDCG candidates 4. not that i see MRR candidate set MRR candidate set 1. join our csa 5. you can 't see the floor 1. no 2. beautiful code : leading UNK explain how they think 6. 1 is the other is sitting on top of a container 1. letters 2. no i can not 6. 7-9 7. 5 8. not sure 9. 7 or 8 10. around 8 Question:anything else interesting about the photo ? MRR candidate set 1. the colors are very vibrant 2. no 3. the UNK of heaven 7. not that i can see 2. 1 says stop and the other says danger 3. no , i can not 4. the original tour 5. twin peaks with 200 sign on top 8. ys 9. i think so 4. nope 3. pictures Top 10 from the remaining NDCG candidates 6. number 9 10. not really Question:is the chocolate donut frosted ? 4. i do n't understand the question 1. no , not at all 7. not sure MRR candidate set 2. not in my view 8. UNK 5. a flickr UNK 1. yes 3. no just 1 person 10. grass Question:what color is the sign ? 1. yes , it is 1. 3 9. not visibly , but i 'd say probably so 1. yes 9. not completely sure , does n't look like it 9. it 's a dessert Question:is there a computer on the desk ? MRR candidate set 1. no 2. yes 3. i think so 4. looks like it Top 10 from the remaining NDCG candidates Question:how many stories is it ? MRR candidate set 1. ca n't tell 2. i can not tell 3. ca n't tell because the photo is cut but at least 3 4. i ca n't tell 5. maybe 3 6. not sure Top 10 from the remaining NDCG candidates 5. looks great 6. just the sea doo 7. 0 8. no there are n't MRR candidate set 8. i do n't think so Question:is there a clock ? 8. yes , i see like 6 billboard signs 7. yes , they are in both 6. i think so 7. a few penguins out in penguin element 5. i think it 6. a track and a little gate building 3. not really 4. nope Top 10 from the remaining NDCG candidates 1. yes 2. not that i can see 3. no , there are n't 4. yes , there 's some white frosting 5. not sure 4. i do n't think so 9. no 2. no Top 10 from the remaining NDCG candidates 4. i do n't think so 10. alaska 3. yes , it is 1. look like a game 5. not that i can see 6. i 'm not sure 4. yes it is 2. land in the background 7. no , only people Top 10 from the remaining NDCG candidates 8. not really 3. some website not sure 1. it is 9. there is no scoreboard 2. nope 3. looks like it 4. UNK 10. sure why not</cell></row><row><cell>10. not that i can see 10. he looks to be maybe 20 or so 10. not sure 10. ues MRR candidate set 1. yeah in the distance 9. 25 10. no people this is the sky Top 10 from the remaining NDCG candidates 10. yes i think so , hard to tell 4. 0 i can see 10. yes it 's large 8. i can not tell MRR candidate set 1. 2 2. not that i can see 6. lots of grass but yellow MRR candidate set 4. i think so 10. he is n't in a kitchen 10. not really</cell><cell>8. looks old 10. no i am unable to tell enough to tell 10. nah 10. i ca n't tell MRR candidate set 10. yes , looks like a box 10. ye MRR candidate set Top 10 from the remaining NDCG candidates 10. yes , but not too many 6. oh yeah 10. i see 6 i think 10. i think so 10. i see 1 arm besides them MRR candidate set 2. not that i can see 2. i think 4 10. yes , it is 2. no 10. not that i can tell 10. large rocks on the side of the road</cell></row></table><note>Dalu Guo, Chang Xu, and Dacheng Tao. 2019. Image- question-answer synergistic network for visual dia- log. In CVPR. Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. 2017. Learning to reason: End-to-end module networks for visual question answering. ICCV. Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR. Unnat Jain, Svetlana Lazebnik, and Alexander Schwing. 2019. Two can play this game: Visual dialog with discriminative question generation and answering. CVPR. Xiaoze Jiang, Jing Yu, Zengchang Qin, Yingying Zhuang, Xingxing Zhang, Yue Hu, and Qi Wu. 2020. Dualvd: An adaptive dual encoding model for deep visual understanding in visual dialogue. AAAI. Gi-Cheon Kang, Jaeseo Lim, and Byoung-Tak Zhang. 2019. Dual attention networks for visual reference resolution in visual dialog. ACL. Satwik Kottur, Jos? MF Moura, Devi Parikh, Dhruv Ba- tra, and Marcus Rohrbach. 2018. Visual coreference resolution in visual dialog using neural module net- works. In ECCV.Mateusz Malinowski and Mario Fritz. 2014. A multi- world approach to question answering about real- world scenes based on uncertain input. In NIPS. George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM.Idan Schwartz, Seunghak Yu, Tamir Hazan, and Alexander G Schwing. 2019b. Factor graph atten- tion. In CVPR. Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, and Leonid Sigal. 2017. Visual reference resolution using attention memory for visual dialog. In NIPS. Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C Courville, and Yoshua Bengio. 2017. A hierarchical latent variable encoder-decoder model for generating dia- logues. In AAAI.A Qualitative Analysis In the following, we show 200 randomly picked visual dialog samples from test-std. For each sam- ple, we provide the ranked MRR candidate set and the next 10 NDCG candidates. The analysis re- veals the answers' ambiguity and thatQuestion:are any beverages nearby ? MRR candidate set</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Yftah Ziser, Itai Gat, Alexander Schwing and Tamir Hazan for useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual dialog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Question:is this in color ? MRR candidate set</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
