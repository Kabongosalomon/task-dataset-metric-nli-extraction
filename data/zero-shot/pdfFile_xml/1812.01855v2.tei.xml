<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explainable and Explicit Visual Reasoning over Scene Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
							<email>lijuanzi@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Explainable and Explicit Visual Reasoning over Scene Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim to dismantle the prevalent black-box neural architectures used in complex visual reasoning tasks, into the proposed eXplainable and eXplicit Neural Modules (XNMs), which advance beyond existing neural module networks towards using scene graphs -objects as nodes and the pairwise relationships as edges -for explainable and explicit reasoning with structured knowledge. XNMs allow us to pay more attention to teach machines how to "think", regardless of what they "look". As we will show in the paper, by using scene graphs as an inductive bias, 1) we can design XNMs in a concise and flexible fashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce the number of parameters by 10 to 100 times, and 2) we can explicitly trace the reasoning-flow in terms of graph attentions. XNMs are so generic that they support a wide range of scene graph implementations with various qualities. For example, when the graphs are detected perfectly, XNMs achieve 100% accuracy on both CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound for visual reasoning; when the graphs are noisily detected from real-world images, XNMs are still robust to achieve a competitive 67.5% accuracy on VQAv2.0, surpassing the popular bag-of-objects attention models without graph structures. * The work was done when Jiaxin Shi was an intern at Nanyang Technological University.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The prosperity of A.I. -mastering super-human skills in game playing <ref type="bibr" target="#b22">[23]</ref>, speech recognition <ref type="bibr" target="#b0">[1]</ref>, and image recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref> -is mainly attributed to the "winning streak" of connectionism, more specifically, the deep neural networks <ref type="bibr" target="#b15">[16]</ref>, over the "old-school" symbolism, where their controversy can be dated back to the birth of A.I. in 1950s <ref type="bibr" target="#b18">[19]</ref>. With massive training data and powerful computing resources, the key advantage of deep neural networks is the end-to-end design that generalizes to a large spec- trum of domains, minimizing the human efforts in domainspecific knowledge engineering. However, large gaps between human and machines can be still observed in "highlevel" vision-language tasks such as visual Q&amp;A <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>, which inherently requires composite reasoning (cf. <ref type="figure" target="#fig_0">Figure 1</ref>). In particular, recent studies show that the end-to-end models are easily optimized to learn the dataset "shortcut bias" but not reasoning <ref type="bibr" target="#b11">[12]</ref>.</p><p>Neural module networks (NMNs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27]</ref> show a promising direction in conferring reasoning ability for the end-to-end design by learning to compose the networks on-demand from the language counterpart, which implies the logical compositions. Take the question "How many objects are left of the red cube?" as an example, we can program the reasoning path into a composition of functional modules <ref type="bibr" target="#b17">[18]</ref>: Attend[cube], Attend[red], Relate[left], and Count, and then execute them with the input image. We attribute the success of NMNs to the eXplainable and eXplicit (dubbed X) language understanding. By explicitly parsing the question into an explainable module assembly, NMNs effectively prevent the languageto-reasoning shortcut, which are frequent when using the implicit fused question representations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> (e.g., the answer can be directly inferred according to certain language patterns).</p><p>However, the vision-to-reasoning shortcut still exists as an obstacle on the way of NMNs towards the real X visual reasoning. This is mainly because that the visual perception counterpart is still attached to reasoning <ref type="bibr" target="#b17">[18]</ref>, which is inevitably biased to certain vision patterns. For example, on the CLEVR CoGenT task, which provides novel object attributes to test the model's generalization ability (e.g., cubes are blue in the training set but red in the test set), we observe significant performance drop of existing NMNs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref> (e.g., red cubes in the test set cannot be recognized as "cube"). Besides, the reusability of the current module design is limited. For example, the network structure of the Relate module in <ref type="bibr" target="#b17">[18]</ref> must be carefully designed using a series of dilated convolutions to achieve good performance. Therefore, how to design a complete inventory of X modules is still an tricky engineering.</p><p>In this paper, we advance NMN towards X visual reasoning by using the proposed eXplainable and eXplicit Neural Modules (XNMs) reasoning over scene graphs. By doing this, we can insulate the "low-level" visual perception from the modules, and thus can prevent reasoning shortcut of both language and vision counterpart. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, a scene graph is the knowledge representation of a visual input, where the nodes are the entities (e.g., cylinder, horse) and the edges are the relationships between entities (e.g., left, ride). In particular, we note that scene graph detection per se is still a challenging task in computer vision <ref type="bibr" target="#b28">[29]</ref>, therefore, we allow XNMs to accept scene graphs with different detection qualities. For example, the left-hand side of <ref type="figure" target="#fig_0">Figure 1</ref> is one extreme when the visual scene is clean and closed-vocabulary, e.g., in CLEVR <ref type="bibr" target="#b11">[12]</ref>, we can have almost perfect scene graphs where the nodes and edges can be represented by one-hot class labels; the right-hand side shows another extreme when the scene is cluttered and open-vocabulary in practice, the best we have might be merely a set of object proposals. Then, the nodes are RoI features and the edges are their concatenations.</p><p>Thanks to scene graphs, our XNMs only have 4 metatypes: 1) AttendNode, finding the queried entities, 2) AttendEdge, finding the queried relationships, 3) Transfer, transforming the node attentions along the attentive edges, and 4) Logic, performing basic logical operations on attention maps. All types are fully X as their outputs are pure graph attentions that are easily traceable and visible. Moreover, these meta modules are only specific to the generic graph structures, and are highly reusable to constitute different composite modules for more complex functions. For example, we do not need to carefully design the internal implementation details for the module Relate as in <ref type="bibr" target="#b17">[18]</ref>; instead, we only need to combine AttendEdge and Transfer in XNMs.</p><p>We conduct extensive experiments 1 on two visual Q&amp;A benchmarks and demonstrate the following advantages of using XNMs reasoning over scene graphs: 1. We achieve 100% accuracy by using the ground-truth scene graphs and programs on both CLEVR <ref type="bibr" target="#b11">[12]</ref> and CLEVR-CoGent, revealing the performance upperbound of XNMs, and the benefits of disentangling "high-level" reasoning from "low-level" perception. 2. Our network requires significantly less parameters while achieves better performance than previous state-of-theart neural module networks, due to the conciseness and high-reusability of XNMs. 3. XNMs are flexible to different graph qualities, e.g., it achieves competitive accuracy on VQAv2.0 <ref type="bibr" target="#b5">[6]</ref> when scene graphs are noisily detected. 4. We show qualitative results to demonstrate that our XNMs reasoning is highly explainable and explicit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Reasoning. It is the process of analyzing visual information and solving problems based on it. The most representative benchmark of visual reasoning is CLEVR <ref type="bibr" target="#b11">[12]</ref>, a diagnostic visual Q&amp;A dataset for compositional language and elementary visual reasoning. The majority of existing methods on CLEVR can be categorized into two families: 1) holistic approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b10">11]</ref>, which embed both the image and question into a feature space and infer the answer by feature fusion; 2) neural module approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27]</ref>, which first parse the question into a program assembly of neural modules, and then execute the modules over the image features for visual reasoning. Our XNM belongs to the second one but replaces the visual feature input with scene graphs.</p><p>Neural Module Networks. They dismantle a complex question into several sub-tasks, which are easier to answer and more transparent to follow the intermediate outputs.</p><p>Modules are pre-defined neural networks that implement the corresponding functions of sub-tasks, and then are assembled into a layout dynamically, usually by a sequenceto-sequence program generator given the input question. The assembled program is finally executed for answer prediction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. In particular, the program generator is trained based on the human annotations of desired layout or with the help of reinforcement learning due to the nondifferentiability of layout selection. Recently, Hu et al. <ref type="bibr" target="#b8">[9]</ref> proposed StackNMN, which replaces the hard-layout with soft and continuous module layout and performs well even without layout annotations at all. Our XNM experiments on VQAv2.0 follows their soft-program generator.</p><p>Recently, NS-VQA <ref type="bibr" target="#b26">[27]</ref> firstly built the reasoning over the object-level structural scene representation, improving the accuracy on CLEVR from the previous state-of-the-art 99.1% <ref type="bibr" target="#b17">[18]</ref> to an almost perfect 99.8%. Their scene structure consists of objects with detected labels, but lacked the relationships between objects, which limited its application on real-world datasets such as VQAv2.0 <ref type="bibr" target="#b5">[6]</ref>. In this paper, we propose a much more generic framework for visual reasoning over scene graphs, including object nodes and relationship edges represented by either labels or visual features. Our scene graph is more flexible and more powerful than the table structure of NS-VQA. Scene Graphs. This task is to produce graph representations of images in terms of objects and their relationships. Scene graphs have been shown effective in boosting several vision-language tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5]</ref>. To the best of our knowledge, we are the first to design neural module networks that can reason over scene graphs. However, scene graph detection is far from satisfactory compared to object detection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17]</ref>. To this end, our scene graph implementation also supports cluttered and open-vocabulary in real-world scene graph detection, where the nodes are merely RoI features and the edges are their concatenations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We build our neural module network over scene graphs to tackle the visual reasoning challenge. As shown in <ref type="figure">Figure 2</ref>, given an input image and a question, we first parse the image into a scene graph and parse the question into a module program, and then execute the program over the scene graph. In this paper, we propose a set of generic base modules that can conduct reasoning over scene graphs -eXplainable and eXplicit Neural Modules (XNMs)as the reasoning building blocks. We can easily assemble these XNMs to form more complex modules under specific scenarios. Besides, our XNMs are totally attention-based, making all the intermediate reasoning steps transparent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scene Graph Representations</head><p>We formulate the scene graph of an image as (V, E), where V = {v 1 , ? ? ? , v N } are graph nodes corresponding to N detected objects, and v i denotes the feature representation of the i-th object. E = {e ij |i, j = 1, ? ? ? , N } are graph edges corresponding to relations between each object pairs, and e ij denotes the feature representation of the relation from object i to object j (Note that edges are directed).</p><p>Our XNMs are generic for scene graphs of different quality levels of detection. We consider two extreme settings in this paper. The first is the ground-truth scene graph with labels, denoted by GT, that is, using ground-truth objects as nodes, ground-truth object label embeddings as node features, and ground-truth relation label embeddings as edge features. In this setting, scene graphs are annotated with fixed-vocabulary object labels and relationship labels, e.g., defined in CLEVR dataset <ref type="bibr" target="#b11">[12]</ref>. We collect all the C labels into a dictionary, and use an embedding matrix D ? R C?d to map a label into a d-dimensional vector. We represent the nodes and edges using the concatenation of their corresponding label embeddings.</p><p>The second setting is totally detected and label-agnostic, denoted by Det, that is, using detected objects as nodes, RoI visual features as node features, and the fusion of two node features as the edge features. For example, the edge features can be represented by concatenating the two related node features, i.e., e ij = v i ; v j . As an another example, in CLEVR where the edges are only about spatial relationships, we use the difference between detected coordinates of object pairs as the edge embedding. More details are in Section 4.</p><p>We use the GT setting to demonstrate the performance upper-bound of our approach when a perfect scene graph detector is available along with the rapid development of visual recognition, and use the Det setting to demonstrate the practicality in open domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">X Neural Modules</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our XNMs have four meta-types and are totally attention-based. We denote the node attention weight vector by a ? [0, 1] N and the weight of the i-th node by a i . The edge attention weight matrix is denoted by W ? [0, 1] N ?N , where W ij represents the weight of edge from node i to node j.</p><p>AttendNode[query]. This most basic and intuitive operation is to find the relevant objects given an input query (e.g., find all ["cubes"]). For the purpose of semantic computation, we first encode the query into a vector q. This X module takes the query vector as input, and produces the node attention vector by the following function:</p><formula xml:id="formula_0">a = f (V, q).<label>(1)</label></formula><p>The implementation of f is designed according to a specific scene graph representation, as long as f is differentiable and range(f ) = [0, 1].   <ref type="figure">Figure 2</ref>: To answer a question about an image, we need to 1) parse the image into a scene graph, 2) parse the question into a module program, and 3) reasoning over the scene graph. Here, we show the reasoning details of an example from CLEVR. The nodes and edges in red are attended. Scene is a dummy placeholder module that attends all nodes. All intermediate steps of our XNMs are explainable and explicit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AttendEdge[query].</head><p>Though object attention is a widely-used mechanism for better visual understanding, it is unable to capture the interaction between objects and thus is weak in the complex visual reasoning <ref type="bibr" target="#b29">[30]</ref>. This X module aims to find the relevant edges given an input query (e.g., find all edges that are ["left"]). After encoding the query into q, we compute the edge attention matrix by the following function:</p><formula xml:id="formula_1">W = g(E, q),<label>(2)</label></formula><p>where g is defined according to a specific scene graph representation, as long as g is differentiable and range(g) = [0, 1]. Transfer. With the node attention vector a and the edge attention matrix W, we can transfer the node weights along the attentive relations to find new objects (e.g., find objects that are ["left"] to the ["cube"]). Thanks to the graph structure, to obtain the updated node attention a , we merely need to perform a simple matrix multiplication:</p><formula xml:id="formula_2">a = norm(W a),<label>(3)</label></formula><p>where norm assert the values in [0, 1] by dividing the maximum value if any entry exceeds 1. Here, W ij indicates how many weights will flow from object i to object j, and a i = N j=1 W ji a j is the total received weights of object i. This module reallocates node attention in an efficient and fully-differentiable manner.</p><p>Logic. Logical operations are crucial in complex reasoning cases. In XNM, logical operations are performed on one or more attention weights to produce a new attention. We define three logical X modules: And, Or, and Not. Without loss of generality, we discuss all these logical modules on node attention vectors, and the extension to edge attention is similar. The And and Or modules are binary, that is, take two attentions as inputs, while the Not module is unary. The implementation of these logical X modules are as follows:</p><p>And(a 1 , a 2 ) = min(a 1 , a 2 ), Not(a) = 1 ? a, Or(a 1 , a 2 ) = max(a 1 , a 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(4)</head><p>These four meta-types of XNMs constitute the base of our graph reasoning. They are explicitly executed on attention maps, and all intermediate results are explainable. Besides, these X modules are totally differentiable. We can flexibly assemble them into composite modules for more complex functions, which can be still trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementations</head><p>To apply XNMs in practice, we need to consider these questions: (1) How to implement the attention functions f, g in Eq. (1) and Eq. (2)? (2) How to compose our X modules into composite reasoning modules? (3) How to predict the answer according to the attentive results? (4) How to parse the input question to an executable module program?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Attention Functions</head><p>We use different attention functions for different scene graph settings. In the GT setting, as annotated labels are mostly mutually exclusive (e.g., "red" and "green"), we compute the node attention using the softmax function over the label space. Specifically, given a query vector q ? R d , we first compute its attention distribution over all labels by b = softmax(D ? q), where length(b) = C and b c represents the weight of the c-th label. Then we capture the node and edge attention by summing up corresponding label weights:</p><formula xml:id="formula_3">a i = f (V, q) i = c?Ci b c , W ij = g(E, q) ij = c?Cij b c ,<label>(5)</label></formula><p>where C i and C ij denote the (multi-) labels of node i and edge ij respectively.</p><p>In the Det setting, we use the sigmoid function to compute the attention weights. Given the query q ? R d , the node and edge attentions are:</p><formula xml:id="formula_4">a i = f (V, q) i = sigmoid MLP(v i ) q , W ij = g(E, q) ij = sigmoid MLP(e ij ) q ,<label>(6)</label></formula><p>where the MLP maps v i and e ij to the dimension d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Composite Reasoning Modules</head><p>We list our composite reasoning modules and their implementations (i.e., how they are composed by basic X modules) in the top section of <ref type="table" target="#tab_1">Table 1</ref>. For example, Same module is to find other objects that have the same attribute value as the input objects (e.g., find other objects with the same ["color"]). In particular, Describe used in Same is to obtain the corresponding attribute value (e.g., describe one object's ["color"]), and will be introduced in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Feature Output Modules</head><p>Besides the above reasoning modules, we also need another kind of modules to map the intermediate attention to a hidden embedding h for feature representation, which is fed into a softmax layer to predict the final answer, or into some modules for further reasoning. We list our output modules in the bottom section of <ref type="table" target="#tab_1">Table 1</ref>. Exist and Count sum up the node attention weights to answer yes/no and counting questions. Compare is for attribute or number comparisons, which takes two hidden features as inputs. Describe[query] is to transform the attentive node features to an embedding that describes the specified attribute value (e.g., what is the ["color"] of attended objects).</p><p>To implement the Describe module, we first obtain the "raw" attentive node feature b?</p><formula xml:id="formula_5">v = N i=1 a i v i N i=1 a i ,<label>(7)</label></formula><p>and then project it into several "fine-grained" sub-spaces -describing different attribute aspects such as color and shape -using different transformation matrices. Specifically, we define K projection matrices M 1 , ? ? ? , M K to mapv into different aspects (e.g., M 1v represents the color, M 2v represents the shape, etc.), where K is a hyperparameter related to the specific scene graph vocabulary. The output feature is computed by</p><formula xml:id="formula_6">Describe(a, q) = K k=1 c k (M kv ),<label>(8)</label></formula><p>where c = Softmax(MLP(q)) represents a probability distribution over these K aspects, and c k denotes the k-th probability. The mapping matrixes can be learned end-to-end automatically. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Program Generation &amp; Training</head><p>For datasets that have ground-truth program annotations (e.g., CLEVR), we directly learn an LSTM sequence-tosequence model <ref type="bibr" target="#b23">[24]</ref> to convert the word sequence into the module program. However, there is no layout annotations in most real-world datasets (e.g., VQAv2.0). In this case, following StackNMN <ref type="bibr" target="#b8">[9]</ref>, we make soft module selection with a differentiable stack structure. Please refer to their papers for more details. We feed our output features from modules (cf <ref type="table" target="#tab_1">. Table 1</ref>) into a softmax layer for the answer prediction. We use the cross entropy loss between our predicted answers and ground-truth answers to train our XNMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CLEVR</head><p>Settings. The CLEVR dataset <ref type="bibr" target="#b11">[12]</ref> is a synthetic diagnostic dataset that tests a range of visual reasoning abilities. In CLEVR, images are annotated with ground-truth object positions and labels, and questions are represented as functional programs that consists of 13 kinds of modules. Except the "Unique" module, which does not have actual operation, all the remaining 12 modules can correspond to our  <ref type="bibr" target="#b11">[12]</ref>). The program option "scratch" means totally without program annotations, "supervised" means using trained end-to-end parser, and "GT" means using ground-truth programs. Our reasoning modules are composed with highly-reusable X modules, leading to a very small number of parameters. Using the ground-truth scene graphs and programs, we can achieve a perfect reasoning on all kinds of questions. modules in <ref type="table" target="#tab_1">Table 1</ref>. CLEVR modules "Equal attribute", "Equal integer", "Greater than" and "Less than" have the same implementation as our Compare, but with different parameters. There are 4 attribute categories in CLEVR, so we set the number of mapping matrixes K = 4.</p><p>We reused the trained sequence-to-sequence program generator of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>, which uses prefix-order traversal to convert the program trees to sequences. Note that their modules are bundled with input, e.g., they regard Filter <ref type="bibr">[red]</ref> and Filter[green] as two different modules. This will cause serious sparseness in the real-world case. We used their program generator, but unpack the module and the input (e.g., Filter[red] and Filter[green] are the same module with different input query).</p><p>In the GT setting, we performed reasoning over the ground-truth scene graphs. In the Det setting, we built the scene graphs by detecting objects and using RoI features as node embeddings and the differences between detected coordinates as edge embeddings. Since CLEVR does not provide the bounding box or segmentation annotations of objects, it is hard to directly train an object detector. NS-VQA <ref type="bibr" target="#b26">[27]</ref> trained a Mask R-CNN <ref type="bibr" target="#b6">[7]</ref> for object segmentation by "hacking" the rendering process <ref type="bibr" target="#b11">[12]</ref>, which could perform very well due to the simplicity of visual scenes of CLEVR. However, as we expected to explore X modules in a noisier case, we chose the trained attention modules of TbD-net <ref type="bibr" target="#b17">[18]</ref> as our object detector. Specifically, we enu-merated all possible combinations of object attributes (e.g., red, cube, metal, large), and tried to find corresponding objects using their attention modules (e.g., intersection of the output mask of Attend[red], Attend[cube], Attend <ref type="bibr">[metal]</ref> and Attend[large], and then regarded each clique as a single object). The detected results have some frequent mistakes, such as inaccurate position, wrongly merged nodes (two adjacent objects with the same attribute values are recognized as one). These detection noises allow us to test whether our XNMs are robust enough.</p><p>Goals.</p><p>We expect to answer the following questions according to the CLEVR experiments: Q1: What is the upper bound of our X reasoning when both the vision and language perceptions are perfect? Q2: Are our XNMs robust for noisy detected scene graphs and parsed programs? Q3: What are the parameter and data efficiency, and the convergence speed of XNMs? Q4: How is the explainability of XNMs?</p><p>Results. Experimental results are listed in <ref type="table" target="#tab_2">Table 2</ref>. A1: When using the ground-truth scene graphs and programs, we can achieve 100% accuracy, indicating an inspiring upper-bound of visual reasoning. By disentangling "high-level" reasoning from "low-level" perception and using XNMs, we may eventually conquer the visual reasoning challenge with the rapid development of visual recognition.</p><p>A2: With noisy detected scene graphs, we can still achieve a competitive 97.9% accuracy using the ground-truth programs, indicating that our X reasoning are robust to different quality levels of scene graphs. When replacing the ground-truth programs with parsed programs, the accuracy drops by 0.1% in both GT and Det settings, which is caused by minor errors of the program parser.</p><p>A3: Due to the conciseness and high-reusability of X modules, our model requires significantly less parameters than existing models. Our GT setting only needs about 0.22M parameters, taking about 500MB memory with batch size of 128, while PG+EE <ref type="bibr" target="#b12">[13]</ref> and TbD-net <ref type="bibr" target="#b17">[18]</ref> bundle modules and inputs together, leading to a large number of modules and parameters. Accuracy when trained with different ratios of training data. To explore the data efficiency, we trained our model with a partial training set and evaluated on the complete validation set. Results are displayed in the left part of <ref type="figure" target="#fig_3">Figure 3</ref>. We can see that our model performs much better than other baselines when the training set is small. Especially, our GT setting can still achieve a 100% accuracy even with only 10% training data. The right part shows the accuracy at each training epoch. We can see that our X reasoning converges very fast.</p><p>A4: As our XNMs are attention-based, the reasoning process is totally transparent and we can easily show intermediate results. <ref type="figure">Figure 4</ref> displays two examples of CLEVR. We can see all reasoning steps are clear and intuitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CLEVR-CoGenT</head><p>Settings. The CLEVR-CoGenT dataset is a benchmark to study the ability of models to recognize novel combinations of attributes at test-time, which is derived from CLEVR but has two different conditions: in Condition A all cubes are colored one of gray, blue, brown, or yellow, and all cylinders are one of red, green, purple, or cyan; in Condition B the color palettes are swapped. The model is trained using the training set of Condition A, and then is tested using Condition B to check whether it can generalize well to the novel attribute combinations. We train our model on the training set of Condition A, and report the accuracy Results. Results of CLEVR-CoGenT are displayed in <ref type="table" target="#tab_4">Table 3</ref>. A1: When using the ground-truth scene graphs, our XNMs perform perfectly on both Condition A and Condition B. Novel combinations of attributes in Condition B do not cause the performance drop at all. However, when using the detected scene graphs, where node embeddings are RoI features that fuse all attribute values, our generalization results on Condition B drops to 72.1%, suffering from the dataset shortcut just like other existing models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>. A2: <ref type="figure" target="#fig_4">Figure 5</ref> shows some typical failure cases of our Det setting on Condition B. In case (a), our model cannot recognize purple cubes as "cube" because all cubes are colored one of gray, blue, brown, or yellow in the training data. Similarly, in case (b) and (c), whether an object is recognized as a "cube" or a "cylinder" by our model is actually determined by its color. However, in our GT setting, which is given the ground-truth visual labels, we can achieve a perfect performance. This gap reveals that the challenge of CLEVR-CoGenT mostly comes from the vision bias, rather than the reasoning shortcut. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Union Count</head><p>Anser: 3 <ref type="figure">Figure 4</ref>: Reasoning visualizations of two CLEVR samples. Question 1: What number of objects are either big objects that are behind the big gray block or tiny brown rubber balls? Question 2: The other small shiny thing that is the same shape as the tiny yellow shiny object is what color? We plot a dot for each object and darker (red) dots indicate higher attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">VQAv2.0</head><p>Settings. VQAv2.0 <ref type="bibr" target="#b5">[6]</ref> is a real-world visual Q&amp;A dataset which does not have annotations about scene graphs and module programs. We used the grounded visual features of <ref type="bibr" target="#b1">[2]</ref> as node features, and concatenated node embeddings as edge features. We set K = 1 and fused the question embedding with our output feature for answer prediction. Following <ref type="bibr" target="#b1">[2]</ref>, we used softmax over objects for node attention computation.</p><p>Goals. We used VQAv2.0 to demonstrate the generality and robustness of our model in the practical case.</p><p>Results. We list the results in <ref type="table" target="#tab_6">Table 4</ref>. We follow Stack-NMN <ref type="bibr" target="#b8">[9]</ref> to build the module program in a stacked soft manner, but our model can achieve better performance as our reasoning over scene graphs is more powerful than their pixel-level operations.</p><p>Recall that <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref> are not applicable in openvocabulary input, and <ref type="bibr" target="#b26">[27]</ref> relies on the fixed label representation, so it is hard to apply them on practical datasets. In contrast, our XNMs are flexible enough for different cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed X neural modules (XNMs) that allows visual reasoning over scene graphs, represented by different detection qualities. Using the ground-truth scene graphs and programs on CLEVR, we can achieve 100% accuracy with only 0.22M parameters. Compared to existing neural module networks, XNMs disentangle the "high-level" reasoning from the "low-level" visual perception, and allow us to pay more attention to teaching A.I. how to "think", regardless of what they "look". We believe that this is an inspiring direction towards explainable machine reasoning. Besides, our experimental results suggest that visual reasoning benefits a lot from high-quality scene graphs, revealing the practical significance of the scene graph research. Question: What number of objects are tiny spheres or brown blocks behind the gray matte object ? <ref type="figure">Figure 6</ref>: Failure cases caused by the inaccurate coordinate detection. Top case: the large brown cube is not behind the gray rubber object. Bottom case: a large red cube is wrongly recognized to be behind the tiny cylinder.</p><p>Question: Is there a blue metal object that has the same size as the gray metal object ?  : Failure cases caused by occluded objects. We show the high-resolution images here, and we can see that 1) in the left image, there is a small blue cube behind the large blue cube occluded; 2) in the right image, there is a red cylinder behind the large brown cylinder occluded. These occluded objects do not have corresponding dots in the reasoning results, leading to a wrong prediction "No" while the actual answer is "Yes".</p><p>Question: There is a object that is both behind the cyan object and in front of the small metal cylinder ; what size is it ? Question: What number of metal objects are large brown objects or tiny spheres ? <ref type="figure">Figure 8</ref>: Failure cases caused by the inaccurate object division. Top case: two dots are assigned to the same object. Bottom case: two adjacent objects with the same attribute values (i.e., large, brown, sphere, metal) are recognized as one object, which makes the predicted number less than the ground truth answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filter</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " Z V 8 Q T F G 5 R T Z / G 4 q R / y u 4 F + Z g n u 4 = " &gt; A A A C W 3 i c b V F N S 8 N A E N 2 k a m v 8 q o o n L 6 t F 8 V Q S E f R Y 9 O J R w a r Q l L L Z T O v S z S b s T q Q l 5 E 9 6 0 o N / R d z W I F p 9 y 8 L j z R t m 9 m 2 U S W H Q 9 9 8 c t 7 a 0 v F J v r H p r 6 x u b W 8 3 t n X u T 5 p p D l 6 c y 1 Y 8 R M y C F g i 4 K l P C Y a W B J J O E h G l / N 6 g / P o I 1 I 1 R 1 O M + g n b K T E U H C G V h o 0 d R j B S K g i S h h q M S k 9 a h E i T L D I c p 1 J K O l x e D A 7 l c q n d l Q M u q R h + M M s m R 5 Z 7 4 I 5 A W S y p F 4 I K v 4 e M W i 2 / L Y / B / 1 L g o q 0 S I W b Q f M l j F O e J 6 C Q S 2 Z M L / A z 7 B d M o + B 2 Q S / M D W S M j 9 k I e p Y q l o D p F / N s S n p k l Z g O U 2 2 v Q j p X f 3 Y U L D F m m k T W a f d 7 M o u 1 m f h f r Z f j 8 K J f C J X l C I p / D R r m k m J K Z 0 H T W G j g K K e W M K 6 F 3 Z X y J 6 Y Z R / s d n g 0 h W H z y X 3 J / 2 g 4 s v z 1 r d S 6 r O B p k n x y S E x K Q c 9 I h 1 + S G d A k n r + T D q T s N 5 9 2 t u Z 6 7 / m V 1 n a p n l / y C u / c J P W a z W w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z V 8 Q T F G 5 R T Z / G 4 q R / y u 4 F + Z g n u 4 = " &gt; A A A C W 3 i c b V F N S 8 N A E N 2 k a m v 8 q o o n L 6 t F 8 V Q S E f R Y 9 O J R w a r Q l L L Z T O v S z S b s T q Q l 5 E 9 6 0 o N / R d z W I F p 9 y 8 L j z R t m 9 m 2 U S W H Q 9 9 8 c t 7 a 0 v F J v r H p r 6 x u b W 8 3 t n X u T 5 p p D l 6 c y 1 Y 8 R M y C F g i 4 K l P C Y a W B J J O E h G l / N 6 g / P o I 1 I 1 R 1 O M + g n b K T E U H C G V h o 0 d R j B S K g i S h h q M S k 9 a h E i T L D I c p 1 J K O l x e D A 7 l c q n d l Q M u q R h + M M s m R 5 Z 7 4 I 5 A W S y p F 4 I K v 4 e M W i 2 / L Y / B / 1 L g o q 0 S I W b Q f M l j F O e J 6 C Q S 2 Z M L / A z 7 B d M o + B 2 Q S / M D W S M j 9 k I e p Y q l o D p F / N s S n p k l Z g O U 2 2 v Q j p X f 3 Y U L D F m m k T W a f d 7 M o u 1 m f h f r Z f j 8 K J f C J X l C I p / D R r m k m J K Z 0 H T W G j g K K e W M K 6 F 3 Z X y J 6 Y Z R / s d n g 0 h W H z y X 3 J / 2 g 4 s v z 1 r d S 6 r O B p k n x y S E x K Q c 9 I h 1 + S G d A k n r + T D q T s N 5 9 2 t u Z 6 7 / m V 1 n a p n l / y C u / c J P W a z W w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z V 8 Q T F G 5 R T Z / G 4 q R / y u 4 F + Z g n u 4 = " &gt; A A A C W 3 i c b V F N S 8 N A E N 2 k a m v 8 q o o n L 6 t F 8 V Q S E f R Y 9 O J R w a r Q l L L Z T O v S z S b s T q Q l 5 E 9 6 0 o N / R d z W I F p 9 y 8 L j z R t m 9 m 2 U S W H Q 9 9 8 c t 7 a 0 v F J v r H p r 6 x u b W 8 3 t n X u T 5 p p D l 6 c y 1 Y 8 R M y C F g i 4 K l P C Y a W B J J O E h G l / N 6 g / P o I 1 I 1 R 1 O M + g n b K T E U H C G V h o 0 d R j B S K g i S h h q M S k 9 a h E i T L D I c p 1 J K O l x e D A 7 l c q n d l Q M u q R h + M M s m R 5 Z 7 4 I 5 A W S y p F 4 I K v 4 e M W i 2 / L Y / B / 1 L g o q 0 S I W b Q f M l j F O e J 6 C Q S 2 Z M L / A z 7 B d M o + B 2 Q S / M D W S M j 9 k I e p Y q l o D p F / N s S n p k l Z g O U 2 2 v Q j p X f 3 Y U L D F m m k T W a f d 7 M o u 1 m f h f r Z f j 8 K J f C J X l C I p / D R r m k m J K Z 0 H T W G j g K K e W M K 6 F 3 Z X y J 6 Y Z R / s d n g 0 h W H z y X 3 J / 2 g 4 s v z 1 r d S 6 r O B p k n x y S E x K Q c 9 I h 1 + S G d A k n r + T D q T s N 5 9 2 t u Z 6 7 / m V 1 n a p n l / y C u / c J P W a z W w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z V 8 Q T F G 5 R T Z / G 4 q R / y u 4 F + Z g n u 4= " &gt; A A A C W 3 i c b V F N S 8 N A E N 2 k a m v 8 q o o n L 6 t F 8 V Q S E f R Y 9 O J R w a r Q l L L Z T O v S z S b s T q Q l 5 E 9 6 0 o N / R d z W I F p 9 y 8 L j z R t m 9 m 2 U S W H Q 9 9 8 c t 7 a 0 v F J v r H p r 6 x u b W 8 3 t n X u T 5 p p D l 6 c y 1 Y 8 R M y C F g i 4 K l P C Y a W B J J O E h G l / N 6 g / P o I 1 I 1 R 1 O M + g n b K T E U H C G V h o 0 d R j B S K g i S h h q M S k 9 a h E i T L D I c p 1 J K O l x e D A 7 l c q n d l Q M u q R h + M M s m R 5 Z 7 4 I 5 A W S y p F 4 I K v 4 e M W i 2 / L Y / B / 1 L g o q 0 S I W b Q f M l j F O e J 6 C Q S 2 Z M L / A z 7 B d M o + B 2 Q S / M D W S M j 9 k I e p Y q l o D p F / N s S n p k l Z g O U 2 2 v Q j p X f 3 Y U L D F m m k T W a f d 7 M o u 1 m f h f r Z f j 8 K J f C J X l C I p / D R r m k m J K Z 0 H T W G j g K K e W M K 6 F 3 Z X y J 6 Y Z R /s d n g 0 h W H z y X 3 J / 2 g 4 s v z 1 r d S 6 r O B p k n x y S E x K Q c 9 I h 1 + S G d A k n r + T D q T s N 5 9 2 t u Z 6 7 / m V 1 n a p n l / y C u / c J P W a z W w = = &lt; / l a t e x i t &gt; ? brown cylinder large metal &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 C i J G k U z V J Y 1 U X 7 t 3 q O p K M F c J T o = " &gt; A A A C W n i c b V F d S 8 M w F E 2 r b r N + z Y 8 3 X 6 J D 8 W m 0 I u i j 6 I u P E 5 w K 6 x h p e j e D a V q T W 3 W U / k l f R P C v C G a z i E 5 P C B z O P Z d 7 c x J l U h j 0 / T f H n Z t f q N U b i 9 7 S 8 s r q W n N 9 4 9 q k u e b Q 5 a l M 9 W 3 E D E i h o I s C J d x m G l g S S b i J 7 s 8 n 9 Z t H 0 E a k 6 g r H G f Q T N l J i K D h D K w 2 a D 2 E E I 6 G K K G G o x X P p U Y s Q 4 R m L S K d P q q T 7 4 c 7 k V C I f 2 0 k x 6 J K G 4 Q + v Z H o E J Z 0 x J 4 B M l t Q L Q c X f E w b N l t / 2 p 6 B / S V C R F q n Q G T R f w j j l e Q I K u W T G 9 A I / w 3 7 B N A o u o f T C 3 E D G + D 0 b Q c 9 S x R I w / W I a T U n 3 r B L T Y a r t V U i n 6 s + O g i X G j J P I O u 1 + d 2 a 2 N h H / q / V y H J 7 0 C 6 G y H E H x r 0 H D X F J M 6 S R n G g s N H O X Y E s a 1 s L t S f s c 0 4 2 h / w 7 M h B L N P / k u u D 9 u B 5 Z d H r d O z K o 4 G 2 S a 7 5 I A E 5 J i c k g v S I V 3 C y S v 5 c G p O 3 X l 3 X X f R X f q y u k 7 V s 0 l + w d 3 6 B D 3 r s u E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 C i J G k U z V J Y 1 U X 7 t 3 q O p K M F c J T o = " &gt; A A A C W n i c b V F d S 8 M w F E 2 r b r N + z Y 8 3 X 6 J D 8 W m 0 I u i j 6 I u P E 5 w K 6 x h p e j e D a V q T W 3 W U / k l f R P C v C G a z i E 5 P C B z O P Z d 7 c x J l U h j 0 / T f H n Z t f q N U b i 9 7 S 8 s r q W n N 9 4 9 q k u e b Q 5 a l M 9 W 3 E D E i h o I s C J d x m G l g S S b i J 7 s 8 n 9 Z t H 0 E a k 6 g r H G f Q T N l J i K D h D K w 2 a D 2 E E I 6 G K K G G o x X P p U Y s Q 4 R m L S K d P q q T 7 4 c 7 k V C I f 2 0 k x 6 J K G 4 Q + v Z H o E J Z 0 x J 4 B M l t Q L Q c X f E w b N l t / 2 p 6 B / S V C R F q n Q G T R f w j j l e Q I K u W T G 9 A I / w 3 7 B N A o u o f T C 3 E D G + D 0 b Q c 9 S x R I w / W I a T U n 3 r B L T Y a r t V U i n 6 s + O g i X G j J P I O u 1 + d 2 a 2 N h H / q / V y H J 7 0 C 6 G y H E H x r 0 H D X F J M 6 S R n G g s N H O X Y E s a 1 s L t S f s c 0 4 2 h / w 7 M h B L N P / k u u D 9 u B 5 Z d H r d O z K o 4 G 2 S a 7 5 I A E 5 J i c k g v S I V 3 C y S v 5 c G p O 3 X l 3 X X f R X f q y u k 7 V s 0 l + w d 3 6 B D 3 r s u E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 C i J G k U z V J Y 1 U X 7 t 3 q O p K M F c J T o = " &gt; A A A C W n i c b V F d S 8 M w F E 2 r b r N + z Y 8 3 X 6 J D 8 W m 0 I u i j 6 I u P E 5 w K 6 x h p e j e D a V q T W 3 W U / k l f R P C v C G a z i E 5 P C B z O P Z d 7 c x J l U h j 0 / T f H n Z t f q N U b i 9 7 S 8 s r q W n N 9 4 9 q k u e b Q 5 a l M 9 W 3 E D E i h o I s C J d x m G l g S S b i J 7 s 8 n 9 Z t H 0 E a k 6 g r H G f Q T N l J i K D h D K w 2 a D 2 E E I 6 G K K G G o x X P p U Y s Q 4 R m L S K d P q q T 7 4 c 7 k V C I f 2 0 k x 6 J K G 4 Q + v Z H o E J Z 0 x J 4 B M l t Q L Q c X f E w b N l t / 2 p 6 B / S V C R F q n Q G T R f w j j l e Q I K u W T G 9 A I / w 3 7 B N A o u o f T C 3 E D G + D 0 b Q c 9 S x R I w / W I a T U n 3 r B L T Y a r t V U i n 6 s + O g i X G j J P I O u 1 + d 2 a 2 N h H / q / V y H J 7 0 C 6 G y H E H x r 0 H D X F J M 6 S R n G g s N H O X Y E s a 1 s L t S f s c 0 4 2 h / w 7 M h B L N P / k u u D 9 u B 5 Z d H r d O z K o 4 G 2 S a 7 5 I A E 5 J i c k g v S I V 3 C y S v 5 c G p O 3 X l 3 X X f R X f q y u k 7 V s 0 l + w d 3 6 B D 3 r s u E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 C i J G k U z V J Y 1 U X 7 t 3 q O p K M F c J T o = " &gt; A A A C W n i c b V F d S 8 M w F E 2 r b r N + z Y 8 3 X 6 J D 8 W m 0 I u i j 6 I u P E 5 w K 6 x h p e j e D a V q T W 3 W U / k l f R P C v C G a z i E 5 P C B z O P Z d 7 c x J l U h j 0 / T f H n Z t f q N U b i 9 7 S 8 s r q W n N 9 4 9 q k u e b Q 5 a l M 9 W 3 E D E i h o I s C J d x m G l g S S b i J 7 s 8 n 9 Z t H 0 E a k 6 g r H G f Q T N l J i K D h D K w 2 a D 2 E E I 6 G K K G G o x X P p U Y s Q 4 R m L S K d P q q T 7 4 c 7 k V C I f 2 0 k x 6 J K G 4 Q + v Z H o E J Z 0 x J 4 B M l t Q L Q c X f E w b N l t / 2 p 6 B / S V C R F q n Q G T R f w j j l e Q I K u W T G 9 A I / w 3 7 B N A o u o f T C 3 E D G + D 0 b Q c 9 S x R I w / W I a T U n 3 r B L T Y a r t V U i n 6 s + O g i X G j J P I O u 1 + d 2 a 2 N h H / q / V y H J 7 0 C 6 G y H E H x r 0 H D X F J M 6 S R n G g s N H O X Y E s a 1 s L t S f s c 0 4 2 h / w 7 M h B L N P / k u u D 9 u B 5 Z d H r d O z K o 4 G 2 S a 7 5 I A E 5 J i c k g v S I V 3 C y S v 5 c G p O 3 X l 3 X X f R X f q y u k 7 V s 0 l + w d 3 6 B D 3 r s u E = &lt; / l a t e x i t &gt; ? red cube large rubber &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H l b R p z + e Y C r m K p c p G c b x d l x S N n M = " &gt; A A A C V X i c b V F d S 8 M w F E 3 r d / 2 a + u h L d C g + S S u C P g 5 9 8 V H B O W E d I 0 n v Z j B N S 3 I r j t I / u R f x n / g i m M 0 i u n l C 4 H D u u c n N C c + V t B i G 7 5 6 / s L i 0 v L K 6 F q x v b G 5 t N 3 Z 2 H 2 x W G A F t k a n M P H J m Q U k N b Z S o 4 D E 3 w F K u o M O f r y f 1 z g s Y K z N 9 j 6 M c e i k b a j m Q g q G T + g 0 V c x h K X f K U o Z G v V U A d Y o R X L A 0 k F T 2 O D y a r l k T B o a J x / M u l m B k 6 b c Z o C s 7 B V D S I Q S c / h / c b z f A 0 n I L O k 6 g m T V L j t t 8 Y x 0 k m i h Q 0 C s W s 7 U Z h j r 2 S G Z R C Q R X E h Y W c i W c 2 h K 6 j m q V g e + U 0 l Y o e O S W h g 8 y 4 r Z F O 1 d 8 d J U u t H a X c O d 1 8 T 3 a 2 N h H / q 3 U L H F z 2 S q n z A k G L 7 4 s G h a K Y 0 U n E N J E G B K q R I 0 w Y 6 W a l 4 o k Z J t B 9 R O B C i G a f P E 8 e z k 4 j x + / O m 6 2 r O o 5 V s k 8 O y Q m J y A V p k R t y S 9 p E k D H 5 8 D z P 9 9 6 8 T 3 / R X / 6 2 + l 7 d s 0 f + w N / + A l 7 B s X U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H l b R p z + e Y C r m K p c p G c b x d l x S N n M = " &gt; A A A C V X i c b V F d S 8 M w F E 3 r d / 2 a + u h L d C g + S S u C P g 5 9 8 V H B O W E d I 0 n v Z j B N S 3 I r j t I / u R f x n / g i m M 0 i u n l C 4 H D u u c n N C c + V t B i G 7 5 6 / s L i 0 v L K 6 F q x v b G 5 t N 3 Z 2 H 2 x W G A F t k a n M P H J m Q U k N b Z S o 4 D E 3 w F K u o M O f r y f 1 z g s Y K z N 9 j 6 M c e i k b a j m Q g q G T + g 0 V c x h K X f K U o Z G v V U A d Y o R X L A 0 k F T 2 O D y a r l k T B o a J x / M u l m B k 6 b c Z o C s 7 B V D S I Q S c / h / c b z f A 0 n I L O k 6 g m T V L j t t 8 Y x 0 k m i h Q 0 C s W s 7 U Z h j r 2 S G Z R C Q R X E h Y W c i W c 2 h K 6 j m q V g e + U 0 l Y o e O S W h g 8 y 4 r Z F O 1 d 8 d J U u t H a X c O d 1 8 T 3 a 2 N h H / q 3 U L H F z 2 S q n z A k G L 7 4 s G h a K Y 0 U n E N J E G B K q R I 0 w Y 6 W a l 4 o k Z J t B 9 R O B C i G a f P E 8 e z k 4 j x + / O m 6 2 r O o 5 V s k 8 O y Q m J y A V p k R t y S 9 p E k D H 5 8 D z P 9 9 6 8 T 3 / R X / 6 2 + l 7 d s 0 f + w N / + A l 7 B s X U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H l b R p z + e Y C r m K p c p G c b x d l x S N n M = " &gt; A A A C V X i c b V F d S 8 M w F E 3 r d / 2 a + u h L d C g + S S u C P g 5 9 8 V H B O W E d I 0 n v Z j B N S 3 I r j t I / u R f x n / g i m M 0 i u n l C 4 H D u u c n N C c + V t B i G 7 5 6 / s L i 0 v L K 6 F q x v b G 5 t N 3 Z 2 H 2 x W G A F t k a n M P H J m Q U k N b Z S o 4 D E 3 w F K u o M O f r y f 1 z g s Y K z N 9 j 6 M c e i k b a j m Q g q G T + g 0 V c x h K X f K U o Z G v V U A d Y o R X L A 0 k F T 2 O D y a r l k T B o a J x / M u l m B k 6 b c Z o C s 7 B V D S I Q S c / h / c b z f A 0 n I L O k 6 g m T V L j t t 8 Y x 0 k m i h Q 0 C s W s 7 U Z h j r 2 S G Z R C Q R X E h Y W c i W c 2 h K 6 j m q V g e + U 0 l Y o e O S W h g 8 y 4 r Z F O 1 d 8 d J U u t H a X c O d 1 8 T 3 a 2 N h H / q 3 U L H F z 2 S q n z A k G L 7 4 s G h a K Y 0 U n E N J E G B K q R I 0 w Y 6 W a l 4 o k Z J t B 9 R O B C i G a f P E 8 e z k 4 j x + / O m 6 2 r O o 5 V s k 8 O y Q m J y A V p k R t y S 9 p E k D H 5 8 D z P 9 9 6 8 T 3 / R X / 6 2 + l 7 d s 0 f + w N / + A l 7 B s X U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H l b R p z + e Y C r m K p c p G c b x d l x S N n M = " &gt; A A A C V X i c b V F d S 8 M w F E 3 r d / 2 a + u h L d C g + S S u C P g 5 9 8 V H B O W E d I 0 n v Z j B N S 3 I r j t I / u R f x n / g i m M 0 i u n l C 4 H D u u c n N C c + V t B i G 7 5 6 / s L i 0 v L K 6 F q x v b G 5 t N 3 Z 2 H 2 x W G A F t k a n M P H J m Q U k N b Z S o 4 D E 3 w F K u o M O f r y f 1 z g s Y K z N 9 j 6 M c e i k b a j m Q g q G T + g 0 V c x h K X f K U o Z G v V U A d Y o R X L A 0 k F T 2 O D y a r l k T B o a J x / M u l m B k 6 b c Z o C s 7 B V D S I Q S c / h / c b z f A 0 n I L O k 6 g m T V L j t t 8 Y x 0 k m i h Q 0 C s W s 7 U Z h j r 2 S G Z R C Q R X E h Y W c i W c 2 h K 6 j m q V g e + U 0 l Y o e O S W h g 8 y 4 r Z F O 1 d 8 d J U u t H a X c O d 1 8 T 3 a 2 N h H / q 3 U L H F z 2 S q n z A k G L 7 4 s G h a K Y 0 U n E N J E G B K q R I 0 w Y 6 W a l 4 o k Z J t B 9 R O B C i G a f P E 8 e z k 4 j x + / O m 6 2 r O o 5 V s k 8 O y Q m J y A V p k R t y S 9 p E k D H 5 8 D z P 9 9 6 8 T 3 / R X / 6 2 + l 7 d s 0 f + w N / + A l 7 B s X U = &lt; / l a t e x i t &gt; The flowchart of using the proposed XNMs reasoning over scene graphs, which can be represented by detected one-hot class labels (left) or RoI feature vectors (colored bars on the right). Feature colors are consistent with the bounding box colors. XNMs have 4 meta-types. Red nodes or edges indicate attentive results. The final module assembly can be obtained by training an off-the-shelf sequence-to-sequence program generator [13].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Q:</head><label></label><figDesc>How many objects are right of the brown metal cylinder and left of the red cube? le ft , fr o n t ri g h t, b e h in d ri g h t, fr o n t le ft , b e h in d left, front right, behind ? purple cylinder large metal &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z V 8 Q T F G 5 R T Z / G 4 q R / y u 4 F + Z g n u 4 = " &gt; A A A C W 3 i c b V F N S 8 N A E N 2 k a m v 8 q o o n L 6 t F 8 V Q S E f R Y 9 O J R w a r Q l L L Z T O v S z S b s T q Q l 5 E 9 6 0 o N / R d z W I F p 9 y 8 L j z R t m 9 m 2 U S W H Q 9 9 8 c t 7 a 0 v F J v r H p r 6 x u b W 8 3 t n X u T 5 p p D l 6 c y 1 Y 8 R M y C F g i 4 K l P C Y a W B J J O E h G l / N 6 g / P o I 1 I 1 R 1 O M + g n b K T E U H C G V h o 0 d R j B S K g i S h h q M S k 9 a h E i T L D I c p 1 J K O l x e D A 7 l c q n d l Q M u q R h + M M s m R 5 Z 7 4 I 5 A W S y p F 4 I K v 4 e M W i 2 / L Y / B / 1 L g o q 0 S I W b Q f M l j F O e J 6 C Q S 2 Z M L / A z 7 B d M o + B 2 Q S / M D W S M j 9 k I e p Y q l o D p F / N s S n p k l Z g O U 2 2 v Q j p X f 3 Y U L D F m m k T W a f d 7 M o u 1 m f h f r Z f j 8 K J f C J X l C I p / D R r m k m J K Z 0 H T W G j g K K e W M K 6 F 3 Z X y J 6 Y Z R / s d n g 0 h W H z y X 3 J / 2 g 4 s v z 1 r d S 6 r O B p k n x y S E x K Q c 9 I h 1 + S G d A k n r + T D q T s N 5 9 2 t u Z 6 7 / m V 1 n a p n l / y C u / c J P W a z W w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z V 8 Q T F G 5 R T Z / G 4 q R / y u 4 F + Z g n u 4 = " &gt; A A A C W 3 i c b V F N S 8 N A E N 2 k a m v 8 q o o n L 6 t F 8 V Q S E f R Y 9 O J R w a r Q l L L Z T O v S z S b s T q Q l 5 E 9 6 0 o N / R d z W I F p 9 y 8 L j z R t m 9 m 2 U S W H Q 9 9 8 c t 7 a 0 v F J v r H p r 6 x u b W 8 3 t n X u T 5 p p D l 6 c y 1 Y 8 R M y C F g i 4 K l P C Y a W B J J O E h G l / N 6 g / P o I 1 I 1 R 1 O M + g n b K T E U H C G V h o 0 d R j B S K g i S h h q M S k 9 a h E i T L D I c p 1 J K O l x e D A 7 l c q n d l Q M u q R h + M M s m R 5 Z 7 4 I 5 A W S y p F 4 I K v 4 e M W i 2 / L Y / B / 1 L g o q 0 S I W b Q f M l j F O e J 6 C Q S 2 Z M L / A z 7 B d M o + B 2 Q S / M D W S M j 9 k I e p Y q l o D p F / N s S n p k l Z g O U 2 2 v Q j p X f 3 Y U L D F m m k T W a f d 7 M o u 1 m f h f r Z f j 8 K J f C J X l C I p / D R r m k m J K Z 0 H T W G j g K K e W M K 6 F 3 Z X y J 6 Y Z R / s d n g 0 h W H z y X 3 J / 2 g 4 s v z 1 r d S 6 r O B p k n x y S E x K Q c 9 I h 1 + S G d A k n r + T D q T s N 5 9 2 t u Z 6 7 / m V 1 n a p n l / y C u / c J P W a z W w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z V 8 Q T F G 5 R T Z / G 4 q R / y u 4 F + Z g n u 4 = " &gt; A A A C W 3 i c b V F N S 8 N A E N 2 k a m v 8 q o o n L 6 t F 8 V Q S E f R Y 9 O J R w a r Q l L L Z T O v S z S b s T q Q l 5 E 9 6 0 o N / R d z W I F p 9 y 8 L j z R t m 9 m 2 U S W H Q 9 9 8 c t 7 a 0 v F J v r H p r 6 x u b W 8 3 t n X u T 5 p p D l 6 c y 1 Y 8 R M y C F g i 4 K l P C Y a W B J J O E h G l / N 6 g / P o I 1 I 1 R 1 O M + g n b K T E U H C G V h o 0 d R j B S K g i S h h q M S k 9 a h E i T L D I c p 1 J K O l x e D A 7 l c q n d l Q M u q R h + M M s m R 5 Z 7 4 I 5 A W S y p F 4 I K v 4 e M W i 2 / L Y / B / 1 L g o q 0 S I W b Q f M l j F O e J 6 C Q S 2 Z M L / A z 7 B d M o + B 2 Q S / M D W S M j 9 k I e p Y q l o D p F / N s S n p k l Z g O U 2 2 v Q j p X f 3 Y U L D F m m k T W a f d 7 M o u 1 m f h f r Z f j 8 K J f C J X l C I p / D R r m k m J K Z 0 H T W G j g K K e W M K 6 F 3 Z X y J 6 Y Z R / s d n g 0 h W H z y X 3 J / 2 g 4 s v z 1 r d S 6 r O B p k n x y S E x K Q c 9 I h 1 + S G d A k n r + T D q T s N 5 9 2 t u Z 6 7 / m V 1 n a p n l / y C u / c J P W a z W w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z V 8 Q T F G 5 R T Z / G 4 q R / y u 4 F + Z g n u 4 = " &gt; A A A C W 3 i c b V F N S 8 N A E N 2 k a m v 8 q o o n L 6 t F 8 V Q S E f R Y 9 O J R w a r Q l L L Z T O v S z S b s T q Q l 5 E 9 6 0 o N / R d z W I F p 9 y 8 L j z R t m 9 m 2 U S W H Q 9 9 8 c t 7 a 0 v F J v r H p r 6 x u b W 8 3 t n X u T 5 p p D l 6 c y 1 Y 8 R M y C F g i 4 K l P C Y a W B J J O E h G l / N 6 g / P o I 1 I 1 R 1 O M + g n b K T E U H C G V h o 0 d R j B S K g i S h h q M S k 9 a h E i T L D I c p 1 J K O l x e D A 7 l c q n d l Q M u q R h + M M s m R 5 Z 7 4 I 5 A W S y p F 4 I K v 4 e M W i 2 / L Y / B / 1 L g o q 0 S I W b Q f M l j F O e J 6 C Q S 2 Z M L / A z 7 B d M o + B 2 Q S / M D W S M j 9 k I e p Y q l o D p F / N s S n p k l Z g O U 2 2 v Q j p X f 3 Y U L D F m m k T W a f d 7 M o u 1 m f h f r Z f j 8 K J f C J X l C I p / D R r m k m J K Z 0 H T W G j g K K e W M K 6 F 3 Z X y J 6 Y Z R / s d n g 0 h W H z y X 3 J / 2 g 4 s v z 1 r d S 6 r O B p k n x y S E x K Q c 9 I h 1 + S G d A k n r + T D q T s N 5 9 2 t u Z 6 7 / m V 1 n a p n l / y C u / c J P W a z W w = = &lt; / l a t e x i t &gt; ? brown cylinder large metal &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 C i J G k U z V J Y 1 U X 7 t 3 q O p K M F c J T o = " &gt; A A A C W n i c b V F d S 8 M w F E 2 r b r N + z Y 8 3 X 6 J D 8 W m 0 I u i j 6 I u P E 5 w K 6 x h p e j e D a V q T W 3 W U / k l f R P C v C G a z i E 5 P C B z O P Z d 7 c x J l U h j 0 / T f H n Z t f q N U b i 9 7 S 8 s r q W n N 9 4 9 q k u e b Q 5 a l M 9 W 3 E D E i h o I s C J d x m G l g S S b i J 7 s 8 n 9 Z t H 0 E a k 6 g r H G f Q T N l J i K D h D K w 2 a D 2 E E I 6 G K K G G o x X P p U Y s Q 4 R m L S K d P q q T 7 4 c 7 k V C I f 2 0 k x 6 J K G 4 Q + v Z H o E J Z 0 x J 4 B M l t Q L Q c X f E w b N l t / 2 p 6 B / S V C R F q n Q G T R f w j j l e Q I K u W T G 9 A I / w 3 7 B N A o u o f T C 3 E D G + D 0 b Q c 9 S x R I w / W I a T U n 3 r B L T Y a r t V U i n 6 s + O g i X G j J P I O u 1 + d 2 a 2 N h H / q / V y H J 7 0 C 6 G y H E H x r 0 H D X F J M 6 S R n G g s N H O X Y E s a 1 s L t S f s c 0 4 2 h / w 7 M h B L N P / k u u D 9 u B 5 Z d H r d O z K o 4 G 2 S a 7 5 I A E 5 J i c k g v S I V 3 C y S v 5 c G p O 3 X l 3 X X f R X f q y u k 7 V s 0 l + w d 3 6 B D 3 r s u E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 C i J G k U z V J Y 1 U X 7 t 3 q O p K M F c J T o = " &gt; A A A C W n i c b V F d S 8 M w F E 2 r b r N + z Y 8 3 X 6 J D 8 W m 0 I u i j 6 I u P E 5 w K 6 x h p e j e D a V q T W 3 W U / k l f R P C v C G a z i E 5 P C B z O P Z d 7 c x J l U h j 0 / T f H n Z t f q N U b i 9 7 S 8 s r q W n N 9 4 9 q k u e b Q 5 a l M 9 W 3 E D E i h o I s C J d x m G l g S S b i J 7 s 8 n 9 Z t H 0 E a k 6 g r H G f Q T N l J i K D h D K w 2 a D 2 E E I 6 G K K G G o x X P p U Y s Q 4 R m L S K d P q q T 7 4 c 7 k V C I f 2 0 k x 6 J K G 4 Q + v Z H o E J Z 0 x J 4 B M l t Q L Q c X f E w b N l t / 2 p 6 B / S V C R F q n Q G T R f w j j l e Q I K u W T G 9 A I / w 3 7 B N A o u o f T C 3 E D G + D 0 b Q c 9 S x R I w / W I a T U n 3 r B L T Y a r t V U i n 6 s + O g i X G j J P I O u 1 + d 2 a 2 N h H / q / V y H J 7 0 C 6 G y H E H x r 0 H D X F J M 6 S R n G g s N H O X Y E s a 1 s L t S f s c 0 4 2 h / w 7 M h B L N P / k u u D 9 u B 5 Z d H r d O z K o 4 G 2 S a 7 5 I A E 5 J i c k g v S I V 3 C y S v 5 c G p O 3 X l 3 X X f R X f q y u k 7 V s 0 l + w d 3 6 B D 3 r s u E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 C i J G k U z V J Y 1 U X 7 t 3 q O p K M F c J T o = " &gt; A A A C W n i c b V F d S 8 M w F E 2 r b r N + z Y 8 3 X 6 J D 8 W m 0 I u i j 6 I u P E 5 w K 6 x h p e j e D a V q T W 3 W U / k l f R P C v C G a z i E 5 P C B z O P Z d 7 c x J l U h j 0 / T f H n Z t f q N U b i 9 7 S 8 s r q W n N 9 4 9 q k u e b Q 5 a l M 9 W 3 E D E i h o I s C J d x m G l g S S b i J 7 s 8 n 9 Z t H 0 E a k 6 g r H G f Q T N l J i K D h D K w 2 a D 2 E E I 6 G K K G G o x X P p U Y s Q 4 R m L S K d P q q T 7 4 c 7 k V C I f 2 0 k x 6 J K G 4 Q + v Z H o E J Z 0 x J 4 B M l t Q L Q c X f E w b N l t / 2 p 6 B / S V C R F q n Q G T R f w j j l e Q I K u W T G 9 A I / w 3 7 B N A o u o f T C 3 E D G + D 0 b Q c 9 S x R I w / W I a T U n 3 r B L T Y a r t V U i n 6 s + O g i X G j J P I O u 1 + d 2 a 2 N h H / q / V y H J 7 0 C 6 G y H E H x r 0 H D X F J M 6 S R n G g s N H O X Y E s a 1 s L t S f s c 0 4 2 h / w 7 M h B L N P / k u u D 9 u B 5 Z d H r d O z K o 4 G 2 S a 7 5 I A E 5 J i c k g v S I V 3 C y S v 5 c G p O 3 X l 3 X X f R X f q y u k 7 V s 0 l + w d 3 6 B D 3 r s u E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 C i J G k U z V J Y 1 U X 7 t 3 q O p K M F c J T o = " &gt; A A A C W n i c b V F d S 8 M w F E 2 r b r N + z Y 8 3 X 6 J D 8 W m 0 I u i j 6 I u P E 5 w K 6 x h p e j e D a V q T W 3 W U / k l f R P C v C G a z i E 5 P C B z O P Z d 7 c x J l U h j 0 / T f H n Z t f q N U b i 9 7 S 8 s r q W n N 9 4 9 q k u e b Q 5 a l M 9 W 3 E D E i h o I s C J d x m G l g S S b i J 7 s 8 n 9 Z t H 0 E a k 6 g r H G f Q T N l J i K D h D K w 2 a D 2 E E I 6 G K K G G o x X P p U Y s Q 4 R m L S K d P q q T 7 4 c 7 k V C I f 2 0 k x 6 J K G 4 Q + v Z H o E J Z 0 x J 4 B M l t Q L Q c X f E w b N l t / 2 p 6 B / S V C R F q n Q G T R f w j j l e Q I K u W T G 9 A I / w 3 7 B N A o u o f T C 3 E D G + D 0 b Q c 9 S x R I w / W I a T U n 3 r B L T Y a r t V U i n 6 s + O g i X G j J P I O u 1 + d 2 a 2 N h H / q / V y H J 7 0 C 6 G y H E H x r 0 H D X F J M 6 S R n G g s N H O X Y E s a 1 s L t S f s c 0 4 2 h / w 7 M h B L N P / k u u D 9 u B 5 Z d H r d O z K o 4 G 2 S a 7 5 I A E 5 J i c k g v S I V 3 C y S v 5 c G p O 3 X l 3 X X f R X f q y u k 7 V s 0 l + w d 3 6 B D 3 r s u E = &lt; / l a t e x i t &gt; ? red cube large rubber &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H l b R p z + e Y C r m K p c p G c b x d l x S N n M = " &gt; A A A C V X i c b V F d S 8 M w F E 3 r d / 2 a + u h L d C g + S S u C P g 5 9 8 V H B O W E d I 0 n v Z j B N S 3 I r j t I / u R f x n / g i m M 0 i u n l C 4 H D u u c n N C c + V t B i G 7 5 6 / s L i 0 v L K 6 F q x v b G 5 t N 3 Z 2 H 2 x W G A F t k a n M P H J m Q U k N b Z S o 4 D E 3 w F K u o M O f r y f 1 z g s Y K z N 9 j 6 M c e i k b a j m Q g q G T + g 0 V c x h K X f K U o Z G v V U A d Y o R X L A 0 k F T 2 O D y a r l k T B o a J x / M u l m B k 6 b c Z o C s 7 B V D S I Q S c / h / c b z f A 0 n I L O k 6 g m T V L j t t 8 Y x 0 k m i h Q 0 C s W s 7 U Z h j r 2 S G Z R C Q R X E h Y W c i W c 2 h K 6 j m q V g e + U 0 l Y o e O S W h g 8 y 4 r Z F O 1 d 8 d J U u t H a X c O d 1 8 T 3 a 2 N h H / q 3 U L H F z 2 S q n z A k G L 7 4 s G h a K Y 0 U n E N J E G B K q R I 0 w Y 6 W a l 4 o k Z J t B 9 R O B C i G a f P E 8 e z k 4 j x + / O m 6 2 r O o 5 V s k 8 O y Q m J y A V p k R t y S 9 p E k D H 5 8 D z P 9 9 6 8 T 3 / R X / 6 2 + l 7 d s 0 f + w N / + A l 7 B s X U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H l b R p z + e Y C r m K p c p G c b x d l x S N n M = " &gt; A A A C V X i c b V F d S 8 M w F E 3 r d / 2 a + u h L d C g + S S u C P g 5 9 8 V H B O W E d I 0 n v Z j B N S 3 I r j t I / u R f x n / g i m M 0 i u n l C 4 H D u u c n N C c + V t B i G 7 5 6 / s L i 0 v L K 6 F q x v b G 5 t N 3 Z 2 H 2 x W G A F t k a n M P H J m Q U k N b Z S o 4 D E 3 w F K u o M O f r y f 1 z g s Y K z N 9 j 6 M c e i k b a j m Q g q G T + g 0 V c x h K X f K U o Z G v V U A d Y o R X L A 0 k F T 2 O D y a r l k T B o a J x / M u l m B k 6 b c Z o C s 7 B V D S I Q S c / h / c b z f A 0 n I L O k 6 g m T V L j t t 8 Y x 0 k m i h Q 0 C s W s 7 U Z h j r 2 S G Z R C Q R X E h Y W c i W c 2 h K 6 j m q V g e + U 0 l Y o e O S W h g 8 y 4 r Z F O 1 d 8 d J U u t H a X c O d 1 8 T 3 a 2 N h H / q 3 U L H F z 2 S q n z A k G L 7 4 s G h a K Y 0 U n E N J E G B K q R I 0 w Y 6 W a l 4 o k Z J t B 9 R O B C i G a f P E 8 e z k 4 j x + / O m 6 2 r O o 5 V s k 8 O y Q m J y A V p k R t y S 9 p E k D H 5 8 D z P 9 9 6 8 T 3 / R X / 6 2 + l 7 d s 0 f + w N / + A l 7 B s X U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H l b R p z + e Y C r m K p c p G c b x d l x S N n M = " &gt; A A A C V X i c b V F d S 8 M w F E 3 r d / 2 a + u h L d C g + S S u C P g 5 9 8 V H B O W E d I 0 n v Z j B N S 3 I r j t I / u R f x n / g i m M 0 i u n l C 4 H D u u c n N C c + V t B i G 7 5 6 / s L i 0 v L K 6 F q x v b G 5 t N 3 Z 2 H 2 x W G A F t k a n M P H J m Q U k N b Z S o 4 D E 3 w F K u o M O f r y f 1 z g s Y K z N 9 j 6 M c e i k b a j m Q g q G T + g 0 V c x h K X f K U o Z G v V U A d Y o R X L A 0 k F T 2 O D y a r l k T B o a J x / M u l m B k 6 b c Z o C s 7 B V D S I Q S c / h / c b z f A 0 n I L O k 6 g m T V L j t t 8 Y x 0 k m i h Q 0 C s W s 7 U Z h j r 2 S G Z R C Q R X E h Y W c i W c 2 h K 6 j m q V g e + U 0 l Y o e O S W h g 8 y 4 r Z F O 1 d 8 d J U u t H a X c O d 1 8 T 3 a 2 N h H / q 3 U L H F z 2 S q n z A k G L 7 4 s G h a K Y 0 U n E N J E G B K q R I 0 w Y 6 W a l 4 o k Z J t B 9 R O B C i G a f P E 8 e z k 4 j x + / O m 6 2 r O o 5 V s k 8 O y Q m J y A V p k R t y S 9 p E k D H 5 8 D z P 9 9 6 8 T 3 / R X / 6 2 + l 7 d s 0 f + w N / + A l 7 B s X U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H l b R p z + e Y C r m K p c p G c b x d l x S N n M = " &gt; A A A C V X i c b V F d S 8 M w F E 3 r d / 2 a + u h L d C g + S S u C P g 5 9 8 V H B O W E d I 0 n v Z j B N S 3 I r j t I / u R f x n / g i m M 0 i u n l C 4 H D u u c n N C c + V t B i G 7 5 6 / s L i 0 v L K 6 F q x v b G 5 t N 3 Z 2 H 2 x W G A F t k a n M P H J m Q U k N b Z S o 4 D E 3 w F K u o M O f r y f 1 z g s Y K z N 9 j 6 M c e i k b a j m Q g q G T + g 0 V c x h K X f K U o Z G v V U A d Y o R X L A 0 k F T 2 O D y a r l k T B o a J x / M u l m B k 6 b c Z o C s 7 B V D S I Q S c / h / c b z f A 0 n I L O k 6 g m T V L j t t 8 Y x 0 k m i h Q 0 C s W s 7 U Z h j r 2 S G Z R C Q R X E h Y W c i W c 2 h K 6 j m q V g e + U 0 l Y o e O S W h g 8 y 4 r Z F O 1 d 8 d J U u t H a X c O d 1 8 T 3 a 2 N h H / q 3 U L H F z 2 S q n z A k G L 7 4 s G h a K Y 0 U n E N J E G B K q R I 0 w Y 6 W a l 4 o k Z J t B 9 R O B C i G a f P E 8 e z k 4 j x + / O m 6 2 r O o 5 V s k 8 O y Q m J y A V p k R t y S 9 p E k D H 5 8 D z P 9 9 6 8 T 3 / R X / 6 2 + l 7 d s 0 f + w N / + A l 7 B s X U = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of data efficiency and convergence speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FilterFigure 5 :</head><label>5</label><figDesc>Failure cases of our Det setting on Condition B of CoGenT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Filter</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7</head><label>7</label><figDesc>Figure 7: Failure cases caused by occluded objects. We show the high-resolution images here, and we can see that 1) in the left image, there is a small blue cube behind the large blue cube occluded; 2) in the right image, there is a red cylinder behind the large brown cylinder occluded. These occluded objects do not have corresponding dots in the reasoning results, leading to a wrong prediction "No" while the actual answer is "Yes".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Our composite modules (the top section) and output modules (the bottom section). MLP() consists of several linear and ReLU layers.</figDesc><table><row><cell>Modules</cell><cell>In ? Out</cell><cell>Implementation</cell></row><row><cell>Intersect</cell><cell>a 1 , a 2 ? a</cell><cell>And(a 1 , a 2 )</cell></row><row><cell>Union</cell><cell>a 1 , a 2 ? a</cell><cell>Or(a 1 , a 2 )</cell></row><row><cell>Filter</cell><cell>a, q ? a</cell><cell>And(a, AttendNode(q))</cell></row><row><cell>Same</cell><cell>a, q ? a</cell><cell>Filter(Not(a), Describe(a, q))</cell></row><row><cell>Relate</cell><cell>a, q ? a</cell><cell>Transfer(a, AttendEdge(q))</cell></row><row><cell>Exist Count</cell><cell>a ? h</cell><cell>MLP( i a i )</cell></row><row><cell>Compare</cell><cell>h 1 , h 2 ? h</cell><cell>MLP(h 1 ? h 2 )</cell></row><row><cell>Describe</cell><cell>a, q ? h</cell><cell>Eq. (8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons between neural module networks on the CLEVR dataset. Top section: results of the official test set; Bottom section: results of the validation set (we can only evaluate our GT setting on the validation set since the annotations of the test set are not public</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons between NMNs on CLEVR-CoGenT. Top section: results of the test set; Bottom section: results of the validation set. Using the ground-truth scene graphs, our XNMs generalize very well and do not suffer from shortcuts at all.</figDesc><table><row><cell>Method</cell><cell>Program</cell><cell cols="2">Condition A Condition B</cell></row><row><cell>PG+EE [13]</cell><cell>supervised</cell><cell>96.6</cell><cell>73.7</cell></row><row><cell>TbD-net [18]</cell><cell>supervised</cell><cell>98.8</cell><cell>75.4</cell></row><row><cell>XNM-Det</cell><cell>supervised</cell><cell>98.1</cell><cell>72.6</cell></row><row><cell cols="2">NS-VQA [27] supervised</cell><cell>99.8</cell><cell>63.9</cell></row><row><cell>XNM-Det</cell><cell>supervised</cell><cell>98.2</cell><cell>72.1</cell></row><row><cell>XNM-Det</cell><cell>GT</cell><cell>98.3</cell><cell>72.2</cell></row><row><cell>XNM-GT</cell><cell>supervised</cell><cell>99.9</cell><cell>99.9</cell></row><row><cell>XNM-GT</cell><cell>GT</cell><cell>100</cell><cell>100</cell></row><row><cell cols="2">of both conditions.</cell><cell></cell><cell></cell></row><row><cell cols="4">Goals. Q1: Can our model perform well when meeting</cell></row><row><cell cols="4">the novel attribute combinations? Q2: If not, what actually</cell></row><row><cell cols="2">causes the reasoning shortcut?</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Filter [small] Filter [yellow] Filter [metal] Same [shape] Filter [small] Filter [metal] Query [color] Anser:cyan Filter [large] Filter [gray] Filter [cube] Relate [behind] Filter [large] Filter [small] Filter [brown] Filter [rubber] Filter [sphere]</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Single-model results on VQAv2.0 validation set and test set. ?: values reported in the original papers.</figDesc><table><row><cell>Method</cell><cell>expert layout</cell><cell>validation(%)</cell><cell>test(%)</cell></row><row><cell>Up-Down [2]</cell><cell>no</cell><cell>63.2  ?</cell><cell>66.3</cell></row><row><cell>N2NMN [10]</cell><cell>yes</cell><cell>-</cell><cell>63.3  ?</cell></row><row><cell>StackNMN [9]</cell><cell>no</cell><cell>-</cell><cell>64.1  ?</cell></row><row><cell>XNMs</cell><cell>no</cell><cell>64.7</cell><cell>67.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Filter [green] Filter [rubber] Filter [cylinder] Relate [behind] Filter [small] Filter [rubber] Filter [cylinder] Relate [left] Filter [small] Filter [cylinder] Relate [behind] Filter [large] Filter [red] Filter [metal] Count</head><label></label><figDesc>Question: There is a tiny cylinder left of the tiny matte cylinder that is behind the green rubber cylinder ; how many big red metal things are behind it ?</figDesc><table><row><cell>Filter</cell><cell>Filter</cell><cell>Relate</cell><cell>Filter</cell><cell>Filter</cell></row><row><cell>[gray]</cell><cell>[rubber]</cell><cell>[behind]</cell><cell>[brown]</cell><cell>[cube]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Union</cell><cell>Count</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Prediction:</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ground Truth: 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Filter</cell><cell>Filter</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[small]</cell><cell>[sphere]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Prediction:</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Ground Truth: 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>[small] Filter [metal] Filter [cylinder] Relate [front] Filter [cyan] Relate [behind]</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Intersect</cell><cell>Describe [size]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Prediction: small</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ground Truth:large</cell></row><row><cell>Filter</cell><cell>Filter</cell><cell></cell></row><row><cell>[small]</cell><cell>[sphere]</cell><cell></cell></row><row><cell></cell><cell>Union</cell><cell>Filter [metal]</cell><cell>Count</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Prediction: 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ground Truth:3</cell></row><row><cell>Filter</cell><cell>Filter</cell><cell></cell></row><row><cell>[large]</cell><cell>[brown]</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our codes are public at https://github.com/shijx12/ XNM-Net</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The work is supported by NSFC key projects (U1736204, 61661146007, 61533018), Ministry of Education and China Mobile Research Fund (No. 20181770250), THUNUS NExT Co-Lab, and Alibaba-NTU JRI.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>In both of the GT and Det experiments on the CLEVR dataset, we set the dimension of the label embedding (i.e., d) and the dimension of h in all output modules to 128. The classifier consists of a simple multi-layer perceptron that maps the 128-dimentional features to the number of possible answers (i.e., <ref type="bibr" target="#b27">28)</ref>, and a softmax layer. We used Adam <ref type="bibr" target="#b14">[15]</ref> optimizer with an initial learning rate 0.001 to train our module parameters. We trained for 5 epochs for the GT setting and 10 epochs for the Det setting, and we reduced the learning rate to 0.0001 after the first epoch. Each epoch takes about one hour with an Nvidia 1080Ti graphic card.</p><p>The mapping matrices of the Describe module are implemented differently between the GT and Det setting. In the GT setting, as each object has four attribute values corresponding to four attribute categories (i.e., color, shape, size, material), and our node embedding is the concatenation of attribute label embeddings, the dimension of node embeddings is 4d, where d is the dimension of label embeddings. We fix the order of label embedding concatenation as [color, shape, size, material], so we can extract node i's color feature by</p><p>identity matrix and zero matrix respectively. So in the GT setting, our four mapping matrixes are defines as:</p><p>In the Det setting, we regard M k ? R d?d as parameters and learn them automatically, which leads to an increase of the number of parameters (Det has 0.55M parameters while Gt only has 0.22M).</p><p>As for the attention functions in the CLEVR GT experiments, besides the label-space softmax which is mentioned in Eq. 5, we have also tried the sigmoid activation. Specifically, we fused multiple label vectors into one vector via a fully connected layer, and then applied the sigmoid function like Eq. 6 to separately compute attention weights of each node and edge. Using this attention strategy, we can still obtain 100% accuracy in the GT setting, demonstrating that our model is robust and flexible.</p><p>In the VQAv2.0 dataset, we selected the most frequent 3000 answers from the training set, and predicted the target answer from these candidates. We used an LSTM as the question encoder and fused the 1024-dimensional question embedding with the output feature from our module network for the answer classification. For the training, we used Adam optimizer with an initial learning rate 0.0008 and we set batch size as 256. The learning rate was decayed by half every 50000 training steps and the training lasted for 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Failure Cases of the CLEVR Det Setting</head><p>We classify the failure cases in the CLEVR Det setting into three categories:</p><p>1. The coordinate detection is inaccurate ( <ref type="figure">Figure 6</ref>).</p><p>2. Some objects are occluded <ref type="figure">(Figure 7</ref>). <ref type="figure">(Figure 8)</ref>. Specifically, when we propose objects based on the generated mask from the "Attend" modules of <ref type="bibr" target="#b17">[18]</ref>, we may wrongly propose more or less objects due to the blurring boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The mask-based object division is inaccurate</head><p>In all of <ref type="figure">Figure 6</ref>, 7, and 8, we mark the reasoning steps that cause mistakes in red box. We can see that using our X reasoning over scene graphs, we can easily track the reasoning process and diagnose where and why the model makes a mistake, which is an inspiring step towards the explainable AI.</p><p>C. Case Study on the VQAv2.0</p><p>In the VQAv2.0 experiments, we predict a probability distribution over our modules at each step, and then feed the soft fusion of their outputs into next step. We set the reasoning length to 3 and force the last module to be Describe. We show a typical sample of the VQAv2.0 dataset in <ref type="figure">Figure 9</ref>. The top row is the modules with the most probability at each step, while the bottom row shows the results of Relate at Step 2. We can see that even though the question "what is the man wearing on his head" explicitly requires the relationship reasoning (i.e., find the "man" first, and then move the focus to his head), our model scarcely relies on the results of the Relate module. Instead, it can directly focus on the "head" and give the correct prediction. We think this is due to the simplicity of questions, which is a shortcoming of the VQAv2.0 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AttendNode</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AttendNode Describe</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relate</head><p>Question: what is the man wearing on his head ?</p><p>Step 1</p><p>Step 2</p><p>Step 3</p><p>Hat <ref type="figure">Figure 9</ref>: A typical case of the VQAv2.0 dataset. The top row lists the modules with the maximum probability at each step. We can see that even the question "what is the man wearing on his head" explicitly requires the relationship understanding, the module Relate is still not necessary as the target region can be directly found. We argue the simplicity of question annotations is a major shortcoming of the VQAv2.0 dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scene dynamics: Counterfactual critic multiagent training for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02347</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explainable neural computation via stack neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transparency by design: Closing the gap between performance and interpretability in visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mascharka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soklaski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Logical versus analogical or symbolic versus connectionist or neat versus scruffy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Minsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page">484</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<title level="m">Graphstructured representations for visual question answering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Obj2text: Generating visually descriptive language from object layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning to count objects in natural images for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pr?gel-Bennett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05766</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
