<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Dong</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
							<email>lutong@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<email>lmwang@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Understanding</term>
					<term>Temporal Action Detection</term>
					<term>Baseline</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action detection (TAD) is extensively studied in the video understanding community by following the object detection pipelines in images. However, complex designs are not uncommon in TAD, such as two-stream feature extraction, multi-stage training, complex temporal modeling, and global context fusion. In this paper, we do not aim to introduce any novel technique for TAD. Instead, we study a simple, straightforward, yet must-known baseline given the current status of complex design and low efficiency in TAD. In our simple baseline (BasicTAD), we decompose the TAD pipeline into several essential components: data sampling, backbone design, neck construction, and detection head. We empirically investigate the existing techniques in each component for this baseline and, more importantly, perform end-to-end training over the entire pipeline thanks to the simplicity in design. Our BasicTAD yields an astounding RGB-Only baseline very close to the state-of-the-art methods with two-stream inputs. In addition, we further improve the BasicTAD by preserving more temporal and spatial information in network representation (termed as BasicTAD Plus). Empirical results demonstrate that our BasicTAD Plus is very efficient and significantly outperforms the previous methods on the datasets of THUMOS14 and FineAction. Our approach can serve as a strong baseline for TAD. The code will be released at https://github.com/MCG-NJU/BasicTAD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video understanding is a fundamental and challenging problem in computer vision research. Temporal action detection (TAD) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11]</ref> is particularly important for long-term video understanding, which aims to localize the temporal interval of each action instance in an untrimmed video and recognize its action class. Due to the high complexity in video and lack of a clear definition of action : Equal contribution. : Corresponding author. arXiv:2205.02717v1 [cs.CV] 5 May 2022 boundaries, past efforts on TAD often employ a relatively sophisticated paradigm to solve this problem. For example, they typically use two-stream inputs for feature extraction in advance <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44]</ref>, a multi-stage training strategy for different components <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36]</ref>, temporal reasoning with complex models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b4">5]</ref>, and yielding the detection results with the global classification scores <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref> (e.g., UntrimmedNet <ref type="bibr" target="#b41">[42]</ref>). Unlike object detectors in images, these complex designs prevent the existing TAD methods from being a simple and neat detection framework, which could be easily trained in an end-to-end manner and efficiently deployed in real-world applications.</p><p>Given the current status of complex designs and low efficiency in TAD, there is a need to step back and reconsider the basic principle for designing an efficient TAD framework. To obtain both high accuracy and efficiency, simplicity is the most important requirement for building a TAD method. Inspired by the great success of object detectors, we argue that an optimal action detector should also strictly follow a similar modular design, where each component has its own function, and different components could be easily integrated together. In addition, efficient training is another important property that needs to be considered. Multi-stage training often brings extra computational and storage costs, cannot fully unleash the potential of data. We argue that an optimal action detector should be trainable in an end-to-end manner with an easy training recipe. Finally, being free of pre-processing is another desired property for action detector design and, in particular, important for deployment in real applications. We argue that optical flow is the bottleneck for many TAD methods as it often requires a high computational cost to calculate these inputs explicitly. An RGB-only TAD approach should be more practical for video understanding applications.</p><p>Based on the above analysis, we find it is urgent to re-design a simple baseline for TAD to meet the above requirements. In this paper, our goal is not to introduce any new technique for building a TAD framework. Instead, we study a simpler, straightforward, yet must-known baseline for the TAD task. In contrast to the complex designs in the existing TAD frameworks, we carefully design a modular temporal detection framework, which allows us to easily perform in-depth studies for different components, including data augmentation, backbone, neck, and detection head. While the design for standard object detectors has been highly mature and robust, the good practice for building a temporal detector in videos is yet to be built. For each module design, we choose the most simple and basic options and perform extensive investigation over them to discover the optimal configurations. In addition, we investigate different data sampling and augmentation techniques during training and testing for building an effective TAD method, which might be largely ignored in previous methods. Based on our modular design and extensive empirical investigation, we obtain an astounding RGB-only baseline for TAD, termed as BasicTAD. Our BasicTAD yields an very competitive performance close to the state-of-the-art methods with two-stream inputs. This good performance confirms that some basic design <ref type="table">Table 1</ref>. Comparison of end-to-end TAD methods only with RGB input. We categorize components and settings based on their order in the whole pipeline: i) Data Stream: modal, resolution in temporal and spatial; ii)Network: The backbone with ? times temporal down-sampling(denotes as x?) for feature extraction, the Neck for feature aggregation, and the Head for detecting temporal action segments. AF and AB in Head mean two types of our methods: the anchor-free and the anchor-based methods. <ref type="bibr" target="#b16">[17]</ref> here only use RGB stream to enable a true end-to-end approach.</p><p>Method <ref type="bibr" target="#b45">[46]</ref> [17] <ref type="bibr" target="#b39">[40]</ref> BasicTAD BasicTAD Plus of TAD requires reconsideration, and the research efforts on TAD should focus on more fundamental aspects instead of coming up with sophisticated modules. Encouraged by the outstanding performance of BasicTAD, we strictly follow the basic principle of designing an ideal TAD method, and further improve it with minimal changes. The core idea is to preserve more rich temporal information in our backbone and spatial information in our neck. The implementation is quite simple to achieve these objectives by removing the temporal downsampling operations in the backbone and exchanging the spatial and temporal pooling operations in the neck. These small changes would significantly improve the performance of BasicTAD, increasing the performance from 51.9 to 59.5 on the THUMOS-14 dataset. The resulted TAD method is denoted by BasicTAD Plus, and we ascribe to its good performance to our core idea of keeping rich information and careful design in implementation. We believe our work is timely and should draw the whole TAD community to reconsider the design of TAD methods given the current status of complex design and low efficiency. The basic comparisons of our BasicTAD with previous end-to-end TAD methods are summarized in <ref type="table">Table 1</ref>. In summary, our contributions are threefold:</p><p>-We reconsider the TAD pipeline and present a simple modular detection framework. For each component in our modular framework, we perform extensive studies over the existing basic choices. Through extensive empirical studies, we come up with good practice to build an astounding RGB-Only baseline method for TAD, termed as BasicTAD.</p><p>-Encouraged by the outstanding performance of BasicTAD, we further improve it with minimal change to fully unleash the power of deep networks. Our core idea is to preserve more rich information during our backbone feature extraction and neck compression. This idea could be easily implemented with high efficiency and resulted in BasicTAD Plus significantly improving the TAD performance on the THUMOS-14 dataset. -The extensive experiments on THUMOS-14 and FineAction demonstrate that BasicTAD Plus outperforms previous state-of-the-art methods by a large margin. In particular, we obtain an average mAP of 59.5 on the THUMOS-14 only with RGB input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Action Recognition. Action recognition is an essential task in video understanding. It performs spatio-temporal modeling on video frames to recognize actions in the video. Current deep learning-based action recognition methods can be mainly divided into three types. 2D CNN methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">41]</ref> take RGB frames and optical flow as input to capture appearance and motion information, respectively. 3D CNN methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> capture spatio-temporal information between frames by performing 3D convolution on stacked video frames. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b38">39]</ref> model spatio-temporal features by decoupling 2D and 1D convolutions for reducing computing overhead. In this work, we use the SlowOnly <ref type="bibr" target="#b6">[7]</ref> to generate the spatio-temporal feature of the video.</p><p>One-stage Temporal Action Detection. Some recent works on one-stage TAD, detecting the boundaries and category of action segments in a single shot without using action proposals. The existing methods are divided into anchorbased, anchor-free, and query-based. <ref type="bibr" target="#b19">[20]</ref> presented the first one-stage TAD method using convolutional networks. <ref type="bibr" target="#b28">[29]</ref> proposed to use Gaussian kernels to optimize the scale of each anchor dynamically. Our work selects anchor-based and anchor-free methods as two alternative branches as detection heads.</p><p>Two-stage Temporal Action Detection. A two-stage method for TAD first generates candidate action proposals, further classifying them into action categories, and refining their temporal boundaries. Several previous works focused on action proposal generation. The boundary-based Methods, including <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5]</ref>, predict each frame's start and end confidence, then match the frames with start and end confidences to generate the proposals and evaluate their confidence. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref> generate proposals based on a pre-defined sliding window or anchors and train a classifier to filter anchors. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref> adopt different query-based dynamic networks to generate a sparse action proposal set directly, effectively omitting post-processing steps.</p><p>Training Strategies for Temporal Action Detection. There are various training strategies for TAD. The entire training process of TAD is usually split into multiple independent steps to reduce the difficulty of optimization. Mainstream training strategies can be mainly divided into two types. The first type of training strategy consists of two steps. Firstly, the network pre-trained on TAD Dataset or Action Recognition Dataset, performs sliding-window feature extraction on the TAD dataset. Then, a separate network generates proposals and directly merges them with the video-level classification results to obtain the final result or directly predict boundaries and category of the action segment. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref> all adopted this training strategy. The second type of strategy is directly training the network end-to-end from RGB frames or both RGB frames and optical flow extracted from corresponding RGB frames. Recently, several works, including <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40]</ref>, have adopted this training strategy. But their experimental settings are diverse and complex, and our work is to integrate and explore a simple, general, scalable, end-to-end training framework for TAD only using RGB-Stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we reconsider the design of the TAD pipeline by strictly following the basic principles of simplicity, efficient training, and being free of complex preprocessing. Specifically, we present a modular design for the TAD framework, which is composed of data sampling (augmentation), backbone design, neck construction, and detection head. Each component has its own function, and they could be easily integrated together to yield a simple TAD framework. For each component, we investigate the basic options and perform extensive studies to figure out the optimal configurations, termed as BasicTAD. Based on the Ba-sicTAD, we further present some effective and simple extensions with the core idea of preserving rich information in backbone and neck design. The resulting TAD framework is denoted by BasicTAD Plus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BasicTAD</head><p>Aiming at discovering generic frameworks for facilitating the analysis and development of TAD methods, we confine our search to commonly-adopted elements such as temporal down-sampling networks and temporal feature pyramid networks. An overview of BasicTAD is depicted in <ref type="figure" target="#fig_0">Fig 1.</ref> Data Augmentation. The frames extracted from videos need to be augmented before being fed into the network to increase the robustness and help prevent overfitting. Since the task of TAD relies on mining the clues of scenes in frames, we follow <ref type="bibr" target="#b39">[40]</ref> and adopt various image-based data augmentation methods in our BasicTAD, including Spatial Random Crop, Photo Distortion, Spatial Random Rotation, and Spatial Random Flip. Backbone. When the frames have been augmented, we need to feed frames into the backbone module. Like the image classification backbone, which extracts spatial features in object detection tasks, the action recognition backbone is utilized to extract spatiotemporal features from consecutive input frames. The current widely adopted backbones are I3D <ref type="bibr" target="#b2">[3]</ref> and C3D <ref type="bibr" target="#b36">[37]</ref>. Inside these backbones, eight-fold temporal down-sampling operations reduce the temporal dimension of video features for saving computational overhead and increasing the receptive field to capture actions at larger scales. BasicTAD also adopts this setting for the backbone module.</p><p>Neck. The neck module plays a vital role in downstream tasks. With multistage spatiotemporal down-sampling, video frames are transformed into multiscale spatiotemporal features. For the feature F 3d ? R C?T ?H?W , we squeeze the H and W dimensions using spatial average pooling to get the temporal feature F 1d ? R C?T . The neck is used to perform feature aggregation for the features extracted by the backbone. The most popular necks are the temporal downsampling module(TDM) and the temporal feature pyramids network(TFPN). Due to different ways to use features in TAD, we divide the existing necks into three types: TFPN from backbone, TDM after backbone, and TDM-TFPN after backbone. We will analyze their advantages and disadvantages one by one to explain our choice reasonably.</p><p>TFPN from backbone It is the most natural way to construct TFPN directly by using multi-scale spatiotemporal features taken from different stages of backbone, just like what FPN does in object detection. <ref type="bibr" target="#b43">[44]</ref> uses this method to perform a multi-scale fusion of Two-Stream to obtain more robust multi-scale features. However, we believe that this approach will not perform well when we only use RGB-stream due to the lack of local motion information provided by optical flow. Although shallow layers have a higher temporal resolution, the image features at each temporal location can not capture enough rich temporal semantic information, which will harm the TAD task. TDM after backbone Some works like <ref type="bibr" target="#b19">[20]</ref> use TDM to additionally generate multi-scale temporal features on pre-extracted features (1D temporal features from pooling the output feature of the last layer of the backbone).</p><p>Although the last layer of backbone features is used as the source of multiscale features to ensure rich semantic information at each temporal location, the small-scale temporal features at the bottom of the pyramid structure lack sufficient temporal context, making it challenging to detect short actions accurately. TDM-TFPN after backbone Constructing TDM and TFPN after the backbone will simultaneously solve the above two problems. TFPN integrates high-level semantic information into low-level features. This method increases the receptive field of temporal features and enhances the ability of temporal context aggregation while ensuring sufficient spatial semantics.</p><p>Head. The detection head is an essential factor in determining the detection effect. It is composed of sub-networks and sample allocation mechanisms. Welldesigned sub-networks and reasonable sample allocation mechanisms complement each other to effectively utilize the features extracted before and obtain high-quality action proposals. The most popular detection heads are mainly divided into anchor-based and anchor-free methods. Although these two detection heads are dense detection methods, they have been verified by the industry for a long time, which is enough to prove the robustness of these two methods. Since these two detection methods have their advantages and disadvantages, there is no conclusion as to which of them is better. So which method is better is not the focus of this article. We use these two methods to verify the effectiveness of the construction methods of BasicTAD. So we use both two detection methods in our BasicTAD. We will briefly introduce these two methods and give our settings of them.</p><p>Anchor-based Method. Anchor-based methods generate temporal proposals by assigning dense and multi-scale intervals with pre-defined lengths to uniformly distributed temporal locations in the input video. We use translation-invariant anchors in this work, and the anchors have the increasing temporal size from the bottom to the top of the feature pyramid network. At each level, we add anchors with 5 sizes 2 0 , 2 1/5 , 2 2/5 , 2 3/5 , 2 4/5 of the original set of default anchors for dense scale coverage. Anchors are assigned to ground-truth action segments using the temporal Intersection-over-Union (tIoU) threshold 0.6 and background with the tIoU lower than 0.4. Other anchors with the overlap in [0.4, 0.6) will be eliminated during training.</p><p>Anchor-free Method.</p><p>Anchor-free methods directly regress the offsets to action boundaries at each temporal location and then use these offsets to generate temporal proposals. We first compute the regression offsets for each location on all feature levels. Any location which falls into any ground-truth box will be set as positive samples. The others are negative samples. Each level is responsible for a range of motion detection. If an action proposal at one temporal location is beyond this range, this location will be ignored.</p><p>Here m i is the maximum range border that the feature level i needs to regress. In this work, m 2 , m 3 , m 4 , m 5 , m 6 , m 7 are set as -1,5,10,20,40 and ?, respectively. Even with multi-level prediction, if a location is still assigned to more than one ground-truth box, we choose the ground-truth box with the minimal area as its target.</p><p>We train BasicTAD with both heads in the form of a multitask loss function, including a classification loss L cls , a regression loss L reg .</p><formula xml:id="formula_0">L = L cls + ?L reg .<label>(1)</label></formula><p>We use focal loss <ref type="bibr" target="#b21">[22]</ref> as classification loss and DIoU loss <ref type="bibr" target="#b49">[50]</ref> as regression loss, and weight parameter ? is set to 1. We use four temporal convolutional layers with the kernel size of 3 in both heads, whose parameters are shared on each pyramid level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">From BasicTAD to BasicTAD Plus</head><p>Benefiting from the good practices of BasicTAD, we introduce three novel design concepts for TAD tasks to form our new framework called BasicTAD Plus. We will detail our two new concepts and reasonable training and testing augmentation methods. Then explain their benefits to the TAD task.</p><p>Temporal Preservation for Backbone. Existing methods have the downsampling operation in the Backbone module to reduce the temporal dimension of video features for saving computational overhead and increasing the receptive field to capture actions at larger scales. Unlike object detection, TAD relies on the continuity of actions in the temporal dimension. So we abandon downsampling in the temporal dimension and maintain action continuity through the backbone. In this work, we choose SlowOnly <ref type="bibr" target="#b6">[7]</ref> as our Preservation backbone.</p><p>Spatial Preservation for Neck. Many works before <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40]</ref> squeeze the spatial dimension of features before feeding them into the neck module and process 1D temporal features in the neck. In our experiments of <ref type="table" target="#tab_3">Table 3</ref>, we find the most suitable neck for both heads. We use TDM-TFPN after backbone for anchor-based method and TDM after backbone for anchor-free method. The former adopts convolutional module and the latter adopts maxpooling module as basic operators. In our further experiments, We find that it is essential to preserve the spatial information, so we postpone the spatial pooling after the neck and inflate the 1D operators to 3D operators. This will introduce local spatial context and enhance the robustness of multi-scale spatiotemporal information. Regarding the choice of the basic operator, we also conducted detailed experiments. these details can be found in the supplementary material.</p><p>Detailed Data Augmentation. In the previous section, we have equipped BasicTAD with various image-based data augmentations during the training phase. To optimize our BasicTAD Plus pipeline further, we explore various data augmentation methods for the training and testing phase from the perspective of input image resolution. For the training phase, we adopted a short-side rescaling for the original image while maintaining its aspect ratio. It also prevents objects in the image from being severely deformed. The data augmentation in the testing phase mainly works on the temporal and spatial levels. We attempt to use a bidirectional sliding window to increase the density of temporal patches and combine the results of each temporal patch for post-processing. We employ two spatial-level data augmentation methods for each temporal patch, namely Spatial ThreeCrop and Spatial Flipping, individually or simultaneously. We will perform detailed experiments and analysis on this part in the ablation study section. the validation set for both FineAction and ActivityNet-v1.3. We use Avg to represent average mAP on THUMOS14, FineAction and ActivityNet-v1.3.</p><p>Implementation Details. On THUMOS14 <ref type="bibr" target="#b11">[12]</ref>, we sample RGB frames at 24 FPS and split the video into clips for BasicTAD. While for BasicTAD Plus, 3 FPS is adopted. The length of each clip T is set to 768 frames for BasicTAD and 96 frames for BasicTAD Plus. We uniformly sample clips of 32 seconds, covering over 99.7% action instances. In the training phase, we resize the origin frame resolution to 128 ? 128 for BasicTAD and short-128 (short side of frame is set to 128) for BasicTAD Plus. We set the input resolution of random crop to 112?112. The photo distortion and spatial random rotation are consistent with <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b23">24]</ref>. In the testing phase, the sampling stride between adjacent clips is set to 25% of the clip length. We adopt center crop of 112 ? 112 resolution as default setting. On FineAction <ref type="bibr" target="#b25">[26]</ref>, we sample RGB frames at 2 FPS for space constraints. In the training phase, we resize the origin frame resolution to short-256 and set the input resolution of random crop to 224 ? 224. In the testing phase, we adopt center crop of 224 ? 224 resolution as default setting. On ActivityNet-v1.3 <ref type="bibr" target="#b10">[11]</ref>, we follow the settings of [17] that we sample frames using different fps and ensure the number of video frames is 768 for each video. Thus, each video has only one clip with 768 frames. The frame resolution setting of the input is the same as FineAction. We use SlowOnly <ref type="bibr" target="#b6">[7]</ref> pre-trained on Kinetics <ref type="bibr" target="#b12">[13]</ref> as our backbone where all batch normalization layers are frozen. We train the model using SGD with momentum 0.9 and weight decay 0.0001. The batch size is set as 16. The learning rate schedule is annealing down from 0.01 to 0.0001 every 1200 iterations on THUMOS14 and 20 epochs on ActivityNet-v1.3 using the cosine decay rule. In the first 500 iterations, the learning rate linearly warms up from 0.001 to 0.01. In the post-processing phase, the candidates with classification scores smaller than 0.005 are filtered. We adopt NMS <ref type="bibr" target="#b29">[30]</ref> for anchor-free head and NMW <ref type="bibr" target="#b30">[31]</ref> for anchor-based head to eliminate the redundant action segments. The code for all the experiments would be released to facilitate the future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>This section conducts several ablation studies on THUMOS14 to investigate further the efficiency of critical components and hyper-parameter settings in our proposed BasicTAD and BasicTAD Plus. We only change the corresponding part for all experiments and use the same evaluation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study on Downsampling Locations in BasicTAD.</head><p>Existing works have down-sampling in the temporal dimension among backbones <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref> to obtain high-level abstract semantic information. To construct similar structure, our BasicTAD backbone chooses three different places to operate eight-fold downsampling to follow other works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>. We conduct corresponding experiments and the results are shown in <ref type="table">Table 2</ref>. We find that down-sampling before res2, res3, and res5 performs best among all four settings for both anchor-based and anchor-free methods. <ref type="table">Table 2</ref>. Comparison between different locations of downsampling in Ba-sicTAD. We choose four intervals between these res-layers and three for the maxpool down-sampling operation. If we choose an interval before this res-layer, we will tick it.  Comparison of Neck Design in BasicTAD. We compare three kinds of necks on both anchor-based and anchor-free methods. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. We find that TDM-TFPN after backbone is most suitable for the anchor-based method, and TDM after backbone is most suitable for the anchorfree method.</p><p>Exploring the performance of BasicTAD on different backbones. For a fair comparison, during the exploration of downsampling locations in BasicTAD, we use SlowOnly <ref type="bibr" target="#b6">[7]</ref> as our choice, we further explore to adopt C3D <ref type="bibr" target="#b36">[37]</ref> and I3D <ref type="bibr" target="#b2">[3]</ref> as the backbone. As shown in <ref type="table">Table 4</ref>, BasicTAD with I3D or C3D does not perform as well as BasicTAD with SlowOnly <ref type="bibr" target="#b6">[7]</ref>, further proving that SlowOnly is a more effective backbone when we only use the RGB stream.</p><p>The Choice of basic operator. After our study over the backbone and neck modules, we explore the impact of different basic operators on anchor-free and anchor-based to choose a more suitable basic operator. We compare the 1D convolution operator with the 1D MaxPool operator in <ref type="table">Table 5</ref>. Similar to most anchor-free methods such as <ref type="bibr" target="#b16">[17]</ref>, the 1D MaxPool operator is more suitable for anchor-free methods, and it obtains a precision improvement of 1.9 compared with the convolution operator. Anchor-based method can further improve their performance by using Convolution operations, which gains a 0.4 precision improvement when replacing MaxPool with the 1D Convolution operator. Effectiveness of Temporal Preservation for Backbone. We conduct following experiments to study the effectiveness of Temporal Preservation for backbone. As is shown in <ref type="table" target="#tab_5">Table 6</ref>, in the anchor-based method, using Temporal Preservation for Backbone with 96-frame and 192-frame is better than Basic-TAD in detection results, with the average mAP boosting 1.1% and 3.7% each, and it costs less computing overhead. In the anchor-free method, using Temporal Preservation for Backbone with 96-frame only costs half of the computational overhead and obtains comparable results with BasicTAD. When using 192 frames, the mAP is about 1% higher than BasicTAD.</p><p>Effectiveness of Spatial Preservation Neck. We conduct the experiments to study effectiveness of Spatial Preservation for Neck. As is shown in <ref type="table" target="#tab_5">Table 6</ref>, with the same other settings, using Spatial Preservation for Neck with 96-frame and 192-frame can obtain 0.8% and 0.6% mAP improvement in the anchor-free method and 0.5% and 0.3% mAP improvement in the anchor-based method, respectively.</p><p>Effectiveness of Training Augmentation. We unify the standard training augmentation of input frames. To make a fair comparison with mainstream settings, whose origin size is resized to 128 ? 128 and crop size is 112 ? 112, we try other settings as is shown in <ref type="table" target="#tab_6">Table 7</ref>. We find that when the aspect ratio of the resized image is closer to the original image, the detection results are better. The anchor-free method can obtain nearly 2% and 3% mAP improvement when origin size is resized to 171 ? 128 and short-128 in the anchor-free method. The anchor-based method also can improve about 3% and 4%. If we further enlarge the image resolution and proportionally enlarge the size of the cropped image,  we can get 5.6% mAP improvement in the anchor-free method and 6.1% mAP improvement in the anchor-based method.</p><p>Effectiveness of Testing Augmentation. We conduct ablation experiments to study the effectiveness of testing augmentation. We try to apply four kinds of testing augmentation. CenterCrop means cropping from center of frame as mentioned before. Backward means sampling from the end of the video frames back to front using a sliding window. ThreeCrop means cropping images from front, middle and back of video frames. Flip is a basic operation used in image processing. The results are shown in <ref type="table" target="#tab_7">Table 8</ref>. Using ThreeCrop can get the best results on both methods, obtaining around 0.4% and 0.3% mAP improvement in the anchor-based and anchor-free method.</p><p>The best time to fuse multiple spatial views. After our study over the effectiveness of testing augmentation, we also explore the best time to fuse multiple spatial views in each temporal patch during the testing phase. We take ThreeCrop augmentation as an example and choose four places, including after the backbone, after the neck, after the head, and in the post-processing phase. We use the average operation for the former three places to fuse the features extracted from multiple spatial views. For the fusion in the post-processing  phase, we adopt NMS <ref type="bibr" target="#b29">[30]</ref> or NMW <ref type="bibr" target="#b30">[31]</ref> to fuse the generated action proposals. As shown in <ref type="table" target="#tab_8">Table 9</ref>, we find that multi-scale feature fusion after neck can bring the most significant performance improvement for both the anchor-free and anchor-based methods.</p><p>Study on the Temporal and Spatial Resolution. We further investigate the relationship between temporal and spatial resolution and their influence on the training model. We set the baseline 96-frame setting to be compared for both anchor-based and anchor-free methods. The original frame image will be resized in a short-128 way and cropped to 112 ? 112. As shown in <ref type="table" target="#tab_9">Table 10</ref>, doubling the temporal and spatial resolution respectively both increases mAP by 2.4% in the anchor-free method. It also has 2.1% and 1.8% mAP outperforms the basic one in the anchor-based method, suggesting that spatial and temporal resolutions are equally important for the TAD task. When we further increase both temporal and spatial resolutions, which means quadruple resolution increase, we can obtain 2.5% and 5.5% mAP improvement compared with basic one on anchor-based and anchor-free method.  <ref type="table">Table 11</ref>. Comparison of inference speed. The running speed of previous methods is directly cited from their papers. Note that these methods all use optical flow as inputs, and their running time does not include the optical flow calculation. So, their real running speed is lower than the reported speed.</p><p>Method GPU RGB-Only FPS mAP SS-TAD <ref type="bibr" target="#b1">[2]</ref> TITAN XM &lt; 701 -R-C3D <ref type="bibr" target="#b45">[46]</ref> TITAN XP &lt; 1030 -PBRNet <ref type="bibr" target="#b22">[23]</ref> 1080Ti &lt; 1488 47.1 AFSD <ref type="bibr" target="#b16">[17]</ref> 1080Ti &lt; 3259 52.0 AFSD <ref type="bibr" target="#b16">[17]</ref> V100 &lt; 4057 52.0 BasicTAD Plus(AB) TITAN XP 13715 54.9 BasicTAD Plus(AB) V100 17454 54.9 BasicTAD Plus(AF) TITAN XP 7143 54.0 BasicTAD Plus(AF) V100 8377 54.0</p><p>Efficience Analysis. As mentioned above, the proposed BasicTAD Plus achieves high precision and high efficiency. We compare the inference speed between our method and other state-of-the-art methods with end-to-end training strategies to verify this conclusion. We use the anchor-based method with 96-frame and 3 FPS as an example. As shown in <ref type="table">Table 11</ref>, the anchor-based method of Basic-TAD Plus can operate at a speed of 17454 FPS on V100 GPU, which is nearly four times faster than <ref type="bibr" target="#b16">[17]</ref>. although the anchor-free method of BasicTAD Plus is much slower because of keeping more channels in the neck and head, it still outperforms <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State of the Art</head><p>We compare our proposed BasicTAD and BasicTAD Plus with state-of-the-art methods on THUMOS14 <ref type="bibr" target="#b11">[12]</ref> and summarize the results in <ref type="table" target="#tab_10">Table 12</ref>. We choose the best results of our proposed BasicTAD Plus and standard BasicTAD on both anchor-based and anchor-free methods. It is worth noting that our BasicTAD Plus outperforms all previous methods. Even the BasicTAD has a performance comparable to the previous methods. BasicTAD Plus achieves significant improvement for around 8% mAP higher than BasicTAD at Avg index on both anchor-based and anchor-free methods. We also compare our proposed BasicTAD Plus with the state-of-the-art methods on FineAction <ref type="bibr" target="#b25">[26]</ref> and ActivityNet-v1.3 <ref type="bibr" target="#b10">[11]</ref>, as shown in <ref type="table" target="#tab_3">Table 13</ref> and <ref type="table">Table 14</ref>. On FineAction, we compare our Basic-TAD Plus with three convincing works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b46">47]</ref>. We find that our method can acquire a better performance on average mAP. However, our method perform worse on high mAP@0.95. One possible reason is that all three methods above are two-stage methods which can get refined proposals from previous detection stage or proposal generation stage which leading to better localization results for metric at high threshold like mAP@0.95. On ActivityNet-v1.3, we attempt to adapt our model to predict binary action proposals and obtain the detection results by applying video-level action classifiers. The reason is that such indirect methods could benefit from the video-level predictions because most videos in ActivityNet-v1.3 only contain one action instance belonging to a single class. Our method performs better than <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">47]</ref> when only using RGB frames as input. However, other methods use pre-extracted features from whole video to get richer information and their detection results can get further improved by introducing additional optical flow modality. We only use 768 frames to represent a whole video which will lose fine-grained information. Also, our method is in one-stage form that we don't have any refinement operation like <ref type="bibr" target="#b16">[17]</ref> which also adopt 768 frames as input. To get better results on ActivityNet-v1.3, we need a new sampling strategy to maintain rich information for end-to-end methods and improvement measures for proposals provided from one-stage methods. Our method obtains better performance on both large-scale datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have reconsidered the TAD pipeline and present a simple modular detection framework. Based on this modular design, we perform empirical investigation over the basic options in each component, and finally come up with a simple yet effective TAD baseline, termed as BasicTAD. Furthermore, we improve the BasicTAD with minimal changes by following the core design of preserving rich information in backbone and neck, and the resulted detector is termed as BasicTAD Plus. Extensive experiments demonstrate that our BasicTAD Plus significantly outperforms previous state-of-the-art methods on the THUMOS-14 and FineAction, and achieves competitive results on the ActivityNet-v1.3 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More details of our BasicTAD</head><p>We use SlowOnly <ref type="bibr" target="#b6">[7]</ref> as the backbone and extract a 3D feature map F 3d ? R C? T 8 ? H 32 ? W 32 , where C, T , H, W represents number of channels, temporal length, spatial height, and spatial width, respectively. Then F 3d is average pooled in spatial dimension to the temporal feature map F 1d ? R C? T 8 . We feed F 1d into the neck to obtain the temporal multi-scale feature map. In each scale, we use a simple detection head with shared parameters to predict the category and boundaries of the action segments. To show our model structure in more detail, we list the network structure of the neck and head in <ref type="table" target="#tab_5">Table 15 and Table 16</ref>. <ref type="table">Table 15</ref>. Neck of BasicTAD for both anchor-based and anchor-free methods. We introduce the construction details of the neck module of the two methods. For the anchor-based method, we first use TDM to generate the multi-scale temporal features. Then TFPN enhances the temporal feature in each scale by the temporal feature at a high level. We use 1D convolution layers with a kernel size of 1 as the lateral connection. We use the interpolate up-sampling and 1D convolution layers with a kernel size of 3 to fuse the feature from different scales. We use multiple MaxPool1D operations to generate multi-scale features in the TDM module for the anchor-free method. C means the number of channels, and T means the temporal dimension.  <ref type="table" target="#tab_5">Table 16</ref>. Head of BasicTAD for both anchor-based and anchor-free methods. Both methods use four temporal convolutional layers for both classification and regression branches, respectively. For simplicity, we only list the 4 convolutional layers of one branch in the table where Conv1D-x in two branches does not share parameters. Conv1D cls and Conv1Dreg is denoted as the output layers for classification and regression. NC indicates the number of categories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An overview of BasicTAD. Our BasicTAD follows a modular design framework for the TAD task. It is composed of data sampling (augmentation), backbone, neck, and detection head. With minimal redesigns of existing components, it outperforms the existing state of the arts with high efficiency. (a) BasicTAD adopts a typical onestage network, implemented by both anchor-based and anchor-free methods. The whole pipeline is constructed by input frames, backbone, neck, and head modules. (b) The neck of BasicTAD has three ways of construction for different feature utilization. In neck module, n means the number of stages in backbone module and stage n is the last layer of backbone. stage n* contains extra pooling operations on stage n to construct low temporal resolution feature with rich semantic information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison between three kinds of neck. We test three different kinds of neck and measure the results on THUMOS14.</figDesc><table><row><cell>Method</cell><cell>Neck</cell><cell cols="6">mAP@0.3 mAP@0.4 mAP@0.5 mAP@0.6 mAP@0.7 Avg</cell></row><row><cell></cell><cell>TFPN from backbone</cell><cell>59.1</cell><cell>55.4</cell><cell>50.0</cell><cell>40.9</cell><cell>27.9</cell><cell>46.7</cell></row><row><cell>AB</cell><cell>TDM after backbone</cell><cell>56.7</cell><cell>53.1</cell><cell>48.0</cell><cell>38.8</cell><cell>27.0</cell><cell>44.7</cell></row><row><cell></cell><cell>TDM-TFPN after backbone</cell><cell>61.4</cell><cell>57.8</cell><cell>51.9</cell><cell>43.6</cell><cell>31.8</cell><cell>49.3</cell></row><row><cell></cell><cell>TFPN from backbone</cell><cell>56.1</cell><cell>50.2</cell><cell>43.6</cell><cell>34.1</cell><cell>24.5</cell><cell>41.7</cell></row><row><cell>AF</cell><cell>TDM after backbone</cell><cell>66.1</cell><cell>61.7</cell><cell>53.9</cell><cell>44.5</cell><cell>33.3</cell><cell>51.9</cell></row><row><cell></cell><cell>TDM-TFPN after backbone</cell><cell>61.8</cell><cell>56.6</cell><cell>49.2</cell><cell>39.9</cell><cell>28.7</cell><cell>47.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Performance of BasicTAD on different backbones. Here we use two common backbones C3D<ref type="bibr" target="#b36">[37]</ref> and I3D<ref type="bibr" target="#b2">[3]</ref> for our BasicTAD, where I3D only retains RGB branch. Choice of the basic operator. Here we choose two basic operators in the neck module for both anchor-based and anchor-free methods. The original frame will be rescaled to 128?128, and the crop size is 112?112.</figDesc><table><row><cell></cell><cell cols="9">Method Backbone mAP@0.3 mAP@0.4 mAP@0.5 mAP@0.6 mAP@0.7 Avg</cell></row><row><cell></cell><cell></cell><cell>C3D</cell><cell>56.8</cell><cell cols="2">50.1</cell><cell>44.7</cell><cell>35.2</cell><cell>24.3</cell><cell>42.2</cell></row><row><cell></cell><cell>AF</cell><cell>I3D</cell><cell>61.7</cell><cell cols="2">56.9</cell><cell>49.3</cell><cell>38.7</cell><cell>26.1</cell><cell>46.6</cell></row><row><cell></cell><cell></cell><cell>SlowOnly</cell><cell>66.1</cell><cell cols="2">61.7</cell><cell>53.9</cell><cell>44.5</cell><cell>33.3</cell><cell>51.9</cell></row><row><cell></cell><cell></cell><cell>C3D</cell><cell>54.4</cell><cell cols="2">50.8</cell><cell>45.7</cell><cell>37.4</cell><cell>26.1</cell><cell>42.9</cell></row><row><cell></cell><cell>AB</cell><cell>I3D</cell><cell>59.5</cell><cell cols="2">56.0</cell><cell>51.4</cell><cell>41.8</cell><cell>28.3</cell><cell>47.4</cell></row><row><cell></cell><cell></cell><cell>SlowOnly</cell><cell>61.4</cell><cell cols="2">57.8</cell><cell>51.9</cell><cell>43.6</cell><cell>31.8</cell><cell>49.3</cell></row><row><cell cols="2">Method Operator</cell><cell cols="2">Neck</cell><cell></cell><cell cols="5">mAP@0.3 mAP@0.4 mAP@0.5 mAP@0.6 mAP@0.7 Avg</cell></row><row><cell>AF</cell><cell>Conv MaxPool</cell><cell cols="2">TDM after backbone TDM after backbone</cell><cell></cell><cell>61.8 65.7</cell><cell>57.7 60.7</cell><cell>50.0 52.8</cell><cell>41.5 41.7</cell><cell>29.5 29.3</cell><cell>48.1 50.0</cell></row><row><cell>AB</cell><cell cols="4">Conv TDM-TFPN after backbone MaxPool TDM-TFPN after backbone</cell><cell>63.9 63.0</cell><cell>59.4 59.4</cell><cell>53.7 53.6</cell><cell>44.3 44.2</cell><cell>31.0 30.5</cell><cell>50.5 50.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation study results on Temporal Preservation for Backbone and Spatial Preservation for Neck. TP is denoted as Temporal Preservation for the backbone, and SP is denoted as Spatial Preservation for the neck.</figDesc><table><row><cell cols="8">Head TP SP Frames mAP@0.3 mAP@0.4 mAP@0.5 mAP@0.6 mAP@0.7 Avg GFLOPs</cell></row><row><cell></cell><cell>768</cell><cell>61.4</cell><cell>57.8</cell><cell>51.9</cell><cell>43.6</cell><cell>31.8</cell><cell>49.3 280.22</cell></row><row><cell></cell><cell>96</cell><cell>63.9</cell><cell>59.4</cell><cell>53.7</cell><cell>44.3</cell><cell>31.0</cell><cell>50.5 133.31</cell></row><row><cell>AB</cell><cell>96</cell><cell>63.2</cell><cell>59.7</cell><cell>54.4</cell><cell>45.6</cell><cell>32.2</cell><cell>51.0 136.44</cell></row><row><cell></cell><cell>192</cell><cell>65.4</cell><cell>61.8</cell><cell>56.4</cell><cell>47.9</cell><cell>33.8</cell><cell>53.1 266.03</cell></row><row><cell></cell><cell>192</cell><cell>65.1</cell><cell>62.0</cell><cell>57.1</cell><cell>48.9</cell><cell>33.9</cell><cell>53.4 272.88</cell></row><row><cell></cell><cell>768</cell><cell>66.1</cell><cell>61.7</cell><cell>53.9</cell><cell>44.5</cell><cell>33.3</cell><cell>51.9 298.4</cell></row><row><cell></cell><cell>96</cell><cell>65.7</cell><cell>60.7</cell><cell>52.8</cell><cell>41.7</cell><cell>29.3</cell><cell>50.0 151.4</cell></row><row><cell>AF</cell><cell>96</cell><cell>67.3</cell><cell>61.6</cell><cell>54.4</cell><cell>41.2</cell><cell>29.5</cell><cell>50.8 151.5</cell></row><row><cell></cell><cell>192</cell><cell>67.5</cell><cell>62.6</cell><cell>55.4</cell><cell>45.7</cell><cell>33.7</cell><cell>53.0 284.21</cell></row><row><cell></cell><cell>192</cell><cell>69.9</cell><cell>64.3</cell><cell>56.8</cell><cell>44.7</cell><cell>32.1</cell><cell>53.6 284.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison between different spatial size in training stage. Origin size means image frame before crop.</figDesc><table><row><cell cols="8">Head Origin size Crop size mAP@0.3 mAP@0.4 mAP@0.5 mAP@0.6 mAP@0.7 Avg</cell></row><row><cell></cell><cell>128 ? 128 112 ? 112</cell><cell>63.2</cell><cell>59.7</cell><cell>54.4</cell><cell>45.6</cell><cell>32.2</cell><cell>51.0</cell></row><row><cell>AB</cell><cell>171 ? 128 112 ? 112 short-128 112 ? 112</cell><cell>66.8 68.4</cell><cell>63.3 65.0</cell><cell>57.8 58.6</cell><cell>48.8 49.2</cell><cell>32.9 33.5</cell><cell>53.9 54.9</cell></row><row><cell></cell><cell>short-180 161 ? 161</cell><cell>71.2</cell><cell>66.8</cell><cell>61.2</cell><cell>50.1</cell><cell>36.3</cell><cell>57.1</cell></row><row><cell></cell><cell>128 ? 128 112 ? 112</cell><cell>67.3</cell><cell>61.6</cell><cell>54.4</cell><cell>41.3</cell><cell>29.5</cell><cell>50.8</cell></row><row><cell>AF</cell><cell>171 ? 128 112 ? 112 short-128 112 ? 112</cell><cell>68.1 69.4</cell><cell>64.3 65.1</cell><cell>55.2 57.4</cell><cell>45.3 45.2</cell><cell>31.9 33.2</cell><cell>53.0 54.0</cell></row><row><cell></cell><cell>short-180 161 ? 161</cell><cell>72.5</cell><cell>66.8</cell><cell>59.1</cell><cell>48.4</cell><cell>35.0</cell><cell>56.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Ablation study results on testing augmentation. For ThreeCrop and Flip augmentation, we fuse the features after the neck. For backward augmentation, we fuse the detection results in the post-processing phase. More details of testing augmentations can be found in the supplementary material.</figDesc><table><row><cell>Method</cell><cell>Test Aug</cell><cell cols="7">Crop Size mAP@0.3 mAP@0.4 mAP@0.5 mAP@0.6 mAP@0.7 Avg</cell></row><row><cell></cell><cell>CenterCrop</cell><cell>161 ? 161</cell><cell>71.2</cell><cell>66.8</cell><cell>61.2</cell><cell>50.1</cell><cell>36.3</cell><cell>57.1</cell></row><row><cell></cell><cell>CenterCrop</cell><cell>180 ? 180</cell><cell>71.6</cell><cell>67.3</cell><cell>61.6</cell><cell>50.8</cell><cell>34.7</cell><cell>57.2</cell></row><row><cell>AB</cell><cell cols="2">CenterCrop+Backward 180 ? 180</cell><cell>71.1</cell><cell>67.4</cell><cell>61.1</cell><cell>51.0</cell><cell>34.5</cell><cell>57.0</cell></row><row><cell></cell><cell>ThreeCrop</cell><cell>180 ? 180</cell><cell>71.7</cell><cell>67.9</cell><cell>62.0</cell><cell>50.7</cell><cell>35.6</cell><cell>57.6</cell></row><row><cell></cell><cell>CenterCrop+Flip</cell><cell>180 ? 180</cell><cell>71.9</cell><cell>67.7</cell><cell>62.1</cell><cell>51.0</cell><cell>35.2</cell><cell>57.6</cell></row><row><cell></cell><cell>CenterCrop</cell><cell>161 ? 161</cell><cell>72.5</cell><cell>66.8</cell><cell>59.1</cell><cell>48.4</cell><cell>35.0</cell><cell>56.4</cell></row><row><cell></cell><cell>CenterCrop</cell><cell>180 ? 180</cell><cell>72.9</cell><cell>66.3</cell><cell>59.5</cell><cell>48.2</cell><cell>35.1</cell><cell>56.4</cell></row><row><cell>AF</cell><cell cols="2">CenterCrop+Backward 180 ? 180</cell><cell>72.9</cell><cell>66.7</cell><cell>59.9</cell><cell>47.6</cell><cell>33.9</cell><cell>56.2</cell></row><row><cell></cell><cell>ThreeCrop</cell><cell>180 ? 180</cell><cell>72.3</cell><cell>67.6</cell><cell>59.0</cell><cell>48.5</cell><cell>35.9</cell><cell>56.7</cell></row><row><cell></cell><cell>CenterCrop+Flip</cell><cell>180 ? 180</cell><cell>72.4</cell><cell>66.8</cell><cell>59.7</cell><cell>48.9</cell><cell>35.0</cell><cell>56.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>The best time to fuse multiple spatial views. Here we choose four different stages to fuse our features or detection results. We compare them with Cen-terCrop, which does not use fusion operation and is annotated as 'None' in this table. The original frame image will be resized to short-180, and cropped size is 180?180.</figDesc><table><row><cell cols="8">Method Fusion Time mAP@0.3 mAP@0.4 mAP@0.5 mAP@0.6 mAP@0.7 Avg</cell></row><row><cell></cell><cell>None</cell><cell>71.6</cell><cell>67.3</cell><cell>61.6</cell><cell>50.8</cell><cell>34.7</cell><cell>57.2</cell></row><row><cell></cell><cell>Backbone</cell><cell>71.9</cell><cell>67.8</cell><cell>62.2</cell><cell>50.4</cell><cell>35.4</cell><cell>57.5</cell></row><row><cell>AB</cell><cell>Neck</cell><cell>71.7</cell><cell>67.9</cell><cell>62.0</cell><cell>50.7</cell><cell>35.6</cell><cell>57.6</cell></row><row><cell></cell><cell>Head</cell><cell>71.3</cell><cell>67.4</cell><cell>61.7</cell><cell>50.3</cell><cell>35.5</cell><cell>57.2</cell></row><row><cell></cell><cell>Post</cell><cell>71.6</cell><cell>67.4</cell><cell>61.7</cell><cell>50.6</cell><cell>35.4</cell><cell>57.3</cell></row><row><cell></cell><cell>None</cell><cell>72.9</cell><cell>66.3</cell><cell>59.5</cell><cell>48.2</cell><cell>35.1</cell><cell>56.4</cell></row><row><cell></cell><cell>Backbone</cell><cell>72.4</cell><cell>67.1</cell><cell>59.0</cell><cell>48.5</cell><cell>35.0</cell><cell>56.4</cell></row><row><cell>AF</cell><cell>Neck</cell><cell>72.3</cell><cell>67.6</cell><cell>59.0</cell><cell>48.5</cell><cell>35.9</cell><cell>56.7</cell></row><row><cell></cell><cell>Head</cell><cell>72.4</cell><cell>66.7</cell><cell>59.2</cell><cell>48.5</cell><cell>35.1</cell><cell>56.4</cell></row><row><cell></cell><cell>Post</cell><cell>72.7</cell><cell>66.6</cell><cell>59.0</cell><cell>48.4</cell><cell>35.2</cell><cell>56.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Ablation Study results on spatial and temporal resolution. Overhead represents the increase in computational overhead after expanding the temporal or spatial resolution and the basic one denotes by ?1.</figDesc><table><row><cell cols="10">Method Spatial Temporal Overhead mAP@0.3 mAP@0.4 mAP@0.5 mAP@0.6 mAP@0.7 Avg</cell></row><row><cell></cell><cell>112 ? 112</cell><cell>96</cell><cell>?1</cell><cell>68.4</cell><cell>65.0</cell><cell>58.6</cell><cell>49.2</cell><cell>33.5</cell><cell>54.9</cell></row><row><cell>AB</cell><cell>161 ? 161 112 ? 112</cell><cell>96 192</cell><cell>?2 ?2</cell><cell>71.2 70.2</cell><cell>66.8 66.3</cell><cell>61.2 60.6</cell><cell>50.1 50.3</cell><cell>36.3 36.2</cell><cell>57.1 56.7</cell></row><row><cell></cell><cell>161 ? 161</cell><cell>192</cell><cell>?4</cell><cell>70.5</cell><cell>66.6</cell><cell>61.1</cell><cell>51.5</cell><cell>37.3</cell><cell>57.4</cell></row><row><cell></cell><cell>112 ? 112</cell><cell>96</cell><cell>?1</cell><cell>69.4</cell><cell>65.1</cell><cell>57.4</cell><cell>45.2</cell><cell>33.2</cell><cell>54.0</cell></row><row><cell>AF</cell><cell>161 ? 161 112 ? 112</cell><cell>96 192</cell><cell>?2 ?2</cell><cell>72.5 71.7</cell><cell>67.2 66.9</cell><cell>59.1 59.0</cell><cell>48.2 49.2</cell><cell>35.3 35.3</cell><cell>56.4 56.4</cell></row><row><cell></cell><cell>161 ? 161</cell><cell>192</cell><cell>?4</cell><cell>75.3</cell><cell>70.3</cell><cell>63.4</cell><cell>50.5</cell><cell>37.8</cell><cell>59.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>Comparison with the state of the art on the THUMOS-14 dataset.</figDesc><table><row><cell>Type</cell><cell>Method</cell><cell cols="6">RGB-Only mAP@0.3 mAP@0.4 mAP@0.5 mAP@0.6 mAP@ 0.7 Avg</cell></row><row><cell></cell><cell>BSN [21]</cell><cell>53.5</cell><cell>45.0</cell><cell>36.9</cell><cell>28.4</cell><cell>20.0</cell><cell>36.8</cell></row><row><cell></cell><cell>MGG [27]</cell><cell>53.9</cell><cell>46.8</cell><cell>37.4</cell><cell>29.5</cell><cell>21.3</cell><cell>37.8</cell></row><row><cell></cell><cell>BMN [19]</cell><cell>56.0</cell><cell>47.4</cell><cell>38.8</cell><cell>29.7</cell><cell>20.5</cell><cell>38.5</cell></row><row><cell></cell><cell>DBG [19]</cell><cell>57.8</cell><cell>49.4</cell><cell>39.8</cell><cell>30.2</cell><cell>21.7</cell><cell>39.8</cell></row><row><cell>Multi-stage</cell><cell>RTD-Net [36]</cell><cell>58.5</cell><cell>53.1</cell><cell>45.1</cell><cell>36.4</cell><cell>25.0</cell><cell>43.6</cell></row><row><cell></cell><cell>TCANet [32]</cell><cell>60.6</cell><cell>53.2</cell><cell>44.6</cell><cell>36.8</cell><cell>26.7</cell><cell>44.4</cell></row><row><cell></cell><cell>G-TAD [47]</cell><cell>66.4</cell><cell>60.4</cell><cell>51.6</cell><cell>37.6</cell><cell>22.9</cell><cell>47.8</cell></row><row><cell></cell><cell>AFSD [17]</cell><cell>67.3</cell><cell>62.4</cell><cell>55.5</cell><cell>43.7</cell><cell>31.1</cell><cell>52.0</cell></row><row><cell></cell><cell>DCAN [5]</cell><cell>68.2</cell><cell>62.7</cell><cell>54.1</cell><cell>43.9</cell><cell>32.6</cell><cell>52.3</cell></row><row><cell></cell><cell>SP-TAD [44]</cell><cell>69.2</cell><cell>63.3</cell><cell>55.9</cell><cell>45.7</cell><cell>33.4</cell><cell>53.5</cell></row><row><cell></cell><cell>SSAD [20]</cell><cell>43.0</cell><cell>35.0</cell><cell>24.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>R-C3D [46]</cell><cell>44.8</cell><cell>35.6</cell><cell>28.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DBS [10]</cell><cell>50.6</cell><cell>43.1</cell><cell>34.3</cell><cell>24.4</cell><cell>14.7</cell><cell>33.4</cell></row><row><cell></cell><cell>GTAN [29]</cell><cell>57.8</cell><cell>47.2</cell><cell>38.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>One-stage</cell><cell>A2Net [48] PBRNet [23]</cell><cell>58.6 58.5</cell><cell>54.1 54.6</cell><cell>45.5 51.3</cell><cell>32.5 41.8</cell><cell>17.2 29.5</cell><cell>41.6 47.1</cell></row><row><cell></cell><cell>C-TCN [14]</cell><cell>68.0</cell><cell>62.3</cell><cell>52.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BasicTAD(AB)</cell><cell>61.4</cell><cell>57.8</cell><cell>51.9</cell><cell>43.6</cell><cell>31.8</cell><cell>49.3</cell></row><row><cell></cell><cell>BasicTAD(AF)</cell><cell>66.1</cell><cell>61.7</cell><cell>53.9</cell><cell>44.5</cell><cell>33.3</cell><cell>51.9</cell></row><row><cell></cell><cell>BasicTAD Plus(AB)</cell><cell>71.7</cell><cell>67.9</cell><cell>62.0</cell><cell>50.7</cell><cell>35.6</cell><cell>57.6</cell></row><row><cell></cell><cell>BasicTAD Plus(AF)</cell><cell>75.3</cell><cell>70.3</cell><cell>63.4</cell><cell>50.5</cell><cell>37.8</cell><cell>59.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 .Table 14 .</head><label>1314</label><figDesc>Comparison with the state of the art on the FineAction dataset. Comparison with the state of the art on the ActivityNet-1.3 dataset.</figDesc><table><row><cell>Method</cell><cell cols="4">RGB-Only mAP@0.5 mAP@0.75 mAP@0.95 Avg</cell></row><row><cell>BMN [19]</cell><cell>12.56</cell><cell>7.49</cell><cell>2.62</cell><cell>7.86</cell></row><row><cell>BMN [19]</cell><cell>14.44</cell><cell>8.92</cell><cell>3.12</cell><cell>9.25</cell></row><row><cell>DBG [16]</cell><cell>8.57</cell><cell>5.01</cell><cell>1.93</cell><cell>5.31</cell></row><row><cell>DBG [16]</cell><cell>10.65</cell><cell>6.43</cell><cell>2.50</cell><cell>6.75</cell></row><row><cell>G-TAD [47]</cell><cell>10.88</cell><cell>6.52</cell><cell>2.19</cell><cell>6.87</cell></row><row><cell>G-TAD [47]</cell><cell>13.74</cell><cell>8.83</cell><cell>3.06</cell><cell>9.06</cell></row><row><cell>BasicTAD Plus(AB)</cell><cell>24.34</cell><cell>10.57</cell><cell>0.43</cell><cell>12.15</cell></row><row><cell>BasicTAD Plus(AF)</cell><cell>22.37</cell><cell>10.36</cell><cell>0.83</cell><cell>11.66</cell></row><row><cell>Method</cell><cell cols="4">RGB-Only mAP@0.5 mAP@0.75 mAP@0.95 Avg</cell></row><row><cell>BSN [21]</cell><cell>46.45</cell><cell>29.96</cell><cell>8.02</cell><cell>30.03</cell></row><row><cell>P-GCN [49]</cell><cell>48.26</cell><cell>33.16</cell><cell>3.27</cell><cell>31.11</cell></row><row><cell>BMN [19]</cell><cell>50.07</cell><cell>34.78</cell><cell>8.29</cell><cell>33.85</cell></row><row><cell>G-TAD [47]</cell><cell>50.36</cell><cell>34.60</cell><cell>9.02</cell><cell>34.09</cell></row><row><cell>AFSD [17]</cell><cell>52.40</cell><cell>35.30</cell><cell>6.50</cell><cell>34.40</cell></row><row><cell>TCANet [32]</cell><cell>54.33</cell><cell>39.13</cell><cell>8.41</cell><cell>37.56</cell></row><row><cell>BMN [19]</cell><cell>41.93</cell><cell>30.10</cell><cell>9.00</cell><cell>29.23</cell></row><row><cell>G-TAD [47]</cell><cell>45.68</cell><cell>31.36</cell><cell>7.42</cell><cell>30.98</cell></row><row><cell>BasicTAD Plus(AB)</cell><cell>50.04</cell><cell>33.06</cell><cell>2.39</cell><cell>31.43</cell></row><row><cell>BasicTAD Plus(AF)</cell><cell>50.70</cell><cell>33.05</cell><cell>7.29</cell><cell>32.72</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Layer Kernel Stride Padding Input Channel Output</head> <ref type="table">Channel   AB   Conv1D-1  3  1  1  256  256  Conv1D-2  3  1  1  256  256  Conv1D-3  3  1  1  256  256  Conv1D-4  3  1  1  256  256  Conv1Dcls  3  1  1  256  NC  Conv1Dreg  3  1  1  256  2   AF   Conv1D-1  3  1  1  2048  2048  Conv1D-2  3  1  1  2048  2048  Conv1D-3  3  1  1  2048  2048  Conv1D-4  3  1  1  2048  2048  Conv1Dcls  3  1  1  2048  NC  Conv1Dreg  3  1  1  2048  2</ref> </div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<editor>Vedaldi, A., Bischof, H., Brox, T., Frahm, J.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12373</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-to-end, singlestream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking the faster R-CNN architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">DCAN: improving temporal action detection via dual context aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/2112.03612</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1711.08200</idno>
		<title level="m">Temporal 3d convnets: New architecture and transfer learning for</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Accurate temporal action proposal generation with relation-aware pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">TURN TAP: temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video imprint segmentation for temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu</title>
		<meeting><address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep concept-wise temporal convolutional networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM &apos;20: The 28th ACM International Conference on Multimedia</title>
		<editor>Zimmermann, R.</editor>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TEA: temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning salient boundary feature for anchor-free temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE (2021) 2, 3, 5, 9, 10, 11, 15</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">TSM: temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BMN: boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<editor>Yan, S.</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">BSN: boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<editor>Weiss, Y.</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11208</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end temporal action detection with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno>abs/2106.10271</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fineaction: A fine-grained video dataset for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>CoRR abs/2105.11107 (2021) 1, 9</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006-08" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inception single shot multibox detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops, ICME Workshops</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal context aggregation network for temporal action proposal refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">BSN++: complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Relaxed transformer decoders for direct action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">RGB stream is enough for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<idno>abs/2107.04362</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">TDN: temporal difference networks for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Towards high-quality temporal action detection with sparse proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>CoRR abs/2109.08847 (2021) 2, 4, 5, 7, 10</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<editor>Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y.</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">R-C3D: region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">G-TAD: sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE (2020) 2, 4</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Revisiting anchor mechanisms for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Distance-iou loss: Faster and better learning for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
