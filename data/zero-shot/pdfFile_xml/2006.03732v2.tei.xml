<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WOAD: Weakly Supervised Online Action Detection in Untrimmed Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
							<email>mingfei.gao@salesforce.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
							<email>yingbo.zhou@salesforce.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
							<email>ran.xu@salesforce.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
							<email>rsocher@salesforce.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<email>cxiong@salesforce.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salesforce</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">WOAD: Weakly Supervised Online Action Detection in Untrimmed Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online action detection in untrimmed videos aims to identify an action as it happens, which makes it very important for real-time applications. Previous methods rely on tedious annotations of temporal action boundaries for training, which hinders the scalability of online action detection systems. We propose WOAD, a weakly supervised framework that can be trained using only video-class labels. WOAD contains two jointly-trained modules, i.e., temporal proposal generator (TPG) and online action recognizer (OAR). Supervised by video-class labels, TPG works offline and targets at accurately mining pseudo frame-level labels for OAR. With the supervisory signals from TPG, OAR learns to conduct action detection in an online fashion. Experimental results on THUMOS'14, ActivityNet1.2 and ActivityNet1.3 show that our weakly-supervised method largely outperforms weaklysupervised baselines and achieves comparable performance to the previous strongly-supervised methods. Beyond that, WOAD is flexible to leverage strong supervision when it is available. When strongly supervised, our method obtains the state-of-the-art results in the tasks of both online per-frame action recognition and online detection of action start.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal Action Localization aims to detect temporal action boundaries in long, untrimmed videos. Most previous methods are under offline settings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref>, where they can observe the entire action before making decisions. However, applications such as surveillance systems and autonomous cars, are required to interact with the world in real time based on their accumulative observations up to now. Online Action Detection <ref type="bibr" target="#b5">[6]</ref> is proposed to address this problem, where methods need to identify occurring actions moment-tomoment without access to future information. With different focuses, recent online action detectors consider two subtasks: (1) online per-frame action recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref> and <ref type="bibr" target="#b1">(2)</ref> online detection of action start <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>. The former task focuses on the general capability of recognizing the action category of each coming frame. On the other hand, detecting segment-level anno. video-level anno. segment ? video ? previous methods our method segment ? video ? mixed mixed ? ? <ref type="figure">Figure 1</ref>. Comparison between previous methods and our approach. Previous methods require segment-level annotations (start and end times of actions) during training, which leads to high human labeling cost. In contrast, our method can be trained with video-level annotations (video-level action labels) and is flexible to utilize (full/ partial) segment-level supervision when it is available. action starts in a timely manner is more important to some real-world applications. For example, an autonomous car needs to recognize "line merging" of another vehicle as soon as it starts. While, it is challenging to detect action starts due to the similar appearances near the start points and the lack of training data. The later task specially targets on this problem. Our method jointly addresses these two tasks.</p><p>Although previous methods have achieved promising progress, they rely on segment-level annotations of action boundaries for training (see <ref type="figure">Fig. 1</ref>). However, annotating action boundaries in long, untrimmed videos involves possibly ambiguous decisions and requires significant amount of human labor. This hinders the scalability of model learning, particularly for videos embodying complex semantics. Compared to the segment-level boundaries, video-level action classes are much easier to acquire. With the help of text-based video retrieval techniques, video-class labels may be obtained almost for free from the internet at a large scale.</p><p>To take advantage of the easy-to-obtain video-level annotations, we propose WOAD, a Weakly supervised Online Action Detection framework, that can be trained with videoclass labels only. Detecting actions using weak supervision in an online scenario is challenging since (1) online action detectors generally require per-frame labels for training, so it is hard to utilize video-level labels as supervision and <ref type="formula" target="#formula_5">(2)</ref> it is not trivial for a model to be accurate for action recognition and sensitive to action starts without access to future information. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our proposed WOAD contains two jointly-trained modules, i.e., Temporal Proposal Generator (TPG) and Online Action Recognizer (OAR), each of which focuses on handling one of the challenges. Supervised by video-class labels, TPG mines class-wise temporal action proposals that can be used as pseudo per-frame labels for OAR. While, OAR aims at conducting both per-frame action recognition and start detection jointly in an online fashion.</p><p>The proposed design has the following benefits: (1) TPG is used only during training for pseudo labels generation, so it can fully utilize temporal relation of frames (e.g. grouping nearby frames of the same class to improve proposal generation) without online constraint; (2) the design of OAR directly targets at improving the online tasks without being distracted by the weakly supervised setting and (3) the joint training could help learn better representations.</p><p>Our contributions are summarized as follows: (1) we introduce a novel method for weakly supervised online action detection. To the best of our knowledge, this is the first work that addresses the problem using weak supervision; (2) our method is flexible to combine weak and strong supervision when only a part of videos have strong annotations and <ref type="formula" target="#formula_6">(3)</ref> experimental results show that our method largely outperforms weakly-supervised baselines and achieves comparable performance to the previous strongly-supervised methods. When strongly supervised, our method obtains the state-ofthe-art results in the tasks of both online per-frame action recognition and online detection of action start.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Temporal Action Localization. The goal of temporal action detectors is to find the temporal boundaries of actions in untrimmed videos. Most existing methods work in offline settings, where they can make predictions after processing the entire actions. <ref type="bibr">Shou</ref>   <ref type="bibr" target="#b13">[14]</ref> and improved feature representations in <ref type="bibr" target="#b29">[30]</ref>.</p><p>Online Action Detection. Online action detectors identify the occurring action in untrimmed, streaming videos based on the past and current observations. Geest et al. first posed this problem as online per-frame action recognition and set up several baselines and evaluation metrics in <ref type="bibr" target="#b5">[6]</ref>. Following this direction, Gao et al. introduced RED <ref type="bibr" target="#b8">[9]</ref> which conducts current and future action predictions jointly. Xu et al. proposed TRN <ref type="bibr" target="#b27">[28]</ref> that uses the predicted future actions to improve action recognition at the current time. Eun et al. introduced IDU to accumulate input information based on its relevance to the current action in <ref type="bibr" target="#b6">[7]</ref>. Compared to per-frame action recognition, online detecting action starts is more important for some applications and is more challenging due to the similar appearance near starts and the lack of training data. Shou et al. first proposed an online framework in <ref type="bibr" target="#b23">[24]</ref> and treated the problem as a classification task. <ref type="bibr">Gao et al. presented StartNet [11]</ref> which set the new state-of-the-art performance. However, these methods depend on the annotations of action boundaries for training and are evaluated on either per-frame action recognition or action start detection. Our work jointly handles these two tasks using weak supervision.</p><p>Weakly Supervised Offline Action Detection. Extensive studies have been done in offline action detection with video-class lables as supervision. <ref type="bibr">Wang</ref>   <ref type="bibr" target="#b16">[17]</ref> and BasNet <ref type="bibr" target="#b14">[15]</ref> improved weakly supervised action localization by background suppression. In <ref type="bibr" target="#b19">[20]</ref>, Narayan et al. optimized the models by jointly minimizing category, count and center losses. Yuan et al. proposed MAAN <ref type="bibr" target="#b28">[29]</ref> to relieve the effect of the dominant response of the most salient regions. Luo et al. explicitly modeled the key instances assignment via a EM-MIL approach in <ref type="bibr" target="#b17">[18]</ref>. Nguyen et al. learned a rich notion of actions via background modeling in <ref type="bibr" target="#b20">[21]</ref>. Min et al. proposed A2CL-PT <ref type="bibr" target="#b18">[19]</ref> to learn discriminative features and distinguish background. Existing offline methods are not well suitable in the online setting since (1) offline methods target at predicting action segments that have significant temporal overlap with ground-truth, so they are not designed to be good at per-frame recognition and are not sensitive to start; (2) most of them adopt temporal prediction grouping strategy to improve performance during inference which violates online constraint and (3) technically, they prefer feedforward networks while online action detectors work better with RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weakly Supervised Online Action Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework Overview</head><p>For online action detection, the input to the system is a streaming, untrimmed video, V i , represented as a sequence of image frames [I 1 i , I 2 i , ..., I Ti i ], where i denotes video index and T i is video length. At each time t, the system takes I t i as input. It predicts, a t i , the probability of the current action category (online per-frame action recognition) and determines, as t i , the probability that an action start occurs (online detection of action start). Under the constraint of online setting, no future information is available in the inference phase. Previous approaches require annotations of temporal action boundaries for training. The proposed method can be trained using only video-class labels. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our method contains two modules, i.e., the Temporal Proposal Generator (TPG) and the Online Action Recognizer (OAR). During training, TPG is supervised by video-class labels and outputs class-wise temporal proposals (Sec. 3.2). The proposals serve as pseudo ground truth of action boundaries which can be used as per-frame labels to supervise the training of OAR (Sec. 3.3). During inference, only OAR is used for online action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Proposal Generator</head><p>There are different options to implement our Temporal Proposal Generator (TPG). In this work, we focus on utilizing Multiple Instance Learning (MIL) loss and Co-Activity Similarity (CAS) loss proposed in <ref type="bibr" target="#b21">[22]</ref>. Next, we will revisit the definitions of these two losses</p><formula xml:id="formula_0">Let F i = [f 1 i , f 2 i , .</formula><p>.., f Ti i ] ? R Ti?D indicates the features of V i just before the TPG module as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, where f t i indicates the feature of the frame at time step t, and T i denotes the number of frames in V i . We obtain per-frame scores,</p><formula xml:id="formula_1">S i = [s 1 i , s 2 i , ..., s Ti i ] ? R Ti?C by projecting the features to action class space and s t i = [s t i1 , s t i2 , ..., s t iC ] ? R C indicates scores of frame t over c = [1, 2, ..., C] classes.</formula><p>For each class c, a video-level score,? ic , is obtained by averaging over the top K i frame scores as? ic = 1 Ki t?Kic s t ic , where K ic indicates the set of top K i frames for class c over T i frames, K i = Ti ? and ? is fixed to be 8. MIL loss, L M IL , is defined as the cross entropy loss between the video-class label, y i , and the predicted video-class probability, p i , where p i is obtained by applying softmax in</p><formula xml:id="formula_2">classes over? i = [? i1 ,? i2 , ...,? iC ].</formula><p>CAS loss encourages regions of videos containing sim-ilar activities to have similar feature representations, and those containing different activities to have different representations. High-and low-attention region feature representations, ? and ?, are introduced to achieve this goal. For class c,</p><formula xml:id="formula_3">? ic = F i A ic and ? ic = 1 Ti?1 F i (1 ? A ic ), where A ic ? R Ti</formula><p>is a temporal attention vector, obtained by applying temporal softmax over frame scores, S i Intuitively, ? ic aggregates features of regions with high probability containing the activity, while ? ic aggregates those of regions that are unlikely involving in the activity. For class c, a positive video pair, V i and V j , is constructed if y ic = y jc = 1. Their pair-wise loss is calculated as</p><formula xml:id="formula_4">1 2 {max(0, d(? ic , ? jc ) ? d(? ic , ? jc ) + ?) + max(0, d(? ic , ? jc ) ? d(? ic , ? jc ) + ?)},<label>(1)</label></formula><p>where d(x, z) denotes cosine similarity of x and z, and ? is a margin parameter. CAS loss, L CAS , is the average loss over all positive video pairs of all classes in the training batch. Proposal generation is conducted via a two-stage thresholding strategy. First, a threshold, ? class , is used to discard categories having small video-level confidence scores. Then, a second threshold, ? score , is applied on the frame scores of the remaining categories, s t ic , along the temporal axis. Taking advantage of temporal constraint of frames, nearby frames with the same category are grouped to obtain the class-wise temporal proposals and action starts are thus obtained. After that, the video-class labels are used to filter out the proposals with wrong categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Online Action Recognizer</head><p>Online Action Recognizer (OAR) sequentially takes f t i as input and outputs per-frame action scores over classes including background, a t i ? R (C+1) , and a class-agnostic start score, st t i ? R 2 , indicating the probabilities of this frame being a start point or not.</p><p>Our OAR is constructed by a LSTM with temporal pooling. The LSTM updates its hidden and cell states, h t i and c t i , at each time step as</p><formula xml:id="formula_5">h t i , c t i = LST M (h t?1 i , c t?1 i , f t i ).<label>(2)</label></formula><p>Then, h </p><formula xml:id="formula_6">h t i = max pool(h t?M i , h t?M +1 i , ..., h t i ).<label>(3)</label></formula><p>a t i and st t i are obtained by a linear projection followed by the softmax operation on h t i and h t i , respectively as in Eq. 4, where W a and W st indicate the parameters of the classifiers.</p><formula xml:id="formula_7">a t i = sof tmax(W a h t i ). st t i = sof tmax(W st h t i ).<label>(4)</label></formula><p>In each training batch, we convert the proposal boundaries of each class c (obtained from TPG) to per-frame action labels, l jc and binary start labels, ? jm , where j = {1, 2, .., T } indicates the index of a frame, T is the total number of frames in the training video batch and m ? {0, 1} differentiates the non-start and start. We use cross entropy loss between l jc and the predicted action probability, a jc , to form frame loss and utilize focal loss <ref type="bibr" target="#b15">[16]</ref> between ? jm and the predicted start probability, st jm , to construct start loss as shown in Eq. 5, where ? is a hyper parameter.</p><formula xml:id="formula_8">L OAR = ? 1 T T j=1 C c=0 l jc log a jc f rame loss + ? 1 T T j=1 1 m=0 ? jm (1 ? st jm ) ? log st jm start loss .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Optimization and Inference</head><p>Optimization. Our Temporal Proposal Generator (TPG) and Online Action Recognizer (OAR) are jointly optimized by minimizing</p><formula xml:id="formula_9">L total = L OAR + ?L T P G ,<label>(6)</label></formula><p>where L T P G = L M IL + L CAS . L M IL is computed for each videos and L CAS is calculated using the positive video pairs in the training batch. Each video is segmented to nonoverlapping training sequences which are used to calculate L OAR . As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, proposals for OAR supervision are continuously updated. To reduce computation, we update the proposals every N training iterations.</p><p>Inference. For the online action detection tasks, only OAR is used during inference. Proceeding sequentially, OAR outputs a t i and st t i at each time step t. a t i can be used directly as the per-frame action prediction. Following <ref type="bibr" target="#b10">[11]</ref>, scores of action starts, are obtained by as t i(1:C) = a t i(1:C) * st t i1 and as t i0 = a t i0 * st t i0 , where (1 : C) indicates positive classes and (0) denotes background. Then, we generate action starts following the criteria <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref>: (1) the predicted class? t i = argmax c (as t i ) is an action; (2) the maximum action score as t i? t i exceeds a threshold (set to be 0) and <ref type="formula" target="#formula_6">(3)</ref> c t i =? t?1 i . As indicated, st t i is used to boost the scores if a start is predicted at time t and suppress those otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We conduct experiments on THUMOS'14 <ref type="bibr" target="#b11">[12]</ref>, ActivityNet1.2 and ActivityNet1.3 <ref type="bibr" target="#b7">[8]</ref>. THUMOS'14 contains 20 sport-related action classes. Following prior works, we use the validation set (200 videos) for training and evaluate on the test set (212 videos). Each video contains 15 action instances on average. ActivityNet1.2 contains 100 action classes with an average of 1.5 action instances per video. We train on the training set (4819 videos) and evaluate on validation set (2383 videos). With 200 action classes, ActivityNet1.3 is an enlarged version of ActivityNet1.2. Our model is trained with the 10k training videos and validated using its 5k validation videos. Although ActivityNet datasets are much larger, THUMOS'14 has varying video lengths and much denser temporally annotated actions which make it more challenging.</p><p>Evaluation metrics. Following previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11]</ref>, frame-based average precision (F-AP) and pointbased average precision (P-AP) are used as our evaluation metrics. F-AP focuses on evaluating model performance based on per-frame predictions. P-AP evaluates performance of action starts. P-AP works similarly as the bounding box based AP in the object detection task, except that P-AP uses time difference to determine whether an action start prediction is correct, while the later one uses Intersection over Union between the predicted box and the ground truth. The mean F-AP and mean P-AP are calculated by averaging F-APs and P-APs over classes, respectively.</p><p>Baselines. Since our TPG is implemented based on W-TALC <ref type="bibr" target="#b21">[22]</ref>, we compare with this baseline to show the advantage of our framework under the weakly supervised setting. <ref type="bibr" target="#b0">1</ref> Then, we compare against two strongly-supervised methods, TRN <ref type="bibr" target="#b27">[28]</ref> and StartNet <ref type="bibr" target="#b10">[11]</ref>. TRN is a state-of-the-art (SOTA) method for the online per-frame action recognition task and StartNet is the SOTA method for the task of online detection of action starts.</p><p>Feature description. On THUMOS'14 and Activi-tyNet1.2, two-stream (optical flow stream and RGB stream) I3D network <ref type="bibr" target="#b2">[3]</ref> pre-trained on Kinetics is used as the feature extractor. Features are extracted at the chunk level. Video frames are extracted at 25 FPS and the chunk size is 16. The final features are the concatenation of the outputs of the two streams, resulting in a dimension of 2048. To perform fair comparison, our method and the baselines use the preextracted features provided by the authors of <ref type="bibr" target="#b21">[22]</ref>. To avoid heavy feature extraction, we adopt the C3D features 2 of Ac-tivityNet1.3 officially released by the ActivityNet Challenge. Implementation details. Our method is implemented using Pytorch. The update interval of temporal proposals is set to be N = 100 for THUMOS'14, N = 500 for Ac-tivityNet1.2 and 1000 for ActivityNet1.3. For OAR, the dimension of h t i is set to be 4096 and the length of training sequence for LSTM is 64. M in temporal pooling is fixed to be 3. ? in Eq. 5 is set to be 2. Since starts are sparsely located in each video, we use all positive frames and randomly sample 3 times negative ones in each training batch to compute start loss. ? is fixed to be 0.5. Batch size of training videos is set to be 10. We use Adam <ref type="bibr" target="#b12">[13]</ref> with weight decay 5 ? 10 ?4 and set learning rate to be 1 ? 10 ?4 .</p><p>Supervision combination strategy. When segmentlevel (strong) annotations exist, frame and start losses are computed using a combination of ground-truth and pseudo labels. The intuition is that the boundary annotations usually involve ambiguous decisions, so the noisy labels may serve as a type of regularization by making the label set reasonably diverse. We conduct the combination by randomly selecting 90% videos using ground-truth supervision and other videos use the noisy proposal supervision. The proposals and the combination set are updated during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">WOAD with Weak Supervision</head><p>Our main focus is weakly supervised online action detection. So, we first conduct experiments under this setting.</p><p>Online detection of action start. Comparisons in terms of P-AP between our approach and our baseline, W-TALC, are shown in <ref type="table">Table 1</ref> and 2. Our method outperforms W-TALC over all the time thresholds. Specifically, we obtain 5.7% and 2.7% higher mean P-AP when time threshold is 1 second on THUMOS'14 and ActivityNet1.2, respectively.</p><p>When only using video-level annotations, our method obtains better performance than strongly-supervised StartNet in general on THUMOS'14, ActivityNet1.2 and ActivityNet1.3 as shown in <ref type="table" target="#tab_3">Table 3</ref>, 4 and 5.</p><p>Online per-frame action recognition. Comparisons between our method and W-TALC in terms of F-AP are shown in <ref type="table">Table 1</ref> and 2 (last column). Our method improves the baseline largely by 6.4% and 12.9% mean F-AP on THU-MOS'14 and ActivityNet1.2 which demonstrate the effectiveness of our framework.</p><p>We also compare our weakly-supervised method with strongly-supervised baselines in <ref type="table">Table 5</ref>, 6 and 8. Our weakly-supervised method achieves 54.4% mean F-AP improving the strongly-supervised TRN by 3.4% on THU-MOS'14, and obtains 66.7% mean F-AP which is only 2.4% lower than TRN on ActivityNet1.2. Although using video-level labels, our method achieves comparable results to strongly-supervised TRN. THUMOS'14 is a more challenging dataset, containing 10? action instances per video as compared to ActivityNet, so it leaves more room for our model to improve the performance. This is why our method gains much better results on THUMOS'14. For future reference, our method obtains 67.9% using weighted maIA, a newly proposed metric in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">WOAD with Strong Supervision</head><p>Full strong supervision. When using all segment-level annotations, our method largely outperforms TRN on THU- MOS'14 ( by 16.1% mean F-AP, see <ref type="table">Table 7</ref>). On Ac-tivityNet1.2 and ActivityNet1.3, our method achieves new state-of-the-art performance of 70.7% mean F-AP <ref type="table">(Table 8)</ref> and 46.8% <ref type="table">(Table 5)</ref>, respectively. For online detection of action start, our method outperforms StarNet consistently for all time thresholds on both THUMOS'14 <ref type="table" target="#tab_3">(Table 3)</ref> and Ac-tivityNet1.2 <ref type="table">(Table 4</ref>). Interestingly, we observe that the gap of our performance between strong-and weak-supervised settings is only 4% and 2.8% on ActivityNet1.2 and 1.3, whereas the gap is 12.7% on THUMOS'14. This may be because that the average ratio of action length over video length in ActivityNet is ? 40%, while the ratio is only 2% in THU-MOS'14. So, our method is not as sensitive to the boundary shift of a noisy proposal in ActivityNet as in THUMOS'14.</p><p>WOAD with Mixed Supervision. One advantage of our method is the flexibility of taking different forms of supervi-sion for different videos. We evaluate our model when only a portion of randomly selected videos have segment-level annotations. As shown in <ref type="table">Table 7</ref> and 8, the performance of our model improves when more segment-level labels are available. On ActivityNet1.2, our method achieves comparable performance to previous SOTA method when only 70% of data contains segment-level annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Model Ablation and Analysis</head><p>Our superior performance may attribute to (1) the improvements by jointly training TPG and OAR; (2) the effect of the supervision combination strategy and (3) our desirable structure. Ablation studies are conducted to analyze the effect of each component of WOAD.</p><p>TPG and OAR joint training. Training two modules together has following benefits: (1) the diverse pseudo labels generated in different iterations could serve as regularization during training and (2) the shared features can be potentially improved by multi-task learning. We validate its effect by removing L T P G when strong labels are available. As shown in <ref type="table">Table 9</ref>, disabling L T P G (W/O L T P G ) results in 5.9% and 3.5% lower mean F-AP and mean P-AP@1.</p><p>Supervision combination. Should we use the pseudo labels generated from TPG when strong labels are available? As shown in <ref type="table">Table 9</ref>, using only segment-level supervision (W/O weak sup.) results in degradation of mean F-AP by 3.2% and mean P-AP@1 by 2.6%. We observe that small amount of pseudo labels could serve as a type of regularization thus relieve overfitting. However, adding too much noise would lead to performance degradation. For example, when we use pseudo labels for 90% of videos, the mean F-AP is decreased to 58.0%.</p><p>Effect of start point prediction. As shown in <ref type="table">Table 9</ref> AP@1 by 1.7%. As expected, start point prediction improves the accuracy of action start generation by suppressing false positives at non-start frames. Effect of temporal pooling. Information of the current frame may not be the best indicator for start prediction, so we use temporal pooling to make our model more flexible to take temporal information. When it is removed, our model (W/O temp. pool in <ref type="table">Table 9</ref>) obtains worse performances.</p><p>Effect of RNN. Our OAR utilizes LSTM to aggregate temporal information. To investigate the effect of the recurrent architecture, we replace the LSTM with two fully connected layers of size 4096. The performance (W/O RNN in <ref type="table">Table 9</ref>) under both weak and strong supervised settings are much worse than our method which demonstrate the usefulness of RNN in our architecture.</p><p>Effect of ?. The hyper parameter ? in Eq. 6 controls the contribution of the losses from our TPG and OAR modules to the total loss. ? is set to be 0.5 as default. Our method is relatively robust in this hyper-parameter choice. With video-level supervision, our method achieves 54.4%, 55.0% and 54.6% mean F-AP when ? equals 0.5, 1.0 and 2.0, respectively. When strongly supervised, our method obtains 67.1%, 66.3% and 66.6% mean F-AP accordingly.</p><p>WOAD with another TPG option. Although, our TPG is based on <ref type="bibr" target="#b21">[22]</ref>, other weakly supervised techniques may also serve as TPG. We experiment with another offline weakly supervised method, BaS-Net <ref type="bibr" target="#b14">[15]</ref>, to generates action proposals. The results in <ref type="table">Table 10</ref> show that our method outperforms the baselines of W-TALC and BaS-Net largely by 6.4% and 6.8% mean F-AP, and by 5.7% and 2.8% mean P-AP@1, respectively. The clear gaps demonstrate the effectiveness of our design. Besides, WOAD+ performs much better than WOAD which suggests that our method achieves better results when using a more accurate TPG model.</p><p>Model efficiency analysis. Since our model and the baselines use the same features, we compare the inference times after feature extraction. We test all the models under the same environment with a single Tesla V100 GPU. The perframe inference times of TRN, StartNet and our method averaging over the entire test set of THUMOS'14 are 2.60 ms, 0.56 ms and 0.40 ms respectively. The results suggests that our method is the fastest, around 6? faster than TRN. Model size is another key factor, especially for online tasks. Given similar model accuracy, smaller models are preferable, since they require less memory. Number of parameters of TRN, StartNet and our method (TPG+OAR) are 314M, 118M and 110M. Our method has the least number of parameters (3? smaller than TRN).</p><p>Qualitative results. In <ref type="figure">Fig. 3</ref>, we visualize our predicted action and start scores of three representative cases. As it is shown, our method performs fairly good in the case of Long Jump. In the second case (JavelinThrow), our action scores are desirable. Since the visual appearance across frames near start points are very similar, the start scores are (although still reasonable) not as good as those in the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Background <ref type="figure">Figure 3</ref>. Qualitative results of our weakly-supervised method. The last row of each group indicates the predicted start scores (blue bars) and the second to the last row (colored bars) indicates the predicted action scores of the ground-truth class.</p><p>first case. In the third case (Diving), there are two actions occurred with a very short break in between. This makes it very hard to estimate starts based on the subtle visual and motion differences. Thus, the gap of start scores between start and non-start points are relatively small.</p><p>Our weaknesses. Our TPG is basically an offline weakly supervised method which performs poorly in long videos especially when there are very few training videos available. The low accuracy of the generated pseudo labels may result in an unsatisfactory performance of our method in the above scenarios. We conducted experiments on TVSeries <ref type="bibr" target="#b5">[6]</ref> dataset which contains only 20 long videos for training. With the two-stream features used in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>, our method achieves 59.1% and 82.2% mean cAP in weakly and strongly supervised settings. <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b27">[28]</ref> reported 79.2% and 83.7%, respectively. As suggested by the results, our method is not as competitive as it is in other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose WOAD to address online action detection using weak supervision. Previous methods rely on segmentlevel annotations for training which leads to significant amount of human effort and hinders the model scalability. Our proposed WOAD can be trained using only video-level labels and is largely improved when strong labels are available. Experimental results demonstrate that our method with weak supervision obtains comparable performance to the existing strongly-supervised approaches on the online action detection tasks and achieves the state-of-the-art results when strongly supervised.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the proposed WOAD in the training phase. A feature extractor is used to extract features of the input video. Frame features, Fi, are then obtained by a fully connected layer (FC) activated with ReLU and serve as inputs to both Temporal Proposal Generator (TPG) and Online Action Recognizer (OAR). TPG is trained using video-class labels and its generated class-wise temporal proposals are used as pseudo ground truth of action boundaries to supervise the training of OAR. See details in Sec. 3.2 and Sec. 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>ti</head><label></label><figDesc>is obtained by applying max pooling along temporal axis from h t?M i up to h t i as in Eq. 3, where M indicates the length of the temporal window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2 http://activity-net.org/challenges/2016/download.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al. proposed S-CNN [25] to conduct action proposal generation, classification and regression via multi-stage networks. Dai et al. introduced TCN [5] that utilizes temporal context of proposals to improve proposal generation. Xu et al. presented R-C3D [27] that improves model efficiency by sharing the processing stages of proposal generation and classification. Buch et al. proposed SST [2] to conduct fast proposal generation. Zeng et al. modeled relations among proposals using Graph Convolutional Networks</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>WOAD 21.9 32.9 40.5 44.4 48.1 49.8 50.8 51.7 52.4 53.1 54.4 Comparison with our baseline under weakly supervised setting on THUMOS'14. V indicates video-level (weak) supervision. Comparison with our baseline under weakly supervised setting on ActivityNet1.2. V indicates video-level (weak) supervision.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">mean P-AP@ Time Threshold (Seconds)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Sup.</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>mean F-AP</cell></row><row><cell>W-TALC [22]</cell><cell>V</cell><cell cols="10">16.2 26.0 31.3 34.6 36.2 37.6 38.6 39.3 39.9 40.3</cell><cell>48.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">mean P-AP@ Time Threshold (Seconds)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Sup.</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>mean F-AP</cell></row><row><cell>W-TALC [22]</cell><cell>V</cell><cell cols="10">5.2 8.5 10.7 12.8 14.5 15.9 17.1 18.1 19.1 20.1</cell><cell>53.8</cell></row><row><cell>WOAD</cell><cell></cell><cell cols="10">7.9 11.6 14.3 16.4 18.8 20.3 22.2 23.4 24.7 25.3</cell><cell>66.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>33.5 39.6 42.5 46.2 46.6 47.7 48.3 48.6 49.0 WOAD V 21.9 32.9 40.5 44.4 48.1 49.8 50.8 51.7 52.4 53.1 Comparison with strongly-supervised method for online detection of action start on THUMOS'14. V and S denote video-level (weak) and segment-level (strong) supervision, respectively. Best and second-best per column are highlighted.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">mean P-AP@ Time Threshold (Seconds)</cell></row><row><cell></cell><cell cols="2">Methods</cell><cell>Sup.</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell></cell><cell cols="2">StartNet [11]</cell><cell cols="10">S 21.9 S 28.0 40.6 45.7 48.0 50.1 51.0 51.9 52.4 53.0 53.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">mean P-AP@ Time Threshold (Seconds)</cell></row><row><cell></cell><cell cols="2">Methods</cell><cell>Sup.</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell></cell><cell cols="2">StartNet [11]</cell><cell>S</cell><cell cols="9">7.5 11.5 14.1 16.5 18.4 19.7 20.9 21.8 22.9 23.6</cell></row><row><cell></cell><cell cols="2">WOAD</cell><cell>V</cell><cell cols="9">7.9 11.6 14.3 16.4 18.8 20.3 22.2 23.4 24.7 25.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>S</cell><cell cols="9">8.7 13.6 17.0 19.7 21.6 23.0 24.7 25.8 26.8 27.7</cell></row><row><cell cols="13">Table 4. Comparison with strongly-supervised method for online detection of action start on ActivityNet1.2. V and S denote video-level</cell></row><row><cell cols="13">(weak) and segment-level (strong) supervision, respectively. Best and second-best per column are highlighted.</cell></row><row><cell>Methods</cell><cell></cell><cell cols="5">Sup. mean F-AP mean P-AP@ 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">TRN [28] StartNet [11]</cell><cell>S</cell><cell>43.8 -</cell><cell></cell><cell>-4.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WOAD</cell><cell></cell><cell>V</cell><cell>44.0</cell><cell></cell><cell>5.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>S</cell><cell>46.8</cell><cell></cell><cell>5.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Table 5. Comparison with strongly-supervised methods on Activ-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">ityNet1.3. V and S denote video-level (weak) and segment-level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(strong) supervision.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">Sup. Param. # Infer time mean F-AP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TRN [28]</cell><cell>S</cell><cell cols="2">314M</cell><cell>2.60 ms</cell><cell></cell><cell>51.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WOAD</cell><cell>V</cell><cell cols="2">110M</cell><cell>0.40 ms</cell><cell></cell><cell>54.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Table 6. Comparison with strongly-supervised method for online</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">per-frame action recognition on THUMOS'14. The reported times</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">do not include the processing time of feature extraction. V and S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">indicate video-level (weak) and segment-level (strong) supervision.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .Table 10 .</head><label>710</label><figDesc>Comparison with strongly-supervised SOTA method on THUMOS'14. V+10%S means that 10% of videos have segment-level (strong) annotations and others have video-level (weak) annotations. Comparison of our method implemented with different modules as TPG on THUMOS'14 dataset. WOAD+ indicates our framework using BaS-Net as TPG.</figDesc><table><row><cell>,</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To perform fair comparison, we evaluate on its frame-level predictions (S i inFig. 2)during inference under the online constraint.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement. Discussions with Peng Tang and Kathy</head><p>Baxter are gratefully acknowledged. We thank Zuxuan Wu, Zeyuan Chen and Salesforce researchers for the help of improving the writing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rethinking online action detection in untrimmed videos: A novel online evaluation protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Baptista-R?os</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><forename type="middle">J</forename><surname>L?pez-Sastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Acevedo-Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saturnino</forename><surname>Maldonado-Basc?n</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SST: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cees Snoek, and Tinne Tuytelaars. Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Roeland De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to discriminate information for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjun</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RED: Reinforced encoder-decoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">TURN TAP: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Startnet: Online detection of action start in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with expectation-maximization multi-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial background-aware loss for weakly-supervised temporal activity localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3c-net: Category count and center loss for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Phuc Xuan Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wtalc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online action detection in untrimmed, streaming videos-modeling and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><forename type="middle">Giroi</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">R-C3D: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal recurrent networks for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Marginalized average attentional network for weakly-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
