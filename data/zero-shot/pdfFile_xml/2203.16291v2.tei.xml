<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Yildiz</surname></persName>
							<email>s:b.yildiz@tudelft.nl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyran</forename><surname>Khademi</surname></persName>
							<email>s.khademi@tudelft.nl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">Maria</forename><surname>Siebes</surname></persName>
							<email>r.m.siebes@vu.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
							<email>j.c.vangemert@tudelft.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce AmsterTime: a challenging dataset to benchmark visual place recognition (VPR) in presence of a severe domain shift. AmsterTime offers a collection of 2,500 well-curated images matching the same scene from a street view matched to historical archival image data from Amsterdam city. The image pairs capture the same place with different cameras, viewpoints, and appearances. Unlike existing benchmark datasets, AmsterTime is directly crowdsourced in a GIS navigation platform (Mapillary). We evaluate various baselines, including non-learning, supervised and self-supervised methods, pre-trained on different relevant datasets, for both verification and retrieval tasks. Our result credits the best accuracy to the ResNet-101 model pre-trained on the Landmarks dataset for both verification and retrieval tasks by 84% and 24%, respectively. Additionally, a subset of Amsterdam landmarks is collected for feature evaluation in a classification task. Classification labels are further used to extract the visual explanations using Grad-CAM for inspection of the learned similar visuals in a deep metric learning models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual place recognition (VPR) involves inferring a geographical location of a single image with broad applications in robotics, consumer photography, social media, and archival repositories. The question of "where was this photo taken?" is answered for a query image, by retrieving the most similar match from a geo-tagged gallery of images. Thus, VPR is conveniently formulated as a content-based image retrieval problem, where the image representation is key.</p><p>The ideal image representation for VPR maps all the images capturing the same place close to each other, regardless of various viewpoints, illuminations, appearances, and capturing sensors. At the same time, similar places are mapped far enough from all other images, in an n-dimensional latent space. <ref type="bibr" target="#b0">[1]</ref>, where the quality of the representation mapping is measured against precision and recall over all queries, commonly captured in mean average precision (mAP) <ref type="bibr" target="#b1">[2]</ref>. In this conduct, ranking perfection is achieved once all images of the same place are ranked higher than all others in the gallery. These defined criteria for the VPR task, lend themselves to a image-similarity learning problem.</p><p>In the past decade, deep similarity learning became a dominant framework by using (dis)similar image pairs to train convolutional neural networks (CNN) such as Siamese <ref type="bibr" target="#b2">[3]</ref> and Triplet models <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. At inference time, the trained CNN model is used as a feature extractor for the image retrieval There exist numerous benchmark datasets for VPR, while none of them are directly crowd-sourced by retrieving similar visual places. A common approach to obtain positive images (most difficult) for VPR task is Geographic Information System based (GIS) annotations, e.g., from street view images with known GIS information <ref type="bibr" target="#b5">[6]</ref>. GIS-based pairing, labels all the images with the same geographical altitude and latitude as similar, despite the fact that all the images taken from a single point do not share visual similarities, an example is orthogonal viewpoints. Others take images, taken by different people in social media or online photography platforms, from known landmarks such as Eiffel, Pyramids, etc <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. A problem with the latter is the undesired bias towards popular geographical hotspots since many common architectural forms and typologies are excluded from the dataset. Other datasets use vehicle trajectories to capture the same scenes in different time frames ranging from seasonal to yearly intervals, tapering the scope of the VPR task to appearance-invariant learning and evaluation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. All of these semi-automated pair mining methods, even though efficient for learning relevant visual features, are either unfaithful to visual similarity notion or relatively facile to trustfully benchmark the VPR task. In this paper, we introduce the first crowd-sourced benchmark dataset for the VPR task based on a visual search to match an archival query with street view images in the Mapillary navigation platform <ref type="bibr" target="#b0">1</ref> . In turn, all the matching pairs are verified by a human expert to verify the correct matches and evaluate the human competence in the VPR task for further references. The properties of our dataset referred to as AmsterTime 2 are summarized as:</p><p>? 1200+ license-free images from the Amsterdam city Archive, representing urban places in the city of Amsterdam, captured in the past century by many photographers. ? All archival queries are matched with street view images from Mapillary. ? All matches are verified by architectural historians and Amsterdam inhabitants. ? Image pairs are archival and street views capturing the same place with different cameras, time lags, structural changes, occlusion, viewpoint, appearance, and illuminations. ? The dataset exhibits a domain shift between query and the gallery due to significant difference between scanned archival and street view images. We embrace data scarcity as a realistic setting and we purposely limit AmsterTime dataset for evaluating the VPR baselines rather than training. We also add visual similarity learning baselines with the latest self-supervision frameworks and visual inspection with Grad-CAM <ref type="bibr" target="#b11">[11]</ref> model to qualitatively evaluate the learned visual features. We list the contributions as: 1) Various baselines including recent self-supervised Sim-Siam <ref type="bibr" target="#b12">[12]</ref> model is evaluated on AmsterTime dataset. 2) VGG-16 <ref type="bibr" target="#b13">[13]</ref> and ResNet-50 <ref type="bibr" target="#b14">[14]</ref> models are trained on a very large Google Landmarks dataset <ref type="bibr" target="#b7">[8]</ref> for visual similarity learning with a self-supervised framework. 3) Relevant landmarks from Amsterdam city are collected into a new classification dataset, from Google Landmarks dataset, to evaluate the learned similarity features, using class activation mapping frameworks such as Grad-CAM <ref type="bibr" target="#b11">[11]</ref>. 4) Visual explanations are generated using Grad-CAM model to inspect the visual similarities learned in the self-supervised models. AmsterTime covers lifelong temporal coverage of Amsterdam city with severe domain shift between query and gallery </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets Imagery</head><p>Oxford RobotCar <ref type="bibr" target="#b15">[15]</ref> Car Traverse Berlin Kudamm <ref type="bibr" target="#b9">[10]</ref> Train Traverse Mapillary SLS <ref type="bibr" target="#b16">[16]</ref> Mapillary Street View Pittsburgh-30k <ref type="bibr" target="#b17">[17]</ref> Google Street View Tokyo247 <ref type="bibr" target="#b18">[18]</ref> Google Street View Nordland <ref type="bibr" target="#b8">[9]</ref> Train Traverse Garden points <ref type="bibr" target="#b19">[19]</ref> Car Traverse AmsterTime (ours) Mapillary Street View + Archive which is uniquely challenging to benchmark VPR models as the baseline results indicate. t-SNE visualization of all the images in the dataset is given in <ref type="figure" target="#fig_0">Fig. 1</ref> and some example image pairs are also given in <ref type="figure" target="#fig_1">Fig. 2</ref>. The dataset and the evaluation code are available at the project repository. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Datasets</head><p>There are valuable survey papers that may be consulted for broader discussion over developments of VPR models and applications <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>. This work is based on <ref type="bibr" target="#b22">[22]</ref> that introduces unsupervised domain adaption and attention mechanism to solve the domain shift between query and gallery images in VPR task. Unlike <ref type="bibr" target="#b22">[22]</ref>, focusing on learning from large unpaired image sets from two domains of archival and street views, we develop a benchmark dataset of cross-domain image pairs to reliably evaluate learned image representations.</p><p>Among popular VPR datasets (Tab. I), the Berlin Kudamm dataset <ref type="bibr" target="#b9">[10]</ref>, exhibit extreme viewpoint variation in the query and reference traverses. This dataset contains recurring and upfront dynamic objects which are uncommon to any other VPR dataset. Nordland dataset <ref type="bibr" target="#b8">[9]</ref> sample images are one of the highly seasonally variant datasets and have manually introduced lateral viewpoint variation. Gardens Point dataset <ref type="bibr" target="#b19">[19]</ref> images are presented here highly illumination variant and accompanied with lateral viewpoint variation. In contrast to these works, our dataset is unique in that it offers all the possible image variations including viewpoints, illuminations, appearances, and capturing sensors resulting in domain shift effect between the image pairs and thus extremely challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-supervised representation learning</head><p>In addition to the dataset, we investigate the performance of self-supervised similarity learning models for solving the VPR task, which is relevant because training data is scarce. In practice, constructing tuple training data and hard negative mining for deep visual similarity learning turned into a scalability bottleneck for suitable training datasets. Recently, Chen et al <ref type="bibr" target="#b12">[12]</ref> introduced a promising self-supervised framework based on Siamese networks that are trained only with positive image pairs, discarding altogether learning from dissimilar  pairs. We are inspired by <ref type="bibr" target="#b12">[12]</ref>, that significantly reduces the combinatorial complexity of contrastive learning.</p><p>In general, self-supervised learning is used for task-agnostic representation learning <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b12">[12]</ref> commonly by a contrastive learning framework and Siamese networks. Interestingly, competitive performance is reported in the literature for self-supervised learning compare to the supervised learning models <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b12">[12]</ref>. Among the self-supervised models, SimCLR <ref type="bibr" target="#b25">[25]</ref> needs both negative and positive pairs with large batch sizes. While, SimSiam <ref type="bibr" target="#b12">[12]</ref> has an extra predictor module on one branch of its network which provides asymmetry and it prevents collapsing even with relatively small batch sizes in absence of negative pairs. Moreover, Barlow Twins <ref type="bibr" target="#b27">[27]</ref> aims redundancy reduction in the representations by using a loss function on the cross-correlation matrix of the embedding which also prevents trivial solutions without the need for asymmetry in the Siamese setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CROWDSOURCING AMSTERTIME A. Data Collection</head><p>Crowdsourcing is a popular way to gather training and evaluation data for deep learning models. Well-known crowdsourcing platforms such as Amazon Mechanical Turk <ref type="bibr" target="#b28">[28]</ref> and AutoML <ref type="bibr" target="#b29">[29]</ref>, are well suited for general crowdsourcing, yet, they are not adequate for our data collection application due to the complexity of the implementation of GIS layers. Therefore, we developed a custom crowdsourcing web application ( <ref type="figure" target="#fig_2">Fig. 3</ref>). This annotation tool shows a participant the combination of an archival image and a 3D street-view navigator from the Mappilary platform. The navigator is positioned close to the expected location where the archival image is being taken, according to the available metadata, allowing the user to expand or zoom and match the archival and contemporary image in a game-like fashion. The tool also provides an evaluation interface, where administrators can manually verify or deny submissions. This task takes only a fraction of the time per image in comparison to the annotation task itself as it is a binary classification task of acceptance or rejection <ref type="bibr" target="#b30">[30]</ref>.</p><p>The selection of archival images originates from a fairly well-documented area of architectural and urban history ( <ref type="figure" target="#fig_3">Fig. 4)</ref>, in the Beeldbank repository of the Amsterdam City Archives 4 -the world's largest city archive. Moreover, the annotators are familiar with the place, from which the data is collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmark Tasks</head><p>AmsterTime includes 1231 matched archival and corresponding street view image pairs. We used these pairs to create  <ref type="table" target="#tab_0">THE MODELS ARE TRAINED  ON THE GIVEN DATASET AND EVALUATED ON AMSTERTIME DATASET. SIFT AND LIFT FEATURES ARE CONVERTED TO 128 DIMENSIONAL BOVW. THE  FOLLOWING THREE CNN ARCHITECTURES ARE IMAGENET-PRETRAINED MODELS USED ONLY FOR EVALUATION. NETVLAD AND AP-GEM  ARCHITECTURES ARE ALSO PRE-TRAINED MODELS AND USED ONLY FOR EVALUATION. EXCEPT THE FIRST SIMSIAM MODEL, WE TRAINED THE REST OF  THEM WITH SELF-SUPERVISION WITH THE COMBINATION OF 2 BACKBONES AND 3 DATASETS. THE FIRST SIMSIAM MODEL IS THE PRE-TRAINED MODEL</ref> FROM THE ORIGINAL PAPER <ref type="bibr" target="#b12">[12]</ref>  both the verification and retrieval tasks that are closely related.</p><p>Verification is a binary classification (auxiliary) task to detect a pair of archival and street view images of the same place. The verification task for AmsterTime dataset has all of the crowdsourced image pairs as positive labeled, where the same number of negative samples are generated by randomly pairing archival and street view images summing up to a total of 2,462 pairs in the verification task.</p><p>Retrieval is the main task corresponding to VPR, in which a given query image is matched with a set of gallery images. For the retrieval task AmsterTime dataset offers 1231 query images where the leave-one-out set serves as the gallery images for each query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. BASELINE EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We take the pairwise distance between two highdimensional feature vectors corresponding to all the images in AmsterTime. The average of all distance values generated by pairwise comparisons in the dataset is used as a threshold distance to classify positive (similar) or negative (dissimilar) pairs. None of the baseline models that we trained uses the dataset labels. Self-supervised models only use AmsterTime images but does not use the pairing annotations.</p><p>We calculate mean average precision mAP , T op1 and T op5 accuracy metrics for retrieval task using cosine distance. For a given query archival image, we first sort all street view images by the distance between the archival image and the street view images in ascending order then the metrics are calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Local Image Features</head><p>We investigated how local image features perform on AmsterTime dataset. The SIFT <ref type="bibr" target="#b31">[31]</ref> descriptors are used to extract local features and they were then aggregated into one global feature per image using bag-of-visual-words encoding (BoVW) <ref type="bibr" target="#b33">[33]</ref>. The process repeated for the descriptors extracted with the LIFT <ref type="bibr" target="#b32">[32]</ref> trained on Piccadilly dataset <ref type="bibr" target="#b34">[34]</ref>. The bag size is chosen 128 which performs best among others. The results for the verification and retrieval tasks using BoVW are given in Tab. II. Accuracy for verification task for SIFT is 58% (8% above the random baseline) and for LIFT 57%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Off-the-shelf (pre-trained) CNN models</head><p>We evaluated the performance of image features extracted from commonly used CNN models pre-trained on different datasets and tasks including image classification and visual place recognition (VPR), on AmsterTime dataset.</p><p>The models VGG-16 <ref type="bibr" target="#b13">[13]</ref>, ResNet-50 <ref type="bibr" target="#b14">[14]</ref>, and ResNet-101 <ref type="bibr" target="#b5">[6]</ref> are pre-trained on ImageNet <ref type="bibr" target="#b35">[35]</ref> for image classification and used directly from PyTorch's library. The CNN models are only used for extracting features of the images in AmsterTime dataset. The features are obtained from the last convolutional layer followed by a ReLU and a maxpooling layers for VGG-16 model and from adaptive average pooling layer for ResNet models. The features are then utilized to calculate scores for verification and retrieval tasks. The results are given in Tab. II. One noticeable point is that VGG-16 works better than both ResNet-50 and ResNet-101 on verification task. That margin is much bigger on retrieval task as VGG-16 has 13% top-1 accuracy while ResNet-50 has 4% and ResNet-101 has 3% top-1 accuracy.</p><p>In the next step, we used NetVLAD <ref type="bibr" target="#b5">[6]</ref> pre-trained on Pittsburgh250k <ref type="bibr" target="#b18">[18]</ref> for VPR task as a close match to our task of image retrieval. In addition, we evaluated AP-GeM <ref type="bibr" target="#b1">[2]</ref> pretrained on Landmaks-clean dataset <ref type="bibr" target="#b6">[7]</ref> on AmsterTime dataset. The NetVLAD model has a VGG-16 backbone while AP-GeM has ResNet-101 backbone. Neither of them are trained further than the publicly available model weights. As usual, the models are used to extract image features for both verification and retrieval task. AP-GeM and NetVLAD models result in 84% and 82% accuracies on verification task, respectively. The pre-trained AP-GeM achieves the best performance among all the baselines as it leverages the largest training dataset which is very similar to images in AmsterTime dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Self-supervised Baseline</head><p>Due to the limited size of AmsterTime, self-supervision is a suitable option to exploit data without labels. SimSiam <ref type="bibr" target="#b12">[12]</ref> is a recent method that combines self-supervision and similarity learning without needing for neither negative samples nor large batches. We evaluated six SimSiam models with different data and architectures presented in the last row section in Tab. II. The list starts from ResNet-50 model trained on ImageNet 5 . We trained two more ResNet-50 models on Google Landmarks (GLDv2) and AmsterTime datasets with same settings except that the batch size is 128 in our trainings. Since ImageNet and GLDv2 are relatively large datasets and AmsterTime is limited dataset, the model trained on GLDv2 is trained for 100 epochs and the model trained on AmsterTime dataset is trained for 10000 epochs to equalize the number of used minibatches during training. Moreover, three VGG-16 6 models are trained on the same datasets (ImageNet, GLDv2 and AmsterTime) with the same settings as bare supervised VGG-16 has better results than ResNet-50 remarked in Sec. IV-C. The models started the self-supervised training from scratch (with random parameters) and after the self-supervised training are completed, the trained models used as usual to extract features from the images in AmsterTime dataset. The results are presented in Tab. II. Contrary to the better results for pretrained VGG-16 model mentioned in Sec. IV-C, ResNet-50 outperformed in self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Supervised Baseline</head><p>To have a real supervised baseline and measure the supervision gap, we also trained a ResNet-18 model in Triplet setting <ref type="bibr" target="#b4">[5]</ref> with grand-truth pair labels. The dataset first divided into train and test with the ratio of 4 : 1. after each 30 epochs), 0.9 momentum and 0.00001 weight decay. The model is then used to extract feature on the test set. The results for verification and retrieval are given in Tab. III. Due to the limited size of AmsterTime dataset, the supervision gap is only around 7% in mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. VISUAL EXPLANATIONS</head><p>We investigate the learned representation of the Sim-Siam <ref type="bibr" target="#b12">[12]</ref> on AmsterTime dataset using Grad-CAM <ref type="bibr" target="#b11">[11]</ref>. Grad-CAM requires a classification layer at the end of CNN architecture while SimSiam-trained models trained on similarity learning. To adapt Grad-CAM, (1) we add a randomly initialized liner classifier at the end of the SimSiam-trained models, (2) train the newly added classifier on a curated similar dataset with class labels (landmarks). The parameters of SimSiam-trained models are frozen after training on Am-sterTime dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset for visualization</head><p>To facilitate visualization we curated a subset of Google Landmarks dataset v2 (GLDv2) <ref type="bibr" target="#b7">[8]</ref> because it is semantically close to AmsterTime. Particularly, 50 landmarks are selected in GLDv2 which are located in the city of Amsterdam. Some of the hand-picked similar images have been given in <ref type="figure">Fig. 5</ref>. We will refer to this subset for the classification </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Labels</head><p>Training Datasets <ref type="figure">Fig. 7</ref>. Grad-CAM visualizations of three ResNet-50 models pre-trained with SimSiam <ref type="bibr" target="#b12">[12]</ref> on three different datasets given in the labels on the left. Top labels denotes both class activation for Grad-CAM and grand-truth class labels. The visualizations suggest that models learn the structure in the images.</p><p>dataset as GLDv2-Amsterdam hereafter. The histogram of class distribution of GLDv2-Amsterdam, presented in <ref type="figure" target="#fig_4">Fig. 6</ref>, shows a highly skewed and imbalanced distribution. To alleviate the training suffering from the imbalanced dataset, of the linear classifier, underrepresented classes were simply duplicated GLDv2-Amsterdam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training the Linear Classifier</head><p>The linear classifier is trained based on <ref type="bibr" target="#b12">[12]</ref>. A randomlyinitialized linear classifier added to a frozen model is trained for 90 epochs with batch size= 256 using SGD with the parameters of cosine-decay-scheduled, initial lr= 30.0 , weight decay= 0, and momentum= 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Grad-CAM visualization</head><p>We created two visualizations: The first is to show how the same models trained on different datasets learn different visual features, and the second one is to see how a model reacts to different class activations of Grad-CAM.</p><p>Firstly, we selected a subset among the intersection of correctly classified images in GLDv2-Amsterdam dataset by three ResNet-50 models pre-trained on ImageNet, GLDv2, and AmsterTime with SimSiam self-supervision. The output of the last convolutional layer of the ResNet-50 models is visualized. The Grad-CAM visualizations for each of the selected images and for each of the models are created and imposed on the original images. In this setting, the grand-truth classes of the images are used as class activation to show the reactions of the models to the images w.r.t. grand-truth class activation. The visualizations are given in <ref type="figure">Fig. 7</ref>. The models give more weight to the landmark objects in the images, indicating that the models correctly focus on landmark features.</p><p>To illustrate the reactions of the models in comparison with other class activations besides the grand-truths, we also created a Grad-CAM visualization matrix given in <ref type="figure" target="#fig_6">Fig. 8</ref>. For this visualization matrix, we used the ResNet-50 model pretrained on AmsterTime dataset with SimSiam self-supervision. The visualizations for images w.r.t. grand-truth class activation appears on the diagonal. This matrix shows that the model relies on the landmark object once the class activation is either the grad-truth class or the class of the images with similar landmarks such as De Gooyer and Molen van Sloten.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We introduced AmsterTime a reliable and challenging evaluation dataset with verification and retrieval benchmark tasks for visual place recognition. AmsterTime dataset consists of ? 2500 archival and street view images matched by human annotators. Various image representation baselines including the local features, supervised and self-supervised models are tested on AmsterTime. The results suggest that supervised model trained on a large and similar dataset of Landmarks outperforms the self-supervised models. Oblation studies are carried out using visual explanations to investigate the learned features confirming the quality of AmsterTime dataset in learning relevant features despite its small size using selfsupervised models. The code for this paper including the image features is available on a GitHub repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGEMENT</head><p>This work is partially supported by Volkswagen Foundation under ArchiMediaL project. We show our gratitude to all the people who helped us to annotate data including Tino Mager, Beate L?ffler, Carola Hein, and Victor de Boer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The visualization of AmsterTime dataset using t-SNE task. Similar to other learned image descriptors (features), the underlying relations in the training data determine similar visual elements. For instance, similar (positive) image pairs with different illuminations in the training, potentially leading to an illumination-agnostic model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Sample image pairs from AmsterTime dataset. Challenges are extreme occlusions, view point changes, camera lens distortions, color changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Screenshot of the ArchiMediaL annotator app</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The urban distribution of AmsterTime dataset shows the concentration of data at the center of Amsterdam following the archival imagery pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Besides ground-truth (positive) pairs, equal number of pseudo negative pairs are randomly generated by pairing the archival and street-view images. The model trained for 90 epochs with SGD optimizer with 128 of batch size, 0.001 learning rate (decayed by 0.1 Histogram of the number of images per selected 50 landmarks located in Amsterdam from Google Landmarks Dataset v2 shows that class distribution is unbalanced and solved simply by duplicating images in underrepresented classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Grad-CAM visualizations of ResNet-50 model pre-trained with SimSiam [12] on AmsterTime dataset. The visualizations on the diagonal and the intersection of De Gooyer and Molen van Sloten show that the model activates more when the activation class images are similar to the input images which indicates the model learned the structures in the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RECENT</head><label>I</label><figDesc>VPR DATASETS (LEFT) WITH THE CORRESPONDING DATA CAPTURING AND ANNOTATION MEDIUM (RIGHT). AMSTERTIME COMBINES TWO IMAGE DOMAINS TO REPRESENT THE SAME PLACE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RESULTS</head><label>II</label><figDesc>FOR VERIFICATION AND RETRIEVAL TASKS. THE BACKBONE ARCHITECTURES ARE GIVEN IN THE PARENTHESES.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>AND USED ONLY FOR EVALUATION. BOLD NUMBERS DENOTE THE BEST SCORES FOR EACH COLUMN.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Verification</cell><cell></cell><cell></cell><cell></cell><cell>Retrieval</cell></row><row><cell>Method</cell><cell>Train Dataset</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="5">Acc ROC AUC mAP Top1 Top5</cell></row><row><cell>SIFT [31] w/ BoVW</cell><cell>N/A</cell><cell>0.57</cell><cell cols="3">0.65 0.61 0.58</cell><cell>0.61</cell><cell>0.03</cell><cell>0.01</cell><cell>0.04</cell></row><row><cell>LIFT [32] w/ BoVW</cell><cell>Piccadilly</cell><cell>0.56</cell><cell cols="3">0.60 0.58 0.57</cell><cell>0.59</cell><cell>0.03</cell><cell>0.01</cell><cell>0.04</cell></row><row><cell>VGG-16 [13]</cell><cell>ImageNet</cell><cell>0.75</cell><cell cols="3">0.63 0.68 0.71</cell><cell>0.78</cell><cell>0.18</cell><cell>0.13</cell><cell>0.23</cell></row><row><cell>ResNet-50 [14]</cell><cell>ImageNet</cell><cell>0.63</cell><cell cols="3">0.66 0.65 0.64</cell><cell>0.69</cell><cell>0.06</cell><cell>0.04</cell><cell>0.08</cell></row><row><cell>ResNet-101 [14]</cell><cell>ImageNet</cell><cell>0.63</cell><cell cols="3">0.67 0.65 0.64</cell><cell>0.69</cell><cell>0.05</cell><cell>0.03</cell><cell>0.07</cell></row><row><cell>NetVLAD (VGG-16) [6]</cell><cell>Pittsburgh250k</cell><cell>0.83</cell><cell>0.80</cell><cell cols="2">0.82 0.82</cell><cell>0.90</cell><cell>0.26</cell><cell>0.17</cell><cell>0.33</cell></row><row><cell>AP-GeM (ResNet-101) [2]</cell><cell>Landmarks</cell><cell>0.88</cell><cell>0.78</cell><cell cols="2">0.83 0.84</cell><cell>0.92</cell><cell>0.35</cell><cell>0.24</cell><cell>0.48</cell></row><row><cell cols="2">SimSiam (ResNet-50) [12] ImageNet</cell><cell>0.75</cell><cell cols="3">0.76 0.75 0.75</cell><cell>0.83</cell><cell>0.19</cell><cell>0.12</cell><cell>0.26</cell></row><row><cell>SimSiam (ResNet-50)</cell><cell>GLDv2</cell><cell>0.80</cell><cell cols="3">0.79 0.80 0.80</cell><cell>0.86</cell><cell>0.23</cell><cell>0.15</cell><cell>0.32</cell></row><row><cell>SimSiam (ResNet-50)</cell><cell>AmsterTime</cell><cell>0.72</cell><cell cols="3">0.75 0.73 0.73</cell><cell>0.81</cell><cell>0.19</cell><cell>0.12</cell><cell>0.26</cell></row><row><cell>SimSiam (VGG-16)</cell><cell>ImageNet</cell><cell>0.63</cell><cell cols="3">0.72 0.67 0.65</cell><cell>0.71</cell><cell>0.10</cell><cell>0.06</cell><cell>0.14</cell></row><row><cell>SimSiam (VGG-16)</cell><cell>GLDv2</cell><cell>0.63</cell><cell cols="3">0.77 0.70 0.66</cell><cell>0.75</cell><cell>0.12</cell><cell>0.07</cell><cell>0.18</cell></row><row><cell>SimSiam (VGG-16)</cell><cell>AmsterTime</cell><cell>0.77</cell><cell cols="3">0.70 0.73 0.74</cell><cell>0.81</cell><cell>0.16</cell><cell>0.10</cell><cell>0.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF SUPERVISED TRAINED RESNET-18 WITH TRIPLET LOSS<ref type="bibr" target="#b4">[5]</ref> ON AMSTERTIMEDATASET. THE NUMBERS ARE MERELY TO QUANTIFY THE SUPERVISION GAP COMPARED TO UNSUPERVISED MODELS IN TAB. II</figDesc><table><row><cell></cell><cell cols="2">Verification</cell><cell></cell><cell></cell><cell>Retrieval</cell></row><row><cell cols="2">Precision Recall</cell><cell cols="5">Acc ROC AUC mAP Top1 Top5</cell></row><row><cell>0.85</cell><cell cols="2">0.89 0.87</cell><cell>0.93</cell><cell>0.42</cell><cell>0.30</cell><cell>0.53</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.mapillary.com 2 This project is partly funded by ArchiMediaL project.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/seyrankhademi/AmsterTime</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Beeldbank Stadsarchief Amsterdam. The Beeldbank contains several hundred thousand images taken in the streets of Amsterdam since the nineteenth century, among them many images of facades, buildings, and streets. https://archief.amsterdam/beeldbank/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This model is used from SimSiam authors' shared models.<ref type="bibr" target="#b5">6</ref> VGG-16 model architecture is with batch normalization which is used directly from PyTorch's library.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">75684  184025  148923  73205  129964  36882  36160  113010  10763  22511  87237  179579  112036  75142  133792  139117  179829  166495  88579  21368  143693  190190  175609  54967  120509  150368  160303  89319  74844  105461  105232  200159  79233  40600  27248  19357  71305  46523  23356  61753  199957  42838  34224  181963  181349  100245  118813  105339  99505  46847</ref> </div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On sensitive minima in margin-based deep distance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serajeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="067" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with average precision: Training image retrieval with a listwise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R D</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIMBAD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors with triplets and shallow convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>in European conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single-view place recognition under seasonal changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Olid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>F?cil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPNIV Workshop at</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Only look once, mining distinctive landmarks from convnet for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maffra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364916679498</idno>
		<ptr target="http://dx.doi.org/10.1177/02783649166794982" />
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mapillary street-level sequences: A dataset for lifelong place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Warburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gargallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual place recognition with repetitive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the performance of convnet features for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<idno>abs/1501.04158</idno>
		<ptr target="http://arxiv.org/abs/1501.041582" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on deep visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Masone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="19" to="516" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual place recognition: A survey from deep learning perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention-aware ageagnostic visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Big selfsupervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Amazon mechanical turk: A research tool for organizations and information systems scholars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crowston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shaping the future of ict research. methods and approaches</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="210" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">AutoML Vision</title>
		<ptr target="https://cloud.google.com/vision/automl/docs.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning from history</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siebes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research and Education in Urban History in the Age of Digital Libraries</title>
		<editor>F. Niebling, S. M?nster, and H. Messemer</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="213" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1470" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Robust global translations with 1dsfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="61" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
