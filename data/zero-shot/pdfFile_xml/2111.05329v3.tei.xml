<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritam</forename><surname>Sarkar</surname></persName>
							<email>pritam.sarkar@queensu.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Etemad</surname></persName>
							<email>ali.etemad@queensu.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present CrissCross, a self-supervised framework for learning audio-visual representations. A novel notion is introduced in our framework whereby in addition to learning the intra-modal and standard 'synchronous' cross-modal relations, CrissCross also learns 'asynchronous' cross-modal relationships. We show that by relaxing the temporal synchronicity between the audio and visual modalities, the network learns strong generalized representations. Our experiments show that strong augmentations for both audio and visual modalities with relaxation of cross-modal temporal synchronicity optimize performance. To pretrain our proposed framework, we use 3 different datasets with varying sizes, Kinetics-Sound, Kinetics400, and AudioSet. The learned representations are evaluated on a number of downstream tasks namely action recognition, sound classification, and retrieval. CrissCross shows state-of-the-art performances on action recognition (UCF101 and HMDB51) and sound classification (ESC50 and DCASE). The codes and pretrained models will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, self-supervised learning has shown great promise in learning strong representations without human-annotated labels <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref>, and emerged as a strong competitor for fully-supervised pretraining. There are a number of benefits to such methods. Firstly, they reduce the time and resources required for expensive human-annotations and allow researchers to directly use large uncurated datasets for learning meaningful representations. Moreover, the models trained in a self-supervised fashion learn more abstract representations, which can be useful in solving a variety of downstream tasks without needing to train the models from scratch.</p><p>Given the abundance of videos, their spatio-temporal information-rich nature, and the fact that in most cases they contain both audio and visual streams, self-supervised approaches are strong alternatives to fully-supervised methods for video representation learning. Moreover, the high dimensionality and multi-modal nature of videos makes them difficult to annotate, further motivating the use of self-supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-modal Loss Synchronous Cross-modal Loss Asynchronous Cross-modal Loss</head><p>: : <ref type="figure">Fig. 1</ref>: Overview. CrissCross learns effective audio-visual representations by exploiting not only intra-modal, but also synchronous and asynchronous cross-modal relations. The sample frames are obtained from Kinetics400 <ref type="bibr" target="#b22">[23]</ref>.</p><p>The common and standard practice in self-supervised audio-visual representations learning is to learn intra-modal and synchronous cross-modal relationships between the audio and visual streams. In this regard, existing solutions try to learn audio-visual representations by maintaining a tight temporal synchronicity between the two modalities <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>. Yet, the impact of learning temporally asynchronous cross-modal relationships in the context of self-supervised learning has not been explored. This notion deserves deeper exploration as learning such temporally asynchronous cross-modal relationships may in fact result in increased invariance and distinctiveness in the learned representations.</p><p>In this study, in an attempt to explore the notion mentioned above, we present CrissCross a novel framework to learn robust generalized representations from videos (a simple illustration is presented in <ref type="figure">Figure 1</ref>). In addition to learning intra-modal and standard synchronous cross-modal relations, CrissCross introduces a novel concept to learn cross-modal representations through relaxing time-synchronicity between corresponding audio and visual segments. We refer to this as 'asynchronous cross-modal' optimization, a concept that has not been explored in prior works. We use 3 datasets of different sizes: Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref>, Kinetics400 <ref type="bibr" target="#b22">[23]</ref>, and AudioSet <ref type="bibr" target="#b15">[16]</ref>, to pretrain CrissCross. We evaluate CrissCross on different downstream tasks, namely action recognition, sound classification, and retrieval. We use 2 popular benchmarks UCF101 <ref type="bibr" target="#b57">[58]</ref> and HMDB51 <ref type="bibr" target="#b27">[28]</ref> to perform action recognition and retrieval, while ESC50 <ref type="bibr" target="#b46">[47]</ref> and DCASE <ref type="bibr" target="#b58">[59]</ref> are used for sound classification. Contributions.The key contributions of this work are as follows:</p><p>? We present a novel framework for multi-modal self-supervised learning by relaxing the audio-visual temporal synchronicity to learn effective generalized representations. Our method is simple, data efficient, and less resource intensive, yet learns robust multi-modal representations for a variety of downstream tasks. ? We perform an in-depth study to explore the performance of the proposed framework and its major concepts. Additionally we extensively investigate a wide range of audio-visual augmentation techniques capable of learning strong audio-visual representations within our framework. ? Comparing the performance of our method to prior works, CrissCross achieves state-of-the-arts on UCF101 <ref type="bibr" target="#b57">[58]</ref>, HMDB <ref type="bibr" target="#b27">[28]</ref>, ESC50 <ref type="bibr" target="#b46">[47]</ref>, and DCASE <ref type="bibr" target="#b58">[59]</ref> when pretrained on Kinetics400 <ref type="bibr" target="#b22">[23]</ref>. Moreover, when trained with AudioSet <ref type="bibr" target="#b15">[16]</ref>, CrissCross achieves better or competitive performances versus the current state-of-the-arts.</p><p>? Lastly, when pretrained on the small-scale dataset Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref>, Criss-Cross outperforms fully-supervised pretraining <ref type="bibr" target="#b32">[33]</ref> by 1.4% and 7.4%, as well as prior self-supervised state-of-the-art <ref type="bibr" target="#b32">[33]</ref> by 11.1% and 19.9% on UCF101 and HMDB51 respectively. To the best of our knowledge, very few prior works have attempted to pretrain on such small datasets, and in fact this is the first time where self-supervised pretraining outperforms full supervision on action recognition in this setup. We hope our proposed self-supervised method can motivate researchers to further explore the notion of asynchronous multi-modal representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-supervised Learning</head><p>Self-supervised learning aims to learn generalized representations of data without any human annotated labels through properly designed pseudo tasks (also known as pretext tasks). Self-supervised learning has recently drawn significant attention in different fields of deep learning such as image <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b51">52]</ref>, video <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">37]</ref>, and wearable data <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref> analysis among others.</p><p>In self-supervised learning, the main focus of interest lies in designing novel pseudo-tasks to learn useful representations. We briefly mention some of the popular categories in the context of self-supervised video representation learning, namely, i) context-based, ii) generation-based, iii) clustering-based, and iv) contrastive learning-based. Various pretext tasks have been proposed in the literature exploring the spatio-temporal context of video frames, for example, temporal order prediction <ref type="bibr" target="#b28">[29]</ref>, puzzle solving <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b0">1]</ref>, rotation prediction <ref type="bibr" target="#b21">[22]</ref>, and others. Generation-based video feature learning methods refer to the process of learning feature representations through video generation <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b52">53]</ref>, video colorization <ref type="bibr" target="#b61">[62]</ref>, and frame or clip prediction <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15]</ref>, among a few others. Clustering-based approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> rely on self-labeling where data is fed to the network and the extracted feature embeddings are clustered using a classical clustering algorithm such as k-means, followed by using the cluster assignments as the pseudo-labels for training the neural network. The key concept of contrastive learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref> is that in the embedding space, 'positive' samples should be similar to each other, and 'negative' samples should have discriminative properties. Using this concept, several prior works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b32">33]</ref> have attempted to learn representations by minimizing the distance between positive pairs and maximizing the distance between negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Audio-Visual Representation Learning</head><p>Typically in multi-modal self-supervised learning, multiple networks are jointly trained on the same pretext tasks towards maximizing the mutual information between multiple data streams <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b56">57]</ref>. Following, we briefly discuss some of the prior works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33]</ref> on audio-visual multi-modal representation learning. A multi-modal self-supervised task introduced in AVTS <ref type="bibr" target="#b26">[27]</ref>, leveraging the natural synergy between audio-visual data. The network is trained to distinguish whether the given audio and visual sequences are 'in sync' or 'out of sync'. The authors propose a two-stream network, where one stream receives audio as input and the other network is fed with the visual data. Next, audio and visual embeddings are fused at the end of the convolution layers, and the joint representations are used to minimize the contrastive loss. In XDC <ref type="bibr" target="#b2">[3]</ref>, the authors introduce a framework to learn cross-modal representations through a self-labelling process. In XDC, cluster assignments obtained from the audio-visual representations are used as pseudo-labels to train the backbones. Specifically, the pseudo-labels computed from audio embeddings are used to train the visual backbone, while the pseudo-labels computed using visual embeddings are used to train the audio network. A self-supervised learning framework based on contrastive learning is proposed in AVID <ref type="bibr" target="#b40">[41]</ref>, to learn audio-visual representations from video. AVID performs instance discrimination as the pretext task. AVID <ref type="bibr" target="#b40">[41]</ref> redefines the notion of positive and negative pairs based on their similarity and dissimilarity in the feature space, followed by optimizing a noise contrastive estimator loss to learn multi-modal representations. This is different from AVTS <ref type="bibr" target="#b26">[27]</ref>, where audio-visual segments originated from the same samples are considered as positive pairs, and segments originated from different samples are considered as negative pairs. Distinctions to our work.We acknowledge that earlier works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33]</ref> show great promise in learning strong multimodal representations. However, we identify some limitations in prior work, which we attempt to address in our study. Most earlier works based on contrastive learning try to find negative and positive pairs through a complex process. Moreover, we notice that over time, the definition of 'positive' and 'negative' pairs have been changing. For instance, we find distinct differences in such definitions amongst some earlier works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40]</ref>. In this study, our goal is to propose a simple yet effective solution for learning multi-modal representations. Additionally, we would like to highlight that earlier works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref> use massive distributed GPU setups , which are significant bottlenecks when computing resources are limited. In this study, we effectively train our method on only 4-8 GPUs. Lastly, as discussed earlier, we hypothesize that to learn effective generalized features, the synchronicity between audio and visual segments could be relaxed. Interestingly, it may appear that our approach is in contrast to some prior works that suggest synchronization <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">50]</ref> is helpful in learning strong multi-modal representations. Nonetheless, our framework exploits both synchronous and asynchronous cross-modal relationships in an attempt to learn both time-dependant and time-invariant representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Here we present the core concepts of our proposed framework. First, we briefly discuss the uni-modal concepts that our model is built on, which are adopted from an earlier work, SimSiam <ref type="bibr" target="#b11">[12]</ref>. Next, we introduce the multi-modal concepts of our framework to jointly learn self-supervised audio-visual representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Uni-modal Learning</head><p>To separately learn visual and audio representations, we follow the setup proposed in <ref type="bibr" target="#b11">[12]</ref>, which we briefly mention here for the sake of completeness. Let's, assume an encoder f , where f is composed of a convolutional backbone followed  We present the uni-modal baselines and the multi-modal setup. The uni-modal setups (Visual-only and Audio-only) are presented in (A) and (B) respectively. The multi-modal framework, CrissCross, is presented in (C). In case of uni-modal setups, we show the predictor heads, as well as the stop-grad to elaborate on the frameworks. However, in case of CrissCross, we skip those components for the sake of simplicity.</p><p>by an MLP projection head, and an MLP prediction head h. Two augmented views of a sample x are created as x 1 and x 2 . Accordingly, the objective is to minimize the symmetrized loss:</p><formula xml:id="formula_0">L x1,x2 = 1 2 D(p 1 , S(z 2 )) + 1 2 D(p 2 , S(z 1 )),<label>(1)</label></formula><p>where D denotes the negative cosine similarity, and output vectors p 1 and z 2 are computed as h(f (x 1 )) and f (x 2 ) respectively. Similarly, p 2 and z 1 are computed as h(f (x 2 )) and f (x 1 ) respectively. Further, we apply stop-grad on the latent vector z 1 and z 2 , which is denoted by S. We extend this concept to learn visual and audio representations as discussed below.</p><p>To learn visual representations from videos, we use a visual encoder f v and a predictor head h v . We generate two augmented views of a sample v as v 1 and v 2 , where v 1 belongs to timestamp t 1 , and v 2 belongs to timestamp t 2 . Finally, we optimize the loss L v1,v2 using Equation 1. We present this concept in <ref type="figure" target="#fig_1">Figure  2A</ref>. Similarly we generate two augmented views of an audio sample a as a 1 and a 2 , where a 1 and a 2 belong to timestamps t 1 and t 2 respectively. We use a 1 and a 2 to optimize L a1,a2 (following Equation 1) using an audio encoder f a and a predictor head h a to learn audio representations. A pictorial representation of this method is depicted in <ref type="figure" target="#fig_1">Figure 2B</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-modal Learning</head><p>Here, we discuss the multi-modal learning components of our proposed framework. We present different ways to learn multi-modal representations, namely Intra-modal, Synchronous Cross-modal, Asynchronous Cross-modal, and finally, CrissCross, which blends all three previous methods. We explain each of these concepts below. Intra-modal Representations. To learn multi-modal representations, our first approach is a joint representation learning method where we train the visual and audio networks with a common objective function L intra . Here, L intra is calculated as (L v1,v2 + L a1,a2 )/2, where L v1,v2 and L a1,a2 are uni-modal losses for visual and audio learning as discussed earlier. Synchronous Cross-modal Representations. To learn cross-modal audiovisual representations, we calculate the distance between the two different modalities, particularly by calculating L a1,v1 corresponding to a 1 , v 1 , and L a2,v2 , corresponding to a 2 , v 2 . Finally, we optimize the synchronous cross-modal loss L sync , which is calculated as (L a1,v1 + L a2,v2 )/2. Asynchronous Cross-modal Representations. Next, we introduce an asynchronous (or cross-time) cross-modal loss to learn local time-invariant representations. Here, we attempt to optimize asynchronous cross-modal representations by calculating L a1,v2 to minimize the distance between feature vectors corresponding to a 1 and v 2 . Similarly, we calculate L a2,v1 to minimize the distance between feature vectors corresponding to a 2 and v 1 . Finally, we calculate the asynchronous cross-modal loss L async as (L a1,v2 + L a2,v1 )/2. CrissCross. Our proposed multi-modal representation learning method is named CrissCross. In this setup, we combine the objective functions of Intra-modal, Synchronous Cross-modal, and Asynchronous Cross-modal learning. Accordingly, we define the final objective function L CrissCross as (L intra + L sync + L async )/3, which gives:</p><formula xml:id="formula_1">L CrissCross = 1 6 (L a1,a2 + L v1,v2 + L a1,v1 + L a2,v2 + L a1,v2 + L a2,v1 ).<label>(2)</label></formula><p>We present the proposed CrissCross framework in <ref type="figure" target="#fig_1">Figure 2C</ref> and its pseudocode in Section S2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relaxing Time Synchronicity</head><p>Audio and visual modalities from the same source clip generally maintain a very strong correlation, which makes them suitable for multi-modal representation learning as one modality can be used as a supervisory signal for the other in a self-supervised setup. However, our intuition behind CrissCross is that these cross-modal temporal correlations do not necessarily need to follow a strict frame-wise coupling. Instead, we hypothesize that relaxing cross-modal temporal synchronicity to some extent can help in learning more generalized representations.</p><p>To facilitate this idea within CrissCross, we present 5 different temporal sampling methods to create the augmented views of a source clip. These 5 temporal sampling methods are designed to explore varying amounts of temporal synchronicity when learning cross-modal relationships. (i ) Sametimestamp: where both the audio and visual segments are sampled from the exact same time window (denoted as none in terms of temporal relaxation). (ii ) Overlapped: where the two views of the audio-visual segments share 50% overlap amongst them (denoted as mild relaxation). (iii ) Adjacent: where adjacent frame sequences and audio segments are sampled (denoted as medium relaxation). (iv ) Far-apart: in which we sample one view from the first half of the source clip, while the other view is sampled from the second half of the source clip (denoted as extreme relaxation). (v ) Random: where the two audio-visual segments are sampled in a temporally random manner (denoted as mixed relaxation). It should be noted that the concept of relaxing cross-modal time synchronicity doesn't apply to the uni-modal setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>The details of the experiment setup and the findings of our thorough empirical studies for investigating the major concepts of our proposed framework are presented here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets. We use 3 datasets of different sizes for pretraining purposes, namely, Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref>, Kinetics400 <ref type="bibr" target="#b22">[23]</ref>, and AudioSet <ref type="bibr" target="#b15">[16]</ref>. Following the standard practices of prior works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref>, we evaluate our self-supervised methods on two types of downstream tasks, (i ) action recognition using UCF101 <ref type="bibr" target="#b57">[58]</ref> and HMDB51 <ref type="bibr" target="#b27">[28]</ref>, and (ii ) sound classification using ESC50 <ref type="bibr" target="#b46">[47]</ref> and DCASE <ref type="bibr" target="#b58">[59]</ref>. We provide additional details for all the datasets in Section S3. Architectures. Following the standard practice among prior works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b44">45]</ref> we use R(2+1)D <ref type="bibr" target="#b62">[63]</ref> and ResNet <ref type="bibr" target="#b20">[21]</ref> as the visual and audio backbones. We use a slightly modified version <ref type="bibr" target="#b40">[41]</ref> of R(2+1)D-18 <ref type="bibr" target="#b62">[63]</ref> as the backbone for visual feature extraction. To extract the audio features, we use ResNet-18 <ref type="bibr" target="#b20">[21]</ref>. The projector and predictor heads of the self-supervised framework are composed of MLPs. The details of all of the architectures are presented in Section S6. Pretraining Details. To train the network in a self-supervised fashion with audio-visual inputs, we downsample the visual streams to 16 frames per second, and feed 0.5-second frame sequences to the visual encoder. We resize the spatial resolution to 112 2 , so the final input dimension to the visual encoder becomes 3 ? 8 ? 112 2 , where 3 represents the 3 channels of RGB. Next, we downsample the audio signals to 16kHz, and segment them into 2-second segments. Next, we transform the segmented raw audio waveforms to mel-spectrograms using 80 mel filters, we set the hop size as 10 milliseconds, and FFT window length as 1024. Finally, we feed spectrograms of shape 80 ? 200 to the audio encoder. We use Adam <ref type="bibr" target="#b25">[26]</ref> optimizer with a cosine learning rate scheduler <ref type="bibr" target="#b30">[31]</ref> to pretrain the encoders and use a fixed learning rate to train the predictors. We provide additional details of the hyperparameters in Section S7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Empirical Study</head><p>Here we present the empirical study performed to investigate the major concepts of our proposed framework. During the empirical study all of the models are trained using Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref> for 100 epochs, unless stated otherwise. We perform transfer learning to evaluate visual and audio representations. Linear evaluation is performed using a one-vs-all SVM classifier (linear kernel) on the fixed features to quickly evaluate our models on downstream tasks. We prefer one-vs-all SVM over training an FC layer to limit parameter tuning at this point. Moreover, to limit memory overhead, we use 0.5 seconds (8 frames) of visual input and 2 seconds of audio input to extract the fixed features. The details of the linear evaluation protocol is mentioned in Section S5. We use UCF101 to evaluate visual representations on action recognition, and ESC50 to evaluate <ref type="table">Table 1</ref>: Ablation study. We present the results of CrissCross and its uni-modal and multi-modal ablation variants. The differences between the ablated variants and CrissCross are also presented in red.</p><formula xml:id="formula_2"># Method Pretrain Db. Downstream Db. UCF101 ESC50 (a) L v 1 ,v 2 Kinetics-Sound 69.1(? 5.7) - (b) L a 1 ,a 2 Kinetics-Sound - 62.0(? 17.0) (c) L intra Kinetics-Sound 69.7(? 5.1) 71.8(? 7.2) (d ) L sync Kinetics-Sound 70.1(? 4.7) 75.8(? 3.2) (e) L async Kinetics-Sound 69.1(? 5.7) 74.8(? 4.2) (f ) L sync + L intra Kinetics-Sound 73.8(? 1.0) 78.0(? 1.0) (g) L async + L intra Kinetics-Sound 72.4(? 2.4) 75.3(? 3.7) (h) L async + L sync Kinetics-Sound 69.1(? 5.7) 74.8(? 4.2) (i) L CrissCross Kinetics-Sound 74.8 79.0 (j ) L sync + L intra Kinetics400 75.8(? 4.1) 78.5(? 3.5) (k ) L async + L intra Kinetics400 74.9(? 5.0) 76.3(? 5.7) (l ) L CrissCross</formula><p>Kinetics400 79.9 82.0 audio representation on sound classification. All of our empirical studies are evaluated using split-1 of both the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Ablation Study</head><p>We present the ablation results in <ref type="table">Table 1</ref> to show the improvement made by optimizing intra-modal, synchronous cross-modal, and asynchronous cross-modal losses. To illustrate the benefits of learning asynchronous relations, we perform ablation studies on 2 pretraining datasets, Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref> and Kinetics400 <ref type="bibr" target="#b22">[23]</ref>. First, using Kinetics-Sound, we train the framework in uni-modal setups, denoted as L v1,v2 and L a1,a2 . We report the top-1 accuracy of UCF101 and ESC50 as 69.1% and 62.0% respectively. Next, we train the network in a multi-modal setup, where we find that L sync outperforms the other multi-modal variants with singleterm losses (L intra and L async ) as well as uni-modal baselines (L v1,v2 and L a1,a2 ). Further study shows that combining different multi-modal losses improve the model performance. Specifically, we notice that L CrissCross outperforms L sync by 4.7% and 3.2% on action recognition and sound classification, respectively. We further investigate the benefits of L CrissCros versus the top 2 ablation competitors (L sync + L intra and L async + L intra ) on the large and diverse Kinet-ics400 <ref type="bibr" target="#b22">[23]</ref>. We observe that L CrissCros outperforms these variants by 4.1% and 3.5% in action recognition and sound classification, respectively, showing the significance of asynchronous cross-modal optimization in a multi-modal setup. Our intuition is that as Kinetics-Sound consists of a few hand-picked classes that are prominently manifested in both audio and visual modalities, the performance gain of CrissCross is less prominent. However, Kinetics400 is considerably larger in scale and is comprised of highly diverse action classes. It therefore benefits more from the more generalized representations learned by asynchronous cross-modal optimization.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Understanding Relaxed Time-synchronicity</head><p>In this subsection we study how different amounts of temporal relaxation in cross-modal synchronicity impacts CrissCross. To do so, we exploit 5 different temporal sampling methods as discussed earlier in Section 3. We further aim to identify the best temporal sampling method in a uni-modal setup. We train Visual-only, Audio-only, and CrissCross frameworks using different temporal sampling methods. The results presented in <ref type="table" target="#tab_0">Table 2</ref> show that the overlapped sampling method works the best overall for both the uni-modal setups. The same temporal sampling method shows poor performance on the visual-only model. However, it performs as good as the overlapped sampling method on the audioonly model. Interestingly, the far-apart sampling shows the worst performance amongst other methods on the Audio-only model, whereas, the Visual-only model works reasonably well with the far-apart sampling method. Next, we test these 5 temporal sampling methods on CrissCross and present the results in <ref type="table" target="#tab_0">Table 2</ref>.</p><p>Interestingly, we notice that the same and far-apart methods, which work poorly in the uni-modal setups, perform reasonably well in a multi-modal setup. We believe, the improvement of performance here is because of the strong supervision received from the other modality. Nonetheless, we find that the overlapped temporal sampling method (mild temporal relaxation) performs relatively better, outperforming the other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Exploring Audio-Visual Augmentations</head><p>We perform an in-depth study to explore the impact of different audio and visual augmentations. Visual Augmentations. We explore a wide range of visual augmentations. As a starting point, we adopt the basic spatial augmentations used in <ref type="bibr" target="#b40">[41]</ref>, which consists of Multi-Scale Crop (MSC), Horizontal Flip (HF), and Color Jitter (CJ). Additionally, we explore other augmentations, namely Gray Scale (GS), Gaussian Blur (GB) <ref type="bibr" target="#b10">[11]</ref>, and Cutout (C) <ref type="bibr" target="#b13">[14]</ref>, which show great performance in image-based self-supervised learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b64">65]</ref>. We explore almost all the possible combinations of different visual augmentations in a uni-modal setup and present the results in <ref type="table" target="#tab_1">Table 3</ref>. The results show that strong augmentations improve the top-1 accuracy by 6.8% in comparison to basic augmentations used in <ref type="bibr" target="#b40">[41]</ref>. We mention the augmentation parameters and implementation details in Section S4. Temporal Consistency of Spatial Augmentations. While investigating different spatial augmentations, we are also interested to know if the spatial aug-mentations should be consistent at the frame level or whether they should be random (i.e., vary among consecutive frames within a sequence). We refer to these concepts as temporarily consistent or temporarily random. We perform an experiment where we apply MSC-HF-CJ-GS randomly at the frame level, and compare the results to applying the same augmentations consistently across all the frames of a sequence. Our results show that maintaining temporal consistency in spatial augmentations across consecutive frames is beneficial, which is in line with the findings in <ref type="bibr" target="#b48">[49]</ref>. Specifically, Temporally random augmentations, results in top-1 accuracy of 53.69%, whereas, the same augmentations applied in a temporally consistent manner results in 68.09%. Audio Augmentations. Similar to visual augmentations, we thoroughly investigate a variety of audio augmentations. Our audio augmentations include, Volume Jitter (VJ), Time and Frequency Masking (Mask) <ref type="bibr" target="#b42">[43]</ref>, Random Crop (RC) <ref type="bibr" target="#b41">[42]</ref>, and Time Warping (TW) <ref type="bibr" target="#b42">[43]</ref>. We also explore almost all the possible combinations of these augmentations, and present the results in <ref type="table" target="#tab_1">Table 3</ref>. Our findings show that time-frequency masking and random crop improve the top-1 accuracy by 17.25% compared to the base variant. We also notice that time warping doesn't improve the performance and is also quite computationally expensive. Hence, going forward we do not use time warping during pretraining. We present the augmentation parameters and additional implementation details in Section S4. Audio-Visual Augmentations. We conduct further experiments on a few combinations of augmentations in a multi-modal setup. We pick the top performing augmentations obtained from the uni-modal variants and apply them concurrently. The results are presented in <ref type="table" target="#tab_1">Table 3</ref> where we find that the results are consistent with the uni-modal setups, as the combination of MSC-HF-CJ-GS-GB-C and VJ-M-RC performs the best in comparison to the other combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Exploring Design Choices</head><p>Predictor. Our empirical study shows that the predictor head plays an important role to effectively train the audio and visual encoders to learn good representations. The predictor architecture is similar to <ref type="bibr" target="#b11">[12]</ref>. For the sake of completeness, we provide the details of the predictor head in Section S6. We explore (i ) different learning rates, and (ii ) using a common vs. a separate predictor in the multimodal setup. It should be noted that none of the variants cause a collapse, even-though we notice considerable differences in performance. We present the findings in the following paragraphs. Additionally, the training curves of both experiments are presented in Section S1.</p><p>Similar to <ref type="bibr" target="#b11">[12]</ref>, we use a constant learning rate for the predictors. However, unlike <ref type="bibr" target="#b11">[12]</ref>, where the predictor learning rate is the same as the base learning rate of the encoder, we find that a higher predictor learning rate helps the network to learn better representations in both uni-modal and multi-modal setups. In case of CrissCross, setting the predictor learning rate to be the same as the base learning rate results in unstable training, and the loss curve shows oscillating behavior. We empirically find that setting the predictor learning rate to 10 times the base learning rate, works well. We present the results in <ref type="table" target="#tab_2">Table 4</ref>.   Next, we evaluate whether the framework can be trained in a multi-modal setup with a common predictor head instead of separate predictor heads (default setup). In simple terms, one predictor head would work towards identity mapping for both audio and video feature vectors. To test this, l2-normalized feature vectors f v (v) and f a (a) are fed to the predictor, which are then used in a usual manner to optimize the cost function. The results are presented in <ref type="table" target="#tab_3">Table 5</ref>. We observe that though such a setup works somewhat well, having separate predictors is beneficial for learning better representations. Projector. We present a comparative study of projection heads with 2 layers vs. 3 layers. We notice 2.4% and 4% improvements in top-1 accuracies when using 3 layers instead of 2 on action recognition and sound classification respectively (please see <ref type="table" target="#tab_3">Table 5</ref>). Note that we use 3 fully-connected layers as the default setup for the projectors. The architecture details are presented in Section S6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Linear Evaluation</head><p>To evaluate the quality of the representations learned through pretraining, we perform linear evaluation on action recognition (HMDB51, UCF101, and Kinetics400) and sound classification (ESC50 and DCASE). As mentioned earlier, we use 3 different sized datasets, i.e., Kinetics-Sound, Kinetics400, and AudioSet for pretraining. In <ref type="table" target="#tab_4">Table 6</ref> we report the top-1 accuracies averaged over all the splits. We provide the details of the evaluation protocol in Section S5. We notice a steady improvement in performance as the dataset size increases, which shows CrissCross can further be scaled on even larger datasets like IG65M <ref type="bibr" target="#b16">[17]</ref>. To evaluate CrissCross in a more critical setting, we test the downstream task performance on Kinetics400 while pretrained with the small-scale Kinetics-Sound (10% of Kinetics400). Our results show CrissCross performs reasonably well even in such a difficult setting, achieving a top-1 accuracy of 39.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison to the State-of-the-Arts</head><p>We compare the proposed CrissCross framework against the state-of-the-arts methods. We validate visual representations on action recognition, and audio representations on sound classification. We present the details in the following. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Action Recognition</head><p>Full-Finetuning. In line with <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b32">33]</ref>, we benchmark CrissCross using UCF101 <ref type="bibr" target="#b57">[58]</ref> and HMDB51 <ref type="bibr" target="#b27">[28]</ref> on action recognition. We briefly mention the experimental setup for downstream evaluation here and redirect readers to Section S5 for additional information. We use the pretrained 18-layer R(2+1)D <ref type="bibr" target="#b62">[63]</ref> as the video backbone, and fully finetune it on action recognition. We use the Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref>, Kinetics400 <ref type="bibr" target="#b22">[23]</ref>, and AudioSet <ref type="bibr" target="#b15">[16]</ref> for pretraining. For a fair comparison to earlier works, we adopt 2 setups for finetuning, once with 8 frames, and the other with 32 frames. In both these setups, we use a spatial resolution of 224 2 . We tune the model using the split-1 of both datasets and report the top-1 accuracy averaged over all the splits. We notice large variability in experimental setups in the literature in terms different backbones (e.g., deeper convnets, transformer-based architectures, etc.) <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b45">46]</ref>, pretraining inputs (e.g., the addition of optical flow or text to audio-visual data, etc.) <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b1">2]</ref>, and pretraining datasets, making it impractical to compare to all the prior works. Following the inclusion criteria of earlier works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40]</ref>, we compare CrissCross with methods that use similar backbones, inputs, and pretraining datasets.</p><p>The comparison of CrissCross with recent works is presented in <ref type="table" target="#tab_5">Table 7</ref>. The fully-supervised baselines compared to CrissCross are taken directly from prior works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b2">3]</ref> and have not been implemented by ourselves. When pretrained with Kinetics400, CrissCross achieves state-of-the-arts on UCF101 and HMDB51 in both the fine-tuning setups. CrissCross outperforms current state-of-the-arts AVID <ref type="bibr" target="#b40">[41]</ref> on UCF101 and HMDB51 by 3.2% and 4.8%, respectively, when fine-tuned with 8 frame inputs. Additionally, while fine-tuned with 32 frames, CrissCross outperforms current state-of-the-arts GDT <ref type="bibr" target="#b44">[45]</ref> by 0.6% and 2.4% on UCF101 and HMDB51 respectively.</p><p>Next, CrissCross outperforms the current state-of-the-art AVID <ref type="bibr" target="#b40">[41]</ref>, when pretrained on AudioSet and fine-tuned with 8-frame inputs, on both UCF101 and HMDB51. When fine-tuned with 32-frame inputs, CrissCross achieves competitive results amongst the leading methods. We note that some of the prior works show slightly better performance compared to ours in some settings. We conjecture this to be due to the use of higher spatio-temporal resolution inputs in these models. E.g., BraVe <ref type="bibr" target="#b49">[50]</ref> is trained with 2 views of 32?112 2 and 128?112 2 , and the input size for MMV <ref type="bibr" target="#b1">[2]</ref> and CM-ACC <ref type="bibr" target="#b32">[33]</ref> are 32?224 2 and 16?224 2 , respectively. In comparison, CrissCross is pretrained with visual inputs of size 8?112 2 . However, we expect the performance of our model to improve further by using such higher resolutions, given the trend shown in <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>In addition to the commonly used Kinectis400 and AudioSet, we further evaluate CrissCross while pretrained on the small-scale Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref>. Here, we observe significant improvements compared to CM-ACC <ref type="bibr" target="#b32">[33]</ref> on UCF101 (88.3 vs. 77.2) and HMDB51 (60.5 vs. 40.6). Additionally, CrissCross outperforms fullysupervised pretraining by 1.4% and 7.4% on UCF101 and HMDB51 respectively when both the fully-supervised and self-supervised methods are pretrained on Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref>. To the best of our knowledge, this is the first time that self-supervision outperforms full-supervised pretraining on action recognition using the same small-scale dataset, showing that our method performs well on limited pretraining data.</p><p>Retrieval. In addition to full finetuning, we also compare the performance of CrissCross in an unsupervised setup. Following prior works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b4">5]</ref>, we perform a retrieval experiment. We use the split-1 of both UCF101 <ref type="bibr" target="#b57">[58]</ref> and HMDB51 <ref type="bibr" target="#b27">[28]</ref> and present the comparison with prior works in <ref type="table" target="#tab_6">Table 8</ref>. We observe that CrissCross outperforms the current state-of-the-arts on UCF101, while achieving competitive results for HMDB51. We present additional details for the retrieval experiment setup in Section S5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sound Classification</head><p>To evaluate audio representations learned by CrissCross, we use two popular benchmarks ESC50 <ref type="bibr" target="#b46">[47]</ref> and DCASE <ref type="bibr" target="#b58">[59]</ref> to perform sound classification. We find large variability of experimental setups in the literature for evaluating audio representations. For instance, different backbones, input lengths, datasets, and  evaluation protocols (linear evaluation, full-finetuning) have been used, making it impractical to compare to all the prior works. Following <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b1">2]</ref>, we perform linear evaluation on ESC50 <ref type="bibr" target="#b46">[47]</ref> and DCASE <ref type="bibr" target="#b58">[59]</ref>. To be able to compare our work to a larger number of prior works, we perform linear evaluation using both 2 and 5-second inputs for ESC50, and 1-second input for DCASE. Please refer to Section S5 for additional details on the evaluation protocols. As presented in <ref type="table" target="#tab_7">Table 9</ref>, when pretrained on Kinetics400 and evaluated with 2-second inputs, CrissCross outperforms the current state-of-the-art AVID [41] by 2.4%. Additionally, when pretrained on AudioSet and evaluated with 5-second inputs, CrissCross marginally outperforms the current state-of-the-art, BraVe <ref type="bibr" target="#b49">[50]</ref>. Finally, CrissCross sets new state-of-the-art by outperforming all prior works on DCASE when pretrained on Kinetics400 and AudioSet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>We propose a novel self-supervised framework to learn audio-visual representations by considering intra-modal, as well as, synchronous and asynchronous cross-modal relationships. We conduct a thorough study investigating the major concepts of our framework. Our findings show that properly composed strong augmentations and relaxation of cross-modal temporal synchronicity is beneficial for learning effective audio-visual representations. These representations can then be used for a variety of downstream tasks including action recognition, sound classification, and retrieval. Limitations. The notion of asynchronous cross-modal optimization has not been explored beyond audio-visual modalities. For example, our model can be expanded to consider more than 2 modalities (e.g., audio, visual, and text), which are yet to be studied. Additionally, we notice a considerable performance gap between full-supervision and self-supervision when both methods are pretrained with the same large-scale dataset (Kinetics400 or AudioSet), showing room for further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>The organization of the supplementary material is as follows: -Section S1: Training Statistics -Section S2: Pseudocode; -Section S3: Details of all the datasets; -Section S4: Details of data augmentations; -Section S5: Downstream evaluation protocols; -Section S6: Architecture details; -Section S7: Hyperparameters and training details; -Section S8: Qualitative analysis.</p><p>-Section S9: Broader Impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1 Training Statistics</head><p>To provide a better understanding of our training process, we present the loss curves of the self-supervised pretraining. In <ref type="figure" target="#fig_1">Figures S1 and S2</ref> we present the training curves obtained during design choice explorations on Kinetics-Sound. <ref type="figure">Figure S1</ref> shows that, setting the predictor learning rate to be the same as the base learning rate, results in oscillating behavior at the initial phase of the pretraining, which results in learning poor representations as already discussed in the main paper. Next, we notice that using the common predictor head, results in the training losses saturating very quickly ultimately yielding worse performance compared to the use of separate predictor heads. Additionally, when analysing the training curves in the default setup, the intra-modal losses reach a steady state quite early in comparison to the cross-modal losses. The total loss still decreases beyond our scheduled training iterations. However, we stop the training when no improvement is noticed in the downstream tasks. <ref type="figure">Fig. S1</ref>: We present the loss curves for different predictor learning rates. La1a2 and Lv1v2 refer to the intra-modal losses; La1v1 and La2v2 denote the synchronous cross-modal losses; and finally La1v2 and Lv1a2 refer to the asynchronous crossmodal losses. Note that the minimum possible value for each loss is -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Pseudocode</head><p>We present the pseudocode of our proposed CrissCross framework in Algorithm 1. Please note this pseudocode is written in a Pytorch-like format.  <ref type="figure" target="#fig_1">Fig. S2</ref>: We present the pretraining loss curves for common vs. separate predictor heads. La1a2 and Lv1v2 refer to the intra-modal losses; La1v1 and La2v2 denote the synchronous cross-modal losses; and finally La1v2 and Lv1a2 refer to the asynchronous cross-modal losses. Note that the minimum possible value for each loss is -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3 Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3.1 Pretraining Datasets</head><p>We use 3 datasets of different sizes for pretraining, namely, Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref>, Kinetics400 <ref type="bibr" target="#b22">[23]</ref>, and AudioSet <ref type="bibr" target="#b15">[16]</ref>. Kinetics-Sound is a small-scale action recognition dataset, which has a total of 22K video clips, distributed over 32 action classes. Kinetics400 is a medium-scale human action recognition dataset, originally collected from YouTube. It has a total of 240K training samples and 400 action classes. Please note that Kinetics-Sound is a subset of Kinetics400, and consists of action classes which are prominently manifested audibly and visually <ref type="bibr" target="#b3">[4]</ref>. Lastly, AudioSet <ref type="bibr" target="#b15">[16]</ref> is a large-scale video dataset of audio events consisting of a total of 1.8M audio-video segments originally obtained from YouTube spread over 632 audio classes. Please note that none of the provided labels are used in self-supervised pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3.2 Downstream Datasets</head><p>Following the standard practices of prior works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref>, we evaluate our self-supervised methods on two types of downstream tasks: (i ) action recognition based on visual representations and (ii ) sound classification based on audio representations. To perform action recognition, we use two popular benchmarks, i.e., UCF101 <ref type="bibr" target="#b57">[58]</ref> and HMDB51 <ref type="bibr" target="#b27">[28]</ref>. UCF101 consists of a total of 13K clips distributed among 101 action classes, while HMDB contains nearly 7K video clips distributed over 51 action categories. To perform sound classification, we use two popular benchmarks ESC50 <ref type="bibr" target="#b46">[47]</ref> and DCASE2014 <ref type="bibr" target="#b58">[59]</ref>. ESC50 is a collection of 2K audio events comprised of 50 classes and DCASE2014 is an audio event dataset of 100 recordings spread over 10 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4 Data Augmentation</head><p>Visual Augmentations. The parameters for visual augmentations are presented in <ref type="table">Table S1</ref>. Some of the parameters are chosen from the literature, while the rest are found through empirical search. We set the parameters of Multi Scale Crop, Gaussian Blur, and Gray Scale as suggested in <ref type="bibr" target="#b10">[11]</ref>, and the parameters for Color Jitter are taken from <ref type="bibr" target="#b40">[41]</ref>. We use TorchVision <ref type="bibr" target="#b43">[44]</ref> for all the implementations of visual augmentations, except Cutout where we use the implementation available <ref type="table">Table S1</ref>: Visual augmentation parameter details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmentation Parameters</head><p>Multi Scale Crop min area = 0.08</p><p>Horizontal Flip p = 0.5</p><p>Color Jitter brightness = 0.4 contrast = 0.4 saturation = 0.4 hue = 0.2 Gray Scale p = 0.2</p><p>Gaussian Blur p = 0.5</p><p>Cutout max size = 20 num = 1  <ref type="table" target="#tab_1">Table S3</ref>: Visual augmentation summary.  <ref type="bibr" target="#b2">3</ref> . Please note that for the Cutout transformation, the mask is created with the mean value of the first frame in the sequence. We summarize the augmentation schemes used for pretraining and evaluation in <ref type="table" target="#tab_1">Table S3</ref>. Audio Augmentations. We present the parameters used for audio augmentations in <ref type="table" target="#tab_0">Table S2</ref>. We use the Librosa <ref type="bibr" target="#b34">[35]</ref> library to generate mel-spectrograms. We use the techniques proposed in <ref type="bibr" target="#b42">[43]</ref> to perform Time Mask, Frequency Mask, and Time Warp transformations 4 . The parameters for the audio augmentations are set empirically, except for Random Crop which we adopt from <ref type="bibr" target="#b41">[42]</ref>. We summarize the augmentation scheme of pretraining and evaluation in <ref type="table" target="#tab_2">Table S4</ref>.</p><formula xml:id="formula_3">MSC HF CJ GS GB C Pretraining ? ? ? ? ? ? Full-finetune ? ? ? ? ? ? Linear evaluation ? ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5 Downstream Evaluation Protocol</head><p>To evaluate the representations learned with self-supervised pretraining, we test the proposed framework in different setups, namely linear evaluation, full finetuning, and retrieval. The details of the evaluation protocols are mentioned below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5.1 Linear Evaluation</head><p>To perform linear evaluation of the learned representations on downstream tasks, we extract fixed features (also called frozen features) using the pretrained backbones. We train a linear classifier using the fixed feature representations. The details are presented below. S5.1.1 Action Recognition. To perform linear evaluation on action recognition, we follow standard evaluation protocols laid out in prior works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b40">41]</ref>. The details are presented below. HMDB51 and UCF101. We perform linear evaluation in 2 setups, i.e., 8-frame and 32-frame inputs. We evaluate on 8-frame inputs for the design explorations and 32-frame inputs for large-scale experiments.</p><p>Following the protocols mentioned in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">50]</ref>, we feed 8-frame inputs to the video backbone, with a spatial resolution of 224 2 . During training, we randomly pick 25 clips per sample to extract augmented representations, while during testing, we uniformly select 10 clips per sample and report top-1 accuracy at sample level prediction by averaging clip level predictions. The augmentation techniques are mentioned in Section S4. We don't apply the Gaussian Blur while extracting the training features since it deteriorates the performance. Moreover, to perform a deterministic evaluation, we don't apply any augmentations during validation. The visual features are extracted from the final convolution layer and passed to a max-pool layer with a kernel size of <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4)</ref>  <ref type="bibr" target="#b40">[41]</ref>. Finally, we use the learned visual representations to train a linear SVM classifier, we sweep the cost values between {0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 1} and report the best accuracy.</p><p>When validating on 32-frame inputs, we could not perform SVM as the feature vector is too large to hold in the memory. Hence, we use a linear fully-connected layer at the end of the video backbone. Note that during training the backbone is kept frozen and only the linear layer is trained. we keep the rest of the setup the same as described earlier, with the exception of training where we randomly select 10 clips per sample. Kinetics400. As Kinetics400 <ref type="bibr" target="#b22">[23]</ref> is a large scale dataset, the feature vector is too large to save in memory. Following <ref type="bibr" target="#b40">[41]</ref>, we use a fully connected layer at the end of the frozen backbone and feed 8 ? 224 2 frame inputs. During training, we randomly pick 1 clip per sample, while during validation, we uniformly select 10 clips per sample. Note that the rest of the setups remain the same, as described for HMDB51 and UCF101. Finally, we obtain the sample level prediction by averaging the clip level predictions and report the top-1 accuracy. S5.1.2 Sound Classification. In case of evaluating audio representations, we follow the evaluation protocol laid out in prior works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b49">50]</ref> for respective datasets. The details are mentioned below. ESC50. In case of sound classification on ESC50, we use 2 and 5-second audio inputs to extract audio representations. Following <ref type="bibr" target="#b44">[45]</ref>, we extract 10 epochs worth of augmented feature vectors from the training clips. During testing, when using 2-second inputs, we extract 10 equally spaced audio segments <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b2">3]</ref>, and when using 5-second inputs, we extract 1 segment <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">50]</ref> from each sample. We perform the augmentations mentioned in Section S4 to extract the training features. We notice that unlike self-supervised pretraining, time warping improves the model performance in the linear evaluation. We do not apply any augmentations during validation. We extract the representations from the final convolution layer and pass it through a max-pool layer with a kernel size of (1, 3) and a stride of (1, 2) <ref type="bibr" target="#b44">[45]</ref>. Similar to action recognition, we perform classification using a one-vs-all linear SVM classifier, we sweep the cost values between {0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 1} and report the best accuracy. DCASE. To validate on DCASE, we follow the protocol mentioned in <ref type="bibr" target="#b40">[41]</ref>. We extract 60 clips per sample and train a linear classifier on the extracted representations. Note that the augmentation and feature extraction schemes remain the same as mentioned for ESC50. We report the top-1 sample level accuracies by averaging the clip level predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5.2 Full Finetuning</head><p>Following earlier works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5]</ref>, we use the pretrained visual backbone along with a newly added fully-connected layer for full finetuning on UCF101 <ref type="bibr" target="#b57">[58]</ref> and HMDB51 <ref type="bibr" target="#b27">[28]</ref>. We adopt two setups for full finetuning, 8-frame inputs and 32frame inputs. In both cases we use a spatial resolution of 224 2 . Lastly, we replace the final adaptive average-pooling layer with an adaptive max-pooling layer. We find that applying strong augmentations improves the model performance in full-finetuning. Please see the augmentation details in Section S4. During testing, we extract 10 equally spaced clips from each sample and do not apply any augmentations. We report the top-1 accuracy at sample level prediction by averaging the clip level predictions. We use an SGD optimizer with a multi-step learning rate scheduler to finetune the model. We present the hyperparameters of full-finetuning in <ref type="table" target="#tab_15">Table S10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5.3 Retrieval</head><p>In addition to full-finetuning, we also perform retrieval to test the quality of the representations in an unsupervised setup. We follow the evaluation protocol laid out in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b67">68]</ref>. We uniformly select 10 clips per sample from both training and test splits. We fit 2-second inputs to the backbone to extract representations. We empirically test additional steps such as l2-normalization and applying batchnormalization on the extracted features, and notice that they do not help the performance. Hence, we simply average the features extracted from the test split to query the features of the training split. We compute the cosine distance between the feature vectors of the test clips (query) and the representations of all the training clips (neighbors). We consider a correct prediction if k neighboring clips of a query clip belong to the same class. We calculate accuracies for k = 1, 5, 20. We use the NearestNeighbors 5 API provided in SciKit-Learn in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6 Architecture Details</head><p>In this study we use a slightly modified version of R(2+1)D-18 <ref type="bibr" target="#b62">[63]</ref> as the video backbone as proposed in <ref type="bibr" target="#b40">[41]</ref>, and ResNet-18 <ref type="bibr" target="#b20">[21]</ref> as the audio backbone. For the sake of completeness we present the architecture details in <ref type="table" target="#tab_3">Tables S5 and   5</ref> sklearn.neighbors.NearestNeighbors S6, respectively. The predictor and projector heads are made of fully-connected layers following <ref type="bibr" target="#b11">[12]</ref>, and their architecture details are presented in <ref type="table" target="#tab_5">Table S7</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7 Hyperparameters and Training Details</head><p>In this section, we present the details of the hyperparameters, computation requirements, as well as additional training details of self-supervised pretraining and full finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.1 Pretraining Details</head><p>We present the pretraining hyperparameters of CrissCross in <ref type="table" target="#tab_7">Table S9</ref>. Most of the parameters remain the same across all 3 datasets, with the exception of a few hyperparameters such as learning rates and epoch size which are set depending on the size of the datasets. We train on Kinetics-Sound with a batch size of 512, on a single node with 4 Nvidia RTX-6000 GPUs. Next, when training on Kinetics400 and AudioSet, we use 2 nodes and set the batch size to 2048. Adam <ref type="bibr" target="#b25">[26]</ref> optimizer is used to train our proposed framework. We use LARC 6 <ref type="bibr" target="#b68">[69]</ref>    as a wrapper to the Adam optimizer to clip the gradients while pretraining with a batch size of 2048. In this work, we stick to batch sizes of 512 and 2048, because (i ) as they show stable performance based on the findings of <ref type="bibr" target="#b11">[12]</ref>; (ii ) they fit well with our available GPU setups. Additionally, we perform mixed-precision training <ref type="bibr" target="#b35">[36]</ref> using PyTorch AMP <ref type="bibr" target="#b43">[44]</ref> to reduce the computation overhead. Ablation Parameters. In the ablation study, we keep the training setup exactly identical across all the variants, with the exception of the learning rates, which we tune to find the best performance for that particular variant. For example, we set the base learning rate for visual-only and audio-only models as 0.0001 and 0.00001 respectively. Next, the predictor learning rates are set to 0.001 and 0.0001 for the visual-only and audio-only variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.2 Full Finetuning Details</head><p>The full finetuning hyperparameters for both the benchmarks are presented in <ref type="table" target="#tab_15">Table S10</ref>. We use a batch size of 32 for the 32-frame input and 64 for the 8-frame input. We use an SGD optimizer with multi-step learning rate scheduler to finetune the video backbones. Please note that we perform the full finetuning on a single Nvidia RTX-6000 GPU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8 Qualitative Analyses</head><p>To perform a qualitative analysis of the learned representations, we visualize the nearest neighborhoods of video-to-video and audio-to-audio retrieval. In this experiment, we use Kinetics400 <ref type="bibr" target="#b22">[23]</ref> to pretrain CrissCross. The pretrained backbones are then used to extract training and validation feature vectors from Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref>. We use the Kinetics-Sound for retrieval experiment as it consists of action classes which are prominently manifested both audibly and visually. Next, we use the features extracted from the validation split to query the training features. We present the qualitative results in Figures S3 and S4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9 Broader Impact</head><p>Better self-supervised audio-visual learning can be used for detection of harmful content on the Internet. Additionally, such methods can be used to develop better multimedia systems. Lastly, the notion that relaxed cross-modal temporal synchronicity is useful, can challenge our existing/standard approaches in learning multi-modal representations and result in new directions of inquiry. The authors don't foresee any major negative impacts. <ref type="figure">Fig. S3</ref>: Visualization of video-to-video retrieval. The frames with black borders represent the query embedding, and the next 5 frames represent the top-5 neighborhoods. We highlight the correct retrievals with green, while the wrong ones are marked with red. Additionally, we also mention the class names at the bottom of each frame. We notice very few instances of wrong retrieval, which generally occur when the visual scenes are highly similar, for instance 'playing piano' and 'playing organ', 'playing saxophone' and 'playing clarinet'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. S4</head><p>: Visualization of audio-to-audio retrieval. The frames with black borders represent the query embedding, and the next 5 frames represent the top-5 neighborhoods. We highlight the correct retrievals with green, while the wrong ones are marked with red. Additionally, we also mention the class names at the bottom of each frame. We notice very few instances of wrong retrieval, which generally occur when the sound events are audibly very similar for instance, 'playing keyboard' and 'playing xylophone', 'playing saxophone' and 'playing accordion'.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Our proposed framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-CJ-GS-C 68.3 VJ-M-TW 49.5 MSC-HF-CJ-GS-GB 68.7 VJ-M-RC 62.0 MSC-HF-CJ-GS-GB-C 69.1 Visual + Audio UCF101 ESC50 Multi MSC-HF-CJ-GS-C + VJ-M-RC 73.9 79.0 MSC-HF-CJ-GS-GB + VJ-M-RC 73.5 79.0 MSC-HF-CJ-GS-GB-C + VJ-M-RC 74.8 79.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>= fv(v1), fv(v2) # visual embeddings pv1, pv2 = hv(zv1), hv(zv2) # predictor output # audio za1, za2 = fa(a1), fa(a2) # audio embeddings pa1, pa2 = ha(za1), ha(za2) # predictor output # loss calculation, # D: loss function # D: loss function # asynchronous cross-modal loss Lv1a2 = D(pv1, za2)/2 + D(pa2, zv1)/2 # v1-a2 La1v2 = D(pa1, zv2)/2 + D(pv2, za1)/2 # a1-v2 L_async = (Lv1a2 + La1v2)/2 # synchronous cross-modal loss Lv1a1 = D(pv1, za1)/2 + D(pa1, zv1)/2 # v1-a1 Lv2a2 = D(pv2, za2)/2 + D(pa2, zv2)/2 # v2-a2 L_sync = (Lv1a1 + Lv2a2)/2 # intra-modal loss Lv1v2 = D(pv1, zv2)/2 + D(pv2, zv1)/2 # v1-v2 La1a2 = D(pa1, za2)/2 + D(pa2, za1)/2 # a1-a2 L_intra = (Lv1v2 + La1a2)/2 # total loss L_CrissCross = (L_async + L_sync + L_intra)/3 return L_CrissCross</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>S9: Pretext training parameters. Note the abbreviations used below, KS: Kinetics-Sound, K400: Kinetics400, AS: AudioSet, Adam * : Adam with LARC dataset bs es ep optim lrs lrvb(start/end) lrab(start/end) lrvp lrap wd betas</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>dataset input es bs ep ms optim lrs lr ? wd mtm drp UCF101 8?224 2</head><label>2</label><figDesc>95K 64 20 6/10/14 SGD multi-step 0.0005 0.3 0.0 0.9 0.0 UCF101 32?224 2 95K 32 20 8/12/16 SGD multi-step 0.00007 0.3 0.0 0.9 0.0 HMDB51 8?224 2 35K 64 20 6/10/14 SGD multi-step 0.0005 0.1 0.0 0.9 0.0 HMDB51 32?224 2 35K 32 20 8/12/16 SGD multi-step 0.0001 0.3 0.0 0.9 0.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Temporal sampling. Exploring temporal sampling methods for multi-modal and uni-modal representation learning.</figDesc><table><row><cell>Temp. Sampling</cell><cell cols="2">Uni-modal</cell><cell cols="2">CrissCross</cell></row><row><cell>(Temp. Relaxation)</cell><cell cols="4">UCF101 ESC50 UCF101 ESC50</cell></row><row><cell>Same (None)</cell><cell>55.6</cell><cell>62.0</cell><cell>73.2</cell><cell>77.0</cell></row><row><cell>Overlapped (Mild)</cell><cell>68.3</cell><cell>62.0</cell><cell>73.9</cell><cell>79.0</cell></row><row><cell>Adjacent (Medium)</cell><cell>68.1</cell><cell>58.8</cell><cell>71.5</cell><cell>77.8</cell></row><row><cell>Random (Random)</cell><cell>65.1</cell><cell>60.3</cell><cell>72.5</cell><cell>77.8</cell></row><row><cell>Far-apart (Extreme)</cell><cell>63.8</cell><cell>58.3</cell><cell>72.8</cell><cell>78.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Augmentations. Exploring audio-visual augmentations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>A comparative study of different predictor lr w.r.t the base lr (lr b ).</figDesc><table><row><cell></cell><cell cols="2">UCF101</cell><cell cols="2">ESC50</cell></row><row><cell></cell><cell cols="4">lrb 10?lrb lrb 10?lrb</cell></row><row><cell>La 1,a2</cell><cell>-</cell><cell>-</cell><cell cols="2">60.3 62.0</cell></row><row><cell>Lv 1,v2</cell><cell cols="2">66.0 69.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">LCrissCross 59.0 74.8 62.3 79.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Exploring design choices for the predictor and projector heads.</figDesc><table><row><cell></cell><cell cols="2">Predictor</cell><cell cols="2">Projector</cell></row><row><cell></cell><cell cols="4">Common Separate 2 Layers 3 Layers</cell></row><row><cell>UCF101</cell><cell>73.6</cell><cell>74.8</cell><cell>72.4</cell><cell>74.8</cell></row><row><cell>ESC50</cell><cell>75.3</cell><cell>79.0</cell><cell>75.0</cell><cell>79.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>We present the results of linear evaluation on action recognition and sound classification.</figDesc><table><row><cell cols="7">Pretraining Dataset Size HMDB51 UCF101 Kinetics400 ESC50 DCASE</cell></row><row><cell>Kinetics-Sound</cell><cell>22K</cell><cell>45.7</cell><cell>78.1</cell><cell>39.0</cell><cell>82.8</cell><cell>93.0</cell></row><row><cell>Kinetics400</cell><cell>240K</cell><cell>50.0</cell><cell>83.9</cell><cell>44.5</cell><cell>86.8</cell><cell>96.0</cell></row><row><cell>AudioSet</cell><cell>1.8M</cell><cell>56.2</cell><cell>87.7</cell><cell>49.4</cell><cell>90.5</cell><cell>97.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>SOTA comparison on action recognition. Top-1 accuracy averaged over all the splits on UCF101 and HMDB51 are presented. We group the results based on the pretraining dataset. Additionally, we present the architecture details and finetuning input size of the respective methods.</figDesc><table><row><cell>Method</cell><cell>Pretrain Dataset</cell><cell cols="2">Compute Backbone</cell><cell>Finetune Resolution</cell><cell cols="2">UCF101 HMDB51</cell></row><row><cell cols="2">Fully Supervised[33] KS</cell><cell>-</cell><cell cols="2">3D-ResNet18 32?224 2</cell><cell>86.9</cell><cell>53.1</cell></row><row><cell>CM-ACC[33]</cell><cell>KS</cell><cell cols="3">40 GPUs 3D-ResNet18 32?224 2</cell><cell>77.2</cell><cell>40.6</cell></row><row><cell>CrissCross</cell><cell>KS</cell><cell cols="2">4 GPUs R(2+1)D-18</cell><cell>8?224 2 32?224 2</cell><cell>84.0 88.3</cell><cell>51.2 60.5</cell></row><row><cell cols="2">Fully Supervised[45] K400</cell><cell>-</cell><cell>R(2+1)D-18</cell><cell>32?224 2</cell><cell>95.0</cell><cell>74.0</cell></row><row><cell>XDC [3]</cell><cell>K400</cell><cell cols="2">64 GPUs R(2+1)D-18</cell><cell>8?224 2</cell><cell>74.2</cell><cell>39.0</cell></row><row><cell>AVID [41]</cell><cell>K400</cell><cell cols="2">64 GPUs R(2+1)D-18</cell><cell>8?224 2</cell><cell>83.7</cell><cell>49.5</cell></row><row><cell>Robust-xID [40]</cell><cell>K400</cell><cell cols="2">8 GPUs R(2+1)D-18</cell><cell>8?224 2</cell><cell>81.9</cell><cell>49.5</cell></row><row><cell>CrissCross</cell><cell>K400</cell><cell cols="2">8 GPUs R(2+1)D-18</cell><cell>8?224 2</cell><cell>86.9</cell><cell>54.3</cell></row><row><cell>DPC [19]</cell><cell>K400</cell><cell cols="2">4 GPUs S3D</cell><cell>25?128 2</cell><cell>75.7</cell><cell>35.7</cell></row><row><cell>CBT [60]</cell><cell>K400</cell><cell cols="2">8 GPUs S3D</cell><cell>16?112 2</cell><cell>79.5</cell><cell>44.6</cell></row><row><cell>AVTS [27]</cell><cell>K400</cell><cell cols="2">4 GPUs MC3-18</cell><cell>25?224 2</cell><cell>84.1</cell><cell>52.5</cell></row><row><cell>SeLaVi [5]</cell><cell>K400</cell><cell cols="2">64 GPUs R(2+1)D-18</cell><cell>32?112 2</cell><cell>83.1</cell><cell>47.1</cell></row><row><cell>XDC [3]</cell><cell>K400</cell><cell cols="2">64 GPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>86.8</cell><cell>52.6</cell></row><row><cell>AVID [41]</cell><cell>K400</cell><cell cols="2">64 GPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>87.5</cell><cell>60.8</cell></row><row><cell>GDT [45]</cell><cell>K400</cell><cell cols="2">64 GPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>90.9</cell><cell>62.3</cell></row><row><cell>Robust-xID [40]</cell><cell>K400</cell><cell cols="2">8 GPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>85.6</cell><cell>55.0</cell></row><row><cell>CMAC [37]</cell><cell>K400</cell><cell cols="2">8 GPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>90.3</cell><cell>61.1</cell></row><row><cell>CM-ACC [33]</cell><cell>K700  *</cell><cell cols="3">40 GPUs 3D-ResNet18 32?224 2</cell><cell>90.2</cell><cell>61.8</cell></row><row><cell>CM-ACC [33]</cell><cell>AS  *</cell><cell cols="3">40 GPUs 3D-ResNet18 32?224 2</cell><cell>90.7</cell><cell>62.3</cell></row><row><cell>CrissCross</cell><cell>K400</cell><cell cols="2">8 GPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>91.5</cell><cell>64.7</cell></row><row><cell cols="2">Fully Supervised[50] AS</cell><cell>-</cell><cell>R(2+1)D-18</cell><cell>32?224 2</cell><cell>96.8</cell><cell>75.9</cell></row><row><cell>XDC [3]</cell><cell>AS</cell><cell cols="2">64 GPUs R(2+1)D-18</cell><cell>8?224 2</cell><cell>84.9</cell><cell>48.8</cell></row><row><cell>AVID [41]</cell><cell>AS</cell><cell cols="2">64 GPUs R(2+1)D-18</cell><cell>8?224 2</cell><cell>88.6</cell><cell>57.6</cell></row><row><cell>CrissCross</cell><cell>AS</cell><cell cols="2">8 GPUs R(2+1)D-18</cell><cell>8?224 2</cell><cell>89.4</cell><cell>58.3</cell></row><row><cell>AVTS [27]</cell><cell>AS</cell><cell cols="2">4 GPUs MC3-18</cell><cell>25?224 2</cell><cell>87.7</cell><cell>57.3</cell></row><row><cell>XDC [3]</cell><cell>AS</cell><cell cols="2">64 GPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>93.0</cell><cell>63.7</cell></row><row><cell>AVID [41]</cell><cell>AS</cell><cell cols="2">64 GPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>91.5</cell><cell>64.7</cell></row><row><cell>MMV [2]</cell><cell>AS</cell><cell cols="2">32 TPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>91.5</cell><cell>70.1</cell></row><row><cell>BraVe [50]</cell><cell>AS</cell><cell cols="2">16 TPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>94.1</cell><cell>71.1</cell></row><row><cell>CM-ACC [33]</cell><cell>AS</cell><cell cols="2">40 GPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>93.5</cell><cell>67.2</cell></row><row><cell>CrissCross</cell><cell>AS</cell><cell cols="2">8 GPUs R(2+1)D-18</cell><cell>32?224 2</cell><cell>92.4</cell><cell>66.8</cell></row><row><cell>Note: K700</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* and AS* refer to 240K samples from K700 and AS respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>SOTA comparison on action retrieval. We present the accuracy of video retrieval on UCF and HMDB datasets for different numbers of nearest neighbors, using the video backbone pretrained on Kinetics400.</figDesc><table><row><cell>Method</cell><cell>UCF</cell><cell></cell><cell>HMDB</cell><cell></cell></row><row><cell></cell><cell cols="4">R@1 R@5 R@20 R@1 R@5 R@20</cell></row><row><cell>ST Order [8]</cell><cell>25.7 36.2 49.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SpeedNet [7]</cell><cell>13.0 28.1 49.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">Clip Order [68] 14.1 30.3 51.1 7.6 22.9 48.8</cell></row><row><cell>VCP [32]</cell><cell cols="4">18.6 33.6 53.5 7.6 24.4 53.6</cell></row><row><cell>VSP [13]</cell><cell cols="4">24.6 41.9 76.9 10.3 26.6 54.6</cell></row><row><cell>CoCLR [20]</cell><cell cols="4">55.9 70.8 82.5 26.1 45.8 69.7</cell></row><row><cell>SeLaVi [5]</cell><cell cols="4">52.0 68.6 84.5 24.8 47.6 75.5</cell></row><row><cell>GDT [45]</cell><cell cols="4">57.4 73.4 88.1 25.4 51.4 75.0</cell></row><row><cell cols="5">Robust-xID [40] 60.9 79.4 90.8 30.8 55.8 79.7</cell></row><row><cell>CrissCross</cell><cell cols="4">63.8 78.7 89.9 26.4 50.5 77.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>SOTA</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">comparison on</cell></row><row><cell cols="7">sound classification. Top-1 accura-</cell></row><row><cell cols="7">cies averaged over all the splits on</cell></row><row><cell cols="7">ESC50 and DCASE are presented. Ad-</cell></row><row><cell cols="7">ditionally, the linear evaluation input</cell></row><row><cell cols="7">size for ESC50 and architecture of</cell></row><row><cell cols="7">the respective methods are presented.</cell></row><row><cell cols="7">K400 and AS refer to Kinetics400 and</cell></row><row><cell cols="4">AudioSet respectively.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell></cell><cell>ESC50</cell><cell cols="2">DCASE</cell></row><row><cell></cell><cell></cell><cell cols="5">sec. KS K400 AS KS K400 AS</cell></row><row><cell cols="2">AVTS [27] VGG-8</cell><cell>2</cell><cell cols="4">-76.7 80.6 -91 93</cell></row><row><cell>XDC [3]</cell><cell cols="2">ResNet-18 2</cell><cell cols="4">-78.0 84.8 -91 95</cell></row><row><cell cols="3">AVID [41] ConvNet-9 2</cell><cell cols="4">-79.1 89.1 -93 96</cell></row><row><cell>MMV [2]</cell><cell cols="2">ResNet-50 5</cell><cell>-</cell><cell>-85.6 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">BraVe [50] ResNet-50 5</cell><cell>-</cell><cell>-90.4 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">ResNet-18 2 79.5 81.5 86.7 93 96 97 CrissCross ResNet-18 5 82.8 86.8 90.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S2 :</head><label>S2</label><figDesc>Audio augmentation parameter details.</figDesc><table><row><cell cols="2">Augmentation Parameters</cell></row><row><cell>Volume Jitter</cell><cell>range = ?0.2</cell></row><row><cell>Time Mask</cell><cell>max size = 20 num = 2</cell></row><row><cell>Frequency Mask</cell><cell>max size = 10 num = 2</cell></row><row><cell>Timewarp</cell><cell>wrap window = 20</cell></row><row><cell>Random Crop</cell><cell>range = [0.6,1.5] crop scale = [1.0,1.5]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S4</head><label>S4</label><figDesc></figDesc><table><row><cell cols="4">: Audio augmentation sum-</cell></row><row><cell>mary.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">VJ Mask RC TW</cell></row><row><cell>Pretraining</cell><cell>?</cell><cell>?</cell><cell>? ?</cell></row><row><cell cols="2">Linear evaluation ?</cell><cell>?</cell><cell>? ?</cell></row><row><cell>here</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S5 :</head><label>S5</label><figDesc>Architecture of the video backbone: R(2+1)D-18.</figDesc><table><row><cell>Layer Xs Xt C Ks Kt Ss St</cell></row><row><cell>frames 112 8 3 ----</cell></row><row><cell>conv1 56 8 64 7 3 2 1</cell></row><row><cell>maxpool 28 8 64 3 1 2 1</cell></row><row><cell>block2.1.1 28 8 64 3 3 1 1</cell></row><row><cell>block2.1.2 28 8 64 3 3 1 1</cell></row><row><cell>block2.2.1 28 8 64 3 3 1 1</cell></row><row><cell>block2.2.2 28 8 64 3 3 1 1</cell></row><row><cell>block3.1.1 14 4 128 3 3 2 2</cell></row><row><cell>block3.1.2 14 4 128 3 3 1 1</cell></row><row><cell>block3.2.1 14 4 128 3 3 1 1</cell></row><row><cell>block3.2.2 14 4 128 3 3 1 1</cell></row><row><cell>block4.1.1 7 2 256 3 3 2 2</cell></row><row><cell>block4.1.2 7 2 256 3 3 1 1</cell></row><row><cell>block4.2.1 7 2 256 3 3 1 1</cell></row><row><cell>block4.2.2 7 2 256 3 3 1 1</cell></row><row><cell>block5.1.1 4 1 512 3 3 2 2</cell></row><row><cell>block5.1.2 4 1 512 3 3 1 1</cell></row><row><cell>block5.2.1 4 1 512 3 3 1 1</cell></row><row><cell>block5.2.2 4 1 512 3 3 1 1</cell></row><row><cell>avg-pool --512 ----</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S6 :</head><label>S6</label><figDesc>Architecture of the audio backbone: ResNet-18.</figDesc><table><row><cell cols="2">Layer Xf Xt C Ks Kt Sf St</cell></row><row><cell cols="2">spectrogram 80 200 1 ----</cell></row><row><cell cols="2">conv1 40 100 64 7 7 2 2</cell></row><row><cell cols="2">maxpool 20 50 64 3 3 2 2</cell></row><row><cell cols="2">block2.1.1 20 50 64 3 3 2 2</cell></row><row><cell cols="2">block2.1.2 20 50 64 3 3 2 2</cell></row><row><cell cols="2">block2.2.1 20 50 64 3 3 2 2</cell></row><row><cell cols="2">block2.2.2 20 50 64 3 3 2 2</cell></row><row><cell cols="2">block3.1.1 10 25 128 3 3 2 2</cell></row><row><cell cols="2">block3.1.2 10 25 128 3 3 2 2</cell></row><row><cell cols="2">block3.2.1 10 25 128 3 3 2 2</cell></row><row><cell cols="2">block3.2.2 10 25 128 3 3 2 2</cell></row><row><cell cols="2">block4.1.1 5 13 256 3 3 2 2</cell></row><row><cell cols="2">block4.1.2 5 13 256 3 3 2 2</cell></row><row><cell cols="2">block4.2.1 5 13 256 3 3 2 2</cell></row><row><cell cols="2">block4.2.2 5 13 256 3 3 2 2</cell></row><row><cell cols="2">block5.1.1 3 7 512 3 3 2 2</cell></row><row><cell cols="2">block5.1.2 3 7 512 3 3 2 2</cell></row><row><cell cols="2">block5.2.1 3 7 512 3 3 2 2</cell></row><row><cell cols="2">block5.2.2 3 7 512 3 3 2 2</cell></row><row><cell>avg-pool -</cell><cell>-512 ----</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S7 :</head><label>S7</label><figDesc>Architecture of projector (left) and predictor (right) heads.</figDesc><table><row><cell cols="2">Layer Dimensions</cell><cell cols="2">Layer Dimensions</cell></row><row><cell>input fc-bn-relu fc-bn-relu fc-bn</cell><cell>512 2048 2048 2048</cell><cell>input fc-bn-relu fc</cell><cell>2048 512 2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S8 :</head><label>S8</label><figDesc>Abbreviations and descriptions of the hyperparameters.</figDesc><table><row><cell cols="2">Abbreviations Name</cell><cell>Description</cell></row><row><cell>bs</cell><cell>batch size</cell><cell>The size of a mini-batch.</cell></row><row><cell>es</cell><cell>epoch size</cell><cell>The total number of samples per epoch.</cell></row><row><cell>ep</cell><cell>toal epochs</cell><cell>The total number of epochs.</cell></row><row><cell>lr</cell><cell>learning rate</cell><cell></cell></row><row><cell>lrab</cell><cell>audio backbone lr</cell><cell></cell></row><row><cell>lrvb</cell><cell>video backbone lr</cell><cell>The learning rates to train the networks.</cell></row><row><cell>lrap</cell><cell>audio predictor lr</cell><cell></cell></row><row><cell>lrvp</cell><cell>video predictor lr</cell><cell></cell></row><row><cell>lrs</cell><cell cols="2">learning rate scheduler The learning rate scheduler to train the network.</cell></row><row><cell>ms</cell><cell>milestones</cell><cell>At every ms epoch the learning rate is decayed.</cell></row><row><cell>?</cell><cell>lr decay rate</cell><cell>The learning rate is decayed by a factor of ?.</cell></row><row><cell>wd</cell><cell>weight decay</cell><cell>The weight decay used in the SGD optimizer.</cell></row><row><cell>mtm</cell><cell>momentum</cell><cell>The momentum used in the SGD optimizer.</cell></row><row><cell>drp</cell><cell>dropout</cell><cell>The dropout rate.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table S10 :</head><label>S10</label><figDesc>Full-finetuning hyperparameters for action recognition when pretrained on Kinetics400.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/uoguelph-mlrg/Cutout 4 https://github.com/s3prl/s3prl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We are grateful to Bank of Montreal and Mitacs for funding this research. We are thankful to Vector Institute and SciNet HPC Consortium for helping with the computation resources.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Madhok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-supervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selfsupervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeruIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Labelling unlabelled videos from scratch with multi-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note>In: NeurIPS (2020) 2, 3, 4, 7, 12, 13, 14</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Speednet: Learning the speediness in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving spatiotemporal self-supervision by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Self-supervised spatio-temporal representation learning using variable playback speed prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02692</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP. pp</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal feature learning via video rotation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11387</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised learning with crossmodal transformers for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLT. pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeruIPS. pp</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dual motion gan for future-flow embedded video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Video cloze procedure for self-supervised spatio-temporal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Active contrastive learning of audio-visual video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (2020)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Python in Science Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Cross-modal attention consistency for video-audio unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06939</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust audio-visual instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Audio-visual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Byol for audio: Self-supervised learning for general-purpose audio representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06695</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On compositions of transformations in contrastive self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Space-time crop &amp; attend: Improving cross-modal video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Broaden your views for self-supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16559</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Sdc-net: Video prediction using spatially-displaced convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-supervised contrastive learning of multi-view facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMI. pp</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-supervised ecg representation learning for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-supervised learning for ecg-based emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP. pp</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Detection of maternal and fetal stress from ecg with selfsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lobmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Frasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition with transformer-based self supervised feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siriwardhana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaluarachchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nanayakkara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Detection and classification of acoustic scenes and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giannoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Deep end2end voxel2voxel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning by uncovering spatio-temporal statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<title level="m">Large batch training of convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
