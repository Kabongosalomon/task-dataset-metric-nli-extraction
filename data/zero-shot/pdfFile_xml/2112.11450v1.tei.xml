<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Max-Margin Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Shah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Mitsubishi Electric Research Labs</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Max-Margin Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Standard contrastive learning approaches usually require a large number of negatives for effective unsupervised learning and often exhibit slow convergence. We suspect this behavior is due to the suboptimal selection of negatives used for offering contrast to the positives. We counter this difficulty by taking inspiration from support vector machines (SVMs) to present max-margin contrastive learning (MMCL). Our approach selects negatives as the sparse support vectors obtained via a quadratic optimization problem, and contrastiveness is enforced by maximizing the decision margin. As SVM optimization can be computationally demanding, especially in an end-to-end setting, we present simplifications that alleviate the computational burden. We validate our approach on standard vision benchmark datasets, demonstrating better performance in unsupervised representation learning over state-of-the-art, while having better empirical convergence properties.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Learning effective data representations is crucial to the success of any machine learning model. Recent years have seen a surge in algorithms for unsupervised representation learning that leverage the vast amounts of unlabeled data <ref type="bibr" target="#b5">(Chen et al. 2020a;</ref><ref type="bibr" target="#b17">Gidaris, Singh, and Komodakis 2018;</ref><ref type="bibr" target="#b34">Lee et al. 2017;</ref><ref type="bibr" target="#b64">Zhang et al. 2019;</ref><ref type="bibr" target="#b63">Zhan et al. 2020</ref>). In such algorithms, an auxiliary learning objective is typically designed to produce generalizable representations that capture some higher-order properties of the data. The assumption is that such properties could potentially be useful in (supervised) downstream tasks, which may have fewer annotated training samples. <ref type="bibr">For example, in (Noroozi and Favaro 2016;</ref><ref type="bibr" target="#b47">Santa Cruz et al. 2018)</ref>, the pre-text task is to solve patch jigsaw puzzles, so that the representations learned could potentially capture the natural semantic structure of images. Other popular auxiliary objectives include video frame prediction <ref type="bibr" target="#b41">(Oord, Li, and Vinyals 2018)</ref>, image coloring <ref type="bibr" target="#b65">(Zhang, Isola, and Efros 2016)</ref>, and deep clustering <ref type="bibr" target="#b3">(Caron et al. 2018)</ref>, to name a few.</p><p>Among the auxiliary objectives typically used for representation learning, one that has gained significant momentum recently is that of contrastive learning, which is a variant of the standard noise-constrastive estimation (NCE) <ref type="bibr" target="#b21">(Gutmann and Hyv?rinen 2010)</ref> procedure. In NCE, the goal is to learn * Equal Contribution. ? Corresponding Author. Learning framework. For every positive example, we compute a weighted subset of (hard) negatives via computing a discriminative hyperplane by solving an SVM objective. This hyperplane is then used in learning to maximize the similarity between the representations of the positives and minimize the similarity between the representations of the positives against the negatives. The negatives in the figure are actual ones selected by our scheme for the respective positive.</p><p>data distributions by classifying the unlabeled data against random noise. However, recently developed contrastive learning methods learn representations by designing objectives that capture data invariances. Specifically, instead of using random noise as in NCE, these methods transform data samples to sets of samples, each set consisting of transformed variants of a sample, and the auxiliary task is to classify one set (positives) against the rest (negatives). Surprisingly, even by using simple data transformations, such as color jittering, image cropping, or rotations, these methods are able to learn superior and generalizable representations, sometimes even outperforming supervised learning algorithms in downstream tasks (e.g., CMC <ref type="bibr" target="#b54">(Tian, Krishnan, and Isola 2020)</ref>, MoCo <ref type="bibr" target="#b7">(Chen et al. 2020c;</ref><ref type="bibr" target="#b24">He et al. 2020)</ref>, SimCLR <ref type="bibr" target="#b5">(Chen et al. 2020a)</ref>, and BYOL <ref type="bibr" target="#b19">(Grill et al. 2020)</ref>). Typically, contrastive learning methods use the NCE-loss for the learning objective, which is usually a logistic classifier separating the positives from the negatives. However, as is often found in NCE algorithms, the negatives should be close in distribution to the positives for the learned representations to be useful -a criteria that often demands a large number of negatives in practice (e.g., 16K in SimCLR <ref type="bibr" target="#b5">(Chen et al. 2020a)</ref>). Further, standard contrastive learning approaches make the implicit assumption that the positives and negatives belong to distinct classes in the downstream task <ref type="bibr" target="#b1">(Arora et al. 2019</ref>). This requirement is hard to enforce in an unsupervised training regime and defying this assumption may hurt the downstream performance due to beneficial discriminative cues being ignored.</p><p>In this paper, we explore alternative formulations for contrastive learning beyond the standard logistic classifier. Rather than contrasting the positive samples against all the negatives in a batch, our key insight is to design an objective that: (i) selects a suitable subset of negatives to be contrasted against, and (ii) provides a means to relax the effect of false negatives on the learned representations. <ref type="figure" target="#fig_0">Fig. 1</ref> presents an overview of the idea. A natural objective in this regard is the classical support vector machine (SVM), which produces a discriminative hyperplane with the maximum margin separating the positives from the negatives. Inspired by SVMs, we propose a novel objective, max-margin contrastive learning (MMCL), to learn data representations that maximizes the SVM decision margin. MMCL brings in several benefits to representation learning. For example, the kernel trick allows for the use of rich non-linear embeddings that could capture desirable data similarities. Further, the decision margin is directly related to the support vectors, which form a weighted data subset. The ability to use slack variables within the SVM formulation allows for a natural control of the influence of false negatives on the representation learning setup.</p><p>A straightforward use of the MMCL objective could be practically challenging. This is because SVMs involve solving a constrained quadratic optimization problem, solving which exactly could dramatically increase the training time when used within standard deep learning models. To this end, inspired by coordinate descent algorithms, we propose a novel reformulation of the SVM objective using the assumptions typically used in contrastive learning setups. Specifically, we propose to use a single positive data sample to train the SVM against the negatives -a situation for which efficient approximate solutions can be obtained for the discriminative hyperplane. Once the hyperplane is obtained, we propose to use it for representation learning. Thus, we formulate an objective that uses this learned hyperplane to maximize the classification margin between the remaining positives and the negatives. To demonstrate the empirical benefits of our approach to unsupervised learning, we replace the logistic classifier from prior contrastive learning algorithms with the proposed MMCL objectives. We present experiments on standard benchmark datasets; our results reveal that using our max-margin objective leads to faster convergence and needs far fewer negatives than prior approaches and produces representations that are better generalizable to several downstream tasks, including transfer learning for many-shot recognition, few-shot recognition, and surface normal estimation.</p><p>Below, we summarize the key contributions of this work:</p><p>? We propose a novel contrastive learning formulation using SVMs, dubbed max-margin contrastive learning.</p><p>? We present a novel simplification of the SVM objective using the problem setup commonly used in contrastive learning -this simplification allows deriving efficient approximations for the decision hyperplane. ? We explore two approximate solvers for the SVM hyperplane: (i) using projected gradient descent and (ii) closedform using truncated least squares. ? We present experiments on standard computer vision datasets such as ImageNet-1k, ImageNet-100, STL-10, CIFAR-100, and UCF101, demonstrating superior performances against state of the art, while requiring only smaller negative batches. Further, on a wide variety of transfer learning tasks, our pre-trained model shows better generalizability than competing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>While the key ideas in contrastive learning are classical <ref type="bibr" target="#b2">(Becker and Hinton 1992;</ref><ref type="bibr" target="#b21">Gutmann and Hyv?rinen 2010;</ref><ref type="bibr" target="#b22">Hadsell, Chopra, and LeCun 2006)</ref>, it has recently become very popular due to its applications in self-supervised learning. Arguably, objectives based on contrastive learning have outperformed several hand-designed pre-text tasks (Doersch, Gupta, and Efros 2015; <ref type="bibr" target="#b17">Gidaris, Singh, and Komodakis 2018;</ref><ref type="bibr" target="#b33">Larsson, Maire, and Shakhnarovich 2016;</ref><ref type="bibr" target="#b40">Noroozi and Favaro 2016;</ref><ref type="bibr" target="#b65">Zhang, Isola, and Efros 2016)</ref>. Apart from visual representation learning, the idea of contrastive learning is quickly proliferating into several other subdomains in machine learning, including video understanding <ref type="bibr" target="#b23">(Han, Xie, and Zisserman 2020)</ref>, graph representation learning <ref type="bibr" target="#b62">(You et al. 2020;</ref><ref type="bibr" target="#b53">Sun et al. 2020)</ref>, natural language processing (Logeswaran and Lee 2018), and learning audio representations <ref type="bibr" target="#b46">(Saeed, Grangier, and Zeghidour 2021)</ref>.</p><p>In contrastive predictive coding <ref type="bibr" target="#b41">(Oord, Li, and Vinyals 2018)</ref>, which is one of the first works to apply contrastive learning for self-supervised learning, the noise-contrastive loss was re-targeted for representation learning via the pretext task of future prediction in sequences. It is often empirically seen that the quality of the negatives to be contrasted against has a strong influence on the effectiveness of the representation learned. To this end, for visual representation learning tasks, SimCLR <ref type="bibr">(Chen et al. 2020a,b)</ref> proposed a framework that uses a bank of augmentations to generate positives and negatives. As the number of negatives play a crucial role in NCE, many approaches also make use of a memory bank <ref type="bibr" target="#b7">(Chen et al. 2020c;</ref><ref type="bibr" target="#b24">He et al. 2020;</ref><ref type="bibr" target="#b39">Misra and Maaten 2020;</ref><ref type="bibr" target="#b66">Zhuang, Zhai, and Yamins 2019)</ref> to enable efficient bookkeeping of the large batches of negatives. Other contrastive learning objectives include: clustering <ref type="bibr" target="#b3">(Caron et al. 2018</ref><ref type="bibr" target="#b4">(Caron et al. , 2020</ref><ref type="bibr" target="#b35">Li et al. 2020a)</ref>, predicting the representations of augmented views <ref type="bibr" target="#b19">(Grill et al. 2020)</ref>, and learning invariances <ref type="bibr" target="#b54">(Tian, Krishnan, and Isola 2020;</ref><ref type="bibr">Xiao et al. 2020)</ref>. The lack of access to class labels in contrastive learning can lead to incorrect learning; e.g., due to false negatives. Recent works have attempted to tackle this issue via avoiding sampling bias <ref type="bibr" target="#b8">(Chuang et al. 2020</ref>) and adjusting the contrastive loss for the impact of false negatives <ref type="bibr" target="#b44">(Robinson et al. 2021;</ref><ref type="bibr" target="#b26">Huynh et al. 2022;</ref><ref type="bibr" target="#b29">Kalantidis et al. 2020;</ref><ref type="bibr" target="#b27">Iscen et al. 2018)</ref>. In comparison to these methods that make adjustments to the NCE loss, we propose an alternative way to view contrastive learning through the lens of max-margin methods using support vector machines; allowing for an amalgamation of the rich literature of SVMs with modern deep unsupervised representation learning approaches.</p><p>A key idea in our setup is to view the support vectors as hard negatives for contrastive learning via maximizing the decision margin. Conceptually, this idea is reminiscent of hardnegative mining used in classical supervised learning setups, such as deformable parts models <ref type="bibr" target="#b16">(Felzenszwalb et al. 2009</ref>), triplet-based losses <ref type="bibr" target="#b48">(Schroff, Kalenichenko, and Philbin 2015)</ref>, and stochastic negative mining approaches <ref type="bibr" target="#b43">(Reddi et al. 2019</ref>). However, different from these methods, we explore self-supervised losses in this paper, which require novel reformulations of max-margin objectives for making the setup computationally tractable. Our proposed approximations to MMCL result in a one-point-against-all SVM classifier, which is similar to exemplar-SVMs <ref type="bibr" target="#b38">(Malisiewicz, Gupta, and Efros 2011)</ref>; however rather than learning a bank of classifiers for specific tasks, our objective is to learn embeddings that are generalizable and useful for other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>In this section, we review our notation and visit the principles of contrastive learning, support vector machines, and their potential connections, that will set the stage for presenting our approach. We use lower-case for single entities (such as x), and upper-case (e.g., X) for matrices (synonymous with a collection of entities). We use lower-case bold-font (e.g., z) for vectors. For a function, say f , defined on vectors, we sometimes overload it as f (X), by which we mean applying f to each entity in X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Learning</head><formula xml:id="formula_0">Suppose D = {x i } N i=1</formula><p>is a given unlabeled dataset, where each x i ? R d . Let T : R d ? R d denote a random cascade of data transformation maps (e.g., random image crops and rotations). Standard contrastive learning methods use T to augment D, thereby producing sets of data points D = {X 1 , X 2 , ? ? ? , X N }, where each X is a (potentially infinite) set of transformed data samples obtained via randomly applying T on each x, i.e., X = {T (x)}. The task of representation learning then amounts to minimizing an objective that maximizes the similarity between points from within a set against data points from other sets -essentially learning the data manifold in some representation space, with the hope that such representations are useful in subsequent tasks.</p><p>Suppose f ? : R d ? R d denote a function mapping a data point x to its representation, i.e., f ? (x). Then, inspired by noise-contrastive estimation <ref type="bibr" target="#b21">(Gutmann and Hyv?rinen 2010)</ref>, contrastive learning methods learn the function f ? via minimizing the empirical logistic loss (with respect to ?): <ref type="figure">Figure 2</ref>: An illustration of our MMCL approach. Given a positive point (+) and a set of negatives Y ? , MMCL learns the parameters ? of a backbone network f ? via extracting features z + and Z ? using a view x + of the positive + and the negatives Y ? , respectively. These features are then used in an SVM with an RKHS kernel K to find a decision hyperplane parameterized by ? x and ? Y . Next, MMCL uses the remaining positive views z maximizing the similarity between z and z + , while minimizing the similarity between z and Z ? , thereby achieving contrastiveness. This ensuing MMCL loss is then backpropagated through the pipeline, thereby learning ?, which is the goal.</p><formula xml:id="formula_1">? X?B log g(f ? (x), f ? (x + )) g(f ? (x), f ? (x + )) + x ? ?B\X g(f ? (x), f ? (x ? )) ,<label>(1)</label></formula><formula xml:id="formula_2">+ MMCL objective SVM with Kernel</formula><formula xml:id="formula_3">over batches B ? D with positives {x, x + } ? X ? B, neg- atives x ? ? X , where X ? B\X,</formula><p>and using a suitable similarity function g (e.g., a learnable projection-head followed by an exponentiated-cosine distance as in SimCLR <ref type="bibr" target="#b5">(Chen et al. 2020a)</ref>). As alluded to earlier, the contrastive learning loss in (1) poses several challenges from a representation learning perspective. For example, in the absence of any form of supervision, this learning objective needs to derive the training signals from the (thus far learned) representations of the negative pairs, which could be very noisy; thereby requiring very large negative batches. However, having such large batches increases the chances of class collisions, i.e., positives and negatives belonging to the same class in a subsequent downstream task; such collisions have been shown to be detrimental <ref type="bibr" target="#b1">(Arora et al. 2019)</ref>. As alluded to earlier, unlike approaches that attempt to circumvent this issue, such as <ref type="bibr" target="#b26">(Huynh et al. 2022;</ref><ref type="bibr" target="#b44">Robinson et al. 2021;</ref><ref type="bibr" target="#b8">Chuang et al. 2020)</ref>, we seek to explore alternative contrastive learning objectives that are less sensitive to issues discussed above using formulations that maximize the discriminative margin between the positives and the negatives. Note that instead of the InfoNCE loss, as in (1), for contrasting the positives from the negatives, an alternative is perhaps the hinge loss <ref type="bibr" target="#b1">(Arora et al. 2019;</ref><ref type="bibr" target="#b5">Chen et al. 2020a)</ref>, that minimizes (with respect to ?):</p><formula xml:id="formula_4">x,x + ,x ? t ? sim(f ? (x), f ? (x + )) + sim(f ? (x), f ? (x ? )) + ,</formula><p>where [ . ] + = max(0, .) denotes the hinge loss and t is a margin hyperparameter that must be tuned manually. Our proposed scheme avoids the need for this hyperparameter as the margin is an objective of the optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Vector Machines</head><p>Given two sets X + and X ? with labels y x = 1, if x ? X + and ?1 otherwise, the soft-margin SVM solves the objective:</p><formula xml:id="formula_5">min w,b,??0 1 2 w 2 + C x ? x s. t. y x (w x + b) ? 1 ? ? x , ?x ? X + ? X ? , (2)</formula><p>where w denotes the discriminative hyperplane separating the two classes, b is a bias, and ? x is a per-data-point non-negative slack with a penalty C that balances between misclassification of hard points and maximizing the decision margin. It is well-known that 1/ w captures the margin between the positives and the negatives, and thus the objective in (2) attempts to find the hyperplane w that maximizes this margin. The Lagrangian dual of (2) is given by:</p><formula xml:id="formula_6">min 0???C,? y=0 1 2 ? K(X + , X ? )? ? ? 1,<label>(3)</label></formula><p>where K ? S |X + ?X ? | ++ denotes a symmetric positive definite kernel matrix, the ij-th element of which is given by:</p><formula xml:id="formula_7">K ij = y xi y xj K(x i , x j ) for some suitable RKHS kernel K and x i , x j ? X + ? X ? .</formula><p>As the formulations in <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_6">(3)</ref> are convex, a solution ? to (3) provides the exact decision hyperplane for <ref type="formula">(2)</ref> and is given by:</p><formula xml:id="formula_8">w(.) = x?X + ?X ? ? x y x K(x, .).<label>(4)</label></formula><p>As the bias term b in <ref type="formula">(2)</ref> is not essential for the details to follow, we will not need the exact form of this term and will use w(.) to refer to the decision hyperplane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>In this section, we connect the approaches described above deriving our MMCL formulation. An overview of our approach is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Learning Meets SVMs</head><p>The advantages of SVM listed in the last section may seem worthwhile from a contrastive representation learning perspective, and suggest directly using SVM instead of the logistic classifier in (1). Formally, using a soft-constraint variant of (2) with a margin t, the optimization problem in <ref type="formula" target="#formula_1">(1)</ref> can be re-written as:</p><formula xml:id="formula_9">min ? B?D X?B min w X 1 2 w X 2 + [t ? w X , f ? (X) ] + + X ? ?B\X t + w X , f ? (X ? ) + ,<label>(5)</label></formula><p>where X, X ? denote the sets of positives and negatives respectively, and w X captures a max-margin hyperplane separating them. 1 The inner optimization over each w X is what translates into training an SVM. We augment this inner optimization problem in two ways: (i) by including slack variables to model a soft-margin (as in <ref type="formula">(2)</ref>), which results in a 1 Note that f ? (?) we mean applying f ? to each item in set ?.</p><p>hyperparameter C; and (ii) by permitting an additional nonlinear feature map ? so that we may use ?(f ? ) (as in <ref type="formula" target="#formula_6">(3)</ref>) in (5). Using these changes, a contrastive learning formulation via maximizing the SVM classification margin may be derived (by rewriting (5)) as:</p><formula xml:id="formula_10">min ? L(?) := B?D X?B ? X * K (f ? (X), f ? (B\X)) ? * X ,<label>(6)</label></formula><formula xml:id="formula_11">s.t. ? * X = arg min 0???C, ? y=0 1 2 ? K(f ? (X), f ? (B\X))? ? ? 1,<label>(7)</label></formula><p>where</p><formula xml:id="formula_12">K(Z + , Z ? ) = K(Z + , Z + ), ?K(Z + , Z ? ) ?K(Z ? , Z + ), K(Z ? , Z ? ) is a kernel matrix induced by the RKHS kernel K(z, z ) = ?(z), ?(z )</formula><p>. While SVMs have been widely studied in the machine learning literature <ref type="bibr" target="#b51">(Smola and Sch?lkopf 1998;</ref><ref type="bibr" target="#b11">Cortes and Vapnik 1995)</ref>, our idea of linking the fields of SVMs and Contrastive Learning has not been explored before.</p><p>In <ref type="formula" target="#formula_11">(7)</ref>, we use the so-far trained f ? to produce ? * X defining the decision margin, which is then used in (6) to update ? while striving to maximize the margin; doing so, pushes the support vectors from the positive and negative classes away from each other. Unfortunately, despite its intuitive simplicity, the formulation <ref type="formula" target="#formula_10">(6)-(7)</ref> is impractical to use directly. Indeed, it is a challenging bilevel optimization problem <ref type="bibr" target="#b18">(Gould et al. 2016;</ref><ref type="bibr" target="#b0">Amos and Kolter 2017;</ref><ref type="bibr" target="#b56">Wang et al. 2018)</ref>, and if we use an iterative SVM solver for the lower problem (7) within a deep learning framework, it can incur significant slowdown.</p><p>Remarks. There are several interesting aspects of the SVM solution that are perhaps beneficial from a contrastive learning perspective: (i) the dual solution ? is usually sparse 2 , and its active dimensions can be used to identify data points that are the support vectors defining the decision margin, (ii) the slack regularization controls the misclassification rate, and allows tuning the performance against the class collisions, similar to <ref type="bibr" target="#b8">(Chuang et al. 2020</ref>), (iii) the dimensions of ? X are equal to C for misclassified points, which are perhaps hard or false negatives, and thus our formulation allows for identifying these points and mitigate their effects, and (iv) the use of the kernel function provides rich RKHS similarities at our disposal allowing to use, for example, novel structures within the learned representations (e.g., trees, graphs, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max-Margin Contrastive Learning</head><p>The primary method for solving (6) is stochastic gradient descent (SGD), which computes stochastic gradients over the batches B ? D via backpropagation while iteratively updating ?. However, as has been previously observed for bilevel optimization <ref type="bibr" target="#b0">(Amos and Kolter 2017;</ref><ref type="bibr" target="#b18">Gould et al. 2016)</ref>, even obtaining a single stochastic gradient requires solving the lower problem <ref type="formula" target="#formula_11">(7)</ref> exactly, which is impractical.</p><p>Our key idea to overcome this challenge is to introduce a "sample splitting" trick inspired by coordinate descent, which helps to reduce the computational burden. Subsequently, we make additional approximations that lead to our final training procedure.</p><p>Without loss of generality, assume that X consists of the pair (x, x + ); the same idea applies if we permit multiple such positive pairs in X. Instead of solving (7) using all the "coordinates", we split the pair (x, x + ) into two parts: (i) x + , which is used to perform coordinate descent on <ref type="formula" target="#formula_11">(7)</ref>; and (ii) x, which is used to perform the SGD step for (6). This splitting aligns well with contrastive learning, where often one uses only a pair of positives that must be contrasted against the negatives.</p><p>The following proposition states how we perform part (i) of our split to estimate ? * X , which we will henceforth denote as ? x to indicate its dependence on the split sample.</p><formula xml:id="formula_13">Proposition 1. Let (x + , Y ? ) be a tuple consisting of a pos- itive point x + ? R d and a set of n negative points Y ? ? R d?n . Further, let z + = f ? (x + ) and Z ? = f ? (Y ? ). Sup- pose k xx , k xY , and K Y Y denote K(z + , z + ), K(z + , Z ? ), and K(Z ? , Z ? ),</formula><p>respectively. Consider the SVM decision function for a new point z given by</p><formula xml:id="formula_14">w(z) = ? x K(z + , z)1 ? K(Z ? , z) . (8) Let ? = 11 +K Y Y ?k xY 1 ?1k xY , and let P [0,C] denote projection onto the interval [0, C].</formula><p>By suitably selecting ? x in (8) we then obtain the following approximate max-margin solutions:</p><formula xml:id="formula_15">(i) (block) coordinate minimization ? cm x = arg min 0???C g(?) := 1 2 ? ?? ? 2? 1, (ii) m-step projected gradient (MMCL PGD ): ? pg x := ? m = P [0,C] (? m?1 ? ?(?? m?1 ? 21)), for some initial guess ? 0 ? [0, C] n , ? &gt; 0 a step-size, and (iii) greedy truncated least-squares (MMCL INV ): ? ls x = P [0,C] (2? ?1 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The various solutions satisfy g(?</head><formula xml:id="formula_16">?1 1) ? g(? cm x ) ? min{g(? pg x , g(? ls x )}. Moreover, g(? pg x ) ? g(? cm x ) = O exp ?m ? min(?) ? max(?) (g(? 0 ) ? g(? cm x )) .</formula><p>Proof. Choice (i) is obvious. To obtain (ii) and (iii), consider the following dual SVM formulation:</p><formula xml:id="formula_17">min 0?? Y ?C 1 2 ? x ? Y k xx ?k xY ?k xY K Y Y ? x ? Y ? ? x + ? Y 1 , where ? x = ? Y 1. Substituting for ? x , we obtain 3 : min 0?? Y ?C g(? Y ) = 1 2 ? Y ?? Y ? 2? Y 1.</formula><p>Setting ?g(? Y ) = 0, we obtain the unconstrained leastsquares solution 2? ?1 1, which we can greedily truncate to lie in the interval [0, C] to obtain (iii). Solution (ii) runs m iterations of projected gradient descent, and hence it also satisfies a linear convergence rate, which rapidly brings it within the optimal solution ? cm x at the well-known rate depending on the condition number ? max(?) /? min(?) .</p><p>3 Note that ?x is the scalar Lagrangian dual associated with the data point z while ?x is the vector of all dual variables associated with the batch B when considering x as the positive.</p><p>Algorithm 1: Pseudocode for MMCL Input: Dataset D, batch size N , encoder f ? , slackpenalty C, kernel K, augmentation map T for minibatch B = {x k } N k=1 ? D do loss := 0 for k = 1, . . . , N do draw t 1 ? T , t 2 ? T # get embeddings for positives and negatives</p><formula xml:id="formula_18">z + = f ? (t 1 (x k )), z = f ? (t 2 (x k )) Z ? = f ? (t 1 (B \ x k ) ? t 2 (B \ x k )) # calculate kernel similarities k xY = K(z + , Z ? ), K Y Y = K(Z ? , Z ? ) # Solve SVM ? = 11 + K Y Y ? k xY 1 ? 1k xY ? x = svm solver(?, C) # using PGD or INV # calculate the loss loss += ? x (K(Z ? , z) ? K(z + , z)1)<label>end</label></formula><p>for update the model to minimize the loss end for Using Prop. 1, we can reformulate the contrastive learning objective in (6) as maximizing the margin in classifying the other part of the split, namely the positive point x, correctly against the negatives. Here, we introduce an additional simplification by rewriting the margin in terms of the separation between x and Y ? , using the decision hyperplane (8). Let ? x denote the solution obtained from Proposition 1 using the positive point x. Then, we rewrite (6) into our proposed max-margin contrastive learning objective as:</p><formula xml:id="formula_19">min ? (x,x + )?B?D Y ? =B\(x,x + ) ? T x K f ? (Y ? ), f ? (x) ?1K f ? (x + ), f ? (x) .</formula><p>(9) When optimizing for ?, (9) seeks a representation map f ? that improves the similarity between the positives (x, x + ) and the dissimilarity between x and all the points in Y ? , achieving a similar effect as in standard contrastive learning objective in (1), but with the advantage of choosing kernels, selecting the support vectors that matter to the decision margin, as well as finding points that are perhaps hard negatives (those at the upper-bound of the box-constraints), all in one formulation. Note that, using the exact solver (i) in Prop. 1 turned out to be prohibitively expensive in standard contrastive learning pipelines and thus we do not use that variant in our experiments. In Algorithm 1, we provide a pseudocode highlighting the key steps in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>In this section, we systematically study the various components in MMCL, as well as compare performances of MMCLlearned representations for their quality via linear evaluation, and their generalizability on transfer learning tasks. Visual Representation Learning Experiments. We base our experimental setup on the popular SimCLR <ref type="bibr" target="#b5">(Chen et al. 2020a</ref>) baseline, which is widely used, especially to evaluate  <ref type="table">Table 1</ref>: Transfer learning results. We transfer an ImageNet-pre-trained model (using MMCL) on a range of downstream tasks and datasets. We compare with models pre-trained using a similar batch size and epochs. Results on competing approaches are taken from <ref type="bibr" target="#b14">(Ericsson, Gouk, and Hospedales 2021)</ref>. ? Models evaluated using publicly available checkpoints.</p><p>the effectiveness of the "learning loss" against other factors in a self-supervised algorithm (e.g., data augmentations, use of queues, multiple crops). We use a ResNet50 backbone, followed by a two-layer MLP as the projection-head, followed by unit normalization. We pretrain our models on ImageNet-1K <ref type="bibr" target="#b12">(Deng et al. 2009</ref>) using the LARS optimizer (You, Gitman, and Ginsburg 2018) with an initial learning rate of 1.2 for 100 epochs. We also present results on ImageNet-100 (Tian, Krishnan, and Isola 2020) (a subset of ImageNet-1K) and on smaller datasets such as STL-10 (Coates, Ng, and Lee 2011) and CIFAR-100 <ref type="bibr" target="#b32">(Krizhevsky, Hinton et al. 2009</ref>), especially for our ablation studies. We pre-train on ImageNet-100 for 200 epochs in our studies, while pre-training for 400 epochs on the smaller datasets. We use the Adam optimizer with a learning rate of 1e-3 as in <ref type="bibr" target="#b8">(Chuang et al. 2020;</ref><ref type="bibr" target="#b44">Robinson et al. 2021)</ref>. Unless otherwise stated, we use a batch-size of 256 for all ImageNet-1K, CIFAR-100, and STL-10 experiments and 128 for ImageNet-100 experiments. In addition, we also present results on video representation learning using an S3D backbone <ref type="bibr" target="#b60">(Xie et al. 2018</ref>) on the UCF-101 (Soomro, Zamir, and Shah 2012) dataset, pre-trained using MMCL for 300 epochs. Hyperparameters: We mainly use the RBF kernel for the SVM. For CIFAR-100 experiments, we start with a kernel bandwidth ? 2 = 0.02 and increase it by a factor of 10 at 75 and 125 epochs. For STL-10 experiments, we use a kernel bandwidth ? 2 = 1. We used ? 2 = 5 for ImageNet experiments. We set the SVM slack regularization C to 100. For the projected gradient descent optimizer for MMCL, we use a maximum of 1000 steps. Additional details are provided in the Appendix. Practical Considerations: Here, we note a few important but subtle technicalities that need to be addressed when implementing MMCL. Specifically, we found that backpropagating the gradients through ? Y is prohibitively expensive when using PGD iterations. On the other hand, for the least-squares variant, gradients through ? Y was found to be detrimental. This is perhaps unsurprising, because note that, ? Y term includes the term ? ?1 . To improve the decision margin, one needs to make ? an identity matrix, so that the off-diagonal elements go to zero during optimization, which suggests that the training gradients should reduce the magnitude of these terms. However, on the other hand, as ? Y uses ? ?1 one could also maximize the margin by making ? ill-conditioned, via making the off-diagonal elements going to one. Such a tug-of-war between the gradients can essentially destabilize the training. Thus, we found that avoiding any backpropagation through ? is essential for MMCL to learn to produce representations. We also found that using a small regularization ? + ?I (? = 0.1) is necessary for the learning to begin. This is because, initially the representations can be nearly zero, and thus the kernel may be poorly conditioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Transfer Learning</head><p>Recently, models pretrained using various self-supervised learning approaches have shown impressive performance when transferred to various downstream tasks. In this section, we evaluate MMCL-ImageNet pretrained models on such downstream tasks. For these experiments, we follow the experimental protocol provided in <ref type="bibr" target="#b14">(Ericsson, Gouk, and Hospedales 2021)</ref>. We evaluate the models in the fine-tuning setting and use the benchmarking scripts provided in (Ericsson, Gouk, and Hospedales 2021) without any modifications. First, we transfer the MMCL-pretrained backbone model to a collection of many-shot classification datasets used in (Ericsson, Gouk, and Hospedales 2021), namely FGVC Aircraft, Caltech-101, Stanford Cars, CIFAR-10, CIFAR-100, DTD, Oxford Flowers, and Food-101. The setup involves using the pretrained model as the initial checkpoint and attaching a task specific head to the backbone model. The entire network is then finetuned for the downstream task. These datasets vary widely in content and texture compared to ImageNet images. Further, the benchmark datasets include a significant diversity in the number of training images (2K-50K) and the number of classes . For a fair comparison, we only include results for models which are trained for a comparable number of epochs and batch sizes. For few-shot experiments, we follow the setup described in <ref type="bibr" target="#b14">(Ericsson, Gouk, and Hospedales 2021)</ref> for few-shot learning on the Cross-Domain Few-Shot Learning (CD-FSL) benchmark . We evaluate on Crop-Diseases <ref type="bibr" target="#b40">(Mohanty, Hughes, and Salath? 2016)</ref>, EuroSAT <ref type="bibr" target="#b25">(Helber et al. 2019</ref>) datasets for 5-way 20-shot transfer. Finally, we evaluate performance of our model for the dense prediction task of surface normal estimation on NYUv2 <ref type="bibr" target="#b50">(Silberman et al. 2012</ref>) and report the median angular error.</p><p>In <ref type="table">Table 1</ref>, we provide results on the transfer learning experiments. We see that MMCL consistently outperforms the competing self-supervised learning approaches on a wide variety of transfer tasks and across all datasets. Further, MMCL also outperforms the supervised counterpart on several datasets. These results show that MMCL learns highquality generalizable features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Linear Evaluation</head><p>For these experiments, we freeze the weights of the backbone (ResNet-50), and attach a linear layer as in <ref type="bibr" target="#b5">(Chen et al. 2020a)</ref>, which is trained using the class labels available with the dataset. We train this linear layer for 100 epochs. <ref type="table" target="#tab_2">Tables 2  and 3</ref> show our results. We see that MMCL-pretrained model outperforms SimCLR by 6.3% on ImageNet-1K using the same number of negatives. We also compare with the recent memory queue-based methods such as MoCo-v2 <ref type="bibr" target="#b7">(Chen et al. 2020c</ref>) and MoCHI <ref type="bibr" target="#b29">(Kalantidis et al. 2020)</ref>, demonstrating competitive performances while using far fewer negatives (510 vs 65536). We also establish a new state of the art on ImageNet-100 outperforming MoCHI by 1.7% using only 510 negatives (0.008x) and without a memory bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variant</head><p>Negative   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Graph Representation Learning</head><p>Recall that our MMCL formulation works by modifying the contrastive learning loss function; and as a result, our approach is generically applicable to a variety of tasks. In this section, we evaluate our approach on learning graph representations using contrastive learning. We experiment with five common graph benchmark datasets MUTAG (Kriege and Mutzel 2012) -a dataset containing mutagenic compounds, DD(Yanardag and Vishwanathan 2015)-a dataset of biochemical molecules, REDDIT-BINARY, REDDIT-M5K, and IMDB-BINARY <ref type="bibr" target="#b61">(Yanardag and Vishwanathan 2015)</ref> which are social network datasets. Our experiments use GraphCL <ref type="bibr" target="#b62">(You et al. 2020</ref>) -a projection head based on the contrastive learning framework derived from SimCLR while incorporating graph augmentations. For these experiments, we follow the training and evaluation protocols described in <ref type="bibr" target="#b62">(You et al. 2020)</ref>. Specifically, we use the standard ten-fold cross validation using an SVM and report the average performances and their standard deviations. We use the Adam optimizer for training these models. <ref type="table" target="#tab_5">Table 4</ref> shows the results of using MMCL instead of the NCE loss. We see that adding MMCL is comparable or better than GraphCL for these datasets. On MUTAG, we obtain an absolute improvement of 1.62% over GraphCL. These results demonstrate the effectiveness of our approach in learning better representations. Given that the only change from GraphCL is the underlying objective, the results also show that our approach is general and can easily replace NCE based losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Video Action Recognition</head><p>For this experiment, we use the S3D backbone model <ref type="bibr" target="#b60">(Xie et al. 2018)</ref> pre-trained using MMCL on RGB and optical flow images from the UCF-101 dataset. We pre-train the network for 300 epochs, followed by 100 epochs for linear evaluation on the task of action recognition. We report the standard 10-crop test accuracy on split-1, as well as on nearest neighbor retrieval. As seen in <ref type="table" target="#tab_7">Table 5</ref>, MMCL outperforms the baseline by 5.65% on RGB and 1.21% on flow in linear evaluation and 12.5% and 5.74% on Retrieval@1, demonstrating the generalizability of our approach to the video domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation studies and Analyses</head><p>For some of the ablation experiments, we use the smaller datasets: STL-10 and CIFAR-100, and report the readout accuracy calculated using k-NN with k=200 at 200 epochs, besides standard evaluations. Choice of Kernels: Unlike the traditional NCE objective, our approach naturally allows for the use of kernels to better capture the similarity between the data points. In <ref type="table" target="#tab_8">Table 6</ref>, we compare the readout accuracy on CIFAR100 and STL10 for various choices of popular kernels. As is clear from the table, the RBF kernel performs better on both datasets. The best kernel hyperparameters ?, ? were found empirically. We choose the RBF kernel in our subsequent experiments. Effect of slack: A key benefit of our MMCL formulation is the possibility to use a slack that could potentially control the impact of false or hard negatives. To evaluate this effect, we changed the slack penalty C from 0.01 (i.e., low penalty for misclassification) to C = ?. The results on readout accuracy in <ref type="figure" target="#fig_1">Figure 3</ref> shows that C plays a key role in achieving good performance. For example, with C = 0.01, it appears that the performance is consistently low for both the datasets, perhaps because the hard negatives are under-weighted. We also find that using a large C may not be beneficial always. Effect of batch size: We use STL-10 dataset for this experiment and train all models for 400 epochs. We report the Linear evaluation results for this experiment. From Table 7, we see that our model consistently outperforms the SimCLR baseline, while performing much better than other approaches. Indeed, we find that MMCL is about 1-3% better than HCL, which reweights the hard negatives. We also find that MMCL using a batch size of only 128 reaches close to the performance of HCL <ref type="bibr" target="#b44">(Robinson et al. 2021</ref>) trained using a batch size of 256, suggesting that the proposed selection of       hard negatives via support vectors is beneficial.</p><p>Computational time against batch size: In <ref type="figure" target="#fig_2">Figure 4</ref>, we show the time taken per iteration of MMCL variants against those of prior methods, such as SimCLR, for an increasing batch size. The computational cost of our inner optimization for finding the support vectors is directly related to the batch size. These experiments are done on ImageNet-1K with each RTX3090 GPU holding 64 images. We see that the time taken by MMCL is comparable to SimCLR. Performances between MMCL Variants: In <ref type="table" target="#tab_11">Table 8</ref>, we compare performances between MMCL variants: PGD and INV. We see that both variants outperform the SimCLR, while the two MMCL variants show similar performance. In <ref type="figure">Figure 5</ref>, we plot the convergence curves (readout accu-  <ref type="table">Table 7</ref>: Accuracy (in %) against the batch size on STL-10. racy) against training epochs. The plots clearly show that our variants converge to superior performances rapidly than the baseline. Visualization of Support Vectors: Next, we qualitatively analyze if the support vectors found by MMCL are semantically meaningful. To this end, we use an MMCL model pre-trained on STL-10 dataset. We use a batch of examples as input to the model, and choose one of the examples from the batch as a positive and the remaining as negative. We then solve the MMCL objective to find ?, where ? = 0 corresponds to non-support vectors, ? = C are the misclassified points, and ? ? [0, C) are the support vectors. In <ref type="figure">Figure 6</ref>, we show the positive point, and a set of samples from the batch and the respective ? values. The figure clearly shows that object instances from a similar class gets a high ?, suggesting that they lie on or inside the margin and contribute to the loss while batch samples that are irrelevant or easy negatives are not support vectors and do not contribute to the loss. For example, in <ref type="figure">Figure 6</ref>, the yellow bird is an easy negative for a white truck query image and our approach does not include that bird in the support set.</p><p>Longer Training: Our focus in the above experiments has   <ref type="table">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we proposed a new contrastive learning framework, dubbed Max-Margin Contrastive Learning, using which we learn powerful deep representations for selfsupervised learning by maximizing the decision margin sep-arating data pseudo-labeled as positives and negatives. Our approach draws motivations from the classical support vector machines via modeling the selection of useful negatives through support vectors. We obtain consistent improvements over baselines on a variety of downstream tasks. <ref type="bibr" target="#b10">Codella et al. 2019)</ref> and ChestX <ref type="bibr" target="#b57">(Wang et al. 2017)</ref> for fewshot classification, and VOC 2007 <ref type="bibr" target="#b15">(Everingham et al. 2010)</ref> for object detection. We follow the standard benchmarking protocol in <ref type="bibr" target="#b14">(Ericsson, Gouk, and Hospedales 2021)</ref> for evaluating all our results. We report the top-1 accuracy metric on Food, CIFAR-10, CIFAR-100, SUN397, Stanford Cars, and DTD datasets. The mAP accuracy is reported for Aircraft, Pets, Caltech101, and Flowers and the 11-point mAP metric is reported on Pascal VOC 2007. For few-shot transfer experiments, we report the average accuracy along with a 95% confidence interval. We report AP for object detection on VOC and median angular error for surface normal estimation on NYUv2. Please refer to <ref type="bibr" target="#b14">(Ericsson, Gouk, and Hospedales 2021)</ref> for additional details. For a fair comparison, we only compare to models which were pre-trained for a comparable number of epochs (upto 200) and batch-size of 256. To enable comparison with approaches like MoCo-v2 and MOCHI, we download their official pre-trained models (for a batch size of 256) and follow the same benchmarking process to report the numbers. 4 All results are reported in 9. We see that our approach consistently outperforms the prior approaches on most tasks and datasets which show the high-quality nature of representations learned using MMCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Ablation Studies</head><p>In this section, we include some additional analyses and experiments to analyse the MMCL loss. Effect of Slack Penalty C: In the main paper, we explored the effects of the slack penalty C on performance. In this section, we augment that study with an analysis of the behavior of our model towards a changing C. Specifically, note that when starting to train the model, the neural network weights are randomly initialized, and as a result, the generated contrastive learning features might be arbitrary that asking for a misclassification penalty might be unlikely to be useful. To empirically understand this conjencture, we train a model from scratch for 25 epochs using a slack C = ? (i.e., no misclassification is allowed) and then reduce C to a fixed value for the next 75 epochs. In <ref type="table" target="#tab_14">Table 10</ref>, we provide the result of this analysis on CIFAR-100 and STL-10 datasets, where the legend shows the fixed value of C used. The table shows that there is some benefit of using this approach, for example, on STL-10 dataset, where we see that for C=10 shows a slight benefit over C = ? (i.e., not using the proposed approach). However, overall it looks like such a scheme does not reduce the performance dramatically, unless we underpenalize the misclassifications (such as using C=0.1, for example). False Negative Correction: In the main paper, we argue that the use of slack allows to limit the effect that false negatives have on training by limiting the maximum contribution that they can make to the loss. An alternative idea would be to nullify all points that lie inside the margin (i.e., points that are potentially close to positives, and thus are perhaps false negatives). This would amount to the operation ?[? == C] = 0 after finding the optimal dual values ?. For this experiment, we use a single C for the entire training (unlike the above experiment). The result of this experiment is shown in <ref type="figure" target="#fig_0">Figure 11</ref>. The table shows that FN correction has a significant impact on performance when using a small C, while insignificant for larger C. This is perhaps not unexpected given that using a small C will lead to a large number of misclassified points, which if removed from training, will lead to suboptimal contrastive learning, especially in the initial epochs of training. However, with larger values of C, it appears from the table that the impact of removing some points does not impact the performance much. This is perhaps because using the same value of C for all the training epochs is perhaps sub-optimal, and would need a schedule for changing this parameter. We plan to explore this idea in the future. Influence of RBF Kernel Bandwidth ?: In this experiment we vary the RBF Kernel bandwidth for CIFAR-100 and STL-10. From <ref type="figure" target="#fig_3">Figure 7</ref>, we see that the performance can be influenced by the choice of ?, however, using ? = 1 performs reasonably well for these datasets. Effect of Batch Size: In the main paper, we showed the effect of batch size on STL-10 <ref type="table">(Table 7)</ref>. In <ref type="table" target="#tab_2">Table 12</ref>, we augment that with a similar study for CIFAR-100. Our performances on CIFAR-100 are similar, albeit the improvements are not as pronounced as in STL-10, perhaps because its a smaller dataset. Further, the trend in the tables show that similar to other methods, the performance of MMCL increases consistently with increasing batch sizes, while showing signs of diminishing returns as the batch sizes grows larger (e.g., grows from 256 to 512). We still consistently outperform the SimCLR baseline.</p><p>Convergence In <ref type="figure">Figure 5</ref> of the main paper, we show the convergence analysis of training on STL-10. In <ref type="figure">Figure 8</ref> we add a similar analysis for STL-10. Similar to results on STL-10, we see that the MMCL variants converge to superior performances more rapidly than the baseline. Kernels with SimCLR: The loss used in vanilla SimCLR (and subsequently other contrastive learning approaches) is     technically equivalent to using an RBF kernel, and one could perhaps choose other similarity kernels, e.g., tanh. To quantitatively evaluate this intuition and the impact of such a choice, we used a tanh kernel in SimCLR, and experimented with 5 different bandwidths. The results <ref type="table" target="#tab_3">(Table 13)</ref> suggest that while there can be some improvements to SimCLR via selecting other kernel similarities, our proposed formulation demonstrates significantly better performance. This is because our proposed kernel SVM formulation produces a classification margin in an RKHS where the support set is automatically sparse and weighted, which to the best of our knowledge is difficult to be argued for in the SimCLR setup.</p><p>Using an Exact SVM Solver: Our initial approach was to use an exact SVM solver (using Qpth (Amos and Kolter 2017)) to estimate the max-margin hyperplane, however we found that our iterations were considerably slower (0.52 vs 3.07 for batch size 256). Intuition for Slack Penalty C: Note that in contrast to standard SVMs where the penalty C is a trade off between the misclassification error and the margin, in our setup it has an even bigger role as the feature representation itself is evolving. Specifically, when using a larger C, the ? weights on the misclassified points will be equal to C, and thus given our contrastive objective is using representations of data points linearly weighted by ?, the backbone neural network will be updated via gradients to minimize this loss -thereby pushing these misclassified points out of the margin. Thus, in subsequent iterations, the focus of learning will be to increase the margin, as the number of misclassified points will be Method Readout @ 200 SimCLR 48.2 SimCLR tanh ? = 1 50.5 SimCLR tanh ? = 0.5 49.1 SimCLR tanh ? = 0.1 49.9 SimCLR tanh ? = 2 50 SimCLR tanh ? = 10 49.9 MMCL 55 <ref type="table" target="#tab_3">Table 13</ref>: Use of Kernels with SimCLR. We see that while use of kernels does lead to an improvement for the SimCLR loss, our proposed formulation demonstrates significantly better performance smaller. However, C must also account for an estimate of false negatives, and thus a larger value of C is perhaps not appropriate consistently. We found that an empirical evaluation of collisions is quite difficult, especially given that the network weights evolve over the epochs that collisions vanish as the contrastive loss drops. In <ref type="table" target="#tab_15">Table 11</ref>, we attempted to define some heuristics to explicitly correct for false negatives (FN); our results show that when the penalty C is small, correcting for the FNs do show minor benefits (e.g., C=50). However, for larger C, the network weights are perhaps updated very quickly to fix the FN misclassification error in the early epochs that FNs are absent in subsequent epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other Details</head><p>PGD Solver: We used nesterov accelerated PGD solver to get faster convergence. The value of ? before each pretraining step is randomly initialized before the PGD iterations. We use a step size of 0.001 and optionally a step size of 1 ? 2 . ImageNet Experiments: We use an initial learning rate of 1.2 and following SimCLR we use a warmup and an annealing scheme. Our best model uses ? 2 of 5. To keep hyperparameter search tractable, we obtain top-1 validation accuracy after pre-training for 10 epochs and finetuning for 1 epoch. Based on exeriments with ImageNet-100, we found the PGD variant to work slightly better than the INV variant (80.7% vs 80.5%) and so we use PGD variant for all ImageNet experiments. CIFAR-100 and STL-10 Experiments: We choose hyperparameters using the validation accuracy at 200 epochs based protocol mentioned in the main paper (Readout accuracy). UCF-101 Experiments : We use a publicly available implementation for MoCo-v2 for video self supervised learning <ref type="bibr" target="#b23">(Han, Xie, and Zisserman 2020)</ref>. All the hyperparameters are the same except ? 2 of 0.5, C = 1, and learning rate of 1e-4. We use the PGD variant for experiments on UCF-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and Failure Cases</head><p>The proposed approach has some minor overheads. While, we did not see any significant difference in training times (as shown in <ref type="figure" target="#fig_2">Figure 4</ref>), they can be expensive atleast in the initial epochs of training. This is, for example, because, when the model is randomly initialized, the features produced are arbitrary and thus the PGD iterations in MMCL would need longer cycles to finish. However, we see that this trend is only for the initial few epochs and training speed improves over time. We could also perhaps use a different and better solver for box-constrained optimization that leads to much faster convergence (for example, <ref type="bibr" target="#b30">(Kim, Sra, and Dhillon 2012)</ref> or OSQP <ref type="bibr" target="#b49">(Schubiger, Banjac, and Lygeros 2020))</ref> Potential Negative Impact Our paper makes a fundamental contribution to the field of self-supervised and contrastive learning. We do not perceive any obvious negative social impacts of this work. In fact, self-supervision naturally helps to avoid certain biases that can emerge from labeling biases. That said, training selfsupervised learning methods on already biased datasets could be harmful and potentially lead to the biases being inherited by the models during the fine-tuning stage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of our Max-Margin Contrastive</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Effect of slack penalty C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Computations (ImageNet).Figure 5: Convergence (STL-10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Effect of kernel bandwidth 1/? 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>ImageNet-100 linear evaluation.</figDesc><table><row><cell>Variant</cell><cell cols="3">Negative Source Negatives top-1</cell></row><row><cell>SimCLR</cell><cell>Batch</cell><cell>254</cell><cell>57.5</cell></row><row><cell>SimCLR</cell><cell>Batch</cell><cell>510</cell><cell>60.62</cell></row><row><cell cols="2">MoCo-v2 Memory Queue</cell><cell>65536</cell><cell>63.6</cell></row><row><cell>MoCHI</cell><cell>Memory Queue</cell><cell>65536</cell><cell>63.9</cell></row><row><cell>Ours</cell><cell>Batch</cell><cell>510</cell><cell>63.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>ImageNet-1K linear evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>GraphCL 78.62 ? 0.40 86.80 ? 1.34 89.53 ? 0.84 55.99 ? 0.28 71.14 ? 0.44 Ours 78.74 ? 0.30 88.42 ? 1.33 90.41 ? 0.60 56.18 ? 0.29 71.62 ? 0.28</figDesc><table><row><cell>Method</cell><cell>DD</cell><cell>MUTAG</cell><cell>REDDIT-BIN REDDIT-M5K</cell><cell>IMDB-BIN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison with GraphCL. We compare graph representation learning on five graph benchmark datasets. The compared numbers are obtained from the original paper<ref type="bibr" target="#b62">(You et al. 2020)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Video self-supervised learning on UCF-101 dataset.</figDesc><table><row><cell>Kernel (K(x, y))</cell><cell>CIFAR100 STL10</cell></row><row><cell>Linear: x T y</cell><cell>41.43% 74.82%</cell></row><row><cell>Tanh: tanh(??x T y + ?)</cell><cell>54.53% 80.5%</cell></row><row><cell>RBF: exp(? x?y 2 2? 2 )</cell><cell>55.35 % 81.33%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Effect of kernel choice.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Figure 6: Visualizing Support Vectors : We visualize a query image (green box), corresponding support vectors (blue boxes) and non-support vectors (red boxes). We see that the support vectors are plausible hard negatives while in most cases the non-support vectors are easy negatives. The ? corresponding to the various negatives is shown at the bottom left of each image.</figDesc><table><row><cell>Variant</cell><cell cols="2">CIFAR-100 STL-10</cell></row><row><cell>SimCLR</cell><cell>66</cell><cell>80.15</cell></row><row><cell>MMCL PGD</cell><cell>68.0</cell><cell>88.03</cell></row><row><cell>MMCL INV</cell><cell>68.81</cell><cell>88.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Performances on MMCL variants.</figDesc><table><row><cell>been in improving convergence and negative utilization with</cell></row><row><cell>limited training. However, we see competitive performances</cell></row><row><cell>on longer training as well. Using a batch size of 256 (510</cell></row><row><cell>negatives), our model reaches 66.5% in 200 and 69.9% in</cell></row><row><cell>400 epochs, compared to 62% and 64.5% respectively with</cell></row><row><cell>same number of negatives in SimCLR(Chen et al. 2020a).</cell></row><row><cell>Remarkably, our 100 epochs pre-trained models transfer</cell></row><row><cell>better than PCL-v2's (Li et al. 2020b) 200 epoch models on</cell></row><row><cell>most transfer learning tasks (see</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Transfer learning results. We transfer a ImageNet-pretrained model (using MMCL) on a range of downstream tasks and datasets. We compare with models pretrained using a similar batch size and epochs. Results on competing approaches are taken from<ref type="bibr" target="#b14">(Ericsson, Gouk, and Hospedales 2021)</ref>. ? Models evaluated using publicly available checkpoints.</figDesc><table><row><cell></cell><cell cols="2">CIFAR-100 STL-10</cell></row><row><cell>C=0.1</cell><cell>44.41</cell><cell>73.82</cell></row><row><cell>C=1</cell><cell>53.08</cell><cell>78.86</cell></row><row><cell>C=10</cell><cell>55.1</cell><cell>81.25</cell></row><row><cell>C=50</cell><cell>54.95</cell><cell>79.63</cell></row><row><cell>C=?</cell><cell>55.1</cell><cell>79.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Reducing C over epochs.</figDesc><table><row><cell></cell><cell>Readout Acc.</cell></row><row><cell>C=1</cell><cell>61.12</cell></row><row><cell>C=2</cell><cell>69</cell></row><row><cell>C=10</cell><cell>80.05</cell></row><row><cell>C=50</cell><cell>79.91</cell></row><row><cell>No Correction</cell><cell>79.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Using false negative correction for STL-10.</figDesc><table><row><cell>Figure 8: Convergence (CIFAR-100).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Accuracy (in %) against the batch size on CIFAR-100.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">When the K is chosen appropriately.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that comparing with the large batch size (as high as 4096) models and longer pre-trained models (as high as 1000 epochs) is not fair since they require as many as 16 high-performance GPUs<ref type="bibr" target="#b4">(Caron et al. 2020</ref>) and very long pretraining times. For comparison, on our compute setup, training with a 256 batch size model for 100 epochs takes upwards of 3.5 days.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>AS thanks Ketul Shah, Aniket Roy, Shlok Mishra, and Susmija Reddy for their feedback. AS and RC are supported by an ONR MURI grant N00014-20-1-2787. SS acknowledges support from NSF-TRIPODS+X:RES (1839258) and from NSF-BIGDATA (1741341).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Additional Transfer Learning Experiments</head><p>In this section, we report results from several additional experiments we conducted analyzing the generalizability of MMCL-learned representations. Specifically, we used SUN397 <ref type="bibr" target="#b59">(Xiao et al. 2010)</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b42">(Parkhi et al. 2012)</ref>, and VOC2007 <ref type="bibr" target="#b15">(Everingham et al. 2010</ref>) for many-shot classification, ISIC <ref type="bibr" target="#b55">(Tschandl, Rosendahl, and Kittler 2018;</ref><ref type="bibr"></ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">OptNet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Theoretical Analysis of Contrastive Unsupervised Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International conference on Machine Learning</title>
		<meeting>International conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-Organizing Neural Betwork that Discovers Surfaces in Random-Dot Stereograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">6356</biblScope>
			<biblScope unit="page" from="161" to="163" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Clustering for Unsupervised Learning of Visual Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prroceedings of International conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Big Self-Supervised Models are Strong Semi-Supervised Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved Baselines with Momentum Contrastive Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Debiased Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advanced in Neural Information Processing Systems</title>
		<meeting>Advanced in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Analysis of Single-Layer Networks in Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchetti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03368</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Support-Vector Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised Visual Representation Learning by Context Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How Well Do Self-Supervised Models Transfer?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5414" to="5423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part-Based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning by Predicting Image Rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05447</idno>
		<title level="m">On Differentiating Parameterized Argmin and Argmax Problems with Application to Bi-level Optimization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap Your Own Latent: A New Approach To Self-Supervised Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Broader Study of Cross-Domain Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="124" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised Co-Training for Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Larochelle, H.</editor>
		<editor>Ranzato, M.</editor>
		<editor>Hadsell, R.</editor>
		<editor>Balcan, M. F.</editor>
		<editor>and Lin, H.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5679" to="5690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Boosting Contrastive Self-Supervised Learning with False Negative Cancellation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khademi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<title level="m">Mining on Manifolds: Metric Learning Without Labels</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="7642" to="7651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hard Negative Mixing for Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A non-monotonic method for large-scale non-negative least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization Methods and Software (OMS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Subgraph Matching Kernels for Attributed Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Representations for Automatic Colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning by Sorting Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical Contrastive Learning of Unsupervised Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical Contrastive Learning of Unsupervised Representations</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An Efficient Framework for Learning Sentence Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ensemble of Exemplar-SVMs for Object Detection and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning of Pretext-Invariant Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using Deep Learning for Image-based Plant Disease Detection. Frontiers in plant science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salath?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1419" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
	<note>Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stochastic Negative Mining for Learning with Large Output Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holtmann-Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1940" to="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Contrastive Learning with Hard Negative Samples</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Contrastive Learning of General-Purpose Audio Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3875" to="3879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<title level="m">Visual Permutation Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3100" to="3114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">GPU acceleration of ADMM for large-scale quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schubiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Banjac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lygeros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning with Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<title level="m">UCF101: A Dataset of 101 Human Actions Classes from Videos in The Wild</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">In-foGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Contrastive Multiview Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Video Representation Learning using Discriminative Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<editor>Efros, A. A.</editor>
		<editor>and Darrell, T. 2020</editor>
		<meeting><address><addrLine>Wang, X</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
	<note>Proceedings of International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning with Augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, 33. You, Y.; Gitman, I.; and Ginsburg, B. 2018. Large Batch Training of Convolutional Networks. Proceedings of International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Online Deep Clustering for Unsupervised Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6688" to="6697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2547" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Colorful Image Colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Local Aggregation for Unsupervised Learning of Visual Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
