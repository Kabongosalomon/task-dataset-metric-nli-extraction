<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge distillation: A good teacher is patient and consistent</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
							<email>lbeyer@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<email>xzhai@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Am?lie</forename><surname>Royer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larisa</forename><surname>Markeeva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
							<email>akolesnikov@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge distillation: A good teacher is patient and consistent</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is a growing discrepancy in computer vision between large-scale models that achieve state-of-the-art performance and models that are affordable in practical applications. In this paper we address this issue and significantly bridge the gap between these two types of models. Throughout our empirical investigation we do not aim to necessarily propose a new method, but strive to identify a robust and effective recipe for making state-of-the-art large scale models affordable in practice. We demonstrate that, when performed correctly, knowledge distillation can be a powerful tool for reducing the size of large models without compromising their performance. In particular, we uncover that there are certain implicit design choices, which may drastically affect the effectiveness of distillation. Our key contribution is the explicit identification of these design choices, which were not previously articulated in the literature. We back up our findings by a comprehensive empirical study, demonstrate compelling results on a wide range of vision datasets and, in particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which achieves 82.8% top-1 accuracy. 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large-scale vision models currently dominate many areas of computer vision. Recent state-of-the-art models for image classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49]</ref>, object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref> or semantic segmentation <ref type="bibr" target="#b52">[53]</ref> push model size to the limits allowed by modern hardware. Despite their impressive performance, these models are rarely used in practice due to high computational costs. Instead, practitioners typically use much smaller models, such as ResNet-50 <ref type="bibr" target="#b22">[23]</ref> or Mo-bileNet <ref type="bibr" target="#b14">[15]</ref>, which are order(s) of magnitude cheaper to run. According to the download counts of five BiT models equal contribution ? work done at Google, while being a PhD student in IST Austria. ? work done at Google, while being a PhD student in Skoltech. <ref type="bibr" target="#b1">2</ref> We provide code and models in the big vision codebase <ref type="bibr" target="#b2">[3]</ref>. Teacher accuracy consistent teaching fixed teacher function matching <ref type="figure" target="#fig_10">Figure 1</ref>. We demonstrate that distillation works the best when we train patiently for a large number of epochs and provide consistent image views to teacher and student models (green and blue lines). This can be contrasted to a popular setting of distilling with precomputed teacher targets (black line), which works much worse.</p><p>from Tensorflow Hub, the smallest ResNet-50 <ref type="bibr" target="#b11">[12]</ref> model has been downloaded for significantly more times than the larger ones. As a result, many recent improvements in vision do not translate to real-world applications.</p><p>To address this problem, we concentrate on the following task: given a specific application and a large model that performs very well on it, we aim to compress the model to a smaller and more efficient architecture without compromising performance. There are two widely used paradigms that target this task: model pruning <ref type="bibr" target="#b18">[19]</ref> and knowledge distillation <ref type="bibr" target="#b12">[13]</ref>. Model pruning reduces the large model's size by stripping away its parts. This procedure can be restrictive in practice: first, it does not allow changing the model family, say from a ResNet to a MobileNet. Second, there may be architecture-dependent challenges, e.g. if the model uses group normalization <ref type="bibr" target="#b46">[47]</ref>, pruning channels may result in the need to dynamically re-balance channel groups.</p><p>Instead, we concentrate on the knowledge distillation approach which does not suffer from these drawbacks. The idea behind knowledge distillation is to "distill" a teacher model, in our case a large and cumbersome model or ensemble of models, into a small and efficient student model. This works by forcing the student's predictions (or internal activations) to match those of the teacher, thus naturally allowing a change in the model family as part of compression. We closely follow the original distillation setup from <ref type="bibr" target="#b12">[13]</ref> and find it surprisingly effective when done right: We interpret distillation as a task of matching the functions implemented by the teacher and student, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. With this interpretation, we discover two principles of knowledge distillation for model compression. First, teacher and student should process the exact same input image views or, more specifically, same crop and augmentations. Second, we want the functions to match on a large number of support points to generalize well. Using an aggressive variant of mixup <ref type="bibr" target="#b51">[52]</ref>, we can generate support points outside the original image manifold. With this in mind, we experimentally demonstrate that consistent image views, aggressive augmentations and very long training schedules are the key to make model compression via knowledge distillation work well in practice.</p><p>Despite the apparent simplicity of our findings, there are multiple reasons that may commonly prevent researchers (and practitioners) from making the design choices that we suggest. First, it is tempting to precompute the teacher's activations for an image offline once to save compute, especially for very large teachers. As we will show, this fixed teacher approach does not work well. Second, knowledge distillation is also commonly used in different contexts (other than model compression), where authors recommend different or even opposite design choices <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>, see <ref type="figure" target="#fig_1">Figure 2</ref>. Third, knowledge distillation requires an atypically large number of epochs to reach best performance, much more than commonly used for supervised training. Finally, choices which may look suboptimal in training of regular length often end up being best for long runs, and vice-versa.</p><p>In our empirical study, we mostly concentrate on compressing the large BiT-ResNet-152x2 from <ref type="bibr" target="#b22">[23]</ref> that was pretrained on the ImageNet-21k dataset <ref type="bibr" target="#b36">[37]</ref> and fine-tuned to the relevant datasets of interest. We distill it to a standard ResNet-50 architecture <ref type="bibr" target="#b11">[12]</ref> (but replace batch normalization with group normalization) on a range of small and mid-sized datasets without compromising accuracy. We also achieve very strong results on the ImageNet <ref type="bibr" target="#b35">[36]</ref> dataset: with a total number of 9600 epochs for distillation, we set the new ResNet-50 SOTA 82.8% on ImageNet. This is 4.4% bet-ter than the ResNet-50 model from <ref type="bibr" target="#b22">[23]</ref>, and 2.2% better than the best ResNet-50 model in the literature, which uses a more complex setup <ref type="bibr" target="#b37">[38]</ref>. Finally, we demonstrate that our distillation recipe also works when simultaneously compressing and changing the model family, e.g. BiT-ResNet architecture to the MobileNet architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experimental setup</head><p>In this section, we introduce the experimental setup and benchmarks we use throughout the paper. Given a largescale vision model (the teacher, or T) with high accuracy on a particular task, we aim to compress this model to a much smaller model (the student, or S) without compromising its performance. Our compression recipe relies on knowledge distillation, as introduced in <ref type="bibr" target="#b12">[13]</ref>, and a careful investigation of several key ingredients in the training setup.</p><p>Datasets, metrics and evaluation protocol. We conduct experiments on five popular image classification datasets: flowers102 <ref type="bibr" target="#b30">[31]</ref>, pets <ref type="bibr" target="#b32">[33]</ref>, food101 <ref type="bibr" target="#b20">[21]</ref>, sun397 <ref type="bibr" target="#b47">[48]</ref> and ILSVRC-2012 ("ImageNet") <ref type="bibr" target="#b35">[36]</ref>. These datasets span diverse image classification scenarios; In particular, they vary in the number of classes, from 37 to 1000 classes, and total number of training images, from 1020 to 1281167 training images. This allows us to verify our distillation recipe for a broad range of practical settings and ensure its robustness.</p><p>As a metric, we always report classification accuracy. For all datasets, we perform design choices and hyperparameters selection using a validation split, and report final results on the test set. These splits are defined in the appendix E.</p><p>Teacher and student models. Throughout the paper, we opt for using pre-trained teacher models from BiT <ref type="bibr" target="#b22">[23]</ref>, which provides a large collection of ResNet models pretrained on ILSVRC-2012 and ImageNet-21k datasets, with state-of-the-art accuracy. The only significant differences between BiT-ResNets and standard ResNets is their use of group normalization layer <ref type="bibr" target="#b46">[47]</ref> and weight standardization <ref type="bibr" target="#b33">[34]</ref>, which are used instead of batch normalization <ref type="bibr" target="#b16">[17]</ref>.</p><p>In particular, we concentrate on the BiT-M-R152x2 architecture: a BiT-ResNet-152x2 (152 layers, 'x2' indicates the width multiplier) pretrained on ImageNet-21k. This model demonstrates excellent performance on a variety of vision benchmarks and it is still manageable to run extensive ablation studies with it. It is highly expensive to deploy (requires roughly 10x more compute than the standard ResNet-50), and thus effective compression of this model is of practical importance. For the student's architecture, we use a BiT-ResNet-50 variant, referred to as ResNet-50 for brevity.</p><p>Distillation loss. We use the KL-divergence between the teacher's p t , and the student's p s predicted class probability vectors as a distillation loss, as was originally introduced in <ref type="bibr" target="#b12">[13]</ref>. We do not use any additional loss term with respect to the original dataset's hard labels:</p><formula xml:id="formula_0">KL(p t ||p s ) = i?C [?p t,i log p s,i + p t,i log p t,i ] ,<label>(1)</label></formula><p>where C is a set of classes. Also, as in <ref type="bibr" target="#b12">[13]</ref>, we introduce a temperature parameter T , which is used to adjust the entropy of the predicted softmax-probability distributions before they are used in the loss computation: p s ? exp( log ps T ) and p t ? exp( log pt T ). Training setup. For optimization, we train our models with the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with default parameters, except for the initial learning rate that is part of our hyperparameter exploration. We use a cosine learning rate schedule <ref type="bibr" target="#b27">[28]</ref> without warm restarts. We also sweep over the weight decay loss coefficient for all our experiments (for which we use a "decoupled" weight decay convention <ref type="bibr" target="#b28">[29]</ref>). To stabilize training we enable gradient clipping with a threshold of 1.0 on the global L2-norm of a gradient. Finally, we use batch size 512 for all our experiments, except for models trained on ImageNet, where we train with batch size 4096. For the remaining hyperparameters, we discuss their sweeping range together with corresponding experiments in the next section.</p><p>One additional important component of our recipe is the mixup data augmentation strategy <ref type="bibr" target="#b51">[52]</ref>. In particular, we introduce a mixup variant in our "function matching" strategy (see Section 3.1.1), in which we use "agressive" mixing coefficients sampled uniformly from [0, 1], which can be seen as an extreme case of the originally proposed sampling from ?-distribution.</p><p>Unless explicitly specified otherwise, for prepossessing we use an "inception-style" crop <ref type="bibr" target="#b38">[39]</ref> and then resize images to a fixed square size. Furthermore, in order to make our ex-tensive analysis computationally feasible (overall we trained dozens of thousands of models), we use relatively low input resolution and resize input images to 128 ? 128 size, except for our ImageNet experiments, that use the standard input 224 ? 224 resolution.</p><p>For all our experiments we use Google Cloud TPU accelerators <ref type="bibr" target="#b19">[20]</ref>. We also report our batch sizes, epochs or total number of update steps, which allow to estimate resource requirements for any particular experiment of interest. Code and checkpoints are made publicly available <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Distillation for model compression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Investigating the "consistent and patient teacher" hypothesis</head><p>In this section, we perform an experimental verification of our hypothesis formulated in the introduction and visualised in <ref type="figure" target="#fig_1">Figure 2</ref>, that distillation works best when seen as function matching, i.e. when the student and teacher see consistent views of the input images, synthetically "filled" via mixup, and when student is trained using long training schedule (i.e. the "teacher" is patient).</p><p>To make sure that our findings are robust, we perform a very thorough analysis on four small and medium scale datasets, namely Flowers102 <ref type="bibr" target="#b30">[31]</ref> (1020 training images), Pets <ref type="bibr" target="#b32">[33]</ref> (3312 training images), Food101 <ref type="bibr" target="#b20">[21]</ref> (about 68k training images), and SUN397 <ref type="bibr" target="#b47">[48]</ref> (76k training images).</p><p>In an effort to remove any confounding factors, for each individual distillation setting we sweep over all combinations of learning rates {0.0003, 0.001, 0.003, 0.01}, weight decays {1 ? 10 ?5 , 3 ? 10 ?5 , 1 ? 10 ?4 , 3 ? 10 ?4 , 1 ? 10 ?3 }, and distillation temperatures {1, 2, 5, 10}. In all reported figures,   <ref type="figure" target="#fig_1">Figure 2</ref> and Section 3.1.1. Note that while the fixed teacher settings achieve significantly lower distillation loss, they lead to students which do not generalize well. In contrast, consistent teaching and function matching approaches lead to significantly higher student performance. Similar results on more datasets are reported in Appendix C.</p><p>we show every single run as a low opacity curve, and highlight the one with the best final validation accuracy. We provide corresponding test accuracies in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Importance of "consistent" teaching</head><p>First, we demonstrate that the consistency criterion, i.e. student and teacher seeing the same views, is the only way of performing distillation which reaches peak student performance across all datasets consistently. For this study, we define multiple distillation configurations which correspond to instantiations of all four options sketched in <ref type="figure" target="#fig_1">Figure 2</ref>, with the same color coding:</p><p>? Fixed teacher. We explore several options where the teacher's predictions are constant for a given image (precomputed target). The simplest (and worst) method is fix/rs, where the image is just resized to 224 2 px for both student and teacher. fix/cc follows a more common approach of using a fixed central crop for the teacher and a mild random crop for the student. fix/ic ens is a heavy data augmentation approach where the teacher's prediction is the average of 1k inception crops, which we verified to improve the teacher's performance. The student also uses random inception crops. The two latter settings are similar to the input noise strategy from the "noisy student" paper <ref type="bibr" target="#b48">[49]</ref>.</p><p>? Independent noise. We instantiate this common strategy in two ways: ind/rc computes two independent mild random crops for the teacher and student respectively, while ind/ic uses the heavier inception crop instead. A similar setup was used in <ref type="bibr" target="#b40">[41]</ref>.</p><p>? Consistent teaching. In this approach, we randomly crop the image only once, either with mild random cropping (same/rc) or heavy inception crop (same/ic), and use this same crop for the input to both the student and the teacher.</p><p>? Function matching. This approach extends consistent teaching, by expanding an input manifold of images through mixup (mix), and, again, providing consistent inputs to the student and the teacher. For brevity, we sometimes refer to this approach as "FunMatch".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3 shows 10 000 epoch training curves on</head><p>Flow-ers102 dataset in all of these configurations. These results clearly show that "consistency" is the key: all "inconsistent" distillation settings plateau at a lower score, while consistent settings increase student performance significantly, with the function matching approach working the best. Furthermore, the training losses show that, for such small datasets, using a fixed teacher leads to strong overfitting. In contrast, function matching never reaches such loss on the training set while generalizing much better to the validation set. Due to space constraints, we show analogous results for other datasets and training durations in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Importance of "patient" teaching</head><p>One can interpret distillation as a variant of supervised learning, where labels (potentially soft) are provided by a strong teacher model. This is especially true when the teacher predictions are (pre)computed for a single image view. This approach inherits all problems of the standard supervised learning, e.g. aggressive data augmentations may distort actual image label, while less aggressive augmentations may cause overfitting.</p><p>However, things change if we interpret distillation as function matching, and, crucially, make sure to provide consistent inputs to the student and teacher. In this case we can be very aggressive with image augmentations: even if an image view is too distorted, we still will make a progress We empirically confirm our intuition in <ref type="figure">Figure 4</ref>, where for each dataset we show the evolution of test accuracy during training of the best function matching student (according to validation), for different amounts of training epochs. The teacher is shown as a red line and is always reached eventually, after a much larger number of epochs than one would ever use in a supervised training setup. Crucially, there is no overfitting even when we optimize for a 1M epochs.</p><p>We also trained and tuned two more baselines for reference: training a ResNet-50 from scratch using the dataset original hard labels, as well as transferring a ResNet-50 that was pre-trained on ImageNet-21k. For both of these baselines, we heavily tune learning rate and weight decay as described in Section 3.1. The model trained from scratch using the original labels is substantially outperformed by our student. The transfer model fares much better, but is eventually also outperformed.</p><p>Notably, training for a relatively short but common duration of 100 epochs leads to much worse performance than the transfer baseline. Overall, the ResNet-50 student patiently and consistently matches the very strong but much more expensive ResNet-152x2 teacher across the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scaling up to ImageNet</head><p>Based on our insights from the previous sections, we now investigate how the proposed distillation recipe scales to the widely used and more challenging ImageNet dataset <ref type="bibr" target="#b35">[36]</ref>. Following the same protocol as before, in <ref type="figure" target="#fig_5">Figure 5</ref> (left), we report student accuracy curves throughout training for three distillation settings: (1) fixed teacher, (2) consistent teaching and (3) function matching. For reference, our base teacher model reaches a top-1 accuracy of 83.0%. Fixed teacher again suffers from long training schedules, and starts overfitting after 600 epochs. In contrast, the consistent teaching approaches continuously improves performance as the training duration increases. From this we can conclude that consistency is a key to make distillation work on ImageNet, similar to the behaviors on the previously discussed small and mid-sized datasets.</p><p>Compared to simple consistent teaching, function matching performs slightly worse with short schedules, which likely happens due to underfitting. But when we increase the length of training schedule, the improvement of function matching becomes apparent: for instance with only 1200 epochs, it is able to match the performance of consistent teaching at 4800 epochs, thus saving 75% compute resource. Finally, for the longest run of function matching we experimented on, the vanilla ResNet-50 student architecture achieves 82.31% top-1 accuracy on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Distilling across different input resolutions</head><p>So far, we have assumed that both the student and teacher receive the same standard input resolution of 224px. However, it is possible to pass images of different resolution to the student and the teacher, while still being consistent: one simply has to perform the crop on the original high-resolution image, and subsequently resize it differently for the student and the teacher: their views will be consistent, albeit at different resolutions. This insight can be leveraged for learning from a better, higher resolution, teacher <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref>, but also for training a smaller, faster student <ref type="bibr" target="#b1">[2]</ref>. We investigate both directions: first, following <ref type="bibr" target="#b1">[2]</ref>, we train a ResNet-50 student with an input resolution of 160px while leaving the teacher  Middle: Reducing the optimization cost, via Shampoo preconditioning; with 1200 epochs, it is able to match the baseline trained for 4800 epochs. Right: Initializing student with pre-trained weights improves short training runs, but harms for the longest schedules. resolution unchanged (224px). This results in a twice faster model, which still achieves remarkable 80.49% top-1 accuracy (see <ref type="table" target="#tab_1">Table 1</ref>), compared to the best published 78.8% at this resolution using an array of modifications <ref type="bibr" target="#b1">[2]</ref>. Second, following <ref type="bibr" target="#b22">[23]</ref>, we distill a teacher that was finetuned at a resolution of 384px (and attains 83.7% top-1 accuracy), this time leaving the student resolution unchanged, i.e. consuming a 224px input image. Compared to the baseline teacher, this provides a modest but consistent improvement across the board, as shown in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization: A second order preconditioner ( ) improves training efficiency</head><p>We observe that optimization efficacy creates a computational bottleneck for our distillation recipe with "function matching" perspective due to long training schedules. Intuitively, we believe that optimization difficulties stem from the fact that it is much harder to fit a general function with multivariate outputs, rather than fixed image-level labels. Thus, we conduct an initial exploration, whether more powerful optimizers can do a much better job at our task.</p><p>To this end, we change the underlying optimizer from Adam to Shampoo <ref type="bibr" target="#b0">[1]</ref>, with the second order preconditioner. In <ref type="figure" target="#fig_5">Figure 5</ref> (middle) we observe that Shampoo achieves the same test accuracy reached by Adam at 4800 epochs in just 1200 epochs, and with minimal step time overhead. And, in general, we observe consistent improvement over Adam in all our experimental settings. Experimental details on the Shampoo optimizer are provided in the Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Optimization: A good initialization improves short runs but eventually falls behind</head><p>Motivated by transfer learning literature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref> and <ref type="bibr" target="#b37">[38]</ref>, where a good initialization is able to significantly shorten the training cost and achieve a better solution, we try to initialize the student model with a pre-trained BiT-M-ResNet50 weights and show the results in <ref type="figure" target="#fig_5">Figure 5</ref> (right).</p><p>The BiT-M initialization improves more than 2% when the distillation duration is short. However, the gap closes when the training schedule is long enough. Our observation is similar to the conclusion of <ref type="bibr" target="#b10">[11]</ref>. Starting from 1200 epochs, distilling from scratch matches the BiT-M initializated student, and slightly overtakes it for 4800 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Distilling across different model families</head><p>Going beyond using different input resolutions for student and teacher, nothing in principle prevents us from using architectures of different families entirely, as our consistent patient teacher approach still applies in this setting. This allows us to efficiently transfer knowledge from stronger and more complex teachers, e.g. ensembles, while keeping the simple architecture of a ResNet50 student, but also transfer the state-of-the-art performance of large ResNet models to more efficient architectures e.g. MobileNet. We demonstrate this via two experiments. First, we use an ensemble of two models as teacher and show that this further improves performance. Second, we train a MobileNet v3 <ref type="bibr" target="#b13">[14]</ref> student and obtain the best reported MobileNet v3 model to date.</p><p>MobileNet student. We use MobileNet v3 (Large) as a Ensemble teacher. We now try a better teacher: we create a model which consists of averaging the logits from our default teacher at 224px resolution, and our teacher at 384px resolution from the previous section. This is a different, though closely related, type of teacher which is significantly more powerful but also slower. This teacher's student is better than our default teacher's student at every duration we tried (Appendix A) and, after 9600 epochs, reaches a new state-of-the-art top-1 ImageNet accuracy of 82.82%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Comparison to the results from literature.</head><p>Now, when we introduced our key experiments, we compare our best ResNet-50 models to the best ResNet-50 models available in the literature, see <ref type="table" target="#tab_2">Table 2</ref>. In particular, for 224 ? 224 input resolution we compare against the original ResNet-50 model from <ref type="bibr" target="#b11">[12]</ref>, BiT-M-ResNet-50 pretrained on ImageNet-21k dataset <ref type="bibr" target="#b36">[37]</ref> and previous state-of-the-art model from <ref type="bibr" target="#b37">[38]</ref>. For 160 ? 160 input resolution we compare against very recent and competitive model from <ref type="bibr" target="#b1">[2]</ref>. We observe that our distillation recipe leads the state-of-the-art performance in both cases and by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Distilling on the "out-of-domain" data</head><p>By looking at knowledge distillation as "function matching", one can draw a reasonable hypothesis that distillation can be done on arbitrary image inputs. In this section we investigate this hypothesis.</p><p>We conduct experiments on pets and sun397 datasets. We use our distillation recipe to distill pets and sun397 models using out-of-domain images from the food101 and ImageNet datasets and, for the reference results, also run distillation with "in-domain" images from pets and sun397 datasets. <ref type="figure" target="#fig_7">Figure 6</ref> summarizes our results. First we observe that distilling using in-domain data works the best. Somewhat surprisingly, even if the images are completely unrelated, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9.">Finetuning ResNet-50 with augmentations</head><p>To make sure that our observed state-of-the-art distillation results are not an artifact of our well-tuned training setup, namely very long schedule and aggressive mixup augmentations, we train corresponding baseline ResNet-50 models. More specifically, we reuse the distillation training setup for supervised training on ImageNet dataset without distillation loss. To further strengthen our baseline, we additionally try SGD optimizer with momentum, which is known to often work better for ImageNet than Adam optimizer.</p><p>Results are shown in <ref type="figure" target="#fig_8">Figure 7</ref>. We observe that training with labels and without distillation loss leads to significantly worse results and starts to overfit for long training schedules. Thus, we conclude that distillation is necessary to make our training recipe work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related work</head><p>There are many paradigms for compressing neural networks. One of them is pruning, where the general idea is to discard parts of the trained model while making it much more efficient and incurring little or no sacrifise in performance. Model pruning comes in many different flavours: it can be unstructured (i.e. focus on pruning individual connections) or structured (i.e. focus on pruning larger building blocks, e.g. whole channels). It can also come with or without an additional finetuning step, or be iterative or not. Balanced and fair discussion of this topic goes beyond the scope of this paper, so we refer interested reader to recent overviews   as a starting point <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Knowledge distillation <ref type="bibr" target="#b12">[13]</ref> is a technique for transferring knowledge from one model (teacher) to another (student), by optimizing a student model to match certain outputs (or intermediate activations) of a teacher model. This technique is used in numerous distinct contexts, such as semisupervised learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b48">49]</ref> or even self-supervised learning <ref type="bibr" target="#b8">[9]</ref>. In this paper we only consider knowledge distillation as a tool for model compression. The efficiency of distillation has been showcased in numerous works, e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref>, under different depth/width patterns of the student and teacher architectures, and even combined with other compression techniques <ref type="bibr" target="#b29">[30]</ref>. Notably, MEAL <ref type="bibr" target="#b37">[38]</ref> proposes to distill an ensemble of large ResNet teachers to a smaller ResNet student with an adversarial loss and achieves strong results. The main difference of our work to similar works on knowledge distillation for compression, is that our method is simultaneously the simplest and best performing: we do not introduce any new components, but rather discover that correct training setup is sufficient to attain state-of-the art results.</p><p>Weights quantization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref> and decomposition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44]</ref> aim to accelerate and reduce the memory footprint of CNNs by replacing large matrices operations with their lightweight approximations. This line of research is largely orthogonal to this work and can generally be combined with the method from this paper, especially during for the final model deployment stage. We leave exploration of this topic for future research.</p><p>Finally, there is a line of work, which approaches our goal (compact and high performing models) from a different angle, by focusing on altering the architecture and getting good compact models trained from scratch, so there is no need to compress large models. Some notable examples include ResNeXt <ref type="bibr" target="#b49">[50]</ref>, Squeeze-and-Excitation Networks <ref type="bibr" target="#b15">[16]</ref> and Selective Kernel <ref type="bibr" target="#b24">[25]</ref>, which propose modifications that improve model accuracy for a fixed compute budget. These improvements are complementary to the research question tackled in this paper and can be compounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Instead of proposing a new method for model compression, we closely look at the existing common knowledge distillation process and identify how to make it work really well in the context of model compression. Our key findings stem from a specific interpretation of knowledge distillation: we propose to see it as a function matching task. This is not the typical view of knowledge distillation, as normally it is seen as "a strong teacher generates better (soft) labels that are useful for training a better and smaller student model".</p><p>Based on our interpretation we simultaneously incorporate three ingredients: (i) make sure that teacher and student always get identical inputs, including noise, (ii) introduce aggressive data augmentations to enrich the input image manifold (through mixup) and (iii) use very long training schedules. Even though each component of our recipe may seem trivial, our experiments show that one has to apply all of them jointly to get top results.</p><p>We attain very strong empirical results for compressing very large models to the more practical ResNet-50 architecture. We believe that they are very useful from a practical point of view and are a very strong baseline for future research on compressing large-scale models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Full results tables</head><p>We provide a full summary of our experiments on Ima-geNet in <ref type="table" target="#tab_3">Table 3</ref>, with a dash "-" marking settings we did not deem necessary to run, as the cost outweights the potential insights.</p><p>Furthermore, <ref type="table">Table 4</ref> gives numerical results for the models shown in <ref type="figure">Figure 4</ref>, our best models (according to validation) on the four smaller datasets at 128px resolution, together with baselines and the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. BiT models download statistics</head><p>In <ref type="figure">Figure 8</ref>, we show the download statistics for models with different sizes: ResNet50, ResNet50x3, ResNet101, ResNet101x3 and ResNet152x4. It's clear that the smallest ResNet50 model is the most used, with a significant gap compared to the other models. The practitioners' behavior motivates our work of getting the best possible ResNet50 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More consistency plots</head><p>In <ref type="figure" target="#fig_1">Figures 9 to 12</ref>, we show the "consistency" plots (cf <ref type="figure" target="#fig_3">Figure 3</ref> in the main paper) for all datasets and across all training durations. It is noteworthy that (relatively) short runs may provide deceptive signal on the best method, and only with the addition of "patience", e.g. when distilling for a long time, does it become clear that the full function-matching approach is the best choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Shampoo optimization details</head><p>For all experiments the learning rate schedule was a linear warm-up up to 1800 steps followed by a quadratic decay towards zero. Overhead of Shampoo is quite minimal due blocking trick (each preconditioner is atmost 128x128) and inverse is run in a distributed manner across the TPU cores every step, with nesterov momentum. These settings are identical to the the training recipe in <ref type="bibr" target="#b0">[1]</ref> for training a ResNet-50 architecture on ImageNet from scratch efficiently at large batch sizes. All experiments uses weight decay of 0.000375.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training, validation and test splits</head><p>Throughout our experiments we rely on the tensorflow datasets library 3 to access all datasets. A huge advantage of this library is that it enables a unified and reproducible way to access diverse datasets. To this end, we report our train, validation and test splits (following the library's notation) in <ref type="table">Table 5.</ref> 3 https://www.tensorflow.org/datasets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Configuration file for ImageNet distillation</head><p>We present the configuration for performing distillation on ImageNet following the big vision <ref type="bibr" target="#b2">[3]</ref> conventions. <ref type="bibr" target="#b0">1</ref>    Overall <ref type="figure">Figure 8</ref>. BiT models download statistics according to https://tfhub.dev/google/collections/bit. "BiT-S"/"BiT-M" denotes the BiT model for feature extraction, while the figures with a mention of "head" correspond to the classifiers. The rightmost overall plot shows the total download counts for each size. It is clear that ResNet-50 is by far the most widely used model.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Schematic illustrations of various design choices when doing knowledge distillation. Left: Teacher receives a fixed image, while student receives a random augmentation. Center-left: Teacher and student receive independent image augmentations. Center-right: Teacher and student receive consistent image augmentations. Right: Teacher and student receive consistent image augmentations plus the input image manifold is extended by including linear segments between pairs of images (known as mixup [52] augmentation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Experimental validation of the "consistency" requirement on the Flowers102 dataset. Colors match different knowledge distillation design choices as introduced in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Left: Top-1 accuracy on ImageNet of three distillation setups: (1) fixed teacher; (2) consistent teaching; (3) function matching ("FunMatch"). Light color curves show accuracy throughout training, while the solid scatter plots are the final results. The student with a fixed teacher eventually saturates and overfits to it. Both consistent teaching and function matching do not exhibit overfitting or saturation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Distilling pet and sun397 datasets on different data sources. Results indicate that distilling on completely unrelated images works to some extent, even though final results are relatively low. Distilling on "in-domain" data is the best and distilling on related/overlapping images can work reasonably well, but may require extra long training schedule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Baseline ResNet-50 models trained from scratch with labels vs with the ResNet-152x2 teacher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>51 # 52 #</head><label>5152</label><figDesc>17config.student_name = 'bit_paper' 18 config.student = dict(depth=50, width=1)19   20    config.teachers = ['prof_m']<ref type="bibr" target="#b20">21</ref> config.prof_m_name = 'bit_paper'<ref type="bibr" target="#b21">22</ref> config.prof_m_init = 'FILENAME'<ref type="bibr" target="#b22">23</ref> config.prof_m = dict(depth=152, width=2) Eval section shortened for brevity, see code release for full details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Listing 1 .</head><label>1</label><figDesc>Full config for ImageNet distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>Consistency plots for the Flowers102 dataset, when training for 1 000 epochs, 10 000 epochs, and 100 000 epochs, from top to bottom respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 .</head><label>10</label><figDesc>Consistency plots for the Pet37 dataset, when training for 1 000 epochs, 3 000 epochs, 10 000 epochs, and 30 000 epochs, from top to bottom respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 .</head><label>11</label><figDesc>Consistency plots for the Food101 dataset, when training for 100 epochs, 1 000 epochs, and 3 000 epochs, from top to bottom respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 .</head><label>12</label><figDesc>Consistency plots for the SUN397 dataset, when training for 100 epochs, 1 000 epochs, and 3 000 epochs, from top to bottom respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 4. One needs patience along with consistency when doing distillation. Eventually, the teacher will be matched; this is true across various datasets of different scale.towards matching the relevant functions on this input. Thus, we can be more opportunistic with augmentations and avoid overfitting by doing aggressive image augmentations and, if true, optimize for very long time until the student's function comes close to the teacher's.</figDesc><table><row><cell>50 60 70 80 90 100 Test accuracy [%]</cell><cell>1k Student (R50) 10k 100k Flowers102 1020 images Teacher: BiT-M (R152x2) 1M 100 300 1k 3k 10k 30k 65 70 Pet37 3312 images Training duration [epochs] 30 100 300 65 70 Food101 1k 68k images 75 75 80 80 85 85 90 Baseline: from scratch (R50)</cell><cell>3k</cell><cell>30 Baseline: BiT-M transfer (R50) 100 300 1k 50 55 76k images Sun397 60 65 70 75</cell><cell>3k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Top-1 test accuracy for different teacher/student input resolutions (rows) and number of training epochs (columns).</figDesc><table><row><cell>Experiment</cell><cell>300</cell><cell>1200</cell><cell>4800</cell><cell>9600</cell></row><row><cell>T224 ? S224</cell><cell>80.30</cell><cell>81.54</cell><cell>82.18</cell><cell>82.31</cell></row><row><cell>T224 ? S160</cell><cell>78.17</cell><cell>79.61</cell><cell>N/A</cell><cell>80.49</cell></row><row><cell>T384 ? S224</cell><cell>80.46</cell><cell>81.82</cell><cell>82.33</cell><cell>82.64</cell></row><row><cell cols="5">student, for most experiments we opt for the variant which</cell></row><row><cell cols="5">uses GroupNorm (with the default of 8 groups) instead of</cell></row><row><cell cols="5">BatchNorm. We do not use any of the training tricks used</cell></row><row><cell cols="5">in the original paper,we simply perform function matching.</cell></row><row><cell cols="5">Our student reaches 74.60% after 300 epochs, and 76.31%</cell></row><row><cell cols="5">after 1200 epochs, resulting in the best published MobileNet</cell></row><row><cell cols="4">v3 model. More results are in the Appendix A.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of our best and literature ResNet models. The metric is accuracy on ImageNet test split (officially val split).</figDesc><table><row><cell>Model</cell><cell cols="3">Arch. Res. Accuracy</cell></row><row><cell>"Revisiting ResNet" [2]</cell><cell>R50</cell><cell>160</cell><cell>78.8%</cell></row><row><cell>FunMatch (T224)</cell><cell>R50</cell><cell>160</cell><cell>80.5%</cell></row><row><cell>Original ResNet [12]</cell><cell>R50</cell><cell>224</cell><cell>77.2%</cell></row><row><cell>BiT-M-R50 [23]</cell><cell>R50</cell><cell>224</cell><cell>78.4%</cell></row><row><cell>Meal-v2 [38]</cell><cell>R50</cell><cell>224</cell><cell>80.7%</cell></row><row><cell>FunMatch (T384+224)</cell><cell>R50</cell><cell>224</cell><cell>82.8%</cell></row><row><cell>"Revisiting ResNet" [2]</cell><cell cols="2">R152 224</cell><cell>82.8%</cell></row><row><cell cols="4">distillation still works to some extent, though results get</cell></row><row><cell cols="4">worse. This, for example, means that the student model can</cell></row><row><cell cols="4">learn to classify pets with roughly 30% accuracy by only</cell></row><row><cell cols="4">seeing food images (softly) labeled as breeds of pets. Finally,</cell></row><row><cell cols="4">if distillation images are somewhat related or overlapping</cell></row><row><cell cols="4">with the actual "in-domain" images (e.g. Pets and ImageNet,</cell></row><row><cell cols="4">or sun397 and ImageNet), then results can be as good (or</cell></row><row><cell cols="4">almost as good) as using "in-domain" data, but extra long</cell></row><row><cell cols="3">optimization schedule may be required.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Summary of all ImageNet distillation runs. Numbers represent top-1 accuracy on the validation set. By default, the student is always a ResNet50 and the teacher is BiT-M-R152x2.</figDesc><table><row><cell cols="2">Experiment</cell><cell></cell><cell></cell><cell cols="2">30ep</cell><cell>90ep</cell><cell></cell><cell>300ep</cell><cell cols="2">600ep</cell><cell>1200ep</cell><cell>4800ep</cell><cell>9600ep</cell></row><row><cell cols="2">Best from labels</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>76.59</cell><cell></cell><cell>78.08</cell><cell>-</cell><cell></cell><cell>78.15</cell><cell>76.59</cell><cell>-</cell></row><row><cell cols="2">Fixed teacher</cell><cell></cell><cell></cell><cell cols="2">73.75</cell><cell>76.45</cell><cell></cell><cell>77.76</cell><cell cols="2">77.99</cell><cell>78.11</cell><cell>77.56</cell><cell>76.95</cell></row><row><cell cols="2">consistent teacher</cell><cell></cell><cell></cell><cell cols="2">74.95</cell><cell>78.05</cell><cell></cell><cell>80.08</cell><cell cols="2">80.63</cell><cell>81.15</cell><cell>81.58</cell><cell>81.76</cell></row><row><cell cols="4">function matching (FunMatch)</cell><cell cols="2">73.89</cell><cell>78.00</cell><cell></cell><cell>80.30</cell><cell cols="2">81.17</cell><cell>81.54</cell><cell>82.18</cell><cell>82.31</cell></row><row><cell cols="2">consistent teacher</cell><cell></cell><cell></cell><cell cols="2">75.45</cell><cell>78.79</cell><cell></cell><cell>80.54</cell><cell cols="2">81.11</cell><cell>81.44</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">function matching</cell><cell></cell><cell></cell><cell cols="2">75.12</cell><cell>78.70</cell><cell></cell><cell>80.63</cell><cell>-</cell><cell></cell><cell>81.67</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">T224 ? S160 (consistent teacher)</cell><cell cols="2">71.38</cell><cell>75.57</cell><cell></cell><cell>78.01</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">T224 ? S160 (function matching)</cell><cell cols="2">70.22</cell><cell>75.34</cell><cell></cell><cell>78.17</cell><cell cols="2">79.07</cell><cell>79.61</cell><cell>0.10</cell><cell>80.49</cell></row><row><cell cols="4">FunMatch: T384 ? S224</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>80.46</cell><cell>-</cell><cell></cell><cell>81.82</cell><cell>82.33</cell><cell>82.64</cell></row><row><cell cols="4">FunMatch: T384+224 ? S224</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>82.12</cell><cell>82.71</cell><cell>82.82</cell></row><row><cell cols="4">FunMatch: MobileNet v3 (GN)</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>74.60</cell><cell>-</cell><cell></cell><cell>76.31</cell><cell>76.84</cell><cell>76.97</cell></row><row><cell cols="4">FunMatch: MobileNet v3 (GN, 2T)</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>74.85</cell><cell>-</cell><cell></cell><cell>76.51</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">FunMatch: MobileNet v3 (GN, Small)</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>65.61</cell><cell>-</cell><cell></cell><cell>67.57</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">FunMatch: MobileNet v3 (BN)</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>72.32</cell><cell>-</cell><cell></cell><cell>73.51</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">FunMatch: MobileNet v3 (BN, 2T)</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>73.28</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Figure 5 (right): BiT-M-R50 init</cell><cell cols="2">77.52</cell><cell>79.43</cell><cell></cell><cell>80.47</cell><cell cols="2">80.83</cell><cell>81.11</cell><cell>81.45</cell><cell>-</cell></row><row><cell cols="2">Figure 7: SGDM</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>76.59</cell><cell></cell><cell>76.38</cell><cell>-</cell><cell></cell><cell>74.93</cell><cell>73.48</cell><cell>-</cell></row><row><cell cols="2">Figure 7: Adam</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>74.92</cell><cell></cell><cell>74.55</cell><cell>-</cell><cell></cell><cell>73.47</cell><cell>70.66</cell><cell>-</cell></row><row><cell cols="4">Figure 7: SGDM + Mixup</cell><cell>-</cell><cell></cell><cell>76.18</cell><cell></cell><cell>78.06</cell><cell>-</cell><cell></cell><cell>75.01</cell><cell>71.40</cell><cell>-</cell></row><row><cell cols="3">Figure 7: Adam + Mixup</cell><cell></cell><cell>-</cell><cell></cell><cell>76.17</cell><cell></cell><cell>78.08</cell><cell>-</cell><cell></cell><cell>78.15</cell><cell>76.59</cell><cell>-</cell></row><row><cell>7500</cell><cell>BiT-S</cell><cell>7500</cell><cell>BiT-S (i1k head)</cell><cell>7500</cell><cell></cell><cell>BiT-M</cell><cell>7500</cell><cell cols="2">BiT-M (i1k head)</cell><cell>7500</cell><cell cols="2">BiT-M (i21k head)</cell><cell>12500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10000</cell></row><row><cell>5000</cell><cell></cell><cell>5000</cell><cell></cell><cell>5000</cell><cell></cell><cell></cell><cell>5000</cell><cell></cell><cell></cell><cell>5000</cell><cell></cell><cell>7500</cell></row><row><cell>2500</cell><cell></cell><cell>2500</cell><cell></cell><cell>2500</cell><cell></cell><cell></cell><cell>2500</cell><cell></cell><cell></cell><cell>2500</cell><cell></cell><cell>5000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2500</cell></row><row><cell>0</cell><cell>R50 R50x3 R101 R101x3 R152x4</cell><cell>0</cell><cell>R50 R50x3 R101 R101x3 R152x4</cell><cell>0</cell><cell cols="2">R50 R50x3 R101 R101x3 R152x4</cell><cell>0</cell><cell cols="2">R50 R50x3 R101 R101x3 R152x4</cell><cell>0</cell><cell cols="2">R50 R50x3 R101 R101x3 R152x4</cell><cell>0</cell><cell>R50 R50x3 R101 R101x3 R152x4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Tabular representation of the results fromFigure 4. Train, validation and test splits. Split definitions follow notation from the tensorflow datasets library and can be directly used to access relevant data splits using the library.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="3">Epochs Final Test Acc T LR</cell><cell>WD</cell></row><row><cell></cell><cell></cell><cell cols="2">Flowers102</cell><cell></cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>1000</cell><cell>77.51%</cell><cell cols="2">10 0.003 0.001</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>10 000</cell><cell>92.83%</cell><cell cols="2">10 0.003 0.0003</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>100 000</cell><cell>95.54%</cell><cell cols="2">1 0.001 0.0001</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>1 000 000</cell><cell>96.93%</cell><cell cols="2">1 0.0003 1e-05</cell></row><row><cell></cell><cell>ResNet-152x2 teacher</cell><cell>-</cell><cell>97.82%</cell><cell>--</cell><cell>-</cell></row><row><cell></cell><cell>Best transfer ResNet50</cell><cell>10 000</cell><cell>97.50%</cell><cell cols="2">LR=0.01, Mixup=0.0</cell></row><row><cell></cell><cell>Best from-scratch ResNet50</cell><cell>10 000</cell><cell>66.38%</cell><cell cols="2">LR=0.01, Mixup=1.0</cell></row><row><cell></cell><cell></cell><cell>Pet37</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>300</cell><cell>82.75%</cell><cell>2 0.01</cell><cell>1e-05</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>1000</cell><cell>88.01%</cell><cell>5 0.01</cell><cell>0.001</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>3000</cell><cell>90.08%</cell><cell cols="2">10 0.003 0.0003</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>10 000</cell><cell>90.98%</cell><cell cols="2">2 0.001 0.0001</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>30 000</cell><cell>91.06%</cell><cell cols="2">2 0.003 1e-05</cell></row><row><cell></cell><cell>ResNet-152x2 teacher</cell><cell>-</cell><cell>91.03%</cell><cell>--</cell><cell>-</cell></row><row><cell></cell><cell>Best transfer ResNet50</cell><cell>10 000</cell><cell>88.20%</cell><cell cols="2">LR=0.001, Mixup=1.0</cell></row><row><cell></cell><cell>Best from-scratch ResNet50</cell><cell>10 000</cell><cell>74.24%</cell><cell cols="2">LR=0.01, Mixup=1.0</cell></row><row><cell></cell><cell></cell><cell>Food101</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>100</cell><cell>83.29%</cell><cell>10 0.01</cell><cell>0.001</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>1000</cell><cell>86.64%</cell><cell cols="2">10 0.001 0.0003</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>3000</cell><cell>87.20%</cell><cell>5 0.01</cell><cell>0.0001</cell></row><row><cell></cell><cell>ResNet-152x2 teacher</cell><cell>-</cell><cell>86.24%</cell><cell>--</cell><cell>-</cell></row><row><cell></cell><cell>Best transfer ResNet50</cell><cell>1000</cell><cell>85.05%</cell><cell cols="2">LR=0.001, Mixup=1.0</cell></row><row><cell></cell><cell>Best from-scratch ResNet50</cell><cell>1000</cell><cell>74.56%</cell><cell cols="2">LR=0.01, Mixup=1.0</cell></row><row><cell></cell><cell></cell><cell>Sun397</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>100</cell><cell>68.28%</cell><cell>10 0.01</cell><cell>0.001</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>1000</cell><cell>73.46%</cell><cell cols="2">10 0.003 0.0001</cell></row><row><cell></cell><cell>ResNet-50x1 student</cell><cell>3000</cell><cell>74.26%</cell><cell>10 0.01</cell><cell>3e-05</cell></row><row><cell></cell><cell>ResNet-152x2 teacher</cell><cell>-</cell><cell>74.22%</cell><cell>--</cell><cell>-</cell></row><row><cell></cell><cell>Best transfer ResNet50</cell><cell>1000</cell><cell>71.61%</cell><cell cols="2">LR=0.001, Mixup=1.0</cell></row><row><cell></cell><cell>Best from-scratch ResNet50</cell><cell>1000</cell><cell>60.63%</cell><cell cols="2">LR=0.01, Mixup=1.0</cell></row><row><cell>Dataset</cell><cell>train split</cell><cell cols="2">validation split</cell><cell></cell><cell>test split</cell></row><row><cell>Flowers102</cell><cell>train</cell><cell cols="2">validation</cell><cell></cell><cell>test</cell></row><row><cell>Pets37</cell><cell>train[:90%]</cell><cell cols="2">train[90%:]</cell><cell></cell><cell>test</cell></row><row><cell>Food101</cell><cell>train[:90%]</cell><cell cols="2">train[90%:]</cell><cell></cell><cell>test</cell></row><row><cell>Sun397</cell><cell>train</cell><cell cols="2">validation</cell><cell></cell><cell>test</cell></row><row><cell>ImageNet</cell><cell>train[:98%]</cell><cell cols="2">train[98%:]</cell><cell></cell><cell>validation</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Daniel Keysers and Frances Hubis for their valuable feedback on this paper; Ilya Tolstikhin and the Google Brain team at large for providing a supportive research environment.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Scalable second order optimization for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09018</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07579</idno>
		<title level="m">Revisiting resnets: Improved training and scaling strategies</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Big vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/big_vision,2022.1" />
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Javier Gonzalez</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03033</idno>
		<title level="m">What is the state of neural network pruning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the efficacy of knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07177</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automated multi-stage compression of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Gusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksym</forename><surname>Kholiavchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Ponomarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larisa</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Blagoveschensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Rethinking imagenet pre-training. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learing (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pruning versus clipping in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Janowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual international symposium on computer architecture</title>
		<meeting>the 44th annual international symposium on computer architecture</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Combining weakly and webly supervised learning for classifying food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parneet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08730</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fixed point quantization of deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darryl</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Talathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreekanth</forename><surname>Annapureddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learing (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debbie</forename><surname>Marr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Delving deeper into the whorl of flower segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weightedentropy-based quantization for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwhan</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5456" to="5464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10520</idno>
		<title level="m">Micro-batch training with batch-channel normalization and weight standardization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polytechnique</forename><surname>Montr?al</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Universit?</forename><surname>De Montr?al</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08453</idno>
		<title level="m">MEAL V2: Boosting vanilla Resnet-50 to 80%+ top-1 accuracy on ImageNet without tricks</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learing (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Eigendamage: Structured pruning in the kroneckerfactored eigenbasis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learing (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6566" to="6575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06460</idno>
		<title level="m">Emerging paradigms of neural network pruning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Integer quantization for deep learning inference: Principles and empirical evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Isaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09602</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
