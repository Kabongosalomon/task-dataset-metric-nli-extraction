<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Event-driven Dynamic Context for Accident Scene Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
						</author>
						<title level="a" type="main">Exploring Event-driven Dynamic Context for Accident Scene Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic segmentation</term>
					<term>scene understanding</term>
					<term>accident scenarios</term>
					<term>event-based vision</term>
					<term>intelligent vehicles</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The robustness of semantic segmentation on edge cases of traffic scene is a vital factor for the safety of intelligent transportation. However, most of the critical scenes of traffic accidents are extremely dynamic and previously unseen, which seriously harm the performance of semantic segmentation methods. In addition, the delay of the traditional camera during highspeed driving will further reduce the contextual information in the time dimension. Therefore, we propose to extract dynamic context from event-based data with a higher temporal resolution to enhance static RGB images, even for those from traffic accidents with motion blur, collisions, deformations, overturns, etc. Moreover, in order to evaluate the segmentation performance in traffic accidents, we provide a pixel-wise annotated accident dataset, namely DADA-seg, which contains a variety of critical scenarios from traffic accidents. Our experiments indicate that event-based data can provide complementary information to stabilize semantic segmentation under adverse conditions by preserving fine-grained motion of fast-moving foreground (crash objects) in accidents. Our approach achieves +8.2% performance gain on the proposed accident dataset, exceeding more than 20 state-of-the-art semantic segmentation methods. The proposal has been demonstrated to be consistently effective for models learned on multiple source databases including Cityscapes, KITTI-360, BDD, and ApolloScape.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE road safety of Intelligent Vehicles (IV) is being concurrently questioned and researched intensely along with the rapid development of self-driving systems. The general pipeline of self-driving systems can be divided into four sections including (a) the sensing module to observe the environment <ref type="bibr" target="#b0">[1]</ref>; (b) the perception module that uses captured data from the sensors to offer high-level information like recognition results about the surrounding scenes <ref type="bibr" target="#b1">[2]</ref>; (c) the path planning module which plans the motion of the vehicle based on the sensing and perception results <ref type="bibr" target="#b2">[3]</ref>; and (d) the control module that commands the vehicle to make certain actions <ref type="bibr" target="#b3">[4]</ref>. In recent decades, Intelligent Transportation Systems (ITS) have been growing at a rapid pace based on the development of intelligent perception technology, which serves as a vital and basic component of self-driving systems and the quality This work was supported in part by the AccessibleMaps Project by the Federal Ministry of Labor and Social Affairs (BMAS) under Grant 01KM151112, in part by the University of Excellence through the "KIT Future Fields" project, and in part by Hangzhou SurImage Company Ltd. (Corresponding author: <ref type="bibr">Kailun Yang)</ref>.</p><p>The authors are with the Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, 76131 Karlsruhe, Germany (e-mail: jiaming.zhang@kit.edu; kailun.yang@kit.edu; rainer.stiefelhagen@kit.edu).</p><p>Code and dataset will be made publicly available at: https://github.com/ jamycheung/ISSAFE of scene understanding will directly affect the decision-making process. In particular, computer vision models learned on realworld data not only enhance the performance of the perception component but also help to comprehensively understand the surrounding environments <ref type="bibr" target="#b4">[5]</ref>. Most recently, to improve road safety, some works applied vision-based sensing technologies to perform detection <ref type="bibr" target="#b5">[6]</ref>[7] <ref type="bibr" target="#b7">[8]</ref> and prediction <ref type="bibr" target="#b8">[9]</ref> of traffic accidents, as well as estimation of driver attention <ref type="bibr" target="#b9">[10]</ref>. In contrast, image semantic segmentation is generally regarded as an ideal solution for parsing complicated street scenes by providing a unified and dense image classification, containing object categories, shapes, and locations <ref type="bibr" target="#b10">[11]</ref>. In recent years, many state-of-the-art models <ref type="bibr" target="#b11">[12]</ref>[13] <ref type="bibr" target="#b13">[14]</ref>[15] <ref type="bibr" target="#b15">[16]</ref> have achieved impressive successes on major segmentation benchmarks <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b18">[19]</ref>. However, the research of applying semantic segmentation methods on traffic accident images is relatively rare. In addition, a direct transfer of trained models from common street-scene images to those containing traffic accidents will mostly result in a significant performance drop ( <ref type="figure">Fig. 1)</ref>. Thus, this work concentrates on exploring the robustness of semantic segmentation models in processing edge cases during driving, in particular, traffic accident images.</p><p>For this purpose, we create an alternative benchmark DADAseg based on the large-scale DADA-2000 database <ref type="bibr" target="#b9">[10]</ref> for a new task, i.e. accident scene segmentation. To facilitate this research as well as to supplement existing benchmarks <ref type="bibr" target="#b4">[5]</ref>[17] <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b18">[19]</ref> <ref type="bibr" target="#b19">[20]</ref>, we propose an associated dataset which is collected from real-world traffic accidents, involving highly dynamic scenarios and extremely adverse factors. Some cases are shown in <ref type="figure">Fig. 2</ref>, covering diverse situations: motion blur while the pedestrian is dashing across the road, overturning of the motorcyclist during the collision, back-lighting at the intersection, and the occlusions by windshield reflection. As far as known to us, these factors are still challenging for most existing segmentation algorithms and harmful for their performance. Note that the main objective of creating this benchmark is to provide a set of edge cases (critical and accidental) for testing the robustness of models before deployment. Though the direct prediction of traffic accidents is excluded in our scope, the robustness of the semantic segmentation model is a vital factor in IV systems.</p><p>Fusing sensors in an IV system can obtain various data modalities at the sensing stage <ref type="bibr" target="#b0">[1]</ref>. While LiDAR finely scans objects with laser light pulses in a certain range, the RADAR sensor uses radio waves to scan objects in a longer range. However, the LiDAR scans are inaccurate in severe weather that often leads to traffic accidents. RADAR is not affected by weather conditions but has a lower resolution and precision of measurement compared to LiDAR, especially for the perception of small objects. These sensors acquire visual data by discrete time stamps similar to traditional cameras. Therefore, visual information is quantified at a predefined frame rate, which results in the loss of information between adjacent frames <ref type="bibr" target="#b20">[21]</ref>. In addition, the price and size of LiDAR also hinder its wide deployment to IV systems. Unlike traditional framebased sensors, event cameras are bio-inspired novel sensors, such as the Dynamic Vision Sensor (DVS) <ref type="bibr" target="#b21">[22]</ref>, that encode changes of intensity at each pixel asynchronously. Thanks to the characteristics of (a) higher dynamic range (&gt; 120dB), (b) high time resolution (1M Hz clock or ?s timestamp), (c) lower power consumption, and (d) robustness against motion blur <ref type="bibr" target="#b22">[23]</ref>, event-based neuromorphic vision sensors constitute a novel paradigm for perceiving driving scenes. Apart from the higher dynamic range for enhancing overexposed or underexposed scenes, event cameras with higher time resolutions are more sensitive to capture motion information during dynamic driving, especially for fast-moving objects (foreground classes) in accident scenarios, where classic cameras delay between frames and lead to motion blur. Hence, dynamic context, as complementary information to the static RGB image, can be extracted from event-based data to overcome the drawbacks of intensity images. These benefits serve as the motivation for applying the emerging technique of event-based neuromorphic vision to autonomous driving in a sensor fusion manner.</p><p>Finally, as a preliminary exploration on the accident semantic segmentation task, we propose the Event-driven Dynamic Context Network (EDCNet) to capture dynamic context from complicated and volatile driving imagery. For adaptively aggregating the static RGB image and dynamic event data, our EDCNet features Event Attention Module (EAM) and Event Gate Module (EGM) on the lightweight event branch, which serves as the event-based fusion architecture of the multimodal model, as well as a domain bridge connecting the source (normal) and target (accident) data. In accordance with our EDCNet architecture, the robustness of semantic segmentation towards accident scenarios can be significantly improved, as shown in <ref type="figure">Fig. 1</ref>. We benchmark more than 20 representative accuracy-and efficiency-oriented architectures on our established accident scene segmentation dataset, demonstrating that the proposed EDCNet clearly gets ahead these methods. Furthermore, we have performed extensive experiments by using models learned on multiple source databases including Cityscapes <ref type="bibr" target="#b4">[5]</ref>, KITTI-360 <ref type="bibr" target="#b16">[17]</ref>, BDD <ref type="bibr" target="#b17">[18]</ref>, and ApolloScape <ref type="bibr" target="#b18">[19]</ref>, showing that our proposal is consistently effective for enhancing the reliability of semantic segmentation in accident scenes.</p><p>In summary, our main contributions are:</p><p>? We present a rarely addressed task, i.e. accident scene segmentation with an accompanying dataset DADA-seg, aiming to enhance road safety by improving the robustness of perception algorithms against abnormalities during highly dynamic driving. ? We propose EDCNet, integrated with EAM and EGM modules, for exploring event-driven dynamic context. According to the event-aware fusion, comprehensive comparisons and ablation studies are conducted between various datasets for analyzing the benefits and drawbacks of event-based data. ? We verify the EGM module in a cross-modal event-aware domain adaptation model and conduct a comparison with recent UDA methods. The domain adaptation experiment demonstrates that the foreground segmentation can benefit from the domain-consistent event-based data.</p><p>It is noteworthy that this work is the extension of our earlier conference paper <ref type="bibr" target="#b23">[24]</ref>. The extended contents include:</p><p>? The performance gap comparison is expanded to include more than 20 latest CNN-and Transformer-based models. ? The event data representation is extensively analyzed by adding an extra time bin setting and an ablation study on the event intensity in data preprocessing. ? A comprehensive analysis of the trade-off between accuracy, runtime, and model parameters is advanced. ? A new set of quantitative comparisons with recent domain adaptation methods is conducted. ? Other enhanced parts are related work reviews, more detailed qualitative analysis, and additional failure analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Traffic Accident Sensing</head><p>Accompanied by advancing ITS, some existing works tend to explore the edge cases of road driving, aiming to improve the sensing ability on accident scenarios. For identifying the dangerous or safe driving scenes, a binary classifier was boosted by a mixture of experts <ref type="bibr" target="#b6">[7]</ref>. To further subdivide the categories, the potential danger scene was distinguished by a ternary classifier <ref type="bibr" target="#b24">[25]</ref>. Compared with these tasks, we advocate the semantic segmentation task on traffic accident scenarios, which can provide a dense understanding of the driving scene by segmenting most involved objects.</p><p>For forecasting the upcoming driving situation in the near future, a few previous works focused on anticipating the traffic accident. Starting from the anticipation task definition <ref type="bibr" target="#b25">[26]</ref>, an adaptive loss function <ref type="bibr" target="#b26">[27]</ref> was proposed for improving the accuracy of accident anticipation. Different from their bounding box annotations with only four object categories, our proposed dataset has pixel-wise annotations of all 19 object classes. Bao et al. <ref type="bibr" target="#b27">[28]</ref> presented a deep reinforcement learning method with anticipation reward and driver fixation reward, which enabled accident prediction on dashcam videos like the DADA dataset <ref type="bibr" target="#b9">[10]</ref>. In contrast, we focus on robustifying semantic segmentation in accident scenes. Recently, a video dataset <ref type="bibr" target="#b7">[8]</ref> for causality in traffic accident scenes with semantic labels of cause and effect, and their temporal intervals, were introduced for the traffic video classification and localization task. For the video analysis tasks, the higher time resolution of the event data explored in this work has a large potential for more fine-grained anticipation of traffic accidents, as well as the temporal classification and localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantic Segmentation</head><p>Convolutional Neural Networks (CNNs) have dominated the visual understanding field. Since FCN <ref type="bibr" target="#b28">[29]</ref> used fully convolutional layers for pixel-wise prediction on images, a massive number of models <ref type="bibr" target="#b11">[12]</ref>[13] <ref type="bibr" target="#b13">[14]</ref>[15] <ref type="bibr" target="#b15">[16]</ref> have achieved remarkable performance on image semantic segmentation. Among these accuracy-oriented networks, PSPNet <ref type="bibr" target="#b11">[12]</ref> and DeepLabV3+ <ref type="bibr" target="#b12">[13]</ref> leveraged multi-scale feature representations. To harvest contextual information, long-range dependencies <ref type="bibr" target="#b13">[14]</ref>[15] were widely explored. DNL <ref type="bibr" target="#b15">[16]</ref> designed a disentangled non-local block to effectively capture global context. ResNeSt <ref type="bibr" target="#b29">[30]</ref> stacked Split-attention blocks in ResNet style, which greatly benefits downstream semantic segmentation systems. Recently, vision transformers have also been directly employed for dense image segmentation <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b31">[32]</ref>. Moreover, unifying semantic-and instance-specific segmentation, current panoptic segmentation provided by a single model like Panoptic FPN <ref type="bibr" target="#b32">[33]</ref> has achieved excellent results for its sub-tasks and upper-level navigation applications. In addition to high accuracy, other works, such as ERFNet <ref type="bibr" target="#b33">[34]</ref>, SwiftNet <ref type="bibr" target="#b34">[35]</ref>, and Fast-SCNN <ref type="bibr" target="#b35">[36]</ref>, proposed lightweight architectures to improve the efficiency. In specific, ERFNet leveraged early downsampling and filter factorization. SwiftNet designed ladderstyle upsampling, whereas Fast-SCNN followed a multibranch setup. Furthermore, compact classification networks like MobileNet <ref type="bibr" target="#b36">[37]</ref> were frequently applied to accelerate semantic segmentation. In the ITS field, sEnDec <ref type="bibr" target="#b37">[38]</ref> was proposed to achieve more delineated foreground segmentation even in adverse conditions like bad weather and night videos. An end-to-end multi-frame framework was designed in <ref type="bibr" target="#b38">[39]</ref> for moving object segmentation, which embeds edge-and motion features and has been examined on weather-degraded traffic scenes. In this work, we explore dynamic context by mining event-driven semantics like fast moving objects to boost the segmentation performance.</p><p>Most of the above mentioned methods are designed for normal conditions on major segmentation benchmarks <ref type="bibr" target="#b4">[5]</ref>[17] <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b18">[19]</ref>. In spite of the improved accuracy and efficiency, the learned understanding does not adequately generalize well to new scenes such as adverse weathers, which invites Domain Adaptation (DA) strategies, aiming to adapt segmentation models to previously unseen domains. Wang et al. <ref type="bibr" target="#b39">[40]</ref> proposed a differential treatment approach for stuff and thing classes when performing domain alignment. Chen et al. <ref type="bibr" target="#b40">[41]</ref> presented a semi-supervised learning method to make use of video sequence data for creating pseudo labels.</p><p>To list a few of DA methods for adverse conditions, the day-night conversions in <ref type="bibr" target="#b41">[42]</ref> and the adaptations between diverse weathers like foggy <ref type="bibr" target="#b42">[43]</ref> and rainy <ref type="bibr" target="#b43">[44]</ref> environments. However, apart from these natural conditions in real driving scenes, there are many uncontrollable factors in the interaction with other traffic participants. The core purpose of our work is to fill the gap of semantic segmentation in abnormal situations.</p><p>Any ambiguity in machine vision algorithms may cause fatal consequences in autonomous driving <ref type="bibr" target="#b44">[45]</ref>, thus robustness testing in diverse driving conditions is essential. For this reason, WildDash <ref type="bibr" target="#b19">[20]</ref> provided ten different hazards, such as blurs, underexposures or lens distortions, as well as negative test cases against the overreaction of segmentation algorithms. Fishyscapes <ref type="bibr" target="#b45">[46]</ref> evaluated pixel-wise uncertainty estimations towards the segmentation of anomalous objects. MSeg <ref type="bibr" target="#b46">[47]</ref> constructed a composite dataset with the aim of facilitating generalization for multi-domain semantic segmentation. Inspired by these works, we create a new dataset to extend the robustness test from ordinary to accident scenarios. In our DADA-seg dataset, most of the critical or accidental scenes are more difficult by having a large variety of adverse hazards.</p><p>On the other hand of improving robustness, some solutions constructed a multimodal segmentation model by fusing additional information, such as depth information in RFNet <ref type="bibr" target="#b47">[48]</ref>, thermal information in RTFNet <ref type="bibr" target="#b48">[49]</ref>, and optical flow in <ref type="bibr" target="#b49">[50]</ref>. Differing from these classic modalities, event-based data will be explored as a novel auxiliary modality. Specifically, we propose an attention-bridged architecture to extract dynamic context and thereby improve accident scene segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Event-based Vision</head><p>Event cameras, such as the Dynamic Vision Sensor (DVS) <ref type="bibr" target="#b21">[22]</ref> and Asynchronous Time-based Image Sensor (ATIS),are novel perceptual sensors inspired by biology. Its working principle is completely different from traditional frame-based cameras. Whereas the typical camera indistinguishably captures the global scene as an image at a fixed frame rate, the event camera is triggered by dynamic change of intensity at individual pixel in the scene. Compared to normal cameras, the event camera has some complementary characteristics, such as high dynamic range (140dB), no motion blur, and response in microseconds <ref type="bibr" target="#b22">[23]</ref>. Recently, different approaches based on event data have been proposed, such as 3D reconstruction and 6-DOF tracking <ref type="bibr" target="#b50">[51]</ref>, monocular depth prediction <ref type="bibr" target="#b51">[52]</ref>, optical flow estimation <ref type="bibr" target="#b52">[53]</ref>, as well as object detection and recognition <ref type="bibr" target="#b53">[54]</ref>. Event cameras asynchronously encode intensity changes at each pixel with position, time, and polarity: (x, y, t, p). For processing the sparse event data in a deep CNN, the original spatial-temporal event stream is often converted into an image form by remaining the higher time resolution, such as a two-channel event frame by Maqueda et al. <ref type="bibr" target="#b54">[55]</ref>, a four-dimensional grid <ref type="bibr" target="#b52">[53]</ref>, and a Discretized Event Volume (DEV) in x-y-t space by Zhu et al. <ref type="bibr" target="#b55">[56]</ref>.</p><p>According to the image-like transformation, Alonso et al. <ref type="bibr" target="#b56">[57]</ref> introduced a six-channels event representation and constructed a semantic segmentation model Ev-SegNet on an extended event dataset DDD17 <ref type="bibr" target="#b57">[58]</ref>, whose semantic labels are generated by a pre-trained model on Cityscapes and only contain 6 major categories. In contrast, our models are trained with the ground-truth labels of Cityscapes and perform semantic segmentation in all 19 object classes, so that the perception component can deliver a sufficiently dense and finegrained scene understanding result for upper-level assistance systems. Additionally, instead of stacking RGB images with the event frames in the input stage, the event data will be adaptively fused with the RGB image through the proposed attention mechanisms, which are more effective for combining the two heterogeneous modalities.</p><p>However, for event-based vision, labeled event data for semantic segmentation is scarce in the state of the art. Previous works leveraged the existing labeled data of images by simulating their corresponding event data. Rebecq et al. <ref type="bibr" target="#b58">[59]</ref> proposed ESIM to combine a rendering engine with an event simulator, which allows the event simulator to adaptively query visual frames based on the dynamics of the visual signal. In <ref type="bibr" target="#b59">[60]</ref>, event generation and frame upsampling methods were incorporated to recycle the existing video datasets for training networks designed for real event data. Recently, without rendering engine nor upsampling, EventGAN <ref type="bibr" target="#b60">[61]</ref> presented a self-supervised approach and constructed a GAN model to directly generate event frames from the associated images using only modern GPUs.</p><p>In order to guarantee the accuracy of training labels, we primarily select those datasets <ref type="bibr" target="#b4">[5]</ref>[17][18] <ref type="bibr" target="#b18">[19]</ref> with manual annotations as the source domain, which also maintain the category consistency with our proposed accident dataset DADAseg as the target domain. To extend the source and target datasets, we utilize the EventGAN model to generate their associated event data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we state the details of the accident semantic segmentation task and the relevant dataset, as well as our EDCNet architecture, attempting to tackle the performance drop of image semantic segmentation algorithms in accident scenes by capturing event-driven dynamic context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task Definition</head><p>To bring IV systems closer to real applications, we focus on accident scene segmentation. Besides, an associated evaluation set following the same labeling rules as Cityscapes <ref type="bibr" target="#b4">[5]</ref> is provided for quantitative comparison and analysis. All test cases are collected from real-world traffic accidents and most contain adverse factors. We explicitly study the robustness in challenging accident scenarios based on the assumption that the less performance degradation of the algorithm in this unseen dataset, the better its robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Accident Dataset: DADA-seg</head><p>Data Annotation. Our proposed dataset DADA-seg is selected from the DADA-2000 <ref type="bibr" target="#b9">[10]</ref> dataset, which was collected from various mainstream video sites, such as Youtube, Youku, Bilibili, iQiyi, Tencent, etc. It was proposed for driver attention prediction and consists of 2, 000 sequences with the resolution of 1, 584?660. However, it includes only the accident category labels and the attention maps, which are not used in this work. In order to extend to the accident semantic segmentation task, we performed additional laboratory preprocessing work based on all 2, 000 sequences in two stages. In the first stage, we cleaned the whole dataset by removing some sequences in the case with: (i) large watermarking; and (ii) low resolution. Meanwhile, most of the sequences with typical adverse factors were retained, such as those with motion blur, over/underexposure, weak illumination, occlusion, etc. Furthermore, concentrating on accident scenes, we modified the frame distribution of the original accident window, by remaining 10 frames before the accident and 30 frames during the accident. After our selection, a total of 313 sequences containing 12, 520 available frames were retained to constitute our final DADA-seg dataset.</p><p>In the second stage, we manually perform the pixel-wise annotation on every 11-th frame of each sequence by using the polygons to delineate the fine-grained boundaries for individual semantic objects. The object class definition of the annotation follows the labeling rule of Cityscapes. The annotation samples are shown in the t 1 frame in <ref type="figure">Fig. 2</ref>. Finally, the DADA-seg dataset includes 313 labeled images for quantitative analysis of accident semantic segmentation and 12, 207 unlabeled images for event-aware domain adaptation between the normal and abnormal imagery.</p><p>The distribution is described in <ref type="table" target="#tab_0">Table I</ref>. Compared with the Cityscapes dataset, all images from our dataset are taken in a broad range of regions by different cameras from various viewpoints, so as to maintain a larger diversity of driving scenes. Thus, the DADA-seg dataset covers the evaluation and verification of models which are trained on and transferred from other source datasets. Besides, all sequences are collected from traffic accident scenarios, comprising normal, critical, and accidental situations, which can be treated as edge cases of road-driving environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Event-based Data</head><p>Event Data Representation. Event cameras asynchronously encode an event at each individual pixel (x, y) at the corresponding triggering timestamp t, if the change of logarithmic intensity L(x, y, t) = Log(I(x, y, t)) in time variance ?t is greater than a preset threshold C. Eq. (1) denotes the condition of triggering an event at (x, y, t).</p><formula xml:id="formula_0">L(x, y, t) ? L(x, y, t ? ?t) ? pC, p ? {?1, +1} ,<label>(1)</label></formula><p>where polarity p indicates the positive or negative direction of change. As in Eq. (2), a typical volumetric representation of a continuous event stream with size N is a set of 4-tuples, where each event is represented as an independent tuple consisting of the position (x, y), the timestamp t, and the polarity p.</p><formula xml:id="formula_1">V = {e i } N i=1 , where e i = (x i , y i , t i , p i ).<label>(2)</label></formula><p>However, it is still arduous to transmit the asynchronous event spike to the convolutional network by retaining a sufficient time resolution. Hence, we perform a dimensionality reduction operation <ref type="bibr" target="#b60">[61]</ref> in the time dimension. After the ratio of the original timestamp is calculated by (t i ? t 1 )/(t N ? t 1 ), the new timestampt i of i-th event can be calculated via Eq. (3). Then, the original volume V of Eq. (2) is discretized with a fixed length for positive and negative events separately. Each event is locally linearly embedded to the nearest timeseries panel, similar to the bilinear interpolation. According to the preset number of positive time bin B + , a discretized spatial-temporal volume V + is represented as Eq. <ref type="formula" target="#formula_3">(4)</ref>.  <ref type="formula" target="#formula_3">4)</ref>, the high resolution temporal information of the event data can be maintained, which is a vital factor for extracting dynamic features. The detailed setting of time bins B will be discussed in the experiments section.</p><formula xml:id="formula_2">t i = (B + ? 1) (t i ? t 1 ) / (t N ? t 1 ) .<label>(3)</label></formula><formula xml:id="formula_3">V + (x, y,t i ) = B + i max 0, 1 ? t ?t i .<label>(4)</label></formula><p>Event Data Synthesis. Due to the lack of event-based labeled dataset for the semantic segmentation task, we consider synthesizing event data from RGB images that have manually labeled annotations, instead of generating insufficiently accurate pseudo labels based on grayscale images that have real event data. Hence, we utilize the EventGAN <ref type="bibr" target="#b60">[61]</ref> model that was trained from the MVSEC dataset <ref type="bibr" target="#b61">[62]</ref> including around 30-minute real event data and image sequences, to synthesize highly reliable event data on different datasets <ref type="bibr" target="#b4">[5]</ref>[17] <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b18">[19]</ref> for the semantic segmentation task. Different from the fixed frame rate (17Hz) in the Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref>, the sequence in our DADA-seg dataset was acquired with diverse cameras and frame rates, which indicates that its synthesized event data vary from the intensity of motion due to different time intervals. To discover a suitable intensity of simulated event data for DADA-seg, we verify different frame intervals in <ref type="figure" target="#fig_1">Fig. 3</ref>. After qualitative analysis and considering the benefit to obtain more event intensity (see <ref type="figure" target="#fig_1">Fig. 3b</ref>) meanwhile having less tailing issues (see <ref type="figure" target="#fig_1">Fig. 3c</ref>), we select the penultimate frame and stack with its anchor frame that has the semantic annotation, to form as the image pair for event data synthesis. Two cases of the simulated event data are visualized in <ref type="figure" target="#fig_2">Fig. 4</ref> under different temporal dimensions. From this, it can be easily noticed how event data benefits sensing in the dynamic driving scene with moving objects or in the low-lighting environment, meanwhile providing higher time resolution in the volumetric form. In order to evaluate our proposed EDCNet more comprehensively, thorough comparison experiments in different semantic segmentation datasets are conducted, which will be unfolded in the following section. Since the synthesis of event data requires associated image pairs (I t?1 , I t ), where I t is the anchor image with semantic annotation, the previous image with respect to the anchor image must be available in the dataset as the same time. Thus, two conditions should be met to select a dataset: (i) the anchor image has its semantic annotation for model training; and (ii) the image sequence is available.</p><p>Finally, in addition to the Cityscapes dataset with 2, 975 training and 500 validation images, we selected three datasets that meet the above conditions for our study, which are KITTI-360 <ref type="bibr" target="#b16">[17]</ref>, BDD <ref type="bibr" target="#b17">[18]</ref>, and ApolloScape <ref type="bibr" target="#b18">[19]</ref>. The ApolloScape and KITTI-360 datasets have semantic annotations for each frame of their video sequences. Therefore, we only sample one anchor image every 10 frames from the video sequence to prevent overfitting cases. After sampling, the KITTI-360 dataset has 5, 504 training and 612 validation images, while the ApolloScape dataset has 6, 056 training and 673 validation images. However, due to the limited amount of anchor images having annotations in the BDD dataset, only 3, 086 training and 343 validation images were filtered out based on the aforementioned two conditions, which is termed as BDD3K in our work. It should be noted that the category definition of the ApolloScape dataset is different from other datasets, so those models trained on it perform segmentation with only 16 overlapping categories <ref type="bibr" target="#b18">[19]</ref>. Based on the selected RGB image pairs from all datasets, the same method described above is utilized to synthesize the associated event data, and the procedure will be open-sourced to foster future research. Statistics of the datasets are listed in <ref type="table" target="#tab_0">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. EDC: Event-driven Dynamic Context</head><p>For capturing the event-driven dynamic context to enhance the feature extracted from static RGB image we propose the Event-driven Dynamic Context Network (EDCNet). The entire architecture of EDCNet is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Motivated by the high time resolution characteristic of event data, the sparsity of the original event data should be preserved in the process of integrating events, so as to obtain finer-grained motion features from event frames. But in turn, due to the sparsity and dynamics of event data compared to RGB images, the straightforward manners <ref type="bibr" target="#b52">[53]</ref>[55] <ref type="bibr" target="#b56">[57]</ref> by stacking them in the early stage are insufficient to aggregate those two heterogeneous modalities. Thereby, we construct Event Attention Module (EAM) and Event Gate Module (EGM) based on attention mechanisms to enhance the fusion process and apply them in different stages of our EDCNet to ensure that these modalities can be aligned effectively and integrated adaptively.</p><p>With respect to different data characteristics, we mainly explore two fusion modes, i.e. Sparse-to-Dense mode (S2D)</p><p>and Dense-to-Sparse mode (D2S). Correspondingly, the EAM module is applied to the EDCNet in the S2D mode, whereas the EGM module is applied in the D2S mode.</p><p>Event Attention Module. An intuitive sparse-to-dense process mode is capable of treating the sparse event data as input and extracting dense features. Inspired by the design of SwiftNet <ref type="bibr" target="#b34">[35]</ref> and RFNet <ref type="bibr" target="#b47">[48]</ref>, our initial EDCNet in the S2D mode with EAM module extends the multimodal architecture with more sparse event data and an adapted EAM module. In our model, the original fusion mechanism is modified from dense depth information to sparse event-based data, so as to support the event-aware fusion in the proposed S2D mode. As shown in <ref type="figure" target="#fig_4">Fig. 5a</ref>, the EAM module is constructed with channel-wise attention block <ref type="bibr" target="#b62">[63]</ref> for feature selection, which can harvest motion features from the original event data. In the S2D design, while the four residual blocks in the RGB branch gradually extract higher-level features at smaller resolutions with {4, 8, 16, 32} downsampling rates and {64, 128, 256, 512} channels, the accompanying Event branch also processes event data in the same manner. The EAM module is applied between each two blocks corresponding to the RGB and Event branches, so as to adaptively extract motion-related features from dynamic event data. Given the image feature map termed as F i ? R C?H?W and the event feature map as F e ? R? ?H?W , EAM module adaptively merges them to form a new feature map termed as F EAM ? R C?H?W . The calculation inside EAM module is represented as Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_4">F EAM = F i ? ? i [f (F i )] + F e ? ? e [g(F e )] ,<label>(5)</label></formula><p>where both f (?) and g(?) are composed of adaptive global pooling and 1?1 convolution operations, the ? i (?) and ? e (?) denote Sigmoid activate function for the image and event feature map. Benefiting from the channel-wise attended EAM module, the event data with higher time resolution compensate for dynamic context and provide motion-related features insufficient in static images, so as to improve the representational capacity of the merged feature map F EAM . Additionally, its high dynamic range enhances over-/underexposure images. After four residual blocks, the merged feature with a shape of 512 ? H m ? W m will be the straightway input to the Spatial Pyramid Pooling (SPP) module <ref type="bibr" target="#b11">[12]</ref> for capturing multi-scale features, where the m is the downsampling rate. In this S2D mode, we build the SPP module via multi-scale grids in three different resolutions of (h, w) ? { <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b15">16)</ref>, <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b7">8)</ref>, (2, 4)}. These grid features will be uniformly interpolated to the same original resolution, and be concatenated as a new feature map, as shown in the right side of <ref type="figure" target="#fig_4">Fig. 5</ref>. Afterwards, a lightweight decoder, composing of three upsampling modules with blend convolution operations, will perform element-wise addition to fuse the upsampled feature map with 1?1 skip connections from the RGB branch, so as to align different levels of feature maps for the final prediction.   Event Gate Module. Aside from the intuitive S2D mode, inspired by the video restoration from a single blurred image and the event data like <ref type="bibr" target="#b63">[64]</ref>[65], we alternatively leverage the dense-to-sparse (D2S) mode, aiming to capture sparse event information and dynamic context. In the D2S design, the event branch can firstly benefit from the identical dense feature dividing from the RGB branch. Besides, through the supervised learning with the ground-truth event data, the event branch can further guarantee the effectiveness of the dynamic context. For constructing our EDCNet in the D2S mode, we perform three major modifications compared to the previous S2D mode, as illustrated in <ref type="figure" target="#fig_4">Fig. 5b</ref>. Firstly, the standard event residual layers with multiple convolution blocks are replaced by a single block, since such a shallow event branch is more effective to extract features from dense to sparse and it is capable of processing the event feature at a higher spatial resolution without overloading the computation demand. In detail, after the 7?7 stem block of ResNet-18 <ref type="bibr" target="#b65">[66]</ref>, while the RGB branch encodes higher-level features at smaller resolutions with {4, 8, 16, 32} downsampling rates and {64, 128, 256, 512} channels, the event branch gradually shallows event channels in the order of {64, 32, 16, 8}, which also enables event processing at a full resolution. Secondly, we propose a novel gating mechanism named Event Gate Module (EGM), which is capable of aggregating the dense RGB feature map F i ? R C?H?W and the sparse event feature map F e ? R? ?H?W , and forming a new event-aware feature map F EGM ? R? ?H?W . As denoted in Eq. <ref type="formula" target="#formula_5">(6)</ref>, the image and event feature maps are fused by the internal gating process of the EGM module.</p><formula xml:id="formula_5">F EGM = F e ? ? [c(F e , g(F i ))] + F e ,<label>(6)</label></formula><p>where g(?) is a 1?1 convolution, ?(?) and c(?) denote Sigmoid activate function and concatenation operations, respectively. Based on the carefully designed EGM module, we enforce the event branch to deactivate the non-event features according to the higher-level semantic features from the RGB branch. Last but not least, we build the Event-aware Spatial Pyramid Pooling (E-SPP) block by adding an event stream in the aforementioned SPP module. This crucial modification of the context extraction module (E-SPP) allows our EDCNet to integrate significant dynamic context and multi-scale features in high-level dimensions. Due to the two-polarity representation of event data, we select the standard Binary Cross Entropy (BCE) to form the event loss function L event , while the Cross Entropy (CE) is selected for semantic segmentation. The eventrelated supervision will be performed on the event branch before the E-SPP module. Aiming to learn the whole model in an end-to-end fashion, the final loss function is constructed as in Eq. <ref type="formula" target="#formula_6">(7)</ref>, in which the event loss function L event is added to the segmentation loss function L seg . L = L event (e,?) + L seg (y,?),</p><p>where e,?, y and? are the ground-truth and the predicted event, segmentation ground truth and prediction, respectively.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. EDA: Event-aware Domain Adaptation</head><p>Initially, due to the scarcity of training data concerning accidental driving scene, we tend to apply Unsupervised Domain Adaptation (UDA) to perform the transfer learning from normal to abnormal dataset. For exploring potential from the large number of unlabeled samples in our dataset, we investigate the UDA strategy in different aspects, i.e. image level <ref type="bibr" target="#b66">[67]</ref> and/or feature level <ref type="bibr" target="#b67">[68]</ref>. More importantly, compared to textured RGB images, the monochromatic event data, capturing only changes of intensity, is semantically more consistent in both domains. It denotes that the homogeneous event features can serve as a bridge to assist the RGB modal domain adaptation in the feature level as well. Based on this assumption, as shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, we construct our EDCNet in an Event-aware Domain Adaptation (EDA) strategy with two branches, where the lightweight event branch is the same as the aforementioned D2S mode and the RGB branch is created by the ResNet-101 backbone <ref type="bibr" target="#b65">[66]</ref> referring to the CLAN model <ref type="bibr" target="#b67">[68]</ref>. Summarily, in the EDA strategy, we aim to boost the performance of EDCNet by using: (i) unlabeled accident RGB images, (ii) monochromatic event data of two domains, and (iii) a larger backbone. Up to our knowledge, we are making an early attempt to perform cross-modal unsupervised domain adaptation from normal to abnormal driving scenes between two heterogeneous modalities. In order to maintain the consistency of ground-truth labels of both branches, in the corresponding experiment, we mainly discuss the D2S mode, from which the original event data is applied as supervision signals instead of as inputs.</p><p>To distinguish and investigate the impact of diverse domain adaptation strategies, we utilize the CycleGAN model <ref type="bibr" target="#b66">[67]</ref> to translate style of images from Cityscapes to DADA-seg and perform image-level adaptation between the two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, comprehensive experiments in different settings and implementation details are presented. Initially, performance gaps of various semantic segmentation models are investigated. Afterwards, comprehensive experiments verify the effectiveness of the proposed EDCNet in reducing the performance gap. The analysis of the event-aware domain adaptation (EDA) strategy is also conducted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance Gap</head><p>To quantitatively evaluate the robustness of semantic segmentation algorithms, existing accuracy-and efficiencyoriented models are tested on the target dataset, as shown in <ref type="table" target="#tab_0">Table III</ref> and visualized in <ref type="figure">Fig. 1. For a</ref>   <ref type="bibr" target="#b32">[33]</ref> gain high accuracy in the source domain (Cityscapes), they heavily depend on the consistency between training and testing data, which are all normal scenes. It thus hinders their generalization ability and leads to a large performance degradation once taken to the abnormal scenes. The performance degradation of networks on this accident dataset are around 50% ? 60%, of which the 63.6% decline of DNL <ref type="bibr" target="#b15">[16]</ref> is the most serious, from 79.3% to 15.7%. Vision transformer-based methods includ- ing SETR <ref type="bibr" target="#b30">[31]</ref> and SegFormer <ref type="bibr" target="#b31">[32]</ref> designed specifically for semantic segmentation have the capacity to gather longrange context from early layers, but also suffer from huge performance drops in accident scenes. One noticeable insight is that Fast-SCNN <ref type="bibr" target="#b35">[36]</ref> has the highest mIoU among the CNN-based methods on DADA-seg with 26.3% despite only having a score of 69.1% in Cityscapes. Overall, the large gap shows that semantic segmentation in accident scenarios is an extremely challenging task for these top-performance models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head><p>For efficiency reasons, we choose ResNet-18 <ref type="bibr" target="#b65">[66]</ref> as the backbone and the main architecture from SwiftNet <ref type="bibr" target="#b34">[35]</ref>, which is also selected as the baseline model in this work. All models in this subsection are constructed with the encoder-decoder structure and implemented on two GTX1080Ti GPUs with CUDA 11.0, CUDNN 8.0.4 and PyTorch 1.7.0. These models are trained with the Adam optimizer <ref type="bibr" target="#b68">[69]</ref> with a learning rate initialized to 4?10 ?4 . The cosine annealing learning rate scheduling strategy <ref type="bibr" target="#b69">[70]</ref> is used to dynamically adjust the learning rate during the training process. The minimum learning rate of the last epoch is fixed as 1?10 ?6 . The weight decay of the learning rate is set to 1?10 ?4 . We use the weights of the ResNet-18 model pre-trained on ImageNet <ref type="bibr" target="#b70">[71]</ref> to initialize the RGB encoder, and use the Kaiming initialization method <ref type="bibr" target="#b71">[72]</ref> to initialize the whole Event branch as well as the decoder. For parameters of the pre-trained RGB encoder, we update them with a 4? smaller learning rate than the initialized parameters of the event encoder and the decoder, and at the same time apply a 4? smaller weight decay of the learning rate. The data augmentation operation includes a random scaling factor between 0.5 and 2, random horizontal flipping, and random cropping with an output resolution of 1024?512. In the end, we trained the model for 200 epochs with a batch size of 2 per GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on EDC</head><p>Quantitative Analysis. To verify the benefits of extracting dynamic context from event-driven data, we perform the ablation study in EDCNet of two different modes (EAM and EGM) and four various event time bins B={1, 2, 10, 18}. As shown in <ref type="table" target="#tab_0">Table IV</ref>, we train the SwiftNet with RGB only from scratch as a baseline, which obtains 20.1% mIoU in the target domain. Starting exploring event data with event-only SwiftNet, where the event data are processed alone from sparse to dense without RGB images, the time bin settings larger than 1 can achieve a similar performance as the baseline, and the setting of B=10 attains the mIoU of 38.4% in the source domain and 21.7% in the target domain. This indicates that the event data has certain interpretability for the segmentation of driving scenes. Although the accuracy of the Event-only model in the source domain is lower than the RGB-only model, the accuracy in the target domain is close or slightly better. According to such smaller domain gaps, the event data is more domainconsistent than RGB images. This is another insight leading us to develop the Event-aware Domain Adaptation method, in which the domain-consistent event data is processed as auxiliary information with RGB images to further bridge and align two domains. Benefiting from the EAM module fusing RGB images with event data, our EDCNet (EAM) obtains an mIoU improvement of +2.9% in the target domain based on the setting of B=2. At the same time, it maintains a high performance in the source domain.</p><p>Regarding various time bin settings, one reason for the performance drop in the case of B=1 is that all events embedded in a single time bin will cause a problem of dragging behind images of moving objects and then negatively impact RGB features. However, in higher time bins, such as 10 and 18, the events generated in a short interval are dispersed to more bins, which then it leads to insufficient events in a single bin. Therefore, we verify the moderate event representation (B=2) as an effective time bin setting of our EDCNet to perform RGB-Event fusion, because others are either redundant or too sparse for the RGB image.</p><p>Likewise, we conduct the second group of experiments on EDCNet based on EGM in <ref type="table" target="#tab_0">Table IV</ref>. Benefiting from the gating mechanism of the EGM module in the D2S extraction way, the EDCNet in 4 different time bins performs more robustly than the baseline and generally achieves a considerable improvement. Especially, EDCNet in B=2 brings over an +8.2% gain in mIoU in the target domain when compared with the baseline, which is in line with the aforementioned time bin setting. This EDCNet obtains a 28.3% mIoU score  on the accident dataset and it outperforms more than 20 representative accuracy-and efficiency-oriented CNN models listed in <ref type="table" target="#tab_0">Table III</ref>. We leave the Transformer methods in our future work of Transformer-based domain adaptation.</p><p>To summarize briefly, the input from the two data domains are obviously complementary as: (i) When event cameras will not be triggered in static scenes, conventional cameras can perfectly capture the entire scene and provide sufficient textures. (ii) When RGB cameras puzzle over adverse scenes, i.e. fast-moving objects or low lighting environments, the event data can provide auxiliary information, which is particularly important for the segmentation of accident scenes.</p><p>Ablation Study of Event Intensity. Since the intensity of the accumulated event in the resulting Image Warped Events (IWE) affects the score of the reward functions and the performance of downstream event-based applications <ref type="bibr" target="#b72">[73]</ref>, we conduct an ablation study on event intensity (EI) during the preprocessing of the synthesized event-based data. While the default setting of EI within a same time range in Event-GAN <ref type="bibr" target="#b60">[61]</ref> is set as 0.1, we modify it to different settings with EI?{0.1, 0.3, 0.5, 0.7, 1.0}. The event-based data in EI=1.0 has the largest intensity. The visualizations of event-based data in various settings are displayed in <ref type="figure" target="#fig_7">Fig. 7</ref>. Each model is based on the aforementioned D2S event fusion mode, and the event time bin is set as B=2. The models are trained on a single GPU with a batch size of 2 and tested on the DADA-seg dataset. As the results shown in <ref type="table" target="#tab_8">Table V</ref>, EDCNet has a further improvement, elevating mIoU to 21.98%, when EI increases from 0.1 to 0.3. However, if the intensity is too high or reaches the maximum (EI=1.0), the performance will decrease. One reason is that the differentiation of event data in the temporal dimension is limited in a higher EI setting. For instance, when EI=0.7, most of the event pixels in the accumulated IWE will reach the maximum value (yellow pixels in <ref type="figure" target="#fig_7">Fig. 7)</ref>, which leads to excessive homogeneity, thereby losing dynamic information of event data.</p><p>Qualitative Analysis. The comparison of accident scenes segmentation in <ref type="figure" target="#fig_8">Fig. 8</ref> demonstrates that our EDCNet performs significantly better by fusing events and RGB images in those challenging situations. The region of interest in each row is hightlighted in light-blue dashed boxes. As shown in the first four rows of <ref type="figure" target="#fig_8">Fig. 8, our</ref> EDCNet concentrates on the motion information, especially the foreground objects, while the Event-only and RGB-only SwiftNet are both confused with the object categories, such as motorcycle and motorcyclist in the first row, car and truck in the second row, etc. In the fifth row, our model improves the segmentation at the nighttime scene. However, segmentation of night scenes is still challenging, although our method greatly benefits from event data, in contrast to the baseline. In the last row, a case of the initial accident scene is presented, where an overturned car is lying on the road after collision with the fence, where our approach also clearly performs more robustly than the baseline and other compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Efficiency Analysis</head><p>To maintain the efficiency of model similar as Swift-Net <ref type="bibr" target="#b34">[35]</ref>, all of our EDC models are constructed based on the lightweight backbone ResNet-18 <ref type="bibr" target="#b65">[66]</ref>. In order to compare the running time of all models uniformly and fairly, we performed calculations with 100 cases based on the same setting on a workstation with an RTX2070 GPU, and finally listed the average runtime in milliseconds (ms) and the corresponding standard deviation (??ms). As shown in the Runtime column of <ref type="table" target="#tab_0">Table IV</ref>, although our EDC model in the S2D mode contains two completely symmetrical RGB and Event branches, it can still maintain a sufficient real-time speed and only increases the running time by 6ms in general, compared to the original SwiftNet which only has a single branch. However, the design of EDC model in the D2S mode aims to obtain more representative events from dense RGB images, and thus the feature maps in the Event branch are operated at the fullresolution scale. Compared with S2D, the runtime increases to around 50ms, but the number of parameters (see Params column in <ref type="table" target="#tab_0">Table IV</ref>) of D2S is much less than that of S2D, which is almost equal to the single-branch SwiftNet model. In <ref type="figure" target="#fig_9">Fig. 9</ref>, we visualize a comparison between our EDC-Net and existing representative semantic segmentation networks <ref type="bibr" target="#b12">[13]</ref> <ref type="bibr" target="#b13">[14]</ref>[35] <ref type="bibr" target="#b36">[37]</ref>, regarding the trade-offs of runtime (ms), model accuracy (mIoU), and parameters (M ). First, our EDCNet in both modes (i.e. EAM and EGM) has few parameters (around 15M ). EDCNet in EAM mode has similar runtime (around 20ms) compared to the efficiencyoriented models, such as SwiftNet <ref type="bibr" target="#b34">[35]</ref> and MobileNet <ref type="bibr" target="#b36">[37]</ref>, but its accuracy in DADA-seg dataset clearly outperforms theirs. Within the 60ms runtime range, EDCNet in EGM mode achieves much higher performance with a similar or a smaller number of parameters, compared with DeepLabV3+(R18) <ref type="bibr" target="#b12">[13]</ref> and OCRNet(HR48) <ref type="bibr" target="#b13">[14]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Benchmarking and Comparison with the State-of-the-Art</head><p>On Diverse Datasets. For extensive verification of the event-driven dynamic context, we conduct comparisons of our EDCNet with the RGB-only SwiftNet on other datasets, such as ApolloScape <ref type="bibr" target="#b18">[19]</ref>, KITTI-360 <ref type="bibr" target="#b16">[17]</ref>, and BDD3K <ref type="bibr" target="#b17">[18]</ref>. For the event data synthesis on all these datasets, we perform the aforementioned selection process. The contrastive results are shown in <ref type="table" target="#tab_0">Table VI</ref>. The Merge3 dataset is merged from the Cityscapes, KITTI-360, and BDD3K datasets, because these three datasets have the same labels with all 19 classes. In general, our EDCNet is capable of improving the segmentation robustness by extracting and integrating dynamic context from the event-based data. Due to the adverse driving scenes included in the BDD3K dataset, both SwiftNet and EDCNet  <ref type="table" target="#tab_0">Table I</ref>) to cover the evaluation of models trained from different source domains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experiments on EDA</head><p>As mentioned before, apart from the event-aware domain adaptation, we aim to boost the robustness of semantic segmentation by performing different Unsupervised Domain Adaptation (UDA) strategies on the large-scale unlabeled data of our DADA-seg dataset. Additionally, this ablation study includes the comparison between fusing the event-based data and the dense optical flow. To compare diverse strategies, based on the recent model CLAN <ref type="bibr" target="#b67">[68]</ref>, our EDCNet method is performed on two different levels, i.e. feature and/or image level. For an extensive quantitative analysis, we have adopted three different metrics <ref type="bibr" target="#b28">[29]</ref>, namely pixel accuracy (Acc), mean intersection over union (mIoU), and frequency weighted intersection over union (fwIoU), as shown in <ref type="table" target="#tab_0">Table VII</ref>.</p><p>Quantitative Analysis. Firstly, the CLAN model was adapted from the virtual to the real domain, i.e., from the GTA5 <ref type="bibr" target="#b73">[74]</ref> to the Cityscapes dataset. This pre-trained model is tested directly on our DADA-seg dataset without any adjustments, also named source-only CLAN. It gains the mIoU of 16.8% with a 1024?512 resolution and 19.4% with a 512?256 resolution, respectively. Note that here a smaller resolution input results in a higher accuracy in the target domain. There are two main reasons: (i) images of DADAseg are originally with a low resolution, and (ii) a smaller resolution can obtain a larger receptive field with wider context understanding, which indicates that correct classification is more critical in accident scenes than delineating the boundaries. Afterwards, we train the CLAN model from scratch by adapting from Cityscapes to DADA-seg to verify the featurelevel and feature-image-level domain adaptation, whose are termed as f and f + i shortly in Level column of Table VII, whereas the latter obtained a higher mIoU of 64.8% in the source domain. Finally, based on the CLAN model design, our EDCNet in the EDA strategy obtains the highest performance on all three metrics in the DADA-seg dataset, and achieves the top accuracy of 30.0% in mIoU, 42.1% in Acc, and 64.5% in fwIoU at the resolution with 1024?512. In order to understand the impact of event fusion, we list the per-class IoU results of all 10 foreground classes in <ref type="table" target="#tab_0">Table VII</ref>. This demonstrates that the foreground classes indeed benefit more from event data, which is consistent with our assumptions.</p><p>Comparison with Optical Flow. In order to compare the dynamic context from other data modalities, we replace the  event-based data with Dense Optical Flow (DOF) simulated by the Farneback function <ref type="bibr" target="#b74">[75]</ref>. For a fair comparison based on the same sparsity of data, we only utilize the traditional method to generate optical flow data. Under the same architecture and setting of our EDCNet, the EDCNet-DOF model also obtains accuracy improvements. Nevertheless, our EDCNet-Event model performs better in the foreground classes. Although both data are synthesized, motion features with higher time resolution can be extracted from event-based data to boost the foreground segmentation. Besides, compared to optical flow, event cameras actively capture the intensity change and also have a high dynamic range to enhance perception in low-light conditions, which better conforms with our EDCNet design for improving driving safety by fusing event-driven dynamic context.   recommended by the referenced works, to perform domain adaptation from Cityscapes to DADA-seg. Compared with the baseline model CLAN, our model achieves a gain of 1.21% by fusing event data, which reveals our aforementioned insight, the domain-consistency of event-based data. The EDCNet obtains respective 0.29%, 5.52%, and 3.12% gain in mIoU, compared with BDL <ref type="bibr" target="#b75">[76]</ref>, FDA <ref type="bibr" target="#b76">[77]</ref>, and SIM <ref type="bibr" target="#b39">[40]</ref>. Compared to the baseline, in addition to the mIoU improvement, it shows much higher accuracy in foreground objects (such as car, truck, bus, and bicycle). This result is consistent with our assumption that event data are beneficial for foreground segmentation in domain adaptation.</p><p>Qualitative Analysis. A qualitative comparison of semantic segmentation results between the RGB-only SwiftNet, EDCNet-DOF, and EDCNet-Event is shown in <ref type="figure" target="#fig_11">Fig. 10</ref>. These three samples are selected based on complicated driving accident scenes, where images from left to right cover the motionblur condition, the night-time, and the initial accident ahead. The adverse foreground objects are highlighted via yellow dashed boxes in <ref type="figure" target="#fig_11">Fig. 10</ref>. In the motion-blur scenario, when the RGB-only baseline is confused between the foregrounds and backgrounds, our EDCNet accurately segments the foreground object (i.e. the blurred car). Apart from blurred scenes, our EDCNet obtains better performance in the night scene where the RGB-only SwiftNet almost fails. Likewise, an incident car lying horizontally in an initial accident ahead is precisely segmented by our EDCNet. Additionally, <ref type="figure" target="#fig_12">Fig. 11</ref> shows more segmentation results of our EDCNet-Event model on DADA-seg in a sequence manner. From sequence (a) to sequence (e), our model can handle the motion blur, overturns, object occlusions caused by car-pedestrian, car-bus, or car-car collisions. The segmentation sequences of three accident cases in different lighting conditions (under-exposure, over-exposure, and nighttime) reveal that our model can perform semantic segmentation robustly against various adverse situations. These indicate that our EDCNet model significantly stabilizes the segmentation in normal and abnormal scenes by fusing event data, especially for the blurred foreground objects during traffic accidents. the ground truth. "mcycle" is the motorcycle class for short. Zoom in for a better view.</p><p>Failure Analysis. In order to comprehensively analyze the performance of our EDCNet on different cases, some failure segmentation examples are visualized in <ref type="figure">Fig. 12</ref>. In the first row, the segmentation of the pedestrian at a long distance failed, but the model can still segment the rider at a midrange distance with an IoU of 34.7%. In the second failed case, the static sidewalk as the background cannot be segmented in the low-light environment, but the model can still obtain the dynamic context of the moving car via fusing the event data. In the last row, as the motorcycle collided with the car and was overturned, the model was confused and failed in segmenting this motorcycle, but the bicycle next to it can be segmented with an IoU of 43.8%. It is worth noting that segmenting traffic accident scenes in the real world is still tough. To obtain a better perception of highly dynamic driving scenes, panoramic cameras can be used to obtain a wider field of view (i.e., 360 ? ) to perceive pedestrians or cars dashing from the side of ego-vehicle in advance. In addition, fusing event data in transformer-based models that are capable of capturing longdistance context is a potential solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this work, we present the accident semantic segmentation task, aiming to enhance the road safety by perceiving traffic accident scenes during driving. To this end, we build its relevant evaluation dataset DADA-seg with pixel-wise annotations, which serves as a benchmark to assess the robustness and applicability of semantic segmentation algorithms for IV systems. As an initial solution to segment those accidental scenarios, we mainly explore event-driven dynamic context from the generated event data and investigate the possibility of applying event cameras in future vehicles. As a new sensing module, the event camera can be fused with other sensors, such as a normal camera discussed in this work. Imported as a novel perception module in the general pipeline of autonomous vehicles, our proposed semantic segmentation methods mainly focus on improving the perception of the abnormality happened in a traffic accident, so as to provide better scene understanding information for subsequent path planning and control modules. Based on the characteristic of event data, we construct the multimodal segmentation model EDCNet by fusing the RGB with event-based data through different modes and attention mechanisms. Our extensive experiments on various datasets show that dynamic context captured from event data is capable of complementing the RGB image features with temporal information. We further boost the performance by merging diverse datasets and using the EDA strategy. Thus, under extreme driving situations, such as in scenes with motion blur and low illuminations, event cameras are helpful to better perceive the environment and to improve image analysis. Even though our experiments are somewhat limited by the use of synthetic events due to the lack of corresponding event data in common annotated datasets, we have observed consistent and large accuracy gains.</p><p>Eventually, the analysis of traffic accident scenarios is still scarce, especially the dense environment understanding i.e. semantic segmentation. On the one hand, it is still lacking a large-scale dataset with pixel-wise annotation. On the other hand, the traffic accident containing various adverse factors and challenging situations all belong to edge cases of driving scenarios. Thus, our current segmentation performance on those accident scene still has large development space. For this purpose, the large-scale unlabeled data in the DADA-seg dataset may be explored through other learning paradigms, such as contrastive learning. According to the failure analysis, 360 ? cameras and fusing event data in transformerbased model would be alternative techniques to gain robust segmentation, which we leave in our future work. Moreover, an equally intriguing possibility is the accident prediction based on the combination of video semantic segmentation and event regression algorithms, which is a potential approach to avoid traffic hazards and further ensure road traffic safety.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 : 1 Fig. 2 :</head><label>112</label><figDesc>Segmentation accuracy in mIoU(%) across Cityscapes validation and DADA-seg testing datasets. The red star denotes the accuracy of our proposed EDCNet. It achieves +8.2% gain compared to SwiftNet* which is trained from scratch under the same setting. The mark s represents the corresponding small version of the network. Accident sequences from the proposed DADA-seg dataset include diverse hazards (e.g., motion blur, overturns, back light, object occlusions). From top to bottom: event data of the t1 frame, and four frames before and during an accident, where the t1 frame is the ground-truth segmentation for quantitative evaluation, and the others are predictions of our EDCNet model. Zoom in for a better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison of event generation on DADA-seg dataset. Given the same anchor frame at time t, the Eventt?2 generated through the frame pair (t ? 2, t) contains more details than the Eventt?1by (t ? 1, t), especially for the moving foregrounds highlighted by yellow dashed boxes. Zoom in for a better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Visualization of generated event data in B ? H ? W space, where B, H and W denote the time bins, image height and width. From left to right are RGB image pair (It?1, It), different event representations according the time bins number: event volume with higher time resolutions, event polarity frame and event grayscale frame, where blue and red colors indicate positive and negative events, respectively. After the dimensionality reduction for discretization of the respective positive and negative time bins, both positive and negative sub-volumes are concatenated along the time dimension to construct the whole volume V ? R B?W ?H , where B = B + + B ? , and B is the total number of time bins, while W and H are the width and height of spatial resolution, respectively. Based on the reconstructed event volume in Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>RGB</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Model architectures of two different event fusion strategies. In (a) S2D mode with EAM module, event data are fused to RGB branch adaptively from sparse to dense, while in (b) D2S mode with EGM module, event data are extracted from dense image and learned from the sparse ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>RL</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Architecture of EDCNet with the cross-modal event-aware domain adaptation strategy. The Event branch is constructed by the EGM module in the D2S fusion mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Visualization of various settings of event intensity (EI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Contrastive examples between the Event-only SwiftNet, RGB-only SwiftNet and our EDCNet. From top to bottom are different accident scenarios: motorcyclist collision, truck and car dashing, windshield occlusion, car collision at night time, and an initial accident with an overturned car. The abnormalities are highlighted in light-blue boxes. Zoom in for a better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Illustration of trade-off between accuracy, runtime, and parameters across different models. Runtime (ms) and mIoU accuracy (%) on DADA-seg dataset are depicted in x-and y-axis, while the size of the blue dot represents the number of parameter (M ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>Comparison of segmentation results between the RGBonly baseline, EDCNet-DOF, and EDCNet-Event on DADA-seg dataset. These accident scenes are more complicated by involving motion blur, night-time condition, and initial accidents during driving, respectively. The fast moving or adverse foregrounds are highlighted by yellow dashed boxes. Zoom in for a better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :</head><label>11</label><figDesc>(a) blurred pedestrians (b) overturned pedestrians (c) occluded bicyclists (h) cars in night-time scenes (g) cars in over-exposure scenes (d) occluded buses More semantic segmentation results with the respective sequence during a traffic accident of the DADA-seg dataset. The columns correspond to the input images and output predictions of our EDCNet. The objects involved in the collision from (a) to (e) include pedestrians, bicyclists, cars, and buses. Accident scenes from (f) to (h) include under-exposure, over-exposure and night-time conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>8 Fig. 12 :</head><label>812</label><figDesc>Failure analysis of segmentation results. From left to right are the input image, the segmentation result of EDCNet, the ground truth (with zoom-in region), and the difference map (with zoom-in region). The difference map is generated by subtracting the true positive pixels from the segmentation result w.r.t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Distribution of total 313 sequences from DADA-seg dataset under conditions in terms of light, weather, and occasion.</figDesc><table><row><cell>DADA-seg</cell><cell>Light</cell><cell>Weather</cell><cell></cell><cell>Occasion</cell><cell></cell></row><row><cell></cell><cell>day night</cell><cell>sunny rainy</cell><cell cols="3">highway urban rural tunnel</cell></row><row><cell>#sequence</cell><cell>285 28</cell><cell>297 16</cell><cell>32</cell><cell>241 38</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Statistics of datasets after event data synthesis. Merge3 dataset is combined with the Cityscapes, KITTI-360, and BDD3K, having identical label mapping. DADA-seg dataset serves as a target domain and has no training data.</figDesc><table><row><cell>Datasets</cell><cell>Cityscapes</cell><cell>KITTI-360</cell><cell>BDD3K</cell><cell>ApolloScape</cell><cell>Merge3</cell><cell>DADA-seg</cell></row><row><cell>#training</cell><cell>2,975</cell><cell>5,504</cell><cell>3,086</cell><cell>6,056</cell><cell>11,565</cell><cell>0</cell></row><row><cell>#evaluaion</cell><cell>500</cell><cell>612</cell><cell>343</cell><cell>673</cell><cell>14,555</cell><cell>313</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>branch EGM SPP Event SPP EAM</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Event Residual Layer</cell><cell cols="2">RGB Residual Layer</cell><cell cols="3">Upsampling Layer</cell><cell>SPP</cell><cell>E-SPP</cell><cell>A Attention</cell><cell>Add</cell><cell>Multiply</cell><cell>c Concatenate</cell><cell>k?k Conv</cell></row><row><cell></cell><cell cols="2">RL 1</cell><cell>RL 2</cell><cell></cell><cell>RL 3</cell><cell cols="2">RL 4</cell><cell></cell><cell></cell></row><row><cell></cell><cell>H</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>64, /4</cell><cell cols="2">128, /8</cell><cell cols="2">256, /16</cell><cell>512, /32</cell><cell></cell><cell></cell></row><row><cell>B</cell><cell>W</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64, /4 RL 1</cell><cell cols="2">128, /8 RL 2</cell><cell cols="2">256, /16 RL 3</cell><cell cols="2">512, /32 RL 4</cell><cell></cell><cell cols="2">512, H/32, W/32</cell></row><row><cell></cell><cell>1?1</cell><cell>1?1</cell><cell></cell><cell>1?1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>UP 3</cell><cell>UP 2</cell><cell></cell><cell>UP 1</cell><cell></cell><cell>SPP</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Event</cell><cell></cell></row><row><cell></cell><cell>RL 1 64, /1</cell><cell cols="2">RL 2 32, /1</cell><cell>RL 3 16, /1</cell><cell></cell><cell>RL 4 8, /1</cell><cell></cell><cell>branch</cell><cell>?(.)</cell><cell>8, H, W</cell></row><row><cell></cell><cell cols="2">RL 1</cell><cell>RL 2</cell><cell></cell><cell>RL 3</cell><cell cols="2">RL 4</cell><cell></cell><cell>c</cell></row><row><cell></cell><cell></cell><cell>64, /4</cell><cell cols="2">128, /8</cell><cell cols="3">256, /16 512, /32</cell><cell></cell><cell>up, ?32</cell></row><row><cell></cell><cell cols="2">1?1</cell><cell>1?1</cell><cell></cell><cell>1?1</cell><cell></cell><cell></cell><cell></cell><cell>1?1</cell></row><row><cell></cell><cell cols="2">UP 3</cell><cell>UP 2</cell><cell></cell><cell>UP 1</cell><cell>E-SPP</cell><cell></cell><cell>RGB branch</cell><cell>RL 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Performance gap of models, which are trained and validated on Cityscapes and then tested on DADA-seg, both with a 1024?512 resolution. Transformer-based models in the fourth group are tested in 768?768 resolution following their default settings.</figDesc><table><row><cell>Network</cell><cell>Backbone</cell><cell cols="2">Cityscapes DADA-seg</cell><cell>mIoU Gap</cell></row><row><cell>ERFNet [34]</cell><cell>ERFNet</cell><cell>72.1</cell><cell>9.0</cell><cell>-63.1</cell></row><row><cell>PSPNet [12]</cell><cell>MobileNetV2</cell><cell>70.2</cell><cell>17.1</cell><cell>-52.5</cell></row><row><cell>DeepLabV3+ [13]</cell><cell>MobileNetV2</cell><cell>75.2</cell><cell>16.5</cell><cell>-58.7</cell></row><row><cell>MobileNetV3 [37]</cell><cell>MobileNetV3small</cell><cell>64.1</cell><cell>18.2</cell><cell>-45.9</cell></row><row><cell>MobileNetV3 [37]</cell><cell>MobileNetV3</cell><cell>69.5</cell><cell>17.6</cell><cell>-51.9</cell></row><row><cell>SwiftNet [35]</cell><cell>ResNet-18</cell><cell>75.4</cell><cell>20.5</cell><cell>-54.9</cell></row><row><cell>DeepLabV3+ [13]</cell><cell>ResNet-18</cell><cell>76.8</cell><cell>19.1</cell><cell>-57.7</cell></row><row><cell>OCRNet [14]</cell><cell>HRNetV2p-W18small</cell><cell>77.1</cell><cell>20.5</cell><cell>-56.6</cell></row><row><cell>Fast-SCNN [36]</cell><cell>Fast-SCNN</cell><cell>69.1</cell><cell>26.3</cell><cell>-42.8</cell></row><row><cell>DeepLabV3+ [13]</cell><cell>ResNet-50</cell><cell>80.1</cell><cell>18.0</cell><cell>-62.1</cell></row><row><cell>PSPNet [12]</cell><cell>ResNet-50</cell><cell>78.6</cell><cell>16.1</cell><cell>-62.5</cell></row><row><cell>DANet [15]</cell><cell>ResNet-50</cell><cell>79.3</cell><cell>17.6</cell><cell>-61.7</cell></row><row><cell>DNL [16]</cell><cell>ResNet-50</cell><cell>79.3</cell><cell>15.7</cell><cell>-63.6</cell></row><row><cell cols="2">Semantic FPN [33] ResNet-50</cell><cell>74.5</cell><cell>18.2</cell><cell>-56.3</cell></row><row><cell>OCRNet [14]</cell><cell>HRNetV2p-W18</cell><cell>78.6</cell><cell>23.0</cell><cell>-55.6</cell></row><row><cell>DeepLabV3+ [13]</cell><cell>ResNet-101</cell><cell>80.9</cell><cell>21.0</cell><cell>-59.9</cell></row><row><cell>PSPNet [12]</cell><cell>ResNet-101</cell><cell>79.8</cell><cell>20.1</cell><cell>-59.7</cell></row><row><cell>DANet [15]</cell><cell>ResNet-101</cell><cell>80.4</cell><cell>17.8</cell><cell>-62.6</cell></row><row><cell>DNL [16]</cell><cell>ResNet-101</cell><cell>80.4</cell><cell>19.7</cell><cell>-60.7</cell></row><row><cell cols="2">Semantic FPN [33] ResNet-101</cell><cell>75.8</cell><cell>19.6</cell><cell>-56.2</cell></row><row><cell>ResNeSt [30]</cell><cell>ResNeSt-101</cell><cell>79.6</cell><cell>20.0</cell><cell>-59.6</cell></row><row><cell>OCRNet [14]</cell><cell>HRNetV2p-W48</cell><cell>80.7</cell><cell>24.5</cell><cell>-56.2</cell></row><row><cell>SETR-Naive [31]</cell><cell>Transformer-Large</cell><cell>77.9</cell><cell>27.1</cell><cell>-50.8</cell></row><row><cell>SETR-MLA [31]</cell><cell>Transformer-Large</cell><cell>77.2</cell><cell>30.4</cell><cell>-46.8</cell></row><row><cell>SETR-PUP [31]</cell><cell>Transformer-Large</cell><cell>79.3</cell><cell>31.8</cell><cell>-47.5</cell></row><row><cell>SegFormer-B1 [32]</cell><cell>MiT-B1</cell><cell>78.0</cell><cell>16.6</cell><cell>-61.4</cell></row><row><cell>SegFormer-B2 [32]</cell><cell>MiT-B2</cell><cell>80.5</cell><cell>21.2</cell><cell>-59.3</cell></row><row><cell>SegFormer-B3 [32]</cell><cell>MiT-B3</cell><cell>81.5</cell><cell>27.0</cell><cell>-54.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison of different event representations and event fusion approaches. All models use ResNet-18 as backbone and are tested with a 1024?512 resolution. B is short for the time bins of event data. The RGB-only SwiftNet model was selected as baseline. Runtimes in milliseconds (ms) are calculated by an RTX2070 GPU.</figDesc><table><row><cell>Network</cell><cell>RGB</cell><cell>Event</cell><cell>Fusion</cell><cell>Event Bins</cell><cell>Params(M)</cell><cell>Runtime(ms)</cell><cell>Cityscapes</cell><cell>DADA-seg</cell></row><row><cell>SwiftNet [35]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.816</cell><cell>8.1 (?0.17)</cell><cell>69.2</cell><cell>20.1</cell></row><row><cell>SwiftNet [35]</cell><cell></cell><cell></cell><cell></cell><cell>B = 1</cell><cell>11.810</cell><cell>7.9 (?0.15)</cell><cell>35.6</cell><cell>2.3</cell></row><row><cell>SwiftNet [35]</cell><cell></cell><cell></cell><cell></cell><cell>B = 2</cell><cell>11.813</cell><cell>8.0 (?0.32)</cell><cell>36.0</cell><cell>19.7</cell></row><row><cell>SwiftNet [35]</cell><cell></cell><cell></cell><cell></cell><cell>B = 10</cell><cell>11.838</cell><cell>8.6 (?0.07)</cell><cell>38.4</cell><cell>21.7 (+1.6)</cell></row><row><cell>SwiftNet [35]</cell><cell></cell><cell></cell><cell></cell><cell>B = 18</cell><cell>11.863</cell><cell>9.5 (?0.33)</cell><cell>36.6</cell><cell>19.8</cell></row><row><cell>EDCNet</cell><cell></cell><cell></cell><cell>S2D -EAM</cell><cell>B = 1</cell><cell>16.955</cell><cell>13.9 (?0.19)</cell><cell>68.3</cell><cell>16.7</cell></row><row><cell>EDCNet</cell><cell></cell><cell></cell><cell>S2D -EAM</cell><cell>B = 2</cell><cell>16.958</cell><cell>13.9 (?0.19)</cell><cell>68.4</cell><cell>23.0 (+2.9)</cell></row><row><cell>EDCNet</cell><cell></cell><cell></cell><cell>S2D -EAM</cell><cell>B = 10</cell><cell>16.983</cell><cell>14.6 (?0.21)</cell><cell>66.5</cell><cell>18.8</cell></row><row><cell>EDCNet</cell><cell></cell><cell></cell><cell>S2D -EAM</cell><cell>B = 18</cell><cell>17.009</cell><cell>15.4 (?0.22)</cell><cell>67.1</cell><cell>10.4</cell></row><row><cell>EDCNet</cell><cell></cell><cell></cell><cell>D2S -EGM</cell><cell>B = 1</cell><cell>12.012</cell><cell>51.7 (?0.90)</cell><cell>69.0</cell><cell>24.5</cell></row><row><cell>EDCNet</cell><cell></cell><cell></cell><cell>D2S -EGM</cell><cell>B = 2</cell><cell>12.012</cell><cell>56.3 (?2.40)</cell><cell>69.4</cell><cell>28.3 (+8.2)</cell></row><row><cell>EDCNet</cell><cell></cell><cell></cell><cell>D2S -EGM</cell><cell>B = 10</cell><cell>12.012</cell><cell>52.7 (?2.18)</cell><cell>68.8</cell><cell>22.9</cell></row><row><cell>EDCNet</cell><cell></cell><cell></cell><cell>D2S -EGM</cell><cell>B = 18</cell><cell>12.013</cell><cell>52.8 (?2.27)</cell><cell>68.8</cell><cell>24.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Ablation study of event intensity (EI). Models are trained on Cityscapes and evaluated on DADA-seg dataset.</figDesc><table><row><cell>Event Intensity</cell><cell>Acc</cell><cell>mIoU</cell><cell>fwIoU</cell></row><row><cell>0.1</cell><cell>41.06</cell><cell>20.80</cell><cell>49.04</cell></row><row><cell>0.3</cell><cell>43.66</cell><cell>21.98</cell><cell>48.86</cell></row><row><cell>0.5</cell><cell>41.00</cell><cell>21.16</cell><cell>48.08</cell></row><row><cell>0.7</cell><cell>39.55</cell><cell>20.25</cell><cell>46.57</cell></row><row><cell>1.0</cell><cell>40.98</cell><cell>20.33</cell><cell>45.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison of the EDCNet and the SwiftNet on diverse datasets. The SwiftNet is trained only with RGB images as baseline, while our EDCNet is set as the D2S-EGM mode. The Merge3 dataset is combined from Cityscapes, KITTI-360, and BDD3K. +5.9% improvement on the source domain and a +4.7% improvement on the DADA-seg dataset, compared to SwiftNet. On the KITTI-360 and ApolloScape datasets, our model has also considerable improvements in the target domain, precisely +2.4% and +2.8%, respectively. In particular, the mIoU of our EDCNet on the Merge3 dataset reaches the highest score 32.4%. The resulting EDCNet clearly gets ahead all the CNN methods and successfully exceeds state-of-the-art transformer-based models. Overall, the results show that our proposal is consistently and significantly effective for enhancing the reliability of accident scene segmentation. Besides, verifying on diverse datasets demonstrates the reproducibility of our proposed methods if one wants to deploy with or extend to other cameras or datasets. On the other hand, the DADAseg dataset is sufficiently diverse (see</figDesc><table><row><cell>Dataset</cell><cell>Network</cell><cell>Source</cell><cell>DADA-seg</cell></row><row><cell>Cityscapes [5]</cell><cell>SwiftNet [35] EDCNet</cell><cell>69.2 69.4</cell><cell>20.1 28.3 (+8.3)</cell></row><row><cell>BDD3K [18]</cell><cell>SwiftNet [35] EDCNet</cell><cell>30.6 36.5</cell><cell>23.9 28.6 (+4.7)</cell></row><row><cell>KITTI-360 [17]</cell><cell>SwiftNet [35] EDCNet</cell><cell>45.2 46.6</cell><cell>13.7 16.1 (+2.4)</cell></row><row><cell>ApolloScape [19]</cell><cell>SwiftNet [35] EDCNet</cell><cell>61.8 58.8</cell><cell>16.7 19.5 (+2.8)</cell></row><row><cell>Merge3</cell><cell>SwiftNet [35] EDCNet</cell><cell>50.3 61.4</cell><cell>28.5 32.4 (+3.9)</cell></row><row><cell cols="4">have a smaller performance gap between the source and target</cell></row><row><cell cols="4">dataset. Based on the data diversity of BDD3K, our EDCNet</cell></row><row><cell>gains a</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII :</head><label>VII</label><figDesc>Performance comparison of domain adaptation strategies, where f and i represent the feature and image level transfer between the source and target domain. The results of Source and Target are tested with 1024?512 resolution, while Target ? and Foreground classes are with 512?256. To clearly showcase the effect of the event-aware branch, the per-class IoU(%) of ten foreground classes of Target result are listed: Traffic Light, Traffic Sign, Pedestrian, Rider, Car, Truck, Bus, Train, Motorcycle, and Bicycle. Note that the target dataset does not have any Train. Our EDCNet-Event model is adapted on feature and image levels by fusing event data in D2S mode with the EGM module, while the EDCNet-DOF is by fusing dense optical flow data.</figDesc><table><row><cell>Network</cell><cell>Level</cell><cell>Foreground classes</cell><cell>Target ?</cell><cell>Source</cell><cell>Target</cell></row><row><cell></cell><cell></cell><cell cols="4">TLi TSi Ped Rid Car Tru Bus Tra Mot Bic Acc mIoU fwIoU Acc mIoU fwIoU Acc mIoU fwIoU</cell></row><row><cell>CLAN [68]</cell><cell>-</cell><cell>15.2 5.3 4.0 3.4 32.6 8.8 28.8</cell><cell cols="3">-4.2 0.1 34.0 19.4 45.5 56.3 43.7 77.2 28.1 16.8 38.3</cell></row><row><cell>CLAN [68]</cell><cell>f</cell><cell>17.2 21.5 8.4 6.3 63.5 33.4 33.1</cell><cell cols="3">-3.7 6.2 46.3 31.7 67.2 70.4 62.4 87.0 40.1 28.8 63.8</cell></row><row><cell>CLAN [68]</cell><cell>f+i</cell><cell>17.0 20.0 9.4 5.2 64.3 36.8 35.9</cell><cell cols="3">-5.6 7.7 47.3 32.4 66.3 73.2 64.8 87.3 39.4 28.2 60.6</cell></row><row><cell cols="2">EDCNet-DOF f+i</cell><cell>18.1 17.7 9.5 8.1 64.3 34.8 34.9</cell><cell cols="3">-5.1 7.3 48.3 33.4 69.6 71.6 62.9 87.4 40.9 29.2 64.3</cell></row><row><cell cols="2">EDCNet-Event f+i</cell><cell>17.0 19.5 10.0 8.8 65.6 39.5 39.7</cell><cell cols="3">-6.1 7.0 48.2 33.1 68.2 73.2 63.9 87.5 42.1 30.0 64.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Comparison with State-of-the-art DA Methods. Table VIII shows the comparison between our EDCNet and some state-of-the-art domain adaptation methods [40][68][76][77]. Results including mIoU and IoU for each category are tested on the target domain, i.e. DADA-seg dataset. For a fair comparison, these models are trained based on the default settings</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VIII :</head><label>VIII</label><figDesc>Comparison with state-of-the-art domain adaptation methods. Results of mIoU and per-class IoU are calculated in the resolution of 1024?512 on DADA-seg dataset. 79.80 18.61 51.56 8.32 13.60 15.51 17.15 21.51 63.20 21.99 80.53 8.37 6.32 63.47 33.43 33.12 -3.69 6.21 BDL [76] 29.66 81.44 19.18 57.18 8.61 16.26 14.65 8.78 16.77 66.60 26.83 85.87 10.51 7.16 65.45 35.18 34.78 -2.71 5.57 FDA [77] 24.45 67.83 15.36 39.99 4.28 12.98 16.08 6.93 18.26 57.01 9.71 64.50 10.30 9.49 59.39 16.84 39.35 -11.28 4.94 SIM [40] 26.85 79.13 16.93 56.79 4.69 12.38 16.50 7.47 14.04 65.78 18.17 87.27 11.04 3.83 61.33 22.99 28.21 -0.52 2.96 EDCNet (ours) 29.97 80.23 19.51 52.02 6.43 14.68 16.19 17.03 19.50 65.39 21.69 79.84 9.95 8.82 65.60 39.51 39.73 -6.09 7.03</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>Road</cell><cell>Sidewalk</cell><cell>Building</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>Traffic Light</cell><cell>Traffic Sign</cell><cell>Vegetation</cell><cell>Terrain</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Truck</cell><cell>Bus</cell><cell>Train</cell><cell>Motorcycle</cell><cell>Bicycle</cell></row><row><cell>CLAN [68]</cell><cell>28.76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multimodal vision sensor for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SPIE</title>
		<meeting>of SPIE</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11166</biblScope>
			<biblScope unit="page">111660</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stabilization and validation of 3D object position using multimodal sensor fusion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Giosan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1110</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Intervention minimized semiautonomous control using decoupled model predictive control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IV</title>
		<meeting>IV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Control method simulation and application for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ar?kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kayaduman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Im?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">C</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Bak?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karadag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Abbasov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IDAP</title>
		<meeting>IDAP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention R-CNN for accident detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IV, 2020</title>
		<meeting>IV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="313" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolution neural network joint with mixture of extreme learning machines for feature extraction and classification of accident images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pashaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghatee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sajedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Real Time Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1051" to="1066" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Traffic accident benchmark for causality recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="540" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hetero-ConvLSTM: A deep learning approach to traffic accident prediction on heterogeneous spatio-temporal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DADA: Driver attention prediction in driving accident scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haase-Sch?tz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hertlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wiesbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1341" to="1360" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3141" to="3149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic instance annotation of street scenes by 3D to 2D label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3688" to="3697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BDD100K: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2636" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The ApolloScape open dataset for autonomous driving and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2702" to="2719" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WildDash -Creating hazard-aware benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Eventbased neuromorphic vision for autonomous driving: a paradigm shift for bio-inspired visual sensing and perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rohrbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="34" to="49" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A 128?128 120 dB 15 ?s latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbr?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Event-based vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbr?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ISSAFE: Improving semantic segmentation in accidents by fusing event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IROS, 2021</title>
		<meeting>IROS, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classification of crash and near-crash events from dashcam videos and telematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Taccari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sambo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simoncini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ITSC</title>
		<meeting>ITSC</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2460" to="2465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Anticipating accidents in dashcam videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Anticipating traffic accidents with adaptive loss and large-scale incident DB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3521" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DRIVE: Deep reinforced accident anticipation with visual explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV, 2021</title>
		<meeting>ICCV, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="7619" to="7628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">ResNeSt: Splitattention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2021</title>
		<meeting>CVPR, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SegFormer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6392" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized ConvNet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">In defense of pretrained ImageNet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast-SCNN: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">289</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Searching for MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">sEnDec: An improved image to image cnn for foreground localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4435" to="4443" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An end-to-end edge aggregation network for moving object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Biradar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dudhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Murala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="8149" to="8158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Naive-student: Leveraging semi-supervised learning in video sequences for urban scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="695" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nighttime road scene parsing by unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="687" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rainy night scene understanding with near scene semantic adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elezovikj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wasserstein loss with alternative reinforcement learning for severity-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fishyscapes: A benchmark for safe semantic segmentation in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCVW</title>
		<meeting>ICCVW</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MSeg: A composite dataset for multi-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="2879" to="2888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Real-time fusion network for RGB-D semantic segmentation incorporating unexpected obstacle detection for road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5558" to="5565" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">RTFNet: RGB-thermal fusion network for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2576" to="2583" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Optical flow augmented semantic segmentation networks for automated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?zek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Helw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VISAPP</title>
		<meeting>VISAPP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Real-time 3D reconstruction and 6-DoF tracking with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="349" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Combining events and frames using recurrent asynchronous multimodal networks for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?egg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hidalgo-Carri?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2822" to="2829" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">EV-FlowNet: Selfsupervised optical flow estimation for event-based cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. RSS</title>
		<meeting>RSS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Event-based moving object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitrokhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parameshwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IROS</title>
		<meeting>IROS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Event-based vision meets deep learning on steering prediction for selfdriving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Maqueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5419" to="5427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised eventbased learning of optical flow, depth, and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="989" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">EV-SegNet: Semantic segmentation for event-based cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1624" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">DDD17: End-to-end DAVIS driving dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01458</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">ESIM: an open event camera simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoRL</title>
		<meeting>CoRL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="969" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video to events: Recycling video datasets for event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hidalgo-Carri?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="3586" to="3595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">EventGAN: Leveraging large scale image datasets for event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The multivehicle stereo event camera dataset: An event camera dataset for 3D perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>?zaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfrommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2032" to="2039" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">High frame rate video reconstruction based on an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scheerlinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning to extract a video sequence from a single motion-blurred image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6334" to="6342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2502" to="2511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Event cameras, contrast maximization and reward functions: An analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stoffregen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kleeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="300" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farneb?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SCIA</title>
		<meeting>SCIA</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">FDA: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
