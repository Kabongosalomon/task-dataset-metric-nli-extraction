<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gait Recognition in the Wild with Dense 3D Representations and A Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkai</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Explore Academy of JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Wu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Explore Academy of JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Lingxiao He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Explore Academy of JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
							<email>cgyan@hdu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@jd.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Explore Academy of JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gait Recognition in the Wild with Dense 3D Representations and A Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing studies for gait recognition are dominated by 2D representations like the silhouette or skeleton of the human body in constrained scenes. However, humans live and walk in the unconstrained 3D space, so projecting the 3D human body onto the 2D plane will discard a lot of crucial information like the viewpoint, shape, and dynamics for gait recognition. Therefore, this paper aims to explore dense 3D representations for gait recognition in the wild, which is a practical yet neglected problem. In particular, we propose a novel framework to explore the 3D Skinned Multi-Person Linear (SMPL) model of the human body for gait recognition, named SMPLGait. Our framework has two elaborately-designed branches of which one extracts appearance features from silhouettes, the other learns knowledge of 3D viewpoints and shapes from the 3D SMPL model. In addition, due to the lack of suitable datasets, we build the first large-scale 3D representationbased gait recognition dataset, named Gait3D. It contains 4,000 subjects and over 25,000 sequences extracted from 39 cameras in an unconstrained indoor scene. More importantly, it provides 3D SMPL models recovered from video frames which can provide dense 3D information of body shape, viewpoint, and dynamics. Based on Gait3D, we comprehensively compare our method with existing gait recognition approaches, which reflects the superior performance of our framework and the potential of 3D representations for gait recognition in the wild. The code and dataset are available at https://gait3d.github.io.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual gait recognition, which aims to identify a target person using her/his walking pattern in a video, has been studied for over two decades <ref type="bibr" target="#b23">[30,</ref><ref type="bibr" target="#b36">42]</ref>. Existing approaches and datasets are dominated by 2D gait representations such as silhouette sequences <ref type="bibr" target="#b50">[56]</ref>, Gait Energy Images * This work was done when Jinkai Zheng was an intern at Explore Academy of JD.com. ? Corresponding author.  (GEIs) <ref type="bibr">[11]</ref>, 2D skeletons <ref type="bibr" target="#b57">[63]</ref>, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. However, the human body is a 3D non-rigid object, so the 3D-to-2D projection discards a lot of useful information about shapes, viewpoints, and dynamics while presenting ambiguity for gait recognition. Therefore, this paper is focused on 3D gait recognition which is valuable yet neglected by the community. Recently, deep learning-based methods have dominated the state-of-the-art performance on the widely adopted 2D gait recognition benchmarks like CASIA-B <ref type="bibr" target="#b30">[37]</ref> and OU-MVLP <ref type="bibr" target="#b29">[36]</ref> by directly learning discriminative features from silhouette sequences <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b51">57]</ref> or GEIs <ref type="bibr" target="#b40">[46]</ref>. Despite the excellent results on the in-the-lab datasets, these methods cannot work well in the wild scenarios which have more diverse 3D viewpoints of cameras and more complex environmental interference factors like occlusions <ref type="bibr" target="#b57">[63]</ref>. Although several works exploit 3D cylin-ders <ref type="bibr" target="#b2">[3]</ref> or 3D skeletons <ref type="bibr" target="#b35">[41]</ref>, these sparse 3D models also lose helpful information of human bodies like viewpoints and shapes. Fortunately, the development of parameterized human body models like the Skinned Multi-Person Linear (SMPL) model <ref type="bibr" target="#b21">[28]</ref> and 3D human mesh recovery approaches <ref type="bibr" target="#b11">[18,</ref><ref type="bibr" target="#b13">20,</ref><ref type="bibr" target="#b27">34]</ref> makes it possible to estimate precise 3D meshes and viewpoints of human bodies in video frames. The advantages of 3D meshes for gait recognition are two-fold: 1) the 3D mesh can provide not only the pose but also the shape of the human body in the 3D space, which is crucial for learning discriminative features of gait, and 2) the 3D viewpoint can be explored to normalize the orientations of human bodies during cross-view matching.</p><p>To this end, we design a novel 3D SMPL model-based Gait recognition framework, i.e., SMPLGait, to explore the 3D gait representations for human identification. Our SMPLGait framework has two branches based on deep neural networks. One branch takes the silhouette sequence of a person as the input to learn appearance features like clothing, hairstyle, and belongings. However, due to the extreme viewpoint changes in the wild, the shape of the human body can be distorted, which makes the appearance ambiguous, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. To overcome this challenge, we design a 3D Spatial-Transformation Network (3D-STN) as the other branch to learn 3D knowledge of viewpoint and shape from the 3D human mesh. The 3D-STN takes the 3D SMPL model of each frame as the input to learn a spatial transformation matrix. By applying the spatial transformation matrix to the appearance features, these features from different viewpoints are normalized in the latent space. By this means, the gait sequences of the same person will be closer in the feature space.</p><p>Nevertheless, there is no suitable dataset that provides 3D meshes of human bodies in the wild. Therefore, to facilitate the research, we build the first large-scale 3D mesh-based gait recognition dataset, named Gait3D, from high-resolution videos captured in the wild. Compared to existing datasets listed in <ref type="table" target="#tab_0">Table 1</ref>, the Gait3D dataset has the following featured properties: 1) Gait3D contains 4,000 subjects with over 25,000 sequences captured by 39 cameras in an unconstrained indoor scene which makes it scalable for research and applications. 2) It provides precise 3D human meshes recovered from video frames which can provide 3D pose and shape of human bodies as well as accurate viewpoint parameters. 3) It also provides conventional 2D silhouettes and keypoints which can be explored for gait recognition with multi-modal data.</p><p>In summary, the contributions of this paper are as follows:</p><p>? We make one of the first attempts toward 3D gait recognition in the real-world scenario, which aims to explore dense 3D representations of the human body for gait recognition.</p><p>? We propose a novel 3D gait recognition framework based on the SMPL model, named SMPLGait, to explore 3D human meshes for gait recognition.</p><p>? We build the first large-scale 3D gait recognition dataset, named Gait3D, which provides the 3D human meshes of gait collected from unconstrained scenarios.</p><p>Through comprehensive experiments, we not only evaluate existing 2D silhouettes/skeleton-based approaches but also demonstrate the effectiveness of the proposed SMPLGait method, which reflects the potential of 3D representations for gait recognition. Moreover, the combination of 3D and 2D representations further improves the performance which shows the complementarity of multi-modal representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Gait Recognition. We review the 2D and 3D representations-based gait recognition methods separately.</p><p>2D gait recognition methods can be classified into model-based and model-free approaches <ref type="bibr" target="#b36">[42]</ref>.</p><p>Early methods mainly belong to the model-based which defines a structural human body model. Then, gait patterns are modeled by parameters like lengths of limbs, angles of joints, and relative positions of body parts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">48]</ref>. The model-free methods mainly adopt the silhouettes obtained by background subtraction from video frames <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr">11,</ref><ref type="bibr">15,</ref><ref type="bibr" target="#b9">16,</ref><ref type="bibr" target="#b15">22,</ref><ref type="bibr" target="#b25">32,</ref><ref type="bibr" target="#b40">46,</ref><ref type="bibr" target="#b51">57,</ref><ref type="bibr" target="#b52">58]</ref>. In particular, Han et al. proposed to aggregate a sequence of silhouettes into a compact Gait Energy Image (GEI) <ref type="bibr">[11]</ref> which was widely used by the following methods <ref type="bibr" target="#b25">[32,</ref><ref type="bibr" target="#b40">46]</ref>. Recently, due to the success of deep learning for computer vision tasks [7, <ref type="bibr">24-26, 45, 50-54]</ref>, deep Convolutional Neural Networks (CNNs) also dominated the performance of gait recognition. For example, Shiraga et al. <ref type="bibr" target="#b25">[32]</ref> and Wu et al. <ref type="bibr" target="#b40">[46]</ref> proposed to learn effective features from GEIs and significantly outperformed previous methods. The most recent methods started to learn discriminative features directly from the silhouette sequences using larger CNNs or multi-scale structures and achieved state-of-the-art results <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b9">16,</ref><ref type="bibr" target="#b15">22]</ref>. Despite the excellent performance on in-the-lab datasets, e.g., CASIA-B and OU-LP, these methods usually fail in the wild as shown in the experiments on GREW <ref type="bibr" target="#b57">[63]</ref> and our Gait3D.</p><p>3D representations have also been studied since the early years of gait recognition. For example, Urtasun and Fua <ref type="bibr" target="#b35">[41]</ref> proposed an approach to gait analysis that depended on 3D temporal motion models using an articulated skeleton. Zhao et al. <ref type="bibr" target="#b53">[59]</ref> applied a local optimization algorithm to track 3D motion for gait recognition. Yamauchi et al. <ref type="bibr" target="#b43">[49]</ref> proposed the first method using 3D pose estimated from RGB frames for walking human recognition. Ariyanto and Nixon <ref type="bibr" target="#b2">[3]</ref> built a 3D voxel-based dataset using a complex multi-camera system and proposed a structural model of articulated cylinders with 3D Degrees of Freedom at each joint to model the human lower legs. However, these methods either discard rich 3D information like viewpoints and shapes or are limited by devices for real-world applications. In summary, to overcome the problem of 2D methods and explore 3D representations for gait recognition in the wild, we aim to explore 3D mesh as a rich representation of the viewpoint and shape of the human body. Gait Recognition Datasets. Current publicly available gait recognition datasets mainly belong to two series, i.e., the CASIA series <ref type="bibr" target="#b30">[37,</ref><ref type="bibr" target="#b38">44,</ref><ref type="bibr" target="#b50">56]</ref> and the OU-ISIR series <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">14,</ref><ref type="bibr" target="#b10">17,</ref><ref type="bibr" target="#b22">29,</ref><ref type="bibr" target="#b33">39,</ref><ref type="bibr" target="#b34">40,</ref><ref type="bibr" target="#b41">47]</ref> as listed in <ref type="table" target="#tab_0">Table 1</ref>. The CASIA series were built in the early research of gait recognition, which facilitated the initial exploration of RGB images and silhouettes for gait representations <ref type="bibr">[11,</ref><ref type="bibr" target="#b30">37]</ref>. Despite its smaller number of subjects, the CASIA-B <ref type="bibr" target="#b50">[56]</ref> is still the most widely used dataset for the evaluation of silhouettebased methods. The OU-ISIR series were first built ten years ago and developed comprehensive variants such as walking at different speeds <ref type="bibr" target="#b33">[39]</ref>, clothing styles <ref type="bibr">[14]</ref>, and bags <ref type="bibr" target="#b34">[40]</ref>, subjects of different ages <ref type="bibr" target="#b41">[47]</ref>, and annotations of 2D pose <ref type="bibr" target="#b0">[1]</ref>. Due to their large population, the OU-LP <ref type="bibr" target="#b10">[17]</ref> and OU-MVLP <ref type="bibr" target="#b29">[36]</ref> also became the most popular datasets for current research. However, the above datasets were collected in constrained scenes like labs <ref type="bibr" target="#b10">[17,</ref><ref type="bibr" target="#b50">56]</ref> or a small defined area on a campus <ref type="bibr" target="#b24">[31,</ref><ref type="bibr" target="#b38">44]</ref>. Most recently, researchers started to narrow the gap between in-the-lab research and real-world application. As a contemporaneous study of our work, Zhu et al. <ref type="bibr" target="#b57">[63]</ref> constructed the GREW dataset from natural videos collected in an open area. However, there is no dataset that provides rich 3D representations for gait recognition in the wild. Therefore, we need to build a new dataset that is collected from complex scenes and with dense 3D meshes for gait recognition in the wild.</p><p>3D Human Mesh Recovery. 3D representations have attracted a lot of attention in the computer vision community <ref type="bibr" target="#b20">[27,</ref><ref type="bibr" target="#b55">61]</ref>. The 3D human body can be represented by point clouds <ref type="bibr" target="#b16">[23]</ref>, voxels <ref type="bibr" target="#b35">[41]</ref>, parameterized blend shapes <ref type="bibr" target="#b1">[2]</ref>, etc. Among them, the Skinned Multi-Person Linear (SMPL) model <ref type="bibr" target="#b21">[28]</ref> is a skinned vertex-based model that can accurately represent a wide variety of body shapes in natural human poses. With the SMPL model, an arbitrary 3D human body can be represented by a linear combination of a group of shape, pose, scale, and viewpoint parameters. Based on the SMPL model, a series of 3D human mesh recovery approaches are developed to estimate accurate 3D shapes, poses, and viewpoints of human bodies from natural images <ref type="bibr" target="#b11">[18,</ref><ref type="bibr" target="#b13">20,</ref><ref type="bibr" target="#b27">34,</ref><ref type="bibr" target="#b28">35]</ref>. These methods provide us an opportunity to obtain 3D human meshes from in-the-wild videos for 3D mesh-based gait recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The 3D Gait Recognition Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The overall architecture of the proposed 3D SMPLbased Gait Recognition framework, SMPLGait, is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. There are two branches of the framework. For the first branch, we take the sequence of silhouettes as input which has a rich knowledge of the appearance and use a CNN-based model to extract 2D spatial features from each frame. For the second branch, the SMPLs of the human body are fed into the 3D-Spatial-Transformation Network (3D-STN), which aims to learn the latent transformation matrixes from the 3D viewpoints and shapes. Then the 3D Spatial Transformation Module aligns the 2D appearance features in the latent space using the learned transformation matrixes. Finally, the transformed feature of each frame is aggregated into a sequence-level feature for sequence-tosequence matching in training or inference. Next, we will introduce the above modules in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Structure</head><p>The Silhouette Learning Network (SLN) aims to learn the appearance knowledge of humans from silhouettes that contain 2D spatial information like clothing and hairstyle. The SLN has six convolutional layers which are similar to the backbone of GaitSet <ref type="bibr" target="#b4">[5]</ref>. As is shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the sequences of silhouettes are fed into a CNN. We formulate is the i-th binary frame, L is the length of the sequence, H and W are the height and width of the silhouette image. For a frame x i , the process can be formulated as:</p><formula xml:id="formula_0">X sil = {x i } L i=1</formula><formula xml:id="formula_1">F i = F (x i ),<label>(1)</label></formula><p>where F (?) is the CNN-based backbone and F i ? R h?w is the frame-level feature map for frame x i 1 . The 3D Spatial Transformation Network (3D-STN) is proposed to solve viewpoint changes in real 3D scenarios. 3D SMPL parameters related to 3D viewpoints, shapes, and poses are the input of this module. Assuming</p><formula xml:id="formula_2">Y sp = {y i } L i=1</formula><p>is the input SMPLs, where y i ? R D is the SMPL vector of i-th frame, D is the dimension of the SMPL vector which contains 24?3 dimensions of 3D human body pose, 10 dimensions of 3D body shape, and 3 dimensions of camera scale and translation parameters. The 3D-STN consists of three fully connected (FC) layers with neuron number = 128 ? 256 ? h ? w, where h and w are the height and width of the feature map from the Silhouette Learning Network. Each FC layer is followed by batch normalization and the ReLU activation function. We use dropout for the last two FC layers to eliminate overfitting. The forward process of 3D-STN can be formulated as:</p><formula xml:id="formula_3">g i = G(y i ),<label>(2)</label></formula><p>where G(?) is the 3D-STN and g i is the frame-level transformation vector for frame i. The 3D Spatial Transformation Module is designed to align the 2D appearance feature map F i ? R h?w using the transformation vector g i in the feature space, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. We first reshape the transformation vector g i to a matrix G i ? R w?h . Then, for convenience of computation, we expand F i and G i to square matrixes by zero padding on the short edge. After that, we apply G i to F i by</p><formula xml:id="formula_4">F i = F i ? (I + G i ),<label>(3)</label></formula><p>where I is an identity matrix and ? is matrix multiplication. At last, we adopt Set Pooling (SP) and Horizontal Pyramid Pooling (HPP) in GaitSet <ref type="bibr" target="#b4">[5]</ref> to aggregate F i into the final feature vector for sequence-to-sequence matching. For more details of the SMPLGait framework, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference</head><p>Our two-branch 3D gait recognition framework is trained in an end-to-end manner. The network of our framework is optimized by a loss function with two components:</p><formula xml:id="formula_5">L = ?L tri + ?L ce ,<label>(4)</label></formula><p>where L tri is the triplet loss, L ce is the cross entropy loss. ? and ? are the weighting parameters. During inference, we use the sequences of silhouettes and SMPLs as the inputs of the two branches, respectively. The cosine similarity is used to measure the similarity between a query-gallery pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Gait3D Benchmark</head><p>To facilitate the research of 3D gait recognition, we present a novel large-scale dataset, named Gait3D, which has several featured properties compared to existing datasets in <ref type="table" target="#tab_0">Table 1</ref>. First of all, the Gait3D dataset consists of 4,000 subjects, 25,000+ sequences, and over 3 million bounding boxes captured by cameras of arbitrary 3D viewpoints, which makes it more scalable for training deep CNNs. Moreover, it provides accurate 3D human meshes estimated from video frames, which contains the poses and shapes of human bodies as well as viewpoints in the 3D space. Furthermore, Gait3D also provides 2D silhouettes and 2D/3D keypoints obtained by the state-of-the-art image segmentation and pose estimation methods fine-tuned on our dataset. Therefore, multi-modal data can be explored for gait recognition. In addition, Gait3D is collected in a large supermarket in which people usually walk at irregular speeds and routes, and can be occluded by other people or objects. The above properties also make Gait3D a scalable but challenging dataset for gait recognition which can be reflected by the evaluation in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Collection and Pre-processing</head><p>To collect a high-quality in-the-wild dataset for real applications, we collect the seven-day raw videos from 39 cameras mounted in a large supermarket. The scenes of the cameras include the entrance, the goods shelf area, the freezer area, the dining area, the checkout counter, etc. For the videos each day, we randomly sample two segments of continuous two-hour videos. At last, we obtain about 1,090 hours videos with 1,920 ? 1,080 resolution and 25 FPS. Note that, we are authorized by the management of the supermarket to access and process the data for research purposes. In addition, all subjects were noticed that the data is collected only for research purposes. With the videos, we use the open-source FFmpeg 2 to decode the raw videos into frames at 25 FPS to keep the continuity of gait sequences. To guarantee the high quality of the dataset, the annotation process is performed in three main steps as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dataset Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Person detection and tracking from frames</head><p>For each frame extracted from the raw videos, we adopt the CenterNet <ref type="bibr" target="#b56">[62]</ref> fine-tuned on our dataset as the person detector since it is an efficient anchor-free object detector 3 . To achieve accurate person tracking in videos, we exploit the Intersection-over-Union (IoU) and person re-identification (ReID) features of bounding boxes in two adjacent frames to measure their similarity. The ReID feature is extracted by an open-source person ReID framework, FastReID <ref type="bibr" target="#b3">4</ref> [12] pretrained on several public person ReID datasets. When two persons are highly overlapped, the tracking algorithm can easily misjudge them as one person, i.e., ID switching. To solve this problem, we employ human annotators to clean sequences that may contain more than one pedestrian. By this means, we guarantee that each sequence only belongs to one person. Then, we discard the sequences shorter than 25 frames or longer than 500 frames and obtain about 50,000 sequences in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Cross-camera sequence matching</head><p>With the above sequences, we should cluster the sequences of the same person in all cameras. To achieve effective and efficient cross-camera matching of the same person, we also utilize the person ReID features obtained by FastReID <ref type="bibr">[12]</ref>. For each sequence, we first use a pose estimation model, i.e., HRNet 5 <ref type="bibr" target="#b37">[43]</ref> fine-tuned on our dataset 6 , to select a high-quality frame for cross-camera matching. After that, we utilize FastReID to extract the features of the selected frames of all sequences. Through an unsupervised clustering method, i.e., DBSCAN [8], we roughly obtain 5,336 clusters of sequences. Then, we employ human annotators to filter out the outlier sequences in each group. By discarding the groups containing only one sequence, we finally obtain 4,000 subjects and 25,309 sequences for generating the gait representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Generation of gait representations</head><p>With the clean sequences of 4,000 IDs, we generate the 3D SMPL parameter, 3D mesh, 3D pose, 2D silhouette, and 2D pose for each frame. For the 3D SMPL, 3D mesh, and 3D pose, we exploit a state-of-the-art 3D human mesh recovery method, ROMP 7 <ref type="bibr" target="#b27">[34]</ref>, since it can efficiently output these three representations in an end-to-end framework. For the 2D silhouette, we use the semantic segmentation method, HRNet-segmentation 8 <ref type="bibr" target="#b37">[43]</ref>, to obtain the silhouette of the person in each frame. For the 2D pose, we also utilize the HRNet to estimate the 2D keypoints of the person in each frame. We keep the original resolution and aspect ratio of the frame without resizing or normalization. Some examples of gait representations in our dataset are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. It is worth noting that we will only release the generated gait representations but not release any RGB frames to protect the privacy of the subjects.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Dataset Statistics and Evaluation Protocol</head><p>The statistics about the sizes of frames, ID numbers over sequence numbers, and sequence numbers over sequence lengths are shown in <ref type="figure" target="#fig_6">Figure 4</ref>. From <ref type="figure" target="#fig_6">Figure 4</ref> (a), we can find that most frames range from 100 ? 400 ? 200 ? 800 which are larger than person bounding boxes of existing datasets. <ref type="figure" target="#fig_6">Figure 4</ref> (b) shows that most IDs have 2 ? 25 sequences, which guarantees the high reappearance times of subjects. <ref type="figure" target="#fig_6">Figure 4</ref> (c) reflects that most sequences are longer than 50 frames (2 seconds) and the longest sequence has 500 frames, which reflects the complexity of the gait sequences in the unconstrained scenes. The above statistics demonstrate that the Gait3D dataset is scalable but challenging for gait recognition research.</p><p>To facilitate the research, we split the 4,000 IDs of the Gait3D dataset into the train/test subsets with 3,000/1,000 IDs, respectively. For the test set, we further randomly select one sequence from each ID to build the query set with 1,000 sequences, while the rest of the sequences become the gallery set with 5,369 sequences. Our evaluation protocol is based on the open-set instance retrieval setting like existing gait recognition datasets <ref type="bibr" target="#b10">[17]</ref> and the person ReID task <ref type="bibr" target="#b54">[60]</ref>. Given a query sequence, we measure its similarity between all sequences in the gallery set. Then a ranking list of the gallery set is returned by the descending order of the similarities. We report the average Rank-1 and Rank-5 identification rates over all query sequences. We also adopt the mean Average Precision (mAP) and mean Inverse Negative Penalty (mINP) <ref type="bibr" target="#b49">[55]</ref> which consider the recall of multiple instances and hard samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In the experiments, we first evaluate several State-Of-The-Art (SOTA) 2D gait recognition methods and our SMPLGait on the Gait3D dataset. Then, we analyze the influence of the frame size, the sequence length, and the scale of training IDs on the performance of gait recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation of Existing Methods</head><p>Here, we evaluate eight SOTA 2D gait recognition methods including six model-free methods and two model-based methods. We also compare our 3D gait recognition method (SMPLGait) with these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Model-free Approaches</head><p>The details of model-free approaches are as follows:</p><p>1) GEINet <ref type="bibr" target="#b25">[32]</ref> is one of the first methods that adopts a four-layer CNN to learn gait features from GEIs using the cross-entropy loss.</p><p>2) GaitSet <ref type="bibr" target="#b4">[5]</ref> is the representative method that utilizes a 10-layer CNN to directly learn discriminative gait features from silhouette sequences. The GaitSet is trained by the batch all triplet loss <ref type="bibr">[13]</ref>.</p><p>3) GaitPart <ref type="bibr" target="#b6">[9]</ref> adopts the idea of multi-scale feature learning. It horizontally divides a silhouette image into fixed parts to learn discriminative micro-motion features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) GLN [15]</head><p>is an efficient and effective method to learn compact features from gait sequences, which achieves the SOTA performance using only a 256-D feature. 5) GaitGL <ref type="bibr" target="#b15">[22]</ref> is also a CNN-based framework to learn both global and local features from gait sequences. 6) CSTL <ref type="bibr" target="#b9">[16]</ref> applies multi-scale learning on the temporal dimension of the sequence to learn both long-term and short-term motion for gait recognition.</p><p>Implementation Details: During training, we train the above models except GLN with the same configuration. The batch size is 32 ? 4 ? 30, where 32 denotes the number of IDs, 4 denotes the number of training samples per ID, and 30 is the sequence length. The models are trained for 1,200 epochs with the initial Learning Rate (LR)=1e-3 and the LR is multiplied by 0.1 at the 200-th and 600-th epochs. The optimizer is Adam <ref type="bibr" target="#b12">[19]</ref> and the weight decay is set to 5e-4. For GLN, we follow the two-stage training as in <ref type="bibr">[15]</ref>. The model trained in the first stage is used as the pretrained model for the second stage. Both of the two stages are trained with the same configuration of other methods. During testing, we use the cosine similarity to measure the similarity between each pair of query and gallery sequences. For the GaitSet, GaitPart, GLN, and GaitGL models, we adopt the implementations in the open-source OpenGait toolbox 9 since they outperform the original codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Model-based Approaches</head><p>We compare two representative model-based methods which use 2D or 3D skeletons as the input. 1) PoseGait <ref type="bibr" target="#b14">[21]</ref> first exploits OpenPose <ref type="bibr" target="#b3">[4]</ref> to extract the 2D keypoints from RGB frames, then uses the method in [6] to estimate the 3D keypoints of human bodies. Based on the 3D skeletons, it defines several parameters such as joint angle, limb length, and joint motion together with the pose features as the gait representation. In our implementation, we train it for 700 epochs with a batch size of 128. The LR is set to 1e-3. The optimizer is Adam <ref type="bibr" target="#b12">[19]</ref> and weight decay is equal to 5e-4.</p><p>2) GaitGraph <ref type="bibr" target="#b31">[38]</ref> is a recent model-based gait recognition method. It models the 2D skeleton as a graph and adopts a Graph Convolution Network, i.e., the Res-GCN <ref type="bibr" target="#b26">[33]</ref>, to learn features by the contrastive loss. We train GaitGraph in two stages. The setting of the first stage is the same as PoseGait, and the model trained in the first stage is used as the pre-trained model of the second stage. In the second stage, we fine-tune it for 250 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Implementation Details of the SMPLGait</head><p>For our SMPLGait, we use the loss in Equ. 4 for training. In 3D-STN, we set the dropout rate to 0.2 for FC layers. The hyper-parameters in Equ. 4 are set as ?=1.0 and ?=0.1.</p><p>Other settings are the same as those in Section 5.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Experimental Results</head><p>The results of model-free methods, model-based methods, and our SMPLGait are listed in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>For model-free methods, we can first observe that the overall performance of the SOTA methods is much worse 9 https://github.com/ShiqiYu/OpenGait than their performance on in-the-lab datasets like the CASIA-B <ref type="bibr" target="#b50">[56]</ref> and OU-ISIR series <ref type="bibr" target="#b10">[17,</ref><ref type="bibr" target="#b29">36]</ref>. This reflects that there is a huge gap between the in-the-lab research and the in-the-wild application that is much more challenging. Meanwhile, the performance of the SOTA model-free methods varies significantly. For example, the GEI-based method, i.e., GEINet obtains the worst results, which indicates that the GEIs discard too much useful information for gait recognition. Moreover, the methods considering the order of the frames in sequences, i.e., GaitPart, GLN, GaitGL, and CSTL, obtain lower accuracy. It means that the temporal information in the wild scene is hard to learn, because people may stop then continue to walk at varying speeds and routes in unconstrained scenarios. On the contrary, the methods considering frames as an unordered set, i.e., GaitSet, obtain better results.</p><p>For model-based methods, we can find that they are greatly worse than model-free methods on the Gait3D dataset. This is because the input of the model-based methods only has a few sparse human body joints, which seriously lacks useful gait information, such as body shape, appearance, and so on. In addition, the walking speed and route are uncertain in real scenarios, which also greatly affects the performance of the model-based methods that aim to model the temporal dynamics of the human body.</p><p>Finally, our SMPLGait outperforms other methods by a large margin, which indicates the potential of 3D representations for gait recognition in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Ablation Study of SMPLGait</head><p>We also conduct an ablation study on the key components in SMPLGait by removing the 3D branch (SMPLGait w/o 3D). <ref type="bibr" target="#b7">10</ref> The results are listed in <ref type="table" target="#tab_2">Table 2</ref>. This comparison shows that the integration of 2D and 3D representations can better address the challenges of gait recognition in the wild. <ref type="bibr" target="#b12">19</ref>  <ref type="figure">Figure 5</ref>. The effect of frame numbers in sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">More Analysis of the Gait3D Dataset</head><p>We choose two SOTA gait recognition methods, i.e., GaitPart and GaitSet, and our SMPLGait (Ours) to analyze the influence of input size, frame number in sequences, and training ID number on the accuracy. All the models are evaluated on the whole Gait3D test set.</p><p>Input Size. We explore two input sizes of 88 ? 128 and 44 ? 64 for the compared methods, as shown in <ref type="table" target="#tab_2">Table 2</ref>. From the results, we can observe that the performance of almost all methods is improved with larger input size. There is an exception, i.e., GaitGL, which obtain worse accuracy with larger input size. This may be because that GaitGL adopts the 3D CNN as the backbone. When using a larger input size, the 3D CNN learns more misalignment information about frames in physical space, which makes it more difficult to be optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Training Frames</head><p>We randomly sample 10? 50 frames from original gait sequences during training. The Rank-1 accuracy is illustrated in <ref type="figure">Figure 5</ref>. The results show that as the number of frames increases the performance first increases and then decreases, while the best performance occurs around 30 frames per sequence. This indicates that more frames could not bring higher accuracy. The reason may be that there is a lot of redundant or noisy information caused by uncertain speeds and routes of persons, which will bring ambiguous features for gait recognition.</p><p>Scale of Training IDs We fix other settings and use 0.5K ? 3KIDs with an increment of 0.5K for training. As shown in <ref type="figure" target="#fig_7">Figure 6</ref>, the performance of the models grows stably with more training IDs. These results reflect the scalability of our Gait3D dataset.</p><p>More experiments and exemplar results on Gait3D can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Ethical Issues. There are two main ethical issues of this paper: 1) privacy, and 2) data bias. For the first issue, we will try our best to protect the privacy of the subjects involved in our dataset. Firstly, we will not release any human cognizable data like original videos, RGB frames, <ref type="bibr" target="#b9">16</ref>  and bounding boxes of persons. Second, the dataset will be distributed only for research purposes via the case-by-case application with a strict license. To eliminate data bias, the genders and ages of subjects are relatively balanced. Future Work. Despite the proposed baseline method for 3D gait recognition, there are many potential directions for this challenging task. For example, one direction is to study how to design a deep CNN for learning more discriminative features directly from 3D meshes. The second direction is how to learn the temporal information of gait representation, because the walking speed and route in Gait3D are irregular, it is significantly different from the datasets built in the lab. Another interesting direction is how to fuse the multi-modal information like silhouette, 2D/3D skeleton, and 3D mesh for gait recognition in the wild.</p><p>More discussions about the limitations and potential negative impact can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Gait recognition in the wild faces significant challenges such as extreme viewpoint changes, occlusions of the human body, and complex clutter in the environment. Existing methods using 2D silhouettes or skeletons will fail in the wild because crucial information like 3D viewpoints and shapes of human bodies is discarded. Therefore, this paper proposes a 3D SMPL model-based framework (SMPLGait) which is the first method to explore dense 3D representations for gait recognition in the wild. To facilitate the research, we build the first large-scale 3D gait recognition dataset (Gait3D) from cameras deployed in a large supermarket. It provides diverse gait representations including 3D meshes, 3D SMPLs, 3D poses, 2D silhouettes, and 2D poses for over 25,000 gait sequences of 4,000 subjects. We hope Gait3D can provide researchers with a new perspective on gait recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix: Details of the SMPLGait</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Details of the Network Structure</head><p>The Silhouette Learning Network (SLN) consists of six convolutional layers, and each convolutional layer is followed by LeakyReLU whose negative slope is equal to 0.01. The structure of SLN is inspired by GaitSet <ref type="bibr" target="#b4">[5]</ref> and OpenGait 11 . The detailed parameters are listed in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layers</head><p>Kernel # Kernel Size Stride Padding  </p><formula xml:id="formula_6">Conv1 64 5?5 1 2 LeakyReLU (0.01) - - - - Conv2 64 3?3 1 1 LeakyReLU (0.01) - - - - Max Pooling - 2?2 2 0 Conv3 128 3?3 1 1 LeakyReLU (0.01) - - - - Conv4 128 3?3 1 1 LeakyReLU (0.01) - - - - Max Pooling - 2?2 2 0 Conv5 256 3?3 1 1 LeakyReLU (0.01) - - - - Conv6 256 3?3 1 1 LeakyReLU (0.01) - - - -</formula><formula xml:id="formula_7">FC1 128 0.0 BN1 - - ReLU - - FC2 256 0.2 BN2 - - ReLU - - FC3 h ? w 0.2 BN3 - - ReLU - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Appendix: Additional Experimental Results</head><p>In this section, we first analyze the effect of sequence length on accuracy during inference. Then, we conduct the cross-domain experiments to reflect the domain gap between existing datasets and our Gait3D dataset. At last, we provide some exemplar results of our SMPLGait framework for gait recognition to qualitatively demonstrate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Effect of the Lengths of Test Sequences</head><p>This subsection analyzes the influence of the length of testing sequences on the accuracy of gait recognition. We sample 10% ? 100% frames with the 10% increment from the sequences during testing. The plots are demonstrated in <ref type="figure">Figure 7</ref>. From the results, we can observe that the accuracy is improved with the increasing frames of the sequence. Therefore, in real practice, we need to make a trade-off between accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Cross-domain Experiments</head><p>In this subsection, we analyze the domain gap between our Gait3D dataset and existing widely used datasets including CASIA-B <ref type="bibr" target="#b50">[56]</ref>, OU-LP <ref type="bibr" target="#b10">[17]</ref> and the recently released GREW <ref type="bibr" target="#b57">[63]</ref>. Because existing datasets do not provide 3D representations, we only adopt the 2D silhouettes for gait representations in our experiments. We adopt the GaitSet <ref type="bibr" target="#b4">[5]</ref> for the cross-domain experiments since it is the SOTA model-free method on the Gait3D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Evaluation Protocol</head><p>In the cross-domain experiments, we train the GaitSet model on the training set of one source dataset and evaluation the trained model on the testing set of one target dataset. For CASIA-B, OU-LP, and Gait3D, we use the official train/test split for training and testing. Because the test set of GREW <ref type="bibr" target="#b57">[63]</ref> is not released publicly for evaluation, we randomly sample 1,000 IDs from the training set of GREW for evaluation. For the sampled 1,000 IDs, we further randomly select one sequence from each ID to build the query set with 1,000 sequences, while the rest of the sequences becomes the gallery set with 4,095 sequences. We utilize Rank-1 (R-1), Rank-5 (R-5), and mAP as the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Main Results</head><p>The results of the cross-domain evaluation are listed in <ref type="table" target="#tab_7">Table 5</ref>. From the results, we first find that the GaitSet models trained on the in-the-lab datasets, i.e., CASIA-B <ref type="bibr" target="#b50">[56]</ref> and OU-LP <ref type="bibr" target="#b10">[17]</ref> obtain very poor results, only 6.90% and 6.10% Rank-1. This reflects that there is a huge domain gap between the in-the-lab research and the in-the-wild application.</p><p>Next, we observe that the model trained on GREW then tested on Gait3D only obtains <ref type="bibr" target="#b9">16</ref> model trained on Gait3D then tested on GREW achieves a much higher Rank-1, i.e., 43.86%. It is worth noting that the training set of GREW has 20,000 IDs, while our Gait3D only contains 3,000 IDs. This demonstrates the significant domain gap between our Gait3D and GREW although the two datasets are both collected in the wild. Moreover, it indicates that the model trained on Gait3D has a more powerful capability of generalization than the one trained on GREW. At last, we can see that the GaitSet trained on Gait3D achieves competitive accuracy on in-the-lab datasets, i.e., Rank-1 of 66.71% on CASIA-B and 97.84% on OU-LP which is close to the model trained on OU-LP (99.89%) in our implementation). These results further prove that there is a huge gap between the in-the-lab dataset and in-the-wild application, while our Gait3D enables the model to learn more generalized gait representations. From <ref type="figure" target="#fig_8">Figure 8</ref> -10, we can observe that the 3D representations can work well in multi-viewpoint, occluded, and multi-person cases. By this means, 3D meshes can provide more information about shapes, poses, and viewpoints of human bodies, which can help improve the accuracy of gait recognition in real-world scenes. <ref type="figure" target="#fig_1">Figure 11</ref> illustrates a bad case in which the top-3 persons with similar clothes and shapes seriously interfere with the matching results. This indicates that very similar clothing and shapes of persons are one of the main challenges of gait recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Exemplar Results of SMPLGait</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Appendix: Discussion</head><p>Limitations. Although we proposed a 3D gait recognition framework, its performance still has a large space to improve for practical applications. In addition, we only exploit a few frames of gait sequences, e.g., 30 frames, in our framework. The temporal dynamics are not fully explored for gait recognition in the wild.</p><p>Potential Negative Impact. The potential negative outcomes mainly come from the fact that, with the largescale deployment of the urban monitoring network, if the accuracy of gait recognition in the real scenarios is greatly improved in the future, it may cause some privacy and security issues. To minimize these risks, our Gait3D dataset will be distributed only for research purposes via the caseby-case application with a strict license. In summary, we will try our best to protect the privacy of the subjects. We hope that the development of gait recognition can help to create a better human society, as well as better services for human beings, such as assisting the police to solve crimes, looking for lost people, and so on.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Different gait representations of the same person from two viewpoints. Compared with silhouettes and skeletons, 3D meshes retain the shapes and viewpoints of the human body in the 3D space. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the SMPLGait framework for 3D gait recognition in the wild.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Examples of gait representations in the Gait3D dataset. The sizes are normalized for visualization. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Statistics of frame sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ID # over sequence #.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Statistics about the Gait3D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>The effect of different training ID numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 -</head><label>8</label><figDesc>11 provides several exemplar results of our SMPLGait framework on the Gait3D dataset. The top two rows with blue bounding boxes are the silhouette sequence and 3D Mesh sequence of the query, respectively. The rows following the query are the top-5 gallery sequences ranked by their similarities to the query sequence. The results with green bounding boxes are the correctly matched sequences, while those with red bounding boxes are wrong results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .Figure 10 .</head><label>810</label><figDesc>Exemplar results of SMPLGait on the Gait3D. 16 consecutive frames are sampled from each sequence for visualization. This case shows that our method obtains good results when the samples are high-quality. (Best viewed in color.) [6] Ching-Hang Chen and Deva Ramanan. 3d human pose estimation = 2d pose estimation + matching. In CVPR, pages 5759-5767, 2017. 7 [7] Xiaodong Chen, Xinchen Liu, Wu Liu, Xiaoping Zhang, Yongdong Zhang, and Tao Mei. Explainable person re-identification with attribute-guided metric distillation. In ICCV, pages 11793-11802, 2021. 2 [8] Martin Ester, Hans-Peter Kriegel, J?rg Sander, and Xiaowei Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In KDD, pages 226-231, Exemplar results of SMPLGait on the Gait3D. This example shows that even when the silhouettes are of low quality, the 3D meshes can help the model obtain correct results. (Best viewed in color.) [11] Ju Han and Bir Bhanu. Individual recognition using gait energy image. IEEE TPAMI, 28(2):316-322, 2006. 1, 2, 3 [12] Lingxiao He, Xingyu Liao, Wu Liu, Xinchen Liu, Peng Cheng, and Tao Mei. FastReID: A pytorch toolbox for general instance re-identification. CoRR, abs/2006.02631, 2020. 5 [13] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>A bad case of SMPLGait on the Gait3D. The top-3 results contain persons wearing very similar clothes to the query, which is a very challenging condition of gait recognition. (Best viewed in color.) defense of the triplet loss for person re-identification. CoRR, abs/1703.07737, 2017. 6 [14] Md. Altab Hossain, Yasushi Makihara, Junqiu Wang, and Yasushi Yagi. Clothing-invariant gait identification using part-based clothing categorization and adaptive weight control. PR, 43(6):2281-2291, 2010. 3 [15] Saihui Hou, Chunshui Cao, Xu Liu, and Yongzhen Huang. Gait lateral network: Learning discriminative and compact</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of publicly available datasets for gait recognition. Speed, Wild, and 3D-View indicate whether the dataset contains inconstant walking speed, is captured in the wild, and has viewpoint variations in the 3D space, respectively.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Year Subject #</cell><cell>Seq #</cell><cell>Cam #</cell><cell>Data Type</cell><cell>Speed Wild 3D-View</cell></row><row><cell>CASIA-A [44]</cell><cell>2003</cell><cell>20</cell><cell>240</cell><cell>3</cell><cell>RGB, Silh.</cell></row><row><cell>USF HumanID [31]</cell><cell>2005</cell><cell>122</cell><cell>1,870</cell><cell>2</cell><cell>RGB</cell></row><row><cell>CASIA-B [56]</cell><cell>2006</cell><cell>124</cell><cell>13,640</cell><cell>11</cell><cell>RGB, Silh.</cell></row><row><cell>CASIA-C [37]</cell><cell>2006</cell><cell>153</cell><cell>1,530</cell><cell>1</cell><cell>Infrared, Silh.</cell></row><row><cell cols="2">OU-ISIR Speed [39] 2010</cell><cell>34</cell><cell>306</cell><cell>1</cell><cell>Silh.</cell></row><row><cell>OU-ISIR-LP [17]</cell><cell>2012</cell><cell>4007</cell><cell>31,368</cell><cell>2</cell><cell>Silh.</cell></row><row><cell>OU-LP Bag [40]</cell><cell>2018</cell><cell>62,528</cell><cell>187,584</cell><cell>1</cell><cell>Silh.</cell></row><row><cell>OU-MVLP [36]</cell><cell>2018</cell><cell>10,307</cell><cell>288,596</cell><cell>14</cell><cell>Silh.</cell></row><row><cell cols="2">OU-MVLP Pose [1] 2020</cell><cell>10,307</cell><cell>288,596</cell><cell>14</cell><cell>2D Pose</cell></row><row><cell>GREW [63]</cell><cell>2021</cell><cell>26,345</cell><cell>128,671</cell><cell>882</cell><cell>Silh., 2D/3D Pose, Flow</cell></row><row><cell>Gait3D</cell><cell>-</cell><cell>4,000</cell><cell>25,309</cell><cell>39</cell><cell>Silh., 2D/3D Pose, 3D Mesh&amp;SMPL</cell></row></table><note>as the input sequence, where x i ? R H?W</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the state-of-the-art gait recognition methods on Gait3D. As the inputs of the model-based methods, i.e., PoseGait and GaitGraph, are unrelated to the frame size, we only report one group of results.</figDesc><table><row><cell cols="2">Input Size (W?H)</cell><cell></cell><cell cols="2">88?128</cell><cell></cell><cell></cell><cell>44?64</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Publication</cell><cell cols="8">R-1 (%) R-5 (%) mAP (%) mINP R-1 (%) R-5 (%) mAP (%) mINP</cell></row><row><cell>GEINet [32]</cell><cell>ICB 2016</cell><cell>7.00</cell><cell>16.30</cell><cell>6.05</cell><cell>3.77</cell><cell>5.40</cell><cell>14.20</cell><cell>5.06</cell><cell>3.14</cell></row><row><cell>GaitSet [5]</cell><cell>AAAI 2019</cell><cell>42.60</cell><cell>63.10</cell><cell>33.69</cell><cell>19.69</cell><cell>36.70</cell><cell>58.30</cell><cell>30.01</cell><cell>17.30</cell></row><row><cell>GaitPart [9]</cell><cell>CVPR 2020</cell><cell>29.90</cell><cell>50.60</cell><cell>23.34</cell><cell>13.15</cell><cell>28.20</cell><cell>47.60</cell><cell>21.58</cell><cell>12.36</cell></row><row><cell>GLN [15]</cell><cell>ECCV 2020</cell><cell>42.20</cell><cell>64.50</cell><cell>33.14</cell><cell>19.56</cell><cell>31.40</cell><cell>52.90</cell><cell>24.74</cell><cell>13.58</cell></row><row><cell>GaitGL [22]</cell><cell>ICCV 2021</cell><cell>23.50</cell><cell>38.50</cell><cell>16.40</cell><cell>9.20</cell><cell>29.70</cell><cell>48.50</cell><cell>22.29</cell><cell>13.26</cell></row><row><cell>CSTL [16]</cell><cell>ICCV 2021</cell><cell>12.20</cell><cell>21.70</cell><cell>6.44</cell><cell>3.28</cell><cell>11.70</cell><cell>19.20</cell><cell>5.59</cell><cell>2.59</cell></row><row><cell>PoseGait [21]</cell><cell>PR 2020</cell><cell>0.24</cell><cell>1.08</cell><cell>0.47</cell><cell>0.34</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GaitGraph [38]</cell><cell>arXiv 2021</cell><cell>6.25</cell><cell>16.23</cell><cell>5.18</cell><cell>2.42</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">SMPLGait w/o 3D Ours</cell><cell>47.70</cell><cell>67.20</cell><cell>37.62</cell><cell>22.24</cell><cell>42.90</cell><cell>63.90</cell><cell>35.19</cell><cell>20.83</cell></row><row><cell>SMPLGait</cell><cell>Ours</cell><cell>53.20</cell><cell>71.00</cell><cell>42.43</cell><cell>25.97</cell><cell>46.30</cell><cell>64.50</cell><cell>37.16</cell><cell>22.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Details of the SLN Network. Note that the h and w listed in the table are the height and width of the feature map from SLN. For convenient computation, we set h = w = max(h, w) in our experiments.</figDesc><table><row><cell>The 3D Spatial Transformation Network (3D-STN)</cell></row><row><cell>consists of several Fully Connected (FC) layers with</cell></row><row><cell>dropout, Batch Normalization (BN) layers, and ReLU</cell></row><row><cell>activation functions. The details of 3D-STN are listed</cell></row><row><cell>in Layers Neuron # Dropout Rate</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Details of the 3D-STN Network. (Input Size: 88?128).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Results of cross-domain experiments. The method is trained on each source dataset and directly tested on the target datasets.</figDesc><table><row><cell>.50% Rank-1, while the</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the convenience of notation, we omit the channel of the feature map.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://ffmpeg.org/ under the GNU LGPL License v2.1. 3 4,000 person bounding boxes are labeled for fine-tuning the detector. 4 https : / / github . com / JDAI -CV / fast -reid under the Apache 2.0 license.(d) Silhouettes (e) 2D Skeletons (c) 3D Skeletons (b) 3D Meshes (a) RGB Frames</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https : / / github . com / HRNet / HRNet -Human -Pose -Estimation under the MIT License. 6 4,000 images are labeled to fine-tune the pose estimator. 7 https : / / github . com / Arthur151 / ROMP under the MIT License. 8 https : / / github . com / HRNet / HRNet -Semantic -Segmentation under the MIT license.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">It should be noticed that SMPLGait w/o 3D is equal to OpenGait Baseline<ref type="bibr" target="#b7">[10]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://github.com/ShiqiYu/OpenGait</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Performance evaluation of model-based gait on multi-view very large population database with pose sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rijun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TBBIS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SCAPE: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="408" to="416" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model-based 3d gait biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunawan</forename><surname>Ariyanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCB</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">OpenPose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GaitSet: Regarding gait as a set for cross-view gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Exemplar results of SMPLGait on the Gait3D. This example reflects that our method can work well when part of the person is occluded. (Best viewed in color</title>
		<idno>1996. 5</idno>
		<imprint/>
	</monogr>
	<note>Figure 9</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GaitPart: Temporal part-based model for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjie</forename><surname>Chao Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunshui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="14213" to="14221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanfu</forename><surname>Chao Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Opengait</surname></persName>
		</author>
		<ptr target="https://github.com/ShiqiYu/OpenGait.7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">representations for gait recognition</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextsensitive temporal feature learning for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duowang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The OU-ISIR gait database comprising the large population dataset and performance evaluation of gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haruyuki</forename><surname>Iwama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIFS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">VIBE: video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A model-based gait recognition method with body pose and human prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rijun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PR</publisher>
			<biblScope unit="volume">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gait recognition via effective global-local feature representation and local temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beibei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NTU RGB+D 120: A largescale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2684" to="2701" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recent advances in monocular 2d and 3d human pose estimation: A deep learning perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Largescale vehicle re-identification in urban surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyuan</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PROVID: progressive and multimodal vehicle reidentification for large-scale urban surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="645" to="658" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Autogpart: Intermediate supervision search for generalizable 3d part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<idno>abs/2203.06558, 2022. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SMPL: a skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1-248:16</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gait analysis of gender and age using a large-scale multiview gait database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Mannami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing and recognizing walking figures in XYT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sourabh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="469" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The HumanID gait challenge problem: Data sets, performance, and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Jonathon</forename><surname>Sudeep Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyi</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isidro</forename><forename type="middle">Robledo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GEINet: View-invariant gait recognition using a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Shiraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daigo</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomio</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICB</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1625" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Putting people in their place: Monocular regression of 3d people in depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-view large population gait dataset and its performance evaluation for cross-view gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daigo</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomio</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPSJ TCVA</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient night gait recognition based on template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoliang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1000" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">GaitGraph: Graph convolutional network for skeleton-based gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torben</forename><surname>Teepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>H?rmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/2101.11228, 2021. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Silhouette transformation based on walking speed for gait identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Tsuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The OU-ISIR large population gait database with real-life carried object and its performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Zasim Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Ngo Thanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daigo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPSJ TCVA</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d tracking for gait characterization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey on gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vir</forename><forename type="middle">V</forename><surname>Phoha</surname></persName>
		</author>
		<idno>89:1-89:35</idno>
	</analytic>
	<monogr>
		<title level="j">ACM CSUR</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Silhouette analysis-based gait recognition for human identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised graph association for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8320" to="8329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A comprehensive study on cross-view gait based human identification with deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The OU-ISIR gait database comprising the large population dataset with age and performance evaluation of age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gakuto</forename><surname>Ogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPSJ TCVA</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Automated person recognition by walking and running via model-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chew-Yean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Yam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">N</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>PR</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1057" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recognition of walking humans in 3d: Initial results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideo</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep multi-view enhancement hashing for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1445" to="1451" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Taskadaptive attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Depth image denoising using nuclear norm and learning graph model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<idno>122:1-122:17</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TOMM</publisher>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Age-invariant face recognition by multifeature fusionand decomposition with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiehua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1s</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Precise no-reference image quality evaluation based on distortion identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3s</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno>abs/2001.04193</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoliang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Crossview gait recognition with deep universal linear embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9095" to="9104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gait recognition via disentangled representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousef</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4710" to="4719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3d gait recognition using multiple cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for 3d keypoint estimation via view consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Karpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (12)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11216</biblScope>
			<biblScope unit="page" from="141" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Objects as points. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gait recognition in the wild: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianda</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
