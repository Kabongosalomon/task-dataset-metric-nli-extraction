<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zlocha</surname></persName>
							<email>martin.zlocha15@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analysis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
							<email>qi.dou@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analysis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
							<email>b.glocker@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Image Analysis Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate, automated lesion detection in Computed Tomography (CT) is an important yet challenging task due to the large variation of lesion types, sizes, locations and appearances. Recent work on CT lesion detection employs two-stage region proposal based methods trained with centroid or bounding-box annotations. We propose a highly accurate and efficient one-stage lesion detector, by re-designing a RetinaNet to meet the particular challenges in medical imaging. Specifically, we optimize the anchor configurations using a differential evolution search algorithm. For training, we leverage the response evaluation criteria in solid tumors (RECIST) annotation which are measured in clinical routine. We incorporate dense masks from weak RECIST labels, obtained automatically using GrabCut, into the training objective, which in combination with other advancements yields new state-of-the-art performance. We evaluate our method on the public DeepLesion benchmark, consisting of 32,735 lesions across the body. Our one-stage detector achieves a sensitivity of 90.77% at 4 false positives per image, significantly outperforming the best reported methods by over 5%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Detection and localization of abnormalities in Computed Tomography (CT) scans is a critical routine task for radiologists. Accurate, automated detection of suspicious regions could greatly support screening, diagnosis and monitoring of disease progression. Most previous work focuses on a specific type of lesion within a relatively constrained (anatomical) context, such as lymph nodes, lung nodules and brain microbleeds. Recently, Yan et al. <ref type="bibr" target="#b14">[15]</ref> pioneered the study of universal lesion detection and introduced today's largest data repository, i.e., the DeepLesion dataset. Detecting diverse types of lesions across the body using one single model is very challenging due to the large variation of lesion types, sizes, locations and heterogeneous appearances. For example, DeepLesion consists of eight types of lesions with diameters ranging from 0.21 to 342.5 mm. In addition, the lesions may appear with limited contrast compared to nearby normal tissue, which further increases the difficulty of detecting subtle signs of disease.</p><p>Automated lesion detection has been central in medical image computing. Recent work employs two-stage methods with candidate proposal and false positive reduction steps. State-of-the-art performance on the DeepLesion benchmark has been achieved by Yan et al. <ref type="bibr" target="#b12">[13]</ref>. They propose a two-stage, region-based method called 3DCE to effectively incorporate 3D context into 2D regional CNNs. Their method achieves a sensitivity of 85.65% at 4 false positives per image, outperforming the popular detection method of Faster R-CNN <ref type="bibr" target="#b6">[7]</ref> on the same dataset. However, their detection sensitivity for small lesions is much lower, which is an important limitation in the critical context of detecting early signs of diseases.</p><p>Some recent work take advantage of mask information for improving detection accuracy. Jaeger et al. <ref type="bibr" target="#b3">[4]</ref> propose a Retina U-Net, showing that aggregating pixel-wise supervision to train the detector is helpful. Their method shows effectiveness in two scenarios, i.e., lung lesions in CT and breast lesions in MRI. As pixel-wise annotations are tedious and expensive to obtain, Tang et al. <ref type="bibr" target="#b11">[12]</ref> generate pseudo masks by fitting ellipses based on the response evaluation criteria in solid tumors (RECIST) <ref type="bibr" target="#b1">[2]</ref> diameters. Using a 2D Mask R-CNN <ref type="bibr" target="#b2">[3]</ref> with generated lesion masks and other strategies, <ref type="bibr" target="#b11">[12]</ref> achieves a sensitivity of 84.38% at 4 false positives per image on DeepLesion dataset. Their pseudo-mask generation procedure relies heavily on the assumption of elliptical geometry of lesions, which may yield imprecise masks limiting the efficacy of dense supervision.</p><p>We propose a one-stage detector which directly localizes lesions without the need of candidate region proposals. To meet the specific challenge of detecting small lesions, we revisit the RetinaNet <ref type="bibr" target="#b5">[6]</ref> and optimize the feature pyramid scheme and anchor configuration by employing a differential evolution search algorithm. To enhance the model, we leverage high-quality dense masks obtained automatically from weak RECIST labels using GrabCut <ref type="bibr" target="#b7">[8]</ref>. Incorporating these generated masks into pixel-wise supervision shows great benefit for training the detector. In addition, we make use of the coherence between lesion mask predictions and bounding-box regressions to calibrate the detector outputs. We further investigate recent strategies for boosting the detection performance, such as integrating attention mechanism into our feature pyramids. We evaluate the contributions of each part using the DeepLesion benchmark, achieving a new state-of-the-art sensitivity of 90.77% at 4 false positives per image, significantly outperforming the currently best performing method 3DCE <ref type="bibr" target="#b12">[13]</ref> by over 5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Improving RetinaNet</head><p>An overview of our proposed one-stage lesion detector is illustrated in <ref type="figure" target="#fig_1">Fig 1 (a)</ref>. We first describe the model design before elaborating on how we obtain dense masks from weak RECIST labels and incorporate them into training process. We then show the attention mechanism for further improving detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Design with Optimized Anchor Configuration</head><p>The backbone of our approach is a RetinaNet <ref type="bibr" target="#b5">[6]</ref>, a recent one-stage method for object detection. The use of a focal loss addresses the common problem of class imbalance in detection tasks. The feature pyramids and lateral connections with a top-down architecture <ref type="bibr" target="#b4">[5]</ref> are adopted for detecting objects at different scales. This is an important difference with methods such as 3DCE <ref type="bibr" target="#b12">[13]</ref>, since  the feature pyramids can effectively capture information about lesions of varying sizes including very small ones. Our specific network follows the structure of VGG-19 <ref type="bibr" target="#b9">[10]</ref>. We also explored ResNet-50 as used originally, but its performance was worse on DeepLesion, which is in line with results reported in <ref type="bibr" target="#b13">[14]</ref>. The anchor configuration is crucial for the detector, and we find the default anchor sizes (32, 64, 128, 256 and 512), aspect ratios (1:2, 1:1 and 2:1) and scales (2  3 ) turn out to be ineffective for detecting lesions of small size and large ratios. We employ a differential evolution search algorithm <ref type="bibr" target="#b10">[11]</ref> to optimize ratios and scales of anchors on the validation set. This algorithm iteratively improves a population of candidate solutions with regard to an objective function. New solutions are created by combining existing ones. We aim to find the best anchor settings for 3 scales and 5 ratios. The objective is to maximise the overlap between the lesion bounding-box and the best anchor on the validation dataset. We fix one ratio as 1:1, and define other ratios as reciprocal pairs (i.e., if one ratio is 1 : ? then another is ? : 1). Thus, we need to optimise only five variables, i.e, two ratio pairs and three scales. When initialising the population of candidate solutions, all scales are bounded to a range of [0.4, 1.6] and the two ratios are respectively bounded in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. We obtain optimal scales as 0.425, 0.540 and 0.680, and ratios of 3.27:1, 1.78:1, 1:1, 1:1.78, 1:3.27, which fits objects of small size and large ratios. Anchor sizes remain as (32, 64, 128, 256 and 512). These optimised configurations are then used for training the detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dense Mask Supervision from Weak RECIST Labels</head><p>Although annotations of bounding-boxes are relatively easy to obtain, there are other "weak" labels which are routinely generated in clinical practice, such as RECIST diameters. RECIST is used to track lesion growth, and consists of a pair of diameters to measure the lesion extent (cf. <ref type="figure" target="#fig_1">Fig. 1(b)</ref>). To leverage this highly valuable information, we automatically generate dense lesion masks from RECIST labels (provided in the DeepLesion dataset) using GrabCut <ref type="bibr" target="#b7">[8]</ref>. We initialize a trimap into background (T B ), foreground (T F ) and unclear (T U ) pixels. A segmentation mask is generated based on iterative graph-cuts. Initialization can largely affect the final result, as it defines the Gaussian mixture models capturing the foreground and background intensity distributions.</p><p>Cai et al. <ref type="bibr" target="#b0">[1]</ref> previously adopt GrabCut to initialise lesion masks of the RECIST-slice for the task of weakly-supervised lesion segmentation in 3D. Their T B is set as pixels outside the bounding-box defined by RECIST axes, and T F is obtained by dilation of the diameters. Such an initialisation may be sub-optimal, specifically, for large lesions, where a considerable number of lesion pixels, which are quite certain to belong to foreground, are outside the dilation and omitted in T F . For small lesions, the dilation has the risk of hard-labelling background pixels into T F , which cannot be corrected in the optimization.</p><p>To achieve a higher-quality masks using GrabCut, we propose a new strategy, as illustrated in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>. We build a quadrilateral by consecutively connecting the four endpoints of the RECIST diameters. A pixel is labelled as foreground if it falls inside the quadrilateral. As most lesions show convex outlines, this is a simple yet reliable strategy. With the annotation of bounding-box provided in the dataset, the pixels outside the box are hard-labelled as background T B . All remaining pixels are assigned to T U and estimated through GrabCut.</p><p>To exploit these generated dense labels, we add two more upsampling layers (connecting to P2 and P1) and a segmentation prediction layer to the detector. Skip connections are employed by fusing features obtained from C1 (via a 1 ? 1 convolution) and input (via two 3 ? 3 convolutions), as shown in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>. To retain sufficient resolution of feature maps for small lesions, we shift the sub-network operation (i.e., classification and regression) to pyramid levels of P2-P6 from P3-P7. Using dense supervision to help detection task shares the idea with Retina U-Net <ref type="bibr" target="#b3">[4]</ref>, where we avoid the need for tedious labelling, as our dense masks are automatically generated from labels that are already recorded in clinical routine. Additionally, we leverage the IoU between a bounding-box around the predicted segmentation mask and the directly regressed box (yellow sub-networks in <ref type="figure" target="#fig_1">Fig. 1)</ref>, to calibrate the prediction probabilityp = p ? (1 + IoU) of a lesion. High coherence between segmentation and detection results indicates high confidence in lesion prediction, and benefits sensitivity at low FP rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention Mechanism for Gated Feature Fusion</head><p>A recent attention gate (AG) model proposed by Schlemper et al. <ref type="bibr" target="#b8">[9]</ref> learns to focus on target structures by producing an attention map. According to this work, this may be beneficial for small, varying structures. We explore AGs to filter feature responses propagated through skip connections and use features upsampled from coarser scale as the gating signal. The AG module only uses 1?1 convolutions and produces a single attention map, which makes it computationally light-weight. The output of AG is the element-wise multiplication of the attention map and the feature map from the skip connection.</p><p>Training: We follow the loss used in original RetinaNet for detection, and our segmentation uses focal loss with cross-entropy. We employ the Adam optimizer with a learning rate of 10 ?4 which is reduced during training by a factor of 10 when the mean average precision (mAP) has not improved for 2 consecutive epochs. The batch size is 4 during training. To reduce overfitting, early stopping is used if the mAP has not improved for 4 consecutive epochs on the validation set. We use an NVIDIA GeForce GTX 1080 for training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset, Pre-Processing, and Augmentation</head><p>The public DeepLesion dataset <ref type="bibr" target="#b14">[15]</ref> consists of 32,120 axial CT slices from 10,594 studies of 4,427 unique patients. There are 1 ? 3 lesions in each slice, adding up to 32,735 lesions altogether. For each lesion, there is usually 30mm of extra slices above and below the key slice to provide contextual information. In most cases, the slices have 1 or 5 mm thickness, but this varies with some being 0.625 or 2 mm. The 2D bounding-boxes and RECIST diameters for lesions are annotated on the key slice. The dataset covers a wide range of lesions from lung, liver, mediastinum (mainly lymph nodes), kidney, pelvis, bone, abdomen and soft tissue. Sizes vary significantly with diameters ranging from 0.21 to 342.5 mm.</p><p>We perform lightweight pre-processing where images are resized into 512?512 pixels, resulting in a voxel-spacing between 0.175 and 0.977 mm with a mean of 0.802 mm. The Hounsfield units (HU) are clipped into the range of [?1024, 1050]. We normalize the intensities to the range of [?1, 1] as input to the network. In our experiments, we use three adjacent slices after resampling to 2 mm thickness. In rare cases where the neighboring slice of the lesion slice is not provided, we duplicate the lesion slice to fill the missing input channels. We use data augmentation where images are flipped in horizontal and vertical directions with 50% chance. We also use random affine transformations with rotation/shearing up to 0.1 radians, and scaling/translation up to 10% of the image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Detection Results on DeepLesion Benchmark</head><p>The DeepLesion dataset is provided with splits into 70% for training, 15% for validation, and 15% for testing. Thus, our results can be directly compared with numbers reported in the literature. The current best results have been achieved by Yan et al. <ref type="bibr" target="#b12">[13]</ref> and Tang et al. <ref type="bibr" target="#b11">[12]</ref>. We also quote their provided baseline performance using popular detection methods, i.e., Faster R-CNN <ref type="bibr" target="#b6">[7]</ref> (reported in <ref type="bibr" target="#b12">[13]</ref>) and Mask R-CNN <ref type="bibr" target="#b2">[3]</ref> (reported in <ref type="bibr" target="#b11">[12]</ref>). We further provide the results of our own baseline RetinaNet <ref type="bibr" target="#b5">[6]</ref> using its default configuration. A predicted box is regarded as correct if its IoU with a ground truth box is larger than 0.5.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we present the lesion detection sensitivities at different false positives (FP) per image. Our improved RetinaNet consistently outperforms existing methods across all FP rates. Specifically, sensitivity at 4 FPs, which is commonly reported in the literature, we achieve a sensitivity of 90.77%, which is a 5.12% improvement over 3DCE <ref type="bibr" target="#b12">[13]</ref> and 6.39% over ULDor <ref type="bibr" target="#b11">[12]</ref>. The free-response receiver operating characteristics (FROC) curves of different methods are shown on the left in <ref type="figure" target="#fig_3">Fig. 2</ref>. We observe that optimized networks for lesion detection are generally better than out-of-the-box detectors, such as Faster R-CNN, Mask R-CNN and RetinaNet. When comparing sensitivity at low FP rates, our improved  models perform much better than others, indicating the benefit of task-specific optimization and incorporation of additional mask information.</p><p>The sensitivity for detecting different sizes of lesions at 4 FPs are shown on the right in <ref type="figure" target="#fig_3">Fig. 2</ref>. We divide the lesions into three size groups according to the diameter, following <ref type="bibr" target="#b12">[13]</ref> for direct comparison. For small lesions with diameters less than 10 mm, our sensitivity is 88.35% compared to 80% for 3DCE. Using a feature pyramid to retain responses from small lesions together with dense supervision with focal loss seems beneficial for detecting subtle signs of disease. While 3DCE uses richer 3D context, this seems less helpful for small, local structures. Our model works well across all lesion sizes where we improve sensitivity from 87% to 91.73% for lesions of 10 ? 30 mm, and from 84% to 93.02% for lesions larger than 30 mm when compared to 3DCE.</p><p>We also record average inference time for each image during testing, as listed in <ref type="table" target="#tab_0">Table 1</ref>. All the detection results are obtained using a single network without model ensemble nor test augmentation. Our one-stage detector is highly efficient eliminating the need of generating lesion proposals. The integration of dense supervision and attention mechanism has minimal computational overhead, taking about 41 ms for each image. Runtimes are reported for 3DCE and Faster R-CNN in <ref type="bibr" target="#b12">[13]</ref>, but a comparison is only indicative due to different GPUs being used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contribution of Individual Improvements</head><p>We investigate the individual impact of the proposed additions leading to our final improved RetinaNet. In an ablation study, we first evaluate the original RetinaNet <ref type="bibr" target="#b5">[6]</ref> with default settings, then incrementally add our improvements, i.e., automatic anchor optimization, dense supervision using lesion masks from weak labels, and attention mechanism. <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_3">Fig. 2 (left)</ref> summarizes these results. The original RetinaNet with default anchor configuration is performing poorly on the lesion detection task, indicating that out-of-the-box approaches from computer vision are sub-optimal. Remarkably, after employing the automatic search algorithm to optimize the anchor configuration, the simple Reti-naNet already outperforms previous state-of-the-art. The sensitivity at 0.5 FP is 2.34% higher than 3DCE and 11.96% higher than ULDor.</p><p>Adding dense supervision with segmentation masks generated from RECIST diameters significantly boosts detection sensitivity across all FP rates, with 5.42% improvement at 0.5 FP. The pixel-wise supervision adds an important training signal, providing more precise localization information in addition to bounding-boxes. Consistency between bounding-box regression and dense classification helps to reduce false positives. Finally, adding an attention mechanism further improves the performance, achieving a sensitivity of 90.77% at 4 FPs, with an improvement of almost 10% at 0.5 FP over the best reported results.</p><p>Visual examples of detected lesions on test images are shown in <ref type="figure" target="#fig_5">Figs. 3 and 4</ref>. Probability threshold is set to 0.3 yielding 0.5 FP per image. Lesions of various size, appearance and type are localized accurately. Segmentation masks look sensible, indicating good quality of the automatically generated dense labels for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our improved RetinaNet shows impressive performance on CT lesion detection outperforming state-of-the-art by a significant margin. Interestingly, we could show that by task-specific optimization of an out-of-the-box detector we already achieve results superior than the best reported in the literature. Exploitation of clinically available RECIST annotations bears great promise as large amounts of such training data should be available in many hospitals. With a sensitivity of about 91% at 4 FPs per image, our system may reach clinical readiness. Future work will focus on new applications such as whole-body MRI in oncology. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Foreground</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Overview of our improved RetinaNet. (b) Automatic dense mask generation from weak RECIST diameters using GrabCut<ref type="bibr" target="#b7">[8]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>Left: FROC curves for our improved RetinaNet variants and baselines on DeepLesion dataset. Right: Per lesion size results compared to 3DCE<ref type="bibr" target="#b12">[13]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>Visual results for lesion detection at 0.5 FP rate using our improved RetinaNet. The first three columns show different sizes from small to large. The right column shows heatmaps from the segmentation layer overlaid on detections. Yellow boxes are ground truth, green are true positives, red are false positives. Last row shows intriguing failure cases with possibly incorrect ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>More visual results for lesion detection at 0.5 FP rate using our improved RetinaNet. The rows correspond to bone, abdomen, mediastinum, liver, lung, kidney, soft tissue, and pelvis lesions, respectively. Each row contains examples of lesions of different sizes ordered from smallest to largest. Yellow boxes are ground truth, green are true positives, red are false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Detection performance of different methods and our ablation study.</figDesc><table><row><cell>Methods</cell><cell>0.5</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>runtime</cell></row><row><cell>Faster R-CNN [7]</cell><cell cols="6">56.90 67.26 75.57 81.62 85.83 88.74</cell><cell>32 ms</cell></row><row><cell>Mask R-CNN [3]</cell><cell cols="6">39.82 52.66 65.58 77.73 85.54 91.80</cell><cell>-</cell></row><row><cell cols="7">ULDor (Tang et al. [12]) 52.86 64.80 74.84 84.38 87.17 91.80</cell><cell>-</cell></row><row><cell cols="8">3DCE (Yan et al. [13]) 62.48 73.37 80.70 85.65 89.09 91.06 114 ms</cell></row><row><cell cols="7">original RetinaNet [6] 45.80 54.17 62.50 69.80 75.34 79.48</cell><cell>28 ms</cell></row><row><cell cols="7">+ anchor optimization 64.82 74.98 82.29 87.87 92.20 94.90</cell><cell>31 ms</cell></row><row><cell>+ dense supervision</cell><cell cols="6">70.24 78.28 85.10 90.39 93.81 96.01</cell><cell>39 ms</cell></row><row><cell>+ attention gate</cell><cell cols="7">72.15 80.07 86.40 90.77 94.09 96.32 41 ms</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 757173, project MIRA, ERC-2017-STG).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Accurate weakly-supervised deep lesion segmentation using large-scale clinical annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">New response evaluation criteria in solid tumours: revised RECIST guideline (version 1.1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Eisenhauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Therasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bogaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European journal of cancer</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="247" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Retina U-Net: Embarrassingly simple exploitation of segmentation supervision for medical object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bickelhaupt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Kuder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Schlemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08661</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GrabCut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention gated networks: Learning to leverage salient regions in medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schaap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Storn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">ULDor: A universal lesion detector for ct scans with pseudo masks and hard negative example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06359</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">3D context enhanced region-based convolutional neural network for end-to-end lesion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep lesion graphs in the wild: relationship learning and organization of significant radiology image findings in a diverse large-scale lesion database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
