<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonkyu</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of recovering a single person's 3D human mesh from in-the-wild crowded scenes. While much progress has been in 3D human mesh estimation, existing methods struggle when test input has crowded scenes. The first reason for the failure is a domain gap between training and testing data. A motion capture dataset, which provides accurate 3D labels for training, lacks crowd data and impedes a network from learning crowded scene-robust image features of a target person. The second reason is a feature processing that spatially averages the feature map of a localized bounding box containing multiple people. Averaging the whole feature map makes a target person's feature indistinguishable from others. We present 3DCrowdNet that firstly explicitly targets in-the-wild crowded scenes and estimates a robust 3D human mesh by addressing the above issues. First, we leverage 2D human pose estimation that does not require a motion capture dataset with 3D labels for training and does not suffer from the domain gap. Second, we propose a joint-based regressor that distinguishes a target person's feature from others. Our joint-based regressor preserves the spatial activation of a target by sampling features from the target's joint locations and regresses human model parameters. As a result, 3DCrowdNet learns targetfocused features and effectively excludes the irrelevant features of nearby persons. We conduct experiments on various benchmarks and prove the robustness of 3DCrowdNet to the in-the-wild crowded scenes both quantitatively and qualitatively. Codes are available here: https://github. com/hongsukchoi/3DCrowdNet_RELEASE</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Extensive research has been committed to reconstructing an accurate 3D human mesh, which represent both the pose and shape of a human, from a single image. However, 3D human mesh estimation from in-the-wild crowded scenes lack crowded scenes contain crowded scenes (MoCap training data) (in-the-wild crowd testing data)  <ref type="figure">Figure 1</ref>. 3DCrowdNet resolves (a) a domain gap issue in estimating a 3D human mesh from in-the-wild crowded scenes. Due to the large domain gap between motion capture data and in-the-wild crowd data, (b) existing state-of-the-art methods such as SPIN <ref type="bibr" target="#b22">[23]</ref> produce inaccurate results, while 3DCrowdNet gives an accurate 3D human mesh despite severe inter-person occlusion. We conceal a person's face in this paper to abide by the ethical policy. has been barely studied, despite their common presence. Consequently, most of the previous works show results on scenes without inter-person occlusion and provide inaccurate results on crowded scenes. The inter-person occlusion is the essential challenge of in-the-wild crowded scenes, and many practical applications including abnormal behavior detection <ref type="bibr" target="#b9">[10]</ref> and person re-identification <ref type="bibr" target="#b38">[39]</ref> encounter such situations. This paper investigates the limitation of the current literature and proposes a novel method for robust 3D human mesh estimation from in-the-wild crowded scenes.</p><p>The currently dominant training strategy for human mesh recovery is mixed-batch training. It composes a minibatch with one-half data from a motion capture (MoCap) 3D dataset <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref> and the other from an in-the-wild 2D dataset <ref type="bibr" target="#b25">[26]</ref>. To use the 2D dataset for supervision, 3D joints regressed from a predicted mesh are projected onto the image plane, and the distance with 2D annotations is computed. This way of mixing 3D and 2D data is well known to improve accuracy and generalization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref> by implicitly inducing a neural network to benefit from accurate 3D annotations of the 3D data and diverse image appearances in the 2D data. The dominant approach of recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref> is a model-based approach using a global feature vector, which obtains the feature vector with a deep convolutional neural network (CNN) and regresses the human model parameters (e.g. SMPL <ref type="bibr" target="#b27">[28]</ref>) from it. First, they crop an image using a bounding box of a target person detected from off-the-shelf human detectors <ref type="bibr" target="#b11">[12]</ref>. Then they process the target's cropped image with a deep CNN and perform a global average pooling to obtain the global feature vector. The global feature vector is fed to a Multi-Layer Perceptron (MLP)-based regressor that regresses the mesh parameters. The 3D meshes are obtained by forwarding the parameters to the human model layers. While the recent works have shown reasonable results on standard benchmarks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">50]</ref> based on the two wheels of the current literature, in-the-wild crowded scenes remain insurmountable due to the following two reasons. First, a large domain gap between training data from MoCap datasets and testing data from in-the-wild crowded scenes hinders a deep CNN from extracting proper image features of a target person. The domain gap arises from the presence of a human crowd, which entails diverse inter-person occlusion, interacting body poses, and indistinguishable cloth appearances ( <ref type="figure" target="#fig_0">Figure 1a</ref>). The mixed-batch training alone is insufficient to overcome the domain gap, and existing methods struggle to acquire robust image features from in-the-wild crowded scenes, and produce inaccurate meshes <ref type="figure">(Figure 1b</ref>). Intuitively, this tells us that external guidance robust to the domain gap is required for a crowded scene-robust image feature, in addition to the mixed-batch training.</p><p>Next, the global average pooling on a deep CNN feature collapses the spatial information that distinguishes a target person's feature from others. In-the-wild crowded scenes often involve overlapping people and inaccurate human bounding boxes. Thus, a bounding box of a target inevitably includes non-target people. A deep CNN feature retains features of these non-target people, and the global average pooling makes a target person's feature indistinguishable from others. This confuses a regressor and makes it difficult to capture an accurate 3D pose of a target person. For instance, the regressor may miss human parts occluded by another person or predict a different person's pose.</p><p>In this regard, we present 3DCrowdNet, a novel network that learns to estimate a single person's robust 3D human mesh from in-the-wild crowded scenes. This study is one of the earliest works that explicitly tackle 3D human mesh estimation of a target person in a crowd. 3DCrowdNet addresses the two issues of previous works in two folds. First, we resolve the domain gap by explicitly guiding a deep CNN to extract a crowded scene-robust image feature using an off-the-shelf 2D pose estimator. Unlike methods targeting 3D geometry, the 2D pose estimator does not require depth supervision and is not trained on a MoCap dataset. Instead, it is trained only on in-the-wild datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45]</ref> that have images containing human crowds and suffers less from a domain gap regarding the inference on crowded scenes. Consequently, the 2D pose estimator's outputs provide strong evidence of a target person and help 3DCrowd-Net pay attention to a target's feature despite the challenges in in-the-wild crowded scenes. Second, we propose a joint-based regressor that does not blow away the spatial activation of a target person in a feature map with the global average pooling. The joint-based regressor first predicts the spatial locations of joints. Then, it samples image features from a deep CNN feature map with the locations. In particular, we keep the sampling area small to exclude features of non-target people. The target person's feature is distinguished from others, and human model parameters are regressed from the sampled image features. The joint-based regressor differs from the previous regressors that evenly aggregate people's features regardless of the target. <ref type="figure">Figure 2</ref> depicts the overview of 3DCrowdNet.</p><p>Note that 3DCrowdNet substantially differs from prior works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref> that directly lift 2D estimation outputs to 3D-(a) we focus on producing and leveraging image features of a target person in human crowds, and (b) such image features help 3DCrowdNet to resolve the depth and shape ambiguity of a target person, from which the 2D estimation outputs inherently suffer. Thus, we argue that this work takes a step towards accurate 3D human mesh estimation from in-the-wild crowded scenes by distinguishing image features of a target person in densely interacting crowds, which is highly challenging but important. The experiments show that 3DCrowdNet significantly outperforms the previous 3D human mesh estimation methods on in-the-wild crowded scenes. Also, it achieves state-of-the-art accuracy in multiple 3D benchmarks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50]</ref>. Extensive qualitative results are presented in the main manuscript and supplementary material. Our contributions can be summarized as follows:</p><p>? We present 3DCrowdNet, the first approach to 3D human mesh recovery from in-the-wild crowded scenes. It effectively processes image features of a target person in a crowd, which is essential for accurate 3D pose and shape reconstruction.</p><p>? It extracts crowded scene-robust image features by resolving the domain gap with a 2D pose estimator.</p><p>? It distinguishes a target person's image features from others using a joint-based regressor.</p><p>? 3DCrowdNet significantly outperforms previous methods on in-the-wild crowded scenes both quantitatively and qualitatively, and achieves state-of-the-art 3D pose and shape accuracy on multiple 3D benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>2D human pose estimation from crowded scenes. Early works of 2D human pose estimation did not explicitly target crowded scenes. However, their methods are related to diverse challenges of in-the-wild crowded scenes, such as overlapping human bounding boxes, human detection error, and inter-person occlusion. There are two major approaches, namely bottom-up and top-down approaches.</p><p>Bottom-up methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> first detect all joints of the people, and group them to each person. Top-down methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41]</ref> first detect all human bounding boxes, and apply a single-person 2D pose estimation method to each person. Top-down methods generally achieve higher accuracy on traditional 2D pose benchmarks such as MSCOCO <ref type="bibr" target="#b25">[26]</ref>, but underperform on crowded scene benchmarks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b56">57]</ref> than bottom-up methods due to the human detection issues.</p><p>Recently, a few works explicitly addressed crowded scenes 2D pose estimation and reported good accuracy on crowded scene benchmarks. <ref type="bibr" target="#b24">[25]</ref> combined top-down and bottom-up approaches using joint-candidate single person pose estimation and global maximum joints association. <ref type="bibr" target="#b4">[5]</ref> proposed to learn scale-aware representations using highresolution feature pyramids. <ref type="bibr" target="#b16">[17]</ref> made a grouping process of the bottom-up approach differentiable using a graph neural network. <ref type="bibr" target="#b44">[45]</ref> refined invisible joints' prediction using an image-guided progressive graph convolutional network. 3D human geometry estimation from crowded scenes. Several methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59]</ref> have shown reasonable results on multi-person 3D benchmarks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>. However, their focus was on absolute depth estimation of each person, and few works have addressed the inter-person occlusion to estimate robust 3D geometry, such as 3D human pose (i.e. 3D joint coordinates) and meshes, from in-the-wild crowded scenes. XNect <ref type="bibr" target="#b30">[31]</ref> proposed an occlusion-robust method that can be applied to crowded scenes. However, it did not focus on resolving the domain gap. It integrated 2D/3D branches into a single system and trained it on a MoCap dataset <ref type="bibr" target="#b31">[32]</ref>, which barely contains inter-person occlusions. Also, it requires a particular joint (i.e. neck) must be visible for human detection. On the contrary, our key idea is leveraging external 2D pose estimators that are not trained on MoCap data, to alleviate the domain gap between MoCap training data and in-the-wild crowd testing data. In addition, 3DCrowdNet reconstructs full 3D human pose and shape from diverse partially invisible people in crowded scenes.</p><p>ROMP <ref type="bibr" target="#b48">[49]</ref> introduced a bottom-up method for multiperson 3D mesh recovery that can be applied to crowded scenes. It estimates a body center heatmap and a mesh parameter map, and samples each person's mesh parameters from the parameter map using center locations regressed from the heatmap. While the method provides better results on crowded scenes than previous methods, it could still suffer from the domain gap between MoCap training data and testing data from in-the-wild crowded scenes. Also, solely relying on the body center estimation to distinguish a target from others could be unstable in cases of occlusion on the body center. On the other hand, 3DCrowdNet explicitly tackles the domain gap issue with crowded-scene robust 2D poses. Also, we utilize cues from multiple 2D joint locations of the target and refine image features sampled from the locations to handle diverse inter-person occlusion, including occlusion on the body center. 2D geometry to 3D human mesh estimation. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56]</ref> proposed methods that only take 2D geometry without images, such as 2D joint locations, for SMPL parameter regression. While the methods can benefit from 2D estimators robust to in-the-wild crowded scenes, they have two limitations. First, they cannot correct inaccurate 2D input compared to the actual person in images. Instead, they produce the most plausible outputs for the given 2D input, not the 3D pose and shape that best describes the person in images. Second, they do not benefit from image features with rich depth and 3D shape cues of a target person. The cues include subtle light reflection and shadows. 2D geometry hardly contains such cues and could lead to inaccurate 3D human mesh estimation. On the contrary, 3DCrowdNet reconstructs accurate 3D human meshes from possibly inaccurate 2D poses utilizing image features. Also, we focus on extracting the crowded scene-robust image feature of a target person using the 2D pose, rather than directly lifting 2D to 3D as the prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">3DCrowdNet</head><p>3.1. 3DCrowdNet architecture As shown in <ref type="figure">Figure 2</ref>, our architecture comprises a feature extractor followed by a joint-based regressor. The feature extractor is based on ResNet-50 <ref type="bibr" target="#b12">[13]</ref>, and the jointbased regressor is based on <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>. Our network's output is SMPL <ref type="bibr" target="#b27">[28]</ref> parameters, and a single person's 3D mesh is obtained by feeding the parameters to the SMPL layer. Feature extractor. The feature extractor takes a 2D pose and an image as input. The 2D pose is 2D joint coordinates P 2D ? R J?2 predicted by bottom-up off-the-shelf 2D pose estimators <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. J denotes the number of human joints, and it can vary among different 2D pose estimators. During  <ref type="figure">Figure 2</ref>. Overview of 3DCrowdNet. It resolves the domain gap by explicitly guiding a deep CNN to extract a crowded scene-robust feature using an off-the-shelf 2D pose estimator. Then, it distinguishes a target person from others by preserving the target's spatial activation with a joint-based regressor and regresses SMPL <ref type="bibr" target="#b27">[28]</ref> parameters. The parameters are fed to the SMPL layer to get a 3D mesh. For simplicity, we show image feature sampling on only two joints. The numbers in network layers indicate the output channel dimension. The number in the max pooling layer indicates a stride size. The graph convolutional blocks' channel dimension is defined per joint.</p><p>training, we add realistic errors on the ground truth (GT) 2D pose following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref> to mimic erroneous 2D pose outputs in test time, and the noisy 2D pose is used as our input P 2D . We provide the 2D pose P 2D as a heatmap representation H 2D ? R Js?64?64 to the feature extractor by making a Gaussian blob on the 2D joint coordinates. J s = 30 indicates the number of joints in a superset of joint sets defined by multiple datasets. We assign don't-care values to the undefined joints and joint predictions with low confidence in inference time, by multiplying zero to the corresponding joint's heatmap. Modeling don't-care values based on the superset of joints and heatmaps enables 3DCrowdNet to perform inference from various human joint sets with a single network and handle diverse input such as 2D poses with missing joints due to truncation and occlusion.</p><p>The feature extractor uses the 2D pose heatmap H 2D of a target person as guidance and pays attention to the spatial region of a target in a crowd. First, it obtains an early-stage image feature of ResNet F ? R C?64?64 from a cropped image I ? R 3?256?256 . C = 64 is the channel dimension, and I is acquired by cropping and resizing a bounding box area, derived from the 2D pose P 2D . Second, it concatenates F and H 2D along the channel dimension. The concatenated feature is processed by a 3-by-3 convolution block, which keeps the feature's height and width but changes the channel dimension to C. Finally, the feature with C channels is fed back to the remaining part of ResNet, where the output is a crowded scene-robust image feature F ? R C ?8?8 . C = 2048 is the channel dimension. Joint-based regressor. The joint-based regressor first recovers 3D joint coordinates P 3D ? R Jc?3 from F . J c = 15 denotes the number of joints in the intersection of joint sets defined by multiple datasets. (x,y) values of P 3D are defined in a 2D pixel space, and z value of P 3D represents root joint-relative depth. A 1-by-1 convolutional layer outputs a 3D heatmap H 3D ? R Jc?D?8?8 from F after predicting a J c D dimensional 2D feature map and reshaping it to the 3D heatmap. D = 8 decides a descritizated size of depth. P 3D is computed from H 3D , using soft-argmax operation <ref type="bibr" target="#b47">[48]</ref>. As the soft-argmax computes continuous coordinates from a discretized grid, we observed that a heatmap with a low resolution like H 3D gives similar accuracy compared to upsampled ones, while requiring less computational costs.</p><p>Next, the joint-based regressor estimates global rotation of a person ? g ? R 3 , SMPL body rotation parameters ? ? R 21?3 , SMPL shape parameters ? ? R 10 , and camera parameters k ? R 3 for projection. First, image features per joint are sampled from F using the (x,y) pixel positions of P 3D . We use bilinear interpolation, since the (x,y) pixel positions are not in discretized values. The prediction confidence of P 3D is sampled from H 3D in the same manner. Second, we concatenate the sampled image features, P 3D , and the prediction confidence of P 3D , to attain F M ? R Jc?(C +3+1) . Last, we process F M using a graph convolutional network (GCN), and predict ? g , VPoser <ref type="bibr" target="#b42">[43]</ref> latent code z, ?, and k from output features of GCN with separate MLP layers. ? is decoded from z. The GCN shows faster convergence during training than an MLP network, and we think the reason lies behind the character of ?. ? is parent joint-relative joint rotations, and the GCN can exploit the human kinematic prior different from an MLP. For example, the GCN can implicitly learn the valid range of each parent joint-relative joint leveraging the relationship between human joints.</p><p>For the graph convolutional network, we use the jointspecific graph convolution <ref type="bibr" target="#b26">[27]</ref> that learns separate weights for each graph vertex.</p><p>We define learnable weight matrices {W j ? R Cout?Cin } Jc j=1 for all joints of each graph convolution layer, where C in and C out denotes input and output channel dimensions, respectively. Then, the output graph feature of joint j is derived as F out</p><formula xml:id="formula_0">j = ? ReLU ( i?Nj? ji ? BN (W i F in i )),</formula><p>where F in i is the input graph feature of joint i. ? ReLU and ? BN denotes ReLU activation function and 1D batch normalization <ref type="bibr" target="#b13">[14]</ref>, respectively.N j is defined as N j ? {j}, where N j denotes neighbors of a vertex j.? ji is an entry of the normalized adja-</p><formula xml:id="formula_1">cency matrix? at (j, i), where? = D ? 1 2 (A + I)D ? 1 2 . A ? {0, 1}</formula><p>Jc?Jc is the adjacency matrix constructed based on the human skeleton hierarchy and fixed during the training and testing stages. The definition of the human skeleton hierarchy is depicted in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network training</head><p>The feature extractor and joint-based regressor are integrated and trained end-to-end. We use both pseudo-GT SMPL fits obtained by fitting frameworks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43]</ref> and GT annotations from training datasets for supervision following <ref type="bibr" target="#b22">[23]</ref>. Our overall objective is defined as follows:</p><formula xml:id="formula_2">L = L pose + L mesh ,<label>(1)</label></formula><p>where L pose computes the L1 distance between the predicted P 3D and the (pseudo) GT, and L mesh denotes the loss function for predicted SMPL parameters. L mesh is defined as</p><formula xml:id="formula_3">L mesh = L param + L pose ,<label>(2)</label></formula><p>where L param computes the L1 distance between the predicted ? g , ?, and ?, and the pseudo-GT parameters; L pose indicates the L1 distance loss of joints regressed from predicted meshes. To supervise with 2D annotations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>, predicted joints are projected by camera parameters k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation detail</head><p>PyTorch <ref type="bibr" target="#b41">[42]</ref> is used for implementation. We initialize the weights of ResNet <ref type="bibr" target="#b12">[13]</ref> with the pre-trained weights from <ref type="bibr" target="#b52">[53]</ref>. It shows faster convergence during training. We use Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with a mini-batch size of 64. The initial learning rate is 10 ?4 . The model is trained for 6 epochs, and the learning rate is reduced by a factor of 10 after the 3th and 5th epochs. We use four NVIDIA RTX 2080 Ti GPUs for training, and it takes about 9 hours on average. We will release the codes for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3DPW-Crowd</head><p>MuPoTS CMU-Panoptic 3DPW <ref type="figure">Figure 3</ref>. We curate 3DPW-Crowd, a subset of 3DPW, which has much higher bounding box IoU and CrowdIndex <ref type="bibr" target="#b24">[25]</ref> than other 3D benchmarks. CrowdIndex measures other people's joints' ratio over each person's joints in a bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Training sets.</p><p>We use Human3.6M <ref type="bibr" target="#b14">[15]</ref>, MuCo-3DHP <ref type="bibr" target="#b31">[32]</ref>, MSCOCO <ref type="bibr" target="#b25">[26]</ref>, MPII <ref type="bibr" target="#b1">[2]</ref>, and CrowdPose <ref type="bibr" target="#b24">[25]</ref> for training. Only the training sets of the datasets are used, following the standard split protocols. Testing sets. We report accuracy on MuPoTS <ref type="bibr" target="#b31">[32]</ref>, CMU-Panoptic <ref type="bibr" target="#b19">[20]</ref>, 3DPW <ref type="bibr" target="#b49">[50]</ref>, and 3DPW-Crowd. MuPoTS is a multi-person test benchmark captured from indoor and outdoor environments, starring 3 to 4 people. CMU-Panoptic is a large-scale multi-person dataset captured from the Panoptic studio. Following <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b54">55]</ref>, we pick four sequences presenting 3 to 7 people socializing each other for the evaluation. 3DPW is a widely-used 3D benchmark captured from an in-the-wild environment, and we use the test set of 3DPW following the official split protocol. 3DPW-Crowd is a subset of 3DPW and is used to evaluate the a method's robustness to in-the-wild crowded scenes. Refer to more details below about its necessity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation protocols</head><p>Evaluation on crowded scenes: 3DPW-Crowd and CrowdPose. As CrowdPose <ref type="bibr" target="#b24">[25]</ref> addressed, the principal obstacle of pose estimation from crowded scenes is not the number of people, but the inter-person occlusion in a crowd. Thus, MuPoTS <ref type="bibr" target="#b31">[32]</ref> and CMU-Panoptic <ref type="bibr" target="#b19">[20]</ref> have limitations for the evaluation on in-the-wild crowded scenes, not only because they are not in-the-wild data, but also because they show limited interaction.</p><p>To overcome the limitations, we propose 3DPW-Crowd to numerically measure a method's robustness on in-thewild crowded scenes. It contains hugging and dancing sequences that have considerably higher average intersection over union (IoU) of bounding boxes and CrowdIndex <ref type="bibr" target="#b24">[25]</ref>    among 3D benchmarks as shown in <ref type="figure">Figure 3</ref>. We name the subset as 3DPW-Crowd, since it reveals the challenges of in-the-wild crowded scenes, such as overlapping bounding boxes and severe inter-person occlusion. More details about 3DPW-Crowd are in the supplementary material. We also provide extensive qualitative comparison between different methods on the test set of CrowdPose <ref type="bibr" target="#b24">[25]</ref> in this manuscript and supplementary material. Evaluation metrics. We report 3D pose and 3D shape evaluation metrics. For the 3D pose evaluation, we use mean per-joint position error (MPJPE), Procrustes-aligned mean per-joint position error (PA-MPJPE), and 3DPCK proposed in <ref type="bibr" target="#b29">[30]</ref>. Following SPIN <ref type="bibr" target="#b22">[23]</ref>, we use the 3D joint coordinates regressed from a 3D mesh as predictions. For the 3D shape evaluation, we use mean per-vertex position error (MPVPE). All errors are measured after aligning root joints of GT and estimated human body meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>We carry out the ablation study on 3DPW-Crowd.We use HigherHRNet <ref type="bibr" target="#b4">[5]</ref>'s 2D pose outputs in <ref type="table">Table 2</ref> the effectiveness of the crowded scene-robust image feature.The baseline network in the first row crops an image using a GT bounding box and extracts image features without any guidance as in previous methods. The significant error drop in the table proves that the 2D pose can produce crowded scene-robust image features, and the image features are critical to estimate an accurate mesh from the in-the-wild crowded scenes. We further validate our statement that the 2D pose can produce crowded scene-robust image features in <ref type="figure" target="#fig_3">Figure 4</ref>. 3DCrowdNet activates the occluded target male's spatial region, unlike the baseline network, and successfully distinguishes him from the other. As a result, 3DCrowdNet estimates an accurate mesh of the occluded target male, while the baseline predicts a mesh of the occluding female. We conclude that for the 3D mesh estimation on in-the-wild crowded scenes, the domain difference of MoCap train data is the bottleneck, and our idea to exploit the robustness of 2D pose estimators, which do not use MoCap train data, is valid. Joint-based regressor. <ref type="table">Table 2</ref> shows that the joint-based regressor outperforms the SPIN [23]-style regressor, the dominant model-based approach in the current literature, on 3DPW-Crowd. The results prove that preserving the spatial activation of a target person in a deep CNN feature map is essential. SPIN-style regressor shows lower accuracy, since it makes a target person's feature indistinguishable from others by collapsing the spatial information with a global average pooling. We further validate our argument in <ref type="table">Table 3</ref>. Originally, our joint-based regressor samples deep image features from (x,y) positions of the predicted 3D pose. When we enlarge the sampling area, the errors increase. Especially, when the joint-based regressor uses features sampled from the whole feature map, which are the same feature of the SPIN-style regressor, MPJPE becomes similar to that of the SPIN-style regressor. It indicates that most of the accuracy gain in <ref type="table">Table 2</ref> is not from a better network architecture, such as GCN, but from the preservation of the target person's spatial activation. Keeping an appropriate sampling area to less involve non-target people's image features is important to estimate a robust human mesh from in-the-wild crowded scenes. We also verify the effectiveness of estimating a 3D pose instead of a 2D pose in <ref type="table" target="#tab_2">Table 4</ref>. The clear accuracy improvement proves that the depth information can be reliably estimated from a 2D pose and image features, and it is beneficial for the accuracy of the final mesh estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with state-of-the-art methods</head><p>Unless indicated, our 3DCrowdNet is not trained on a CrowdPose <ref type="bibr" target="#b24">[25]</ref> train set in <ref type="table">Table 5</ref>, 6, and 7. Also, we use less or similar training data than other methods, and the details are in the supplementary material. 3DPW-Crowd. We compare our 3DCrowdNet with <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49]</ref> in <ref type="table">Table 5</ref>. They are recent state-of-the-art 3D human mesh estimation methods on 3DPW, and publicly released the codes for evaluation. We make several observations. First, our approach outperforms SPIN <ref type="bibr" target="#b22">[23]</ref>, which takes only the image feature as input and performs a global average pooling on the deep CNN feature map. The result is coherent with the results in <ref type="table" target="#tab_0">Table 1</ref> and 2 of our ablation studies. Next, 3DCrowdNet outperforms ROMP <ref type="bibr" target="#b48">[49]</ref>, a bottom-up method for multi-person 3D mesh estimation. While ROMP achieves higher accuracy than other methods, we think it still suffers from the domain gap issue. For example, it needs to learn how to distinguish body centers of people under diverse inter-person occlusion, but MoCap datasets they used rarely contain such data. On the other hand, 3DCrowdNet explicitly resolves the domain gap using 2D pose input and produces accurate 3D meshes.</p><p>Last, 3DCrowdNet defeats Pose2Mesh <ref type="bibr" target="#b6">[7]</ref>, a method that can also benefit from crowded-scene robust 2D poses. We used the same 2D pose predictions of <ref type="bibr" target="#b4">[5]</ref> for Pose2Mesh and 3DCrowdNet. The result validates 3DCrowdNet's two strengths over Pose2Mesh. First, 3DCrowdNet recovers a 3D mesh that best describes a target person, using rich depth and shape cues in images. On the contrary, Pose2Mesh produces the most plausible 3D mesh for a given 2D pose, and the accuracy depends on it. <ref type="figure" target="#fig_5">Figure 6</ref> shows that 3DCrowd-method 3DPCK? All</p><p>Matched SMPLify-X [43] / OpenPose <ref type="bibr" target="#b2">[3]</ref> 62.8 68.0 HMR <ref type="bibr" target="#b20">[21]</ref> / OpenPose <ref type="bibr" target="#b2">[3]</ref> 66.0 70.9 HMR [21] / Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> 65.6 68.6 Jiang et al. <ref type="bibr" target="#b15">[16]</ref> 69.1 72.2 3DCrowdNet (Ours) / OpenPose <ref type="bibr" target="#b2">[3]</ref> 70.2 70.9 3DCrowdNet (Ours) / HigherHRNet <ref type="bibr" target="#b4">[5]</ref> 72.7 73.3 <ref type="table">Table 6</ref>. Comparison on MuPoTS <ref type="bibr" target="#b31">[32]</ref> between 3DCrowdNet and previous methods. The numbers denote 3DPCK for all annotations (All) and annotations matched to a prediction (Matched), and are brought from <ref type="bibr" target="#b15">[16]</ref>. The method names beside <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref>   <ref type="table">Table 7</ref>. Comparison on CMU-Panoptic <ref type="bibr" target="#b19">[20]</ref>. The numbers denote MPJPE. We follow the evaluation protocol of Jiang et al. <ref type="bibr" target="#b15">[16]</ref>.</p><p>Net recovers accurate 3D meshes, even when a 2D pose is inaccurate. Second, 3DCrowdNet can handle missing joints of 2D pose predictions due to occlusion and truncation owing to don't-care modeling based on the 2D pose's heatmap introduced in Section 3.1. Pose2Mesh takes the 2D pose as coordinates and cannot cope with the missing joints, common in in-the-wild crowded scenes. Please also refer to the qualitative comparison in the supplementary material. MuPoTS. <ref type="table">Table 6</ref> compares our 3DCrowdNet with methods that recover a 3D mesh. It outperforms all the previous methods. Note that the second and fifth rows prove that 3DCrowdNet's high accuracy on the crowded scenes is not simply attributed to better localization derived from bottomup 2D poses. While 3DCrowdNet and HMR use the same 2D poses of OpenPose <ref type="bibr" target="#b2">[3]</ref>, HMR utilizes the 2D pose only to get a bounding box, and 3DCrowdNet additionally uses the 2D pose to guide a feature extractor to extract crowded scene-robust image features. Leveraging more information in given input is natural, and leads to better accuracy. CMU-Panoptic. <ref type="table">Table 7</ref> shows that our 3DCrowdNet significantly outperforms previous 3D human pose and shape estimation methods on CMU-Panoptic. The result demonstrates that the proposed 3DCrowdNet can perform competitively on crowded scenes with daily social activities. Note that no data from CMU-Panoptic are used for training. 3DPW. <ref type="table">Table 8</ref> shows that 3DCrowdNet achieves state-ofthe art accuracy in general in-the-wild scenes. The result validates that 3DCrowdNet is robust to diverse challenges of in-the-wild scenes, although our method is designed to 3DCrowdNet(Ours) SPIN input image 3DCrowdNet(Ours) ROMP input image <ref type="figure">Figure 5</ref>. Qualitative comparison on a CrowdPose <ref type="bibr" target="#b24">[25]</ref> test set with SPIN <ref type="bibr" target="#b22">[23]</ref> and ROMP <ref type="bibr" target="#b48">[49]</ref>. We highlighted their representative failure cases with red circles. The order of 3D meshes is manually assigned.   target crowded scenes. The second row of <ref type="figure">Figure 5</ref> supports our statement, which shows 3DCrowdNet's robustness to truncation and occlusion in in-the-wild images. We provide the qualitative comparison with SPIN [23] and ROMP <ref type="bibr" target="#b48">[49]</ref> in <ref type="figure">Figure 5</ref>. Apparently, 3DCrowdNet produces much more robust 3D meshes on in-the-wild crowded scenes. SPIN predicts a swapped leg pose (top), fails to distinguish different people in the overlapping bounding boxes (middle), and misses the right leg's pose due to inter-person occlusion (bottom). ROMP produces an inaccurate pose for a person under occlusion with similar appearances (top), misses a target whose body center (i.e. torso) is invisible (middle), and estimate an inaccurate global rotation of a target due to occlusion by a nearby person with similar appearance (bottom). Please also refer to more extensive qualitative comparison with <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49]</ref> and failure cases of 3DCrowdNet in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present 3DCrowdNet, the first single image-based 3D human mesh estimation system that explicitly targets in-the-wild crowded scenes. It extracts crowded-scene robust image features of a target person, and effectively distinguishes the target from others. We guide a deep CNN to pay attention to the target using a 2D pose, which is robust to the domain gap between MoCap training data and crowd testing data. The joint-based regressor preserves the spatial activation of the target, and effectively excludes non-target people's image features. We show that 3DCrowdNet highly outperforms previous methods on inthe-wild crowded scenes both quantitatively and qualitatively. 3DCrowdNet could be a baseline for future imagebased methods that target crowded scenes owing to the simple yet effective implementation. Human3.6M <ref type="bibr" target="#b14">[15]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b29">[30]</ref>, MSCOCO <ref type="bibr" target="#b25">[26]</ref>, MPII <ref type="bibr" target="#b1">[2]</ref>, LSP <ref type="bibr" target="#b17">[18]</ref>, LSP-Extended <ref type="bibr" target="#b18">[19]</ref>, AICH <ref type="bibr" target="#b51">[52]</ref>, MuCo-3DHP <ref type="bibr" target="#b31">[32]</ref>, OH <ref type="bibr" target="#b57">[58]</ref>, PoseTrack <ref type="bibr" target="#b0">[1]</ref>, CrowdPose <ref type="bibr" target="#b24">[25]</ref> 104.8 63.9 127.8 3DCrowdNet (Ours) MuCo-3DHP <ref type="bibr" target="#b31">[32]</ref>, MSCOCO <ref type="bibr" target="#b25">[26]</ref> 88.3 59.2 112.8 <ref type="table">Table 9</ref>. Comparison on 3DPW-Crowd between 3DCrowdNet and previous methods. 3DCrowdNet uses the least training datasets and achieves the best accuracy on in-the-wild crowded scenes. Appendix 6. Datasets 6.1. Training sets of different methods <ref type="table">Table 9</ref> demonstrates that the superiority of 3DCrowd-Net does not come from using more training data. It shows the training datasets used in the previous methods of the main manuscript's <ref type="table">Table 5</ref>. We trained 3DCrowdNet on one MoCap dataset and one in-the-wild 2D dataset, which is the least training set among methods, and we tested it on 3DPW-Crowd. It still significantly outperforms the previous methods in all metrics. We used 2D pose outputs of HigherHRNet <ref type="bibr" target="#b4">[5]</ref>, which is trained only on MSCOCO <ref type="bibr" target="#b25">[26]</ref>. The results strongly support that our contributions listed in the main manuscript's Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Details of testing sets</head><p>3DPW-Crowd. The sequence names of 3DPW-Crowd are courtyard hug 00 and courtyard dancing 00, a subset of the 3DPW <ref type="bibr" target="#b49">[50]</ref> validation set. 3DPW-Crowd contains 1073 images and 1923 persons with GT 3D pose and shape annotations. The average bounding box IoU is 37.5%, and the CrowdIndex [25] is 49.3%. We used 14 joints defined by Human3.6M <ref type="bibr" target="#b14">[15]</ref> for evaluating PA-MPJPE and MPJPE following the previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref>. MuPoTS. MuPoTS <ref type="bibr" target="#b31">[32]</ref> contains 20 sequences, 8370 images, and 20899 persons with GT 3D pose annotations. The sequences are captured indoors and outdoors, and GT 3D poses are obtained by a multi-view marker-less motion capture system. The average bounding box IoU is 3.8%, and the CrowdIndex [25] is 13.2%. We used the official MAT-LAB code for evaluation. CMU-Panoptic. We selected four sequences that show people doing social activities, namely Haggling, Mafia, Ultimatum, and Pizza following <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">54]</ref>. Sequences captured by the 16th and 30th cameras are selected. The sequences contain 9600 frames and 21,404 persons with GT 3D pose annotations. The average bounding box IoU is 2.0%, and the CrowdIndex [25] is 11.1%. We used pre-processed GT annotations and followed the evaluation protocol of <ref type="bibr" target="#b15">[16]</ref> in their official code repository. 3DPW. We used the test set of 3DPW <ref type="bibr" target="#b49">[50]</ref> following the official split protocol. The test set contains 26240 images and 35515 persons with GT 3D pose and shape annotations. The average bounding box IoU is 3.7%, and the CrowdIndex <ref type="bibr" target="#b24">[25]</ref> is 4.9%. Sequences starring one actor are excluded in computing the bounding box IoU and the CrowdIndex. We used 14 joints defined by Human3.6M <ref type="bibr" target="#b14">[15]</ref> for evaluating PA-MPJPE and MPJPE following the previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">More qualitative results</head><p>Accurate 3D meshes from erroneous 2D pose input. <ref type="figure" target="#fig_7">Figure 7</ref> shows that our 3DCrowdNet can estimate robust 3D meshes, given inaccurate 2D poses from in-thewild crowded scenes. Due to inter-person occlusion and overlapping bounding boxes between people, 2D pose estimators <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> may produce inaccurate joint predictions as shown in the first, second, and third rows. To handle such cases, the feature extractor of 3DCrowdNet assigns don'tcare values to the joint predictions with low confidence (e.g. lower than 0.1 for outputs from <ref type="bibr" target="#b4">[5]</ref>) using heatmap representation, as discussed in Section 3.1 of the main manuscript. Then, the joint-based regressor of 3DCrowd-Net refines the 2D pose heatmap, while predicting a 3D pose with image features containing the image's context information. The 2D pose heatmap has a different joint set, a superset of joint sets defined by multiple datasets, with the 3D pose's joint set, an intersection of joint sets defined by multiple datasets. <ref type="figure" target="#fig_8">Figure 8</ref>    matic prior and regress parameters of a 3D mesh that best describes a target person in a crowd. The fourth and fifth rows of <ref type="figure" target="#fig_7">Figure 7</ref> prove that our approach is also effective on estimating robust 3D meshes from truncated images, which often have missing 2D joint predictions.</p><p>Comparison with SPIN. We provide more qualitative comparison with SPIN <ref type="bibr" target="#b22">[23]</ref> in <ref type="figure">Figure 9</ref>. SPIN is one of the state-of-the-art methods that are based on the two wheels of the current 3D human mesh estimation literature, the mixed batch training and the model-based approach using a global feature discussed in Section 1 of the main manuscript. Our 3DCrowdNet produces accurate and robust 3D meshes from diverse in-the-wild crowded scenes. On the other hand, SPIN predicts an incorrect overall pose for a person under severe inter-person occlusion Comparison with ROMP. We provide more qualitative comparison with ROMP <ref type="bibr" target="#b48">[49]</ref> in <ref type="figure">Figure 10</ref>. ROMP is a bottom-up method for multi-person 3D mesh estimation. Our 3DCrowdNet produces accurate and robust 3D meshes from diverse in-the-wild crowded scenes. On the contrary, ROMP predicts the wrong global rotation of a target person from in-the-wild crowded scenes (the third and fourth rows) and produces inaccurate leg poses under severe interperson occlusion and crossed human parts (the first and second rows).</p><p>Comparison with Pose2Mesh and I2L-MeshNet. <ref type="figure">Figure 11</ref> shows the qualitative comparison between 3DCrowdNet, Pose2Mesh <ref type="bibr" target="#b6">[7]</ref>, and I2L-MeshNet <ref type="bibr" target="#b34">[35]</ref>. Pose2Mesh and I2L-MeshNet are state-of-the-art modelfree 3D mesh estimators, which predict coordinates of mesh vertices. Especially, Pose2Mesh is one of the most relevant competitors, since it can also benefit from the same 2D pose input. 3DCrowdNet produces much more robust 3D meshes from in-the-wild crowded scenes than the two methods. Pose2Mesh estimates the most plausible 3D mesh for a given 2D pose (the first and fourth rows), not the 3D mesh that best describes a target in a crowd, as discussed in Section 5 of the main manuscript. Also, it often wrongly corrects the 2D pose input and produces common standing leg poses different from the images (the third, fifth, and sixth rows). I2L-MeshNet fails to distinguish different people in overlapping bounding boxes (the fifth and sixth rows). In addition, it tends to provide very noisy 3D pose and shape of a target in crowded scenes, which reveals the method's vulnerability to inter-person occlusion. The results in the second row also validate the superiority of 3DCrowdNet's robustness to truncated bodies.</p><p>Results on 3DPW-Crowd. <ref type="figure">Figure 13</ref> illustrates the 3DCrowdNet's outputs on 3DPW-Crowd. 3DCrowdNet estimates robust 3D pose and shape on images that show people having highly close interaction. Different people in overlapping bounding boxes are disentangled, and occluded body parts are reasonably reconstructed.  <ref type="figure">Figure 9</ref>. Qualitative comparison on the CrowdPose <ref type="bibr" target="#b24">[25]</ref> test set. We highlighted the failure cases of SPIN <ref type="bibr" target="#b22">[23]</ref> with red circles. SPIN tends to be sensitive to occlusion, while 3DCrowdNet provides robust 3D meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3DCrowdNet(Ours)</head><p>ROMP input image <ref type="figure">Figure 10</ref>. Qualitative comparison on the CrowdPose <ref type="bibr" target="#b24">[25]</ref> test set. We highlighted the failure cases of ROMP <ref type="bibr" target="#b48">[49]</ref> with red circles. Wrong global rotation of occluded persons (the third and fourth rows); inaccurate leg poses under inter-person occlusion (the first and third rows). 3DCrowdNet produces much more robust 3D meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Limitation</head><p>Although the proposed 3DCrowdNet highly outperforms the previous 3D mesh estimation methods in in-the-wild crowded scenes, there is a limitation to be resolved in future work. As shown in <ref type="figure">Figure 12</ref>, when the 2D pose is inaccurate and appearances of nearby persons are very similar, 3DCrowdNet fails to produce robust 3D meshes. The topleft and bottom-right cases of <ref type="figure">Figure 12</ref> are the representa-3DCrowdNet(Ours) 2D pose input input image Pose2Mesh I2L-MeshNet <ref type="figure">Figure 11</ref>. Qualitative comparison on the CrowdPose <ref type="bibr" target="#b24">[25]</ref> test set. From left, an input image, 2D pose input, 3DCrowdNet, I2L-MeshNet <ref type="bibr" target="#b34">[35]</ref>, and Pose2Mesh <ref type="bibr" target="#b6">[7]</ref> outputs. Our 3DCrowdNet successfully disentangles a target person from other people in a bounding box compared with I2L-MeshNet. Also, 3DCrowdNet produces a 3D shape that best describes a target person in images, while Pose2Mesh estimates a plausible 3D shape for given 2D poses, which does not correspond to input images. 3DCrowdNet and I2L-MeshNet use the same bounding boxes to crop an image for each person. 3DCrowdNet and Pose2Mesh use the same 2D poses from <ref type="bibr" target="#b4">[5]</ref>. tive cases, which can be easily found in sports images. In such cases, it is challenging for 3DCrowdNet to correct the inaccurate 2D pose with image features, since the context information in image features is ambiguous due to indistinguishable appearances. One way of resolving the challenge could be to model the relative translation between persons to better understand the context. Alternatively, data augmentation to make a network robust to similar appearances would be an interesting direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">2D pose estimators.</head><p>In this work, we used 2D pose outputs from Open-Pose <ref type="bibr" target="#b2">[3]</ref> and HigherHRNet <ref type="bibr" target="#b4">[5]</ref>. The OpenPose outputs used in 3DPW <ref type="bibr" target="#b49">[50]</ref> are included in the annotations of 3DPW <ref type="bibr" target="#b49">[50]</ref>. The OpenPose used in MuPoTS <ref type="bibr" target="#b31">[32]</ref> are obtained by running the third-party PyTorch <ref type="bibr" target="#b41">[42]</ref> code implementation 1 . OpenPose is trained on COCO2017 train <ref type="bibr" target="#b25">[26]</ref> dataset. It achieves 65.3 mAP (mean Average Precision) in COCO2017 val dataset. In the CrowdPose <ref type="bibr" target="#b24">[25]</ref> test set, it achieves 48.7 and 32.3 mAPs for medium and hard cases, respectively. All the HigherHRNet outputs are obtained by running the official code implementation. HigherHRNet is trained on COCO2017 train dataset. It achieves 0.671 mAP on COCO2017 val dataset. In the CrowdPose <ref type="bibr" target="#b24">[25]</ref> test set, it achieves 68.1 and 58.9 mAPs for medium and hard cases, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>License of the Used Assets</head><p>? MSCOCO dataset <ref type="bibr" target="#b25">[26]</ref> belongs to the COCO Consortium and are licensed under a Creative Commons Attribution 4.0 License. ? Human3.6M dataset <ref type="bibr" target="#b14">[15]</ref>'s licenses are limited to academic use only. ? MPII dataset <ref type="bibr" target="#b1">[2]</ref> is released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes. ? 3DPW dataset <ref type="bibr" target="#b49">[50]</ref> is released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes.</p><p>? CrowdPose dataset <ref type="bibr" target="#b24">[25]</ref> is released for academic research only and it is free to researchers from educational or research institutes for non-commercial purposes. ? MuCo-3DHP and MuPoTS <ref type="bibr" target="#b31">[32]</ref> are released for any noncommercial purposes. ? CMU-Panoptic <ref type="bibr" target="#b19">[20]</ref> is released only for research purposes. ? The third party implementation 2 of OpenPose <ref type="bibr" target="#b2">[3]</ref> is licensed under the MIT license. ? HigherHRNet <ref type="bibr" target="#b4">[5]</ref>'s implementation 3 is licensed under the MIT license.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Domain gap (occlusion/pose/appearance, etc)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Comparison between the baseline that takes only image features and 3DCrowdNet. The baseline gives stronger attention to an occluding person (female) instead of an occluded person (male), and produces a wrong 3D mesh. 3DCrowdNet pays attention to the target male and recovers an accurate 3D mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visualization from different viewpoints. 3DCrowdNet uses both 2D pose and image features and provide robust results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>depicts each joint set. Last, the joint-based regressor samples image features using the (x,y) pixel positions of the 3D pose and estimates human model parameters, SMPL<ref type="bibr" target="#b27">[28]</ref> parameters. The joint-based regressor's graph convolutional layers refines the image features of joint predictions by fully exploiting the human kine-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of 3D meshes from different viewpoints. Our 3DCrowdNet can recover a 3D shape that best describes the target person in an image, even when provided with inaccurate 2D poses, using the target person's image features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Illustration of joint sets. The red skeleton of the intersection of joint sets defines the joints' neighborhood of graph convolution used in the joint-based regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(top-left), estimates inaccurate leg poses (bottom-left, bottom-right), and produces noisy 3D meshes (top-right, bottom-left, bottom-right) that show vulnerability to inter-person occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>Failure cases of 3DCrowdNet. 3DCrowdNet's outputs on 3DPW-Crowd.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>input feature</cell><cell cols="2">MPJPE? PA-MPJPE?</cell></row><row><cell>image feature wo. guide</cell><cell>109.6</cell><cell>63.3</cell></row><row><cell>crowded scene-robust image feature</cell><cell>85.8</cell><cell>55.8</cell></row></table><note>. Ablation on the input image features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Ablation of the regressor types. Ablation on the sampling area of image features.</figDesc><table><row><cell>, 3, and 4.</cell></row><row><cell>Crowded scene-robust image feature. Table 1 shows</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation on the intermediate estimation target of the jointbased regressor during training and testing.</figDesc><table><row><cell>estimation target</cell><cell cols="3">MPJPE? PA-MPJPE?</cell></row><row><cell>2D pose</cell><cell>88.3</cell><cell>56.4</cell><cell></cell></row><row><cell>3D pose (Ours)</cell><cell>85.8</cell><cell>55.8</cell><cell></cell></row><row><cell>method</cell><cell cols="3">MPJPE? PA-MPJPE? MPVPE?</cell></row><row><cell>SPIN [23]</cell><cell>121.2</cell><cell>69.9</cell><cell>144.1</cell></row><row><cell>Pose2Mesh [7]</cell><cell>124.8</cell><cell>79.8</cell><cell>149.5</cell></row><row><cell>I2L-MeshNet [35]</cell><cell>115.7</cell><cell>73.5</cell><cell>162.0</cell></row><row><cell>ROMP [49] *</cell><cell>104.8</cell><cell>63.9</cell><cell>127.8</cell></row><row><cell>3DCrowdNet (Ours)</cell><cell>86.8</cell><cell>56.1</cell><cell>109.7</cell></row><row><cell>3DCrowdNet (Ours) *</cell><cell>85.8</cell><cell>55.8</cell><cell>108.5</cell></row><row><cell cols="4">Table 5. Comparison on 3DPW-Crowd between 3DCrowdNet and</cell></row><row><cell cols="4">previous methods. We evaluate other methods with their codes and</cell></row><row><cell>pre-trained models.</cell><cell></cell><cell></cell><cell></cell></row></table><note>* means using CrowdPose [25] for training.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tensorboy/pytorch_Realtime_ Multi-Person_Pose_Estimation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tensorboy/pytorch_Realtime_ Multi-Person_Pose_Estimation 3 https : / / github . com / HRNet / HigherHRNet -Human -Pose-Estimation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HigherHRNet: Scaleaware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond static features for temporally consistent 3D human pose and shape from a video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose2Mesh: Graph convolutional network for 3D human pose and mesh recovery from a 2D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reconstructing 3D human pose by watching humans in the mirror</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Remips: Physically consistent 3d reconstruction of multiple interacting people under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teodor</forename><surname>Szente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Detecting human abnormal behaviour through a video generated model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Seychell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexiei</forename><surname>Dingli</surname></persName>
		</author>
		<editor>ISPA. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Ko?eck?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Differentiable hierarchical graph grouping for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Panoptic Studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CrowdPose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comprehensive study of weight sharing in graph networks for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">XNect: Real-time multi-person 3D motion capture with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<editor>Mohamed Elgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard Pons-Moll, and Christian Theobalt</editor>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3D pose estimation from monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3D multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PoseFix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-Lixel prediction network for accurate 3D human pose and mesh estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11232,2020.5</idno>
		<title level="m">NeuralAnnot: Neural annotator for in-the-wild expressive 3D human pose and mesh training sets</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pose2pose: 3D positional pose-guided 3D rotational pose prediction for expressive 3d human pose and mesh estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11534,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On self-contact and human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Re-identification for online person tracking by modeling space-time continuum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeti</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srirangaraj</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venu</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Associative Embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DeepCut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Peeking into occluded joints: A novel framework for crowd pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingteng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Synthetic training for accurate 3D human pose and shape estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human body model fitting by learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monocular, one-stage, regression of multiple 3D people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">HMOR: Hierarchical multi-person ordinal relations for monocular multi-person 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06475</idno>
		<title level="m">A large-scale dataset for going deeper in image understanding</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3D sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning 3D human shape and pose from dense body parts. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pose2Seg: Detection free human instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">SMAP: Single-shot multiperson absolute 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
