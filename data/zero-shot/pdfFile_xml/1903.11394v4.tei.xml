<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motion Deblurring with an Adaptive Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
							<email>kuldeeppurohit3@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Motion Deblurring with an Adaptive Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the problem of dynamic scene deblurring in the presence of motion blur. Restoration of images affected by severe blur necessitates a network design with a large receptive field, which existing networks attempt to achieve through simple increment in the number of generic convolution layers, kernel-size, or the scales at which the image is processed. However, increasing the network capacity in this manner comes at the expense of increase in model size and inference speed, and ignoring the non-uniform nature of blur. We present a new architecture composed of spatially adaptive residual learning modules that implicitly discover the spatially varying shifts responsible for non-uniform blur in the input image and learn to modulate the filters. This capability is complemented by a self-attentive module which captures non-local relationships among the intermediate features and enhances the receptive field. We then incorporate a spatiotemporal recurrent module in the design to also facilitate efficient video deblurring. Our networks can implicitly model the spatially-varying deblurring process, while dispensing with multi-scale processing and large filters entirely. Extensive qualitative and quantitative comparisons with prior art on benchmark dynamic scene deblurring datasets clearly demonstrate the superiority of the proposed networks via reduction in model-size and significant improvements in accuracy and speed, enabling almost real-time deblurring.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion deblurring is a challenging problem in computer vision due to its ill-posed nature. The past decade has witnessed significant advances in deblurring, wherein major efforts have gone into designing priors that are apt for recovering the underlying undistorted image and the camera trajectory <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40]</ref>. An exhaustive survey of uniform blind deblurring algorithms can be found in <ref type="bibr" target="#b21">[22]</ref>. Few approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> have proposed hybrid algorithms where a Convolutional Neural Net-work (CNN) estimates the blur kernel, which is then used in an alternative optimization framework for recovering the latent image.</p><p>However, these methods have been developed based on a rather strong constraint that the scene is planar and that the blur is governed by only camera motion. This precludes commonly occurring blur in most practical settings. Real-world blur arises from various sources including moving objects, camera shake and depth variations, causing different pixels to acquire different motion trajectories. A class of algorithms involve segmentation methods to relax the static and fronto-parallel scene assumption by independently restoring different blurred regions in the scene <ref type="bibr" target="#b15">[16]</ref>. However, these methods depend heavily on an accurate segmentation-map. Few methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13]</ref> circumvent the segmentation stage by training CNNs to estimate locally linear blur kernels and feeding them to a non-uniform deblurring algorithm based on patch-level prior. However, they are limited in their capability when it comes to general dynamic scenes.</p><p>Conventional approaches for video deblurring are based on image deblurring techniques (using priors on the latent sharp frames and the blur kernels) which remove uniform blurs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b61">61]</ref> and non-uniform blur caused by rotational camera motion <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b62">62]</ref>. However, these approaches are applicable only under the strong assumption of static scenes and absence of depth-dependent distortions. The work in <ref type="bibr" target="#b55">[56]</ref> proposed a segmentation-based approach to address different blurs in foreground and background regions. Kim et al. <ref type="bibr" target="#b16">[17]</ref> further relaxed the constraint on the scene motion by parameterizing spatially varying blur kernel using optical flow.</p><p>With the introduction of labeled realistic motion blur datasets <ref type="bibr" target="#b47">[48]</ref>, deep learning based approaches have been proposed to estimate sharp video frames in an end-to-end manner. Deep Video Deblurring (DVD) <ref type="bibr" target="#b47">[48]</ref> is the first such work to address generalized video deblurring wherein a neural network accepts a stack of neighboring blurry frames for deblurring. They perform off-line stabilization of the blurred frames before feeding them to the network, which learns to exploit the information from multiple frames to de-blur the central frame. Nevertheless, when images are heavily blurred, this method introduces temporal artifacts that become more visible after stabilization. Few methods have also been proposed for burst image deblurring <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b0">1]</ref> which utilize number of observations with independent blurs to restore a scene, but are not trained for general video deblurring. Online Video Deblurring (OVD) <ref type="bibr" target="#b17">[18]</ref> presents a faster design for video deblurring which does not require frame alignment. It utilizes temporal connections to increase the receptive field of the network. Although OVD can handle large motion blur without adding a computational overhead, it lacks in accuracy and is not real-time.</p><p>There are two major limitations shared by prior deblurring works. Firstly, the filters of a generic CNN are spatially invariant (with spatially-uniform receptive field), which is suboptimal to model the process of dynamic scene deblurring and limits their accuracy. Secondly, existing methods achieve high receptive field through networks with a large number of parameters and high computational footprint, making them unsuitable for real-time applications. As the only other work of this kind, <ref type="bibr" target="#b63">[63]</ref> recently proposed a design composed of multiple CNNs and Recurrent Neural Networks (RNN) to learn spatially varying weights for deblurring. However, their performance is inferior to the state-of-the-art <ref type="bibr" target="#b49">[50]</ref> in several aspects. Reaching a trade-off among inference time, accuracy of restoration, and receptive field is a non-trivial task which we address in this paper. We investigate position and motion-aware CNN architecture, which can efficiently handle multiple image segments undergoing motion with different magnitude and direction.</p><p>Following recent developments, we adopt an end-toend learning based approach to directly estimate the restored sharp image. For single image deblurring, we build a fully convolutional architecture equipped with filtertransformation and feature modulation capability suited for the task of motion deblurring. Our design hinges on the fact that motion blur is essentially an aggregation of various spatially varying transformations of the image, and a network that implicitly adapts to the location and direction of such motion, is a better candidate for the restoration task. Next, we address the problem of video deblurring, wherein we extend our single image deblurring network to exploit the redundancy across consecutive frames of a video to guide the process. To this end, we introduce spatio-temporal recurrence at frame and feature-level to efficiently restore sequences of blurred frames.</p><p>Our network contains various layers to spatially transform intermediate filters as well as the feature maps. Its advantages over prior art are three-fold: 1. It is fully convolutional and parametrically efficient: deblurring can be achieved with just a single forward pass through a compact network. 2. Its components can be easily introduced into other architectures and trained in an end-to-end manner us-ing conventional loss functions. 3. The transformations estimated by the network are dynamic and hence can be meaningfully interpreted for any test image.</p><p>The efficiency of our architecture is demonstrated through comprehensive comparisons with the state-of-theart on image and video deblurring benchmarks. While a majority of image and video deblurring networks contain &gt; 7 million parameters, our model achieves superior performance at only a fraction of this size, while being computationally more efficient, resulting in real-time deblurring of images on a single GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Architectures</head><p>An existing technique for accelerating various image processing operations is to down-sample the input image, execute the operation at low resolution, and up-sample the output <ref type="bibr" target="#b5">[6]</ref>. However, this approach discounts the importance of resolution, rendering it unsuitable for image restoration tasks where high-frequency content of the image is of prime importance (deblurring, super-resolution).</p><p>Another efficient design is a CNN with a fixed but very large receptive field (comparable to very-high resolution images), e.g. Cascaded dilated network <ref type="bibr" target="#b6">[7]</ref>, which was proposed to accelerate various image-to-image tasks. However, simple dilated convolutions are not appropriate for restoration tasks (as shown in <ref type="bibr" target="#b23">[24]</ref> for image super-resolution). After several layers of dilated filtering, the output only considers a fixed sparse sampling of input locations, resulting in significant loss of information.</p><p>Until recently, the driving force behind performance improvement in deblurring was use of large number of layers, larger filters, and multi-scale processing which gradually increases the "fixed" receptive field. Not only is it a suboptimal design, it is also difficult to scale since the effective receptive field of deep CNNs is much smaller than the theoretical one (investigated in <ref type="bibr" target="#b24">[25]</ref>).</p><p>We claim that a better alternative is to design a convolutional network whose receptive field is adaptive to input image instances. We show that the latter approach is a far better choice due to its task-specific efficacy and utility for computationally limited environments, and it delivers consistent performance across diverse magnitudes of blur. We explain the need for a network with asymmetric filters. Given a 2D image I and a blur kernel K, the motion blur process can be formulated as: given location [x, y], the sharp intensity can be represented as</p><formula xml:id="formula_0">B[x, y] =</formula><formula xml:id="formula_1">I[x, y] = B[x, y] K[0, 0] ? M/2,M/2 m,n=?M/2 K[m, n]I[x ? n, y ? n] K[0, 0] ,<label>(2)</label></formula><p>which is a 2D infinite impulse response (IIR) model. Recursive expansion of the second term would eventually lead to an expression which contains values from only the blurred image and the kernel as</p><formula xml:id="formula_2">I[x, y] = B[x, y] K[0, 0] ? M/2,M/2 m,n=?M/2 K[m, n]B[x ? m, y ? n] K[0, 0] 2 + M/2,M/2 m,n=?M/2 M/2,M/2 i,j=?M/2 K[m, n]K[i, j]I[x ? n ? i, y ? n ? j] K[0, 0] 2<label>(3)</label></formula><p>The dependence of I[x, y] on a large number of locations in B shows that the deconvolution process requires infinite signal information. If we assume that the boundary of the image is zero, eq. 3 is equivalent to applying an inverse filter to B. As visualized in <ref type="bibr" target="#b63">[63]</ref>, the non-zero region of such an inverse deblurring filter is typically much larger than the blur kernel. Thus, if we use a CNN to model the process, a large receptive field should be considered to cover the pixel positions that are necessary for deblurring. Eq. 3 also shows that only a few coefficients (which are K[m, n] for m, n ? [?M/2, M/2]) need to be estimated by the deblurring model, provided we can find an appropriate operation to cover a large enough receptive field.</p><p>For this theoretical analysis, we will temporarily assume that the motion blur kernel K is linear (assumption used in few prior deblurring works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13]</ref>). Now, consider an image B which is affected by motion blur in the horizontal direction (without loss of generality), implying K[m, n] = 0 for m = 0 (non-zero values present only in the middle row of the kernel). For such a case, eq. 3 translates to</p><formula xml:id="formula_3">I[x, y] = B[x, y] K[0, 0] ? M n=1 K[o, n]B[x, y ? n] K[0, 0] 2 + M n=1 M j=1 K[0, n]K[0, j]I[x, y ? n ? j] K[0, 0] 2 = ...<label>(4)</label></formula><p>It can be seen that for this case, I[x, y] can be expressed as a function of only one row of pixels in the blurred image B, which implies that for a horizontal blur kernel, the deblurring filter is also purely horizontal. We use this observation to state a hypothesis that holds for any motion blur kernel: "Deblurrig filters are directional/asymmetric in shape". This is because motion blur kernels are known to be inherently directional. Such an operation can be efficiently learnt by a CNN with adaptive and asymmetric filters and this forms the basis for our work.</p><p>Inspired by the success of deblurring works that utilize networks composed of residual blocks to directly regress to the sharp image <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">50]</ref>, we build our network over a residual encoder-decoder structure. Such a structure was adopted in Scale Recurrent Network (SRN) <ref type="bibr" target="#b49">[50]</ref>, which is the current state-of-the-art in deblurring. We differentiate our design from SRN in terms of compactness and computational footprint. While SRN is composed of 5 ? 5 conv filters, we employ only 3 ? 3 filters for economy. Unlike <ref type="bibr" target="#b49">[50]</ref>, our single image deblurring network does not contain recurrent units, and most importantly, our approach does not involve multi-scale processing; the input image undergoes only a single pass through the network. Understandably, these changes can drastically reduce the inference time of the network and also decrease the model's representational capacity and receptive field in comparison to SRN, with potential for significant reduction in the deblurring performance. In what follows, we describe our proposed architecture which matches the efficiency of above network while significantly improving representational capacity and performance.</p><p>In our proposed Spatially-Adaptive Residual Network (SARN), the encoder sub-network progressively transforms the input image into feature maps with smaller spatial size and more channels. Our spatially adaptive modules (Deformable Residual Module (DRM) and Spatial Attention (SA) module) operate on the output of the encoder, where the spatial resolution of features is the smallest which leads to minimum additional computations. The resulting features are fed to the Decoder, wherein it is passed through a series of Res-Blocks and deconvolution layers to reconstruct the output image. A schematic of the proposed architecture is shown in <ref type="figure">Fig. 1</ref>, where n (=32) represents the number of channels in the first feature map. Next, we describe the proposed modules in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deformable Residual Module (DRM)</head><p>CNNs operate on fixed locations in a regular grid which limits their ability to model unknown geometric transformations. Spatial Transform Networks (STN) <ref type="bibr" target="#b13">[14]</ref> introduced spatial transformation learning into CNNs, wherein an image-dependent global parametric transformation is estimated and applied on the feature map. However, such warping is computationally expensive and the transformation is considered to be global across the whole image, which is not the case for motion in dynamic and 3D scenes where different regions are affected by different magnitude and direction of motion. Hence, we adopt deformable convolutions <ref type="bibr" target="#b9">[10]</ref>, which enable local transformation learning in an efficient manner. Unlike regular convolutional layers, the deformable convolution <ref type="bibr" target="#b9">[10]</ref> also learns to estimate the shapes of convolution filters conditioned on an input feature map. While maintaining filter weights invariant to the input, a deformable convolution layer first learns a dense offset map from the input, and then applies it to the regular feature map for re-sampling.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our DRM contains the additional capability to a learn positions of the sampling grid used in the convolution. A regular convolution layer is present to estimate the features and another convolution layer to estimate 2D filter offsets for each spatial location. These channels (feature-maps containing red-arrows in <ref type="figure" target="#fig_1">Fig. 2</ref>) represent the estimated 2D offset of each input. The 2D offsets are encoded in the channel dimension i.e., convolution layer of k?k filters is paired with offset predicting convolution layer of 2k 2 channels. These offsets determine the shifting of the k 2 filter locations along horizontal and vertical axes. As a result, the regular convolution filter operates on an irregular grid of pixels. Since the offsets can be fractional, bilinear interpolation is used to sample from the input feature map. All the parts of our network are trainable end-to-end, since bilinear sampling and the grid generation of the warping module are both differentiable <ref type="bibr" target="#b37">[38]</ref>. The offsets are initialized to 0. Finally, the additive link grants the benefits of reusing common features with low redundancy.</p><p>The convolution operator slides a filter or kernel over the input feature map X to produce output feature map Y. For each sliding position p b , a regular convolution with filter weights W, bias term b and stride 1 can be formulated as</p><formula xml:id="formula_4">Y = W * X + b y p b = c pn?R w c,n ? x c,p b +pn + b (5)</formula><p>where c is the index of input channel, p b is the base position of the convolution, n = 1, . . . , N with N = |R| and p n ? R enumerates the locations in the regular grid R.</p><p>The center of R is denoted as p m which is always equal to (0, 0), under the assumption that both height and width of the kernel are odd numbers. This assumption is suitable for most CNNs. m is the index of the central location in R.</p><p>The deformable convolution augments all the sampling locations with learned offsets {?p n |n = 1, . . . , N }. Each offset has a horizontal component and a vertical component. Totally 2N offset parameters are required to be learnt for each sliding position. Equation (5) then becomes</p><formula xml:id="formula_5">y p b = pn?R w n ? x H(pn) + b<label>(6)</label></formula><p>where H(p n ) = p b + p n + ?p n is the learned sampling position on input feature map. The input channel c in <ref type="formula">(5)</ref> is omitted in <ref type="formula" target="#formula_5">(6)</ref> for notational clarity, because the same operation is applied in every channel. The receptive field and the spatial sampling locations are adapted according to the scale, shape, and location of the degradation. Presence of a cascade of DRMs imparts higher accuracy to the network while delivering higher parameter efficiency than the state-of-the-art deblurring approaches. Although the focus of our work is a compact network design, it also provides an effective way to further increase the network capacity since replacing normal Res-Blocks with DRMs is much more efficient than going deeper or wider. In our final network, 6 DRMs are present in the mid-level of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video Deblurring through Spatio-temporal recurrence</head><p>A natural extension to single image deblurring is video deblurring. However, video deblurring is a more structured problem as it can utilize information distributed across multiple observations to mitigate the ill-posedness of deblurring. Existing learning-based approaches <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b17">18]</ref> have proposed generic encoder-decoder architectures to aggregate information from neighboring frames. At each time step, DVD <ref type="bibr" target="#b47">[48]</ref> accepts a stack of neighboring blurred frames as input to network, while OVD <ref type="bibr" target="#b17">[18]</ref> accepts intermediate features extracted from past frames.</p><p>We present an effective technique which elegantly extends our efficient single image deblurring design to restore a sequence of blurred frames. The proposed network encourages recurrent information propagation along the temporal direction at feature-level as well as frame-level to achieve temporal consistency and improve restoration quality. For feature propagation, our network employs Convolutional Long-Short Term Memory (LSTM) modules <ref type="bibr" target="#b46">[47]</ref> which are known to efficiently process spatio-temporal data and perform gated feature propagation. The process can be expressed as</p><formula xml:id="formula_6">f i = Net E (B i , I i?1 ), h i , g i = ConvLSTM(h i?1 , f i ; ? LST M ), I i = Net D (g i ; ? D ),<label>(7)</label></formula><p>where i represents frame index, Net D is the decoder part of our network with parameters ? D and Net E is the portion before the decoder. ? LST M is the set of parameters in ConvLSTM. The hidden state h i contains useful information about intermediate results and blur patterns, which are passed to the network processing the next frame, thus assisting it in sharp feature aggregation.</p><p>Unlike <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b17">18]</ref>, our framework also employs recurrence at frame level wherein previously deblurred estimates are provided at the input to the network that processes subsequent frames. This naturally encourages temporally consistent results by allowing it to assimilate a large number of previous frames without increased computational demands. Our network accepts 5 frames at each time-step (early fusion), of which 2 frames are deblurred estimates from past and 2 are blurred frames from future. As discussed in <ref type="bibr" target="#b1">[2]</ref>, such early fusion allows the initial layers to assimilate complementary information from neighboring frames and improves restoration quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results</head><p>In this section, we carry out quantitative and qualitative comparisons of our architectures with state-of-the-art methods for image as well as video deblurring tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Deblurring</head><p>Due to the complexity of the blur present in general dynamic scenes, conventional uniform blur model based deblurring approaches struggle to perform well <ref type="bibr" target="#b27">[28]</ref>. However, we compare with conventional non-uniform deblurring approaches by Xu et al. <ref type="bibr" target="#b58">[58]</ref> and Whyte et. al. <ref type="bibr" target="#b53">[54]</ref> (proposed for static scenes) and <ref type="bibr" target="#b15">[16]</ref> (proposed for dynamic scenes). Further, we compare with state-of-the-art end-toend learning based methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b49">50]</ref>. The source codes and trained models of competing methods are publicly available on the authors' websites, except for <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b63">[63]</ref> whose results have been reported in previous works <ref type="bibr" target="#b63">[63,</ref><ref type="bibr" target="#b49">50]</ref>. Public implementations with default parameters were used to obtain qualitative results on selected test images. Quantitative Evaluation Quantitative comparisons using PSNR and SSIM scores obtained on the GoPro testing set are presented in <ref type="table" target="#tab_0">Table 1</ref>. Since traditional methods cannot model combined effects of general camera shake and object motion <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b53">54]</ref> or forward motion and depth variations <ref type="bibr" target="#b15">[16]</ref>, they fail to faithfully restore most of the images in the test-set. The below par performance of <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13]</ref> can be attributed to the fact that they use synthetic and simplistic blur kernels to train their CNN and employ traditional deconvolution methods to estimate the sharp image, which severely limits their applicability to general dynamic scenes. On the other hand, the method of <ref type="bibr" target="#b20">[21]</ref> trains a network containing instance-normalization layers using a mixture of deep-feature losses and adversarial losses, but leads to suboptimal performance on images containing large blur. The methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b49">50]</ref> use a multi-scale strategy to improve capability to handle large blur, but fail in challenging situations. One can note that the proposed SARN significantly outperforms all prior works, including the spatially varying RNN based approach <ref type="bibr" target="#b63">[63]</ref>. As compared to the state-of-theart <ref type="bibr" target="#b49">[50]</ref>, our network offers an improvement of ? 0.9 dB. Qualitative Evaluation Visual comparisons on different dynamic and 3D scenes are given in <ref type="figure">Fig. 3</ref>. It shows that results of prior works suffer from incomplete deblurring or ringing artifacts. In contrast, our network is able to restore scene details more faithfully due to its effectiveness in handling large dynamic blur and preserving sharpness. Importantly, our method fares significantly better in terms of model-size and inference-time (70% smaller and 20? faster than the nearest competitor [50] on a single GPU). An additional advantage over <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b53">54]</ref> is that our model waives-off the requirement of parameter tuning during test phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Video Deblurring</head><p>Quantitative Evaluation To demonstrate the superiority of our model, we compare the performance of our network with that of state-of-the-art video deblurring approaches on 10 test videos from the benchmark <ref type="bibr" target="#b47">[48]</ref>. Specifically, we compare our models with conventional model of <ref type="bibr" target="#b10">[11]</ref>, two versions of DVD <ref type="bibr" target="#b47">[48]</ref>, and OVD <ref type="bibr" target="#b17">[18]</ref>. Source codes of competing methods are publicly available on the authors' websites, except for <ref type="bibr" target="#b10">[11]</ref> whose results have been reported in <ref type="bibr" target="#b47">[48]</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows quantitative comparisons between our method and competing methods. We also include a baseline 'Ours-Multi', which refers to a version of our network which takes a stacks 5 consecutive blurred frames as input (configuration of DVD-Noalign). 'Ours-recurrent' refers to our final network involving recurrence at frame as well as feature level. The results indicate that our method significantly outperforms prior methods (? 1 dB higher). Qualitative Evaluation <ref type="figure" target="#fig_3">Fig. 4</ref> contains visual comparisons with <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b17">18]</ref> on different test frames from the qualitative and quantitative subsets of <ref type="bibr" target="#b47">[48]</ref> which suffer from complex blur due to large motion. Although traditional method <ref type="bibr" target="#b16">[17]</ref> models pixel-level blur using optical flow as a cue, its (a) Blurred Image (b) Blurred patch (c) Whyte et al. <ref type="bibr" target="#b53">[54]</ref> (d) Nah et al. <ref type="bibr" target="#b27">[28]</ref> (e) DelurGAN <ref type="bibr" target="#b20">[21]</ref> (f) SRN <ref type="bibr" target="#b49">[50]</ref> (g) Ours <ref type="figure">Figure 3</ref>. Visual comparisons of deblurring results on test images from the GoPro dataset <ref type="bibr" target="#b27">[28]</ref>. Key blurred patches are shown in (b), while zoomed-in patches from the deblurred results are shown in (c)-(g). (best viewed in high resolution).     fails to completely deblur many scenes due to its simplistic assumptions on the kernels and the image properties. Learning based methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b17">18]</ref> fare better than <ref type="bibr" target="#b16">[17]</ref> in several cases but still lead to artifacts in deblurring due to their suboptimal network design. Our method generates sharper results and faithfully restores the scenes, while yielding significant improvements on images affected with large blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>We proposed efficient image and video deblurring architectures composed of convolutional modules that enable spatially adaptive feature learning through filter transformations and feature attention over spatial domain, namely deformable residual module (DRM) and self-attentive (SA) module. The DRMs implicitly address the shifts responsible for the local blur in the input image, while the SA module non-locally connects spatially distributed blurred regions. Presence of these modules awards higher capacity to our compact network without any notable increase in model size. Our network's key strengths are large receptive field and spatially varying adaptive filter learning capability, whose effectiveness is also demonstrated for video deblurring through a recurrent extension of our network. Experiments on dynamic scene deblurring benchmarks showed that our approach performs favorably against prior art and facilitates real-time deblurring. We believe our spatially-aware design can be utilized for other image processing and vision tasks as well, and we shall explore them in the future.</p><p>Refined and complete version of this work appeared in AAAI 2020.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>M/ 2 ,M/ 2 mFigure 1 .</head><label>221</label><figDesc>,n=?M/2 K[m, n]I[x ? n, y ? n], (1) where B is the blurred image, [x, y] represents the pixel coordinates, and M ? M is the size of the blur kernel. At any The proposed deblurring network and its components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Schematic of our deformable residual module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Blurred Image (b) Blurred patch (c) Kim et al.<ref type="bibr" target="#b16">[17]</ref> (d) DVD<ref type="bibr" target="#b47">[48]</ref> (e) OVD<ref type="bibr" target="#b17">[18]</ref> (f) Ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparisons of video deblurring results on two test frames from the DVD dataset<ref type="bibr" target="#b47">[48]</ref>. Key blurred patches are shown in (b), while zoomed-in patches from the deblurred results are shown in (c)-(f). (best viewed in high resolution).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance Comparison of our method with existing deblurring algorithms on single image deblurring benchmark dataset<ref type="bibr" target="#b27">[28]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="10">Xu [58] Whyte [54] Kim [16] Sun[49] MBMF [13] MS-CNN [28] DeblurGAN [21] SRN [50] SVRNN [63] SARN (Ours)</cell></row><row><cell>PSNR (dB)</cell><cell>21</cell><cell>24.6</cell><cell>23.64</cell><cell>24.64</cell><cell>26.4</cell><cell>29.08</cell><cell>28.7</cell><cell>30.26</cell><cell>29.19</cell><cell>31.13</cell></row><row><cell>SSIM</cell><cell>0.7407</cell><cell>0.8458</cell><cell>0.8239</cell><cell>0.843</cell><cell>0.8632</cell><cell>0.914</cell><cell>0.858</cell><cell>0.934</cell><cell>0.931</cell><cell>0.947</cell></row><row><cell>Time (s)</cell><cell>3800</cell><cell>700</cell><cell>3600</cell><cell>1500</cell><cell>1200</cell><cell>6</cell><cell>1</cell><cell>0.4</cell><cell>1</cell><cell>0.02</cell></row><row><cell>Size (MB)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.1</cell><cell>41.2</cell><cell>55</cell><cell>50</cell><cell>28</cell><cell>37.1</cell><cell>11.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison of our method with existing video deblurring approaches on the benchmark dataset<ref type="bibr" target="#b47">[48]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="5">WFA DVD [48] DVD [48] OVD Ours-</cell><cell>Ours-</cell></row><row><cell></cell><cell>[11]</cell><cell>Noalign</cell><cell>Flow</cell><cell>[18]</cell><cell cols="2">Multi Recurrent</cell></row><row><cell cols="2">PSNR (dB) 28.35</cell><cell>30.05</cell><cell>30.05</cell><cell cols="2">29.95 30.60</cell><cell>31.15</cell></row><row><cell>Time (s)</cell><cell>15</cell><cell>0.7</cell><cell>5</cell><cell>0.3</cell><cell>0.02</cell><cell>0.05</cell></row><row><cell>Size (MB)</cell><cell>-</cell><cell>61.2</cell><cell>61.2</cell><cell>11.0</cell><cell>11.2</cell><cell>12.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparisons of different versions of our single image deblurring network on GoPro testset<ref type="bibr" target="#b27">[28]</ref>.</figDesc><table><row><cell>DRMs</cell><cell>0</cell><cell>3</cell><cell>6</cell><cell>6</cell></row><row><cell>SA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">PSNR (dB) 30.64 30.69 31.05 31.13</cell></row><row><cell>Size (MB)</cell><cell>10.7</cell><cell>10.9</cell><cell>11.2</cell><cell>11.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Burst image deblurring using permutation invariant convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2848" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind motion deblurring using multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="5057" to="5071" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inferring image transformation and structure from motion-blurred images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chandramouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bilateral guided upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast image processing with fully-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Registration based non-uniform motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2183" to="2192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hand-held video deblurring via efficient fourier aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Removing camera shake from a single photograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From motion blur to motion flow: A deep learning solution for removing heterogeneous motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3806" to="3815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Psf estimation using sharp edge prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized video deblurring for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5426" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Online video deblurring via dynamic temporal blending network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast image deconvolution using hyper-laplacian priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Blind deconvolution using a normalized sparsity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comparative study for single image blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1701" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating sharp panoramas from motion-blurred videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-level wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="886" to="88609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unconstrained motion deblurring for dual-lens cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Girish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7870" to="7879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep dynamic scene deblurring for unconstrained dual-lens cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nithin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4479" to="4491" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generating high quality pan-shots from motion blurred videos. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nimisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aravind</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="20" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Blur-invariant deep learning for blind-deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nimisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised class-specific deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Nimisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deblurring text images via l0-regularized intensity and gradient prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust kernel estimation with outliers handling for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2800" to="2808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deblurring images via dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2315" to="2328" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shape from sharp and motion-blurred image pair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paramanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="272" to="292" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth from motion and optical blur with an unscented kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paramanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2798" to="2811" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-uniform motion deblurring for bilayer scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paramanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1115" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Workshop on Autodiff</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Region-adaptive dense network for efficient motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11882" to="11889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bringing alive blurred moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning based single image blur detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2202" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Harnessing motion blur to unveil splicing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seetharaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on information forensics and security</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="583" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Inferring plane orientation from a single motion blurred image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seetharaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2089" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A machine learning approach for non-blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chun Woo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="769" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Non-blind deblurring: Handling kernel uncertainty with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Maligireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3272" to="3281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">From local to global: Edge profiles to camera motion in blurred images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4447" to="4456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Non-uniform deblurring in hdr image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paramanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3739" to="3750" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Non-uniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Modeling blurred video with layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="236" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2010</title>
		<editor>K. Daniilidis, P. Maragos, and N. Paragios</editor>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6311</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unnatural l0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Image deblurring via extreme channels prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6978" to="6986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-shot imaging: Joint alignment, deblurring, and resolution-enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2925" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multi-image blind deblurring using a coupled adaptive sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1051" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Intra-frame deblurring by leveraging inter-frame camera motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4036" to="4044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
