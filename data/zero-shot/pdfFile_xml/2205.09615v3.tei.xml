<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EXACT: How to Train Your Accuracy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Karpukhin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Dereka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Kolesnikov</surname></persName>
						</author>
						<title level="a" type="main">EXACT: How to Train Your Accuracy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classification tasks are usually evaluated in terms of accuracy. However, accuracy is discontinuous and cannot be directly optimized using gradient ascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate losses, which can lead to suboptimal results.</p><p>In this paper, we propose a new optimization framework by introducing stochasticity to a model's output and optimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive experiments on linear models and deep image classification show that the proposed optimization method is a powerful alternative to widely used classification losses. 2 https://github.com/tinkoff-ai/exact arXiv:2205.09615v3 [cs.LG] 21 Sep 2022 EXACT: How to Train Your Accuracy Model Logits distribution Accuracy loss Input Figure 2. EXACT training pipeline. The model predicts the mean and variance of the logit vector. EXACT's training objective estimates accuracy, which is differentiable for the stochastic model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accuracy is one of the most used evaluation metrics in computer vision <ref type="bibr" target="#b24">(LeCun et al., 1998;</ref><ref type="bibr" target="#b21">Krizhevsky et al., 2009;</ref>, natural language processing <ref type="bibr" target="#b25">(Maas et al., 2011;</ref><ref type="bibr" target="#b41">Zhang et al., 2015)</ref>, and tabular data classification <ref type="bibr" target="#b1">(Ar?k &amp; Pfister, 2021)</ref>. While accuracy naturally appears in classification tasks, it is discontinuous and difficult to optimize directly. To tackle this problem, multiple surrogate losses were proposed, including cross-entropy and hinge loss <ref type="bibr" target="#b36">(Wang et al., 2022)</ref>. However, as shown below, in our toy example from <ref type="figure">Figure 1</ref>, decreasing cross-entropy or hinge loss can lead to a drop in accuracy. In other words, there is no direct connection between surrogate losses and accuracy optimization.</p><p>In this work, we take a different approach to accuracy optimization. Our idea is to introduce stochasticity to the model's output and then optimize the expected accuracy, i.e. accuracy of the stochastic model, via gradient methods. We call the proposed method EXpected ACcuracy opTimization (EXACT). It directly optimizes the accuracy of the stochastic model in contrast to surrogate losses.</p><p>Contributions of this work can be summarized as follows: 1 Tinkoff.</p><p>Correspondence to: Ivan Karpukhin &lt;i.a.karpukhin@tinkoff.ru&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hinge</head><p>Cross-entropy EXACT Max accuracy Class 1</p><p>Class -1 <ref type="figure">Figure 1</ref>. The toy example, which demonstrates importance of accuracy optimization. The model consists of a single bias parameter (decision threshold), while scaling weight is assumed to be 1. EX-ACT achieves 100% accuracy, while cross-entropy and hinge loss misclassify one element.</p><p>1. We propose a new optimization framework for classification tasks. To the best of our knowledge, it is the first work, where the classification model's accuracy is directly optimized via gradient methods.</p><p>2. We provide an efficient method for evaluating the proposed loss function and its gradient. We do this by presenting a new algorithm for gradient propagation through the orthant integral of the multivariate normal PDF 2 .</p><p>3. We compare the quality of the proposed EXACT method with cross-entropy and hinge losses. According to our results, EXACT improves accuracy in multiple tabular and image classification tasks, including SVHN, CIFAR10, and CIFAR100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Classification Losses</head><p>One of the most used classification loss functions is crossentropy (CE), also known as negative log-likelihood <ref type="bibr" target="#b36">(Wang et al., 2022)</ref>. Minimization of cross-entropy reduces the difference between predicted class probabilities and true posteriors. If a model predicts true posteriors, then selecting the class with maximum probability will lead to maximum accuracy classification <ref type="bibr" target="#b27">(Mitchell &amp; Mitchell, 1997b)</ref>. In practice, cross-entropy can lead to suboptimal results due to several reasons. First, we usually don't have to predict true posteriors in order to achieve maximum accuracy. Any model with logits of the true class exceeding other logits leads to the optimal performance. On the other hand, overfitting and local optima prevent cross-entropy training from true posterior prediction. The limitations of cross-entropy gave a rise to other classification losses.</p><p>One of the widely used classification losses is hinge loss <ref type="bibr" target="#b11">(Gentile &amp; Warmuth, 1998)</ref>. Unlike cross-entropy, hinge loss stops training when scores of ground truth classes exceed alternative scores with the required margin. In some problems, hinge loss shows results on par or better than cross-entropy <ref type="bibr" target="#b10">(Epalle et al., 2021;</ref><ref type="bibr" target="#b17">Jin et al., 2014;</ref><ref type="bibr" target="#b29">Ozyildirim &amp; Kiran, 2021;</ref><ref type="bibr" target="#b34">Peng &amp; Liu, 2018)</ref>.</p><p>Loss functions, such as cross-entropy and hinge loss, correlate with accuracy but do not optimize it directly <ref type="bibr" target="#b14">(Grabocka et al., 2019)</ref>. For this reason, these methods are referred to as proxy or surrogate losses. In this work, we propose a different approach that directly optimizes the accuracy of a specially designed stochastic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Surrogate Losses Beyond Accuracy</head><p>Many machine learning methods, especially deep learning approaches, rely on gradient descent optimization <ref type="bibr" target="#b26">(Mitchell &amp; Mitchell, 1997a)</ref>, which is applicable only to differentiable loss functions. Surrogate losses provide differentiable approximations for non-differentiable target metrics. However, optimization of surrogate losses does not necessary leads to a target metric optimization. Previous works pro-pose differentiable surrogate losses for metrics beyond accuracy, including ROC AUC <ref type="bibr" target="#b6">(Calders &amp; Jaroszewicz, 2007;</ref><ref type="bibr" target="#b39">Yuan et al., 2021)</ref> and F1 scores <ref type="bibr" target="#b2">(B?n?dict et al., 2021)</ref>. For example, AUC ROC can be rewritten in terms of the Heaviside step function, which is approximated via a logistic function or polynomials. Unlike the above-mentioned approaches, we focus on a direct target metric optimization.</p><p>Surrogate loss functions were proposed in domains such as metric learning and ranking <ref type="bibr" target="#b18">(Kaya &amp; Bilge, 2019;</ref><ref type="bibr" target="#b5">Calauzenes et al., 2012)</ref>. Many of them use margin-based losses, similar to hinge loss <ref type="bibr" target="#b37">(Weinberger &amp; Saul, 2009</ref>). Some surrogate losses formulate target metrics in terms of the Heaviside step function, which is then approximated by smooth functions <ref type="bibr" target="#b32">(Patel et al., 2021)</ref>.</p><p>The proposed EXACT method is designed for direct accuracy optimization and does not require any surrogate loss function. Furthermore, the idea of using stochastic prediction can potentially be extended beyond accuracy optimization and applied in other machine learning domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Stochastic Prediction</head><p>There are multiple ways to introduce stochasticity to the model's output. One way is to use probabilistic embeddings <ref type="bibr" target="#b35">(Shi &amp; Jain, 2019)</ref>, where the model predicts a distribution of outputs rather than a single vector. For example, the model can predict a mean and covariance matrix of the multivariate normal distribution. This concept was used in many deep learning approaches, such as regression via mixture density networks <ref type="bibr" target="#b3">(Bishop, 1994)</ref>, variational autoencoders <ref type="bibr" target="#b20">(Kingma &amp; Welling, 2014)</ref>, and metric learning <ref type="bibr" target="#b7">(Chang et al., 2020)</ref>. Another way to introduce stochasticity is to use Bayesian neural networks (BNNs) <ref type="bibr" target="#b13">(Goan &amp; Fookes, 2020)</ref>. In BNNs, the model weights are treated as random variables, and the output depends on particular weights' values.</p><p>One part of the proposed EXACT method is stochastic prediction, which is similar to probabilistic embeddings. However, unlike previous approaches, we use stochastic prediction for accuracy optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivation</head><p>Let's consider a toy example where cross-entropy and hinge loss minimization produces suboptimal results in terms of accuracy. Suppose we solve a binary classification problem via the simple threshold model f (x) = x ? b. The model predicts class? based on the sign of f (x):</p><formula xml:id="formula_0">y = ?1, f (x) &lt; 0 1, f (x) ? 0 .<label>(1)</label></formula><p>The goal of training is to fit the threshold parameter b. Suppose the training set consists of three points: -0.25, 0, and 0.25. The first two points have the label y = ?1, and the last point has the label 1, as shown in <ref type="figure">Figure 1</ref>.</p><p>Cross-entropy loss for binary classification is usually applied to the output of the logistic function g, which converts f (x) to the probability of the positive class:</p><formula xml:id="formula_1">L CE (x, y) = ? log g(yf (x)),<label>(2)</label></formula><formula xml:id="formula_2">g(x) = 1 1 + e ?x .<label>(3)</label></formula><p>Binary hinge loss is computed as follows:</p><formula xml:id="formula_3">L Hinge (x, y) = max(0, 1 ? yf (x)).<label>(4)</label></formula><p>Minimization of cross-entropy and hinge losses fails to find the optimal value of b, which must be in the interval (0, 0.25). The global minimum of the cross-entropy loss is b = 0.7, and 0.75 ? b ? 1 for hinge loss. Gradient ascent optimization of the proposed EXACT method, which we describe below, achieves perfect classification in this example, leading to b = 0.125.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXACT</head><p>The accuracy metric is not differentiable and cannot be directly optimized via gradient ascent. The core idea behind EXACT is to introduce stochasticity to the model's output and then optimize the expected accuracy, i.e. accuracy of the stochastic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Definitions</head><p>The goal of classification training is to minimize the empirical risk of the mapping x ???, where x ? R d is an input feature vector and? ? 1, C is an output class label. In machine learning with gradient methods, including deep learning, the problem is usually solved by a parametric differentiable function f ? : R d ?? R C with parameters ?, which predicts score vectors (or logits) of output classes. Prediction is obtained by taking the class with the maximum score:? In some equations below we will omit ? subscript for simplicity.</p><formula xml:id="formula_4">? (x) = arg max i?1,C f ? (x) i .<label>(5</label></formula><p>In this work, we consider accuracy maximization, i.e. maximization of the correct classification probability of elements (x, y) from some dataset:</p><formula xml:id="formula_5">A(?) = E x,y 1(? ? (x) = y). (6) ? = arg max ? A(?),<label>(7)</label></formula><p>where? is an optimal set of parameters, which we estimate using gradient ascent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Stochastic Model's Accuracy</head><p>Let's consider a modification of the function f : x ? (?, ?) with ? &gt; 0, which predicts the multivariate normal distribution of a score vector s rather than a single vector:</p><formula xml:id="formula_6">s ? N (?(x), ? 2 (x)I),<label>(8)</label></formula><formula xml:id="formula_7">y ? (s) = arg max i?1,C s i .<label>(9)</label></formula><p>In this case, the predicted class? ? (s) is a discrete random variable, which depends on the value of the score vector s.</p><p>We call such a model stochastic since it produces random output. Accuracy of the stochastic model is an expectation of accuracy w.r.t. values of the score vector:</p><formula xml:id="formula_8">A(?) = E x,y E s 1(? ? (s) = y).<label>(10)</label></formula><p>Since the indicator function is a Bernoulli random variable, we can replace the indicator's expectation with probability:</p><formula xml:id="formula_9">A(?) = E x,y P(? ? (s) = y).<label>(11)</label></formula><p>By taking into account Equation 9, we can rewrite accuracy as A(?) = E x,y P(s y &gt; max i =y</p><formula xml:id="formula_10">s i ).<label>(12)</label></formula><p>Here we assume that classification is incorrect if scores of two or more classes are equal to each other.</p><p>Examples of the expected accuracy for different values of ? are presented in <ref type="figure" target="#fig_0">Figure 3</ref>. As we will show below, the accuracy of the stochastic model is differentiable w.r.t. mean vector ?. This property makes optimization via gradient ascent possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optimization</head><p>In this section, we propose an effective approach to expected accuracy computation and differentiation. We base this method on reducing the original problem to an orthant integration of the multivariate normal probability density function (PDF). The latter can be effectively computed using algorithms proposed in previous works.</p><formula xml:id="formula_11">Definition 4.1. Delta matrix D y of the order C for the ground truth label y is a matrix of size C ? 1 ? C with D y i,j = ? ? ? ? ? ? ? ? ? 1, j = y ?1, j &lt; y, i = j ?1, j &gt; y, i = j ? 1 0, otherwise .<label>(13)</label></formula><p>In other words, the delta matrix is used to compute differences between scores of different classes and has the following form:</p><formula xml:id="formula_12">D y = ? ? ? ? ? ? ? ? ? ? ? ? ?1 0 . . . 0 1 0 . . . 0 0 ?1 . . . 0 1 0 . . . 0 . . . . . . . . . 0 0 . . . ?1 1 0 . . . 0 0 0 . . . 0 1 ?1 . . . 0 . . . . . . . . . 0 0 . . . 0 1 0 . . . ?1 ? ? ? ? ? ? ? ? ? ? ? ? .<label>(14)</label></formula><p>Now we can state a key result, necessary for expected accuracy evaluation.</p><p>Theorem 4.2. Suppose the scores vector s is distributed according to multivariate normal distribution N (?, ? 2 I) in R C . In this case, the probability of the y-th score exceeding other scores can be represented as</p><formula xml:id="formula_13">P(s y &gt; max i =y s i ) = ?+ N (t; D y ? ? , D y D T y )dt,<label>(15)</label></formula><p>where N (t; ?, ?) denotes multivariate normal PDF, D y is a delta matrix of the order C for the label y and ? + :</p><formula xml:id="formula_14">{t ? R C?1 , t i ? 0, i = 1, C ? 1} is an orthant in R C?1 .</formula><p>The orthant integral in equation 15 can be effectively computed using Genz algorithm <ref type="bibr" target="#b12">(Genz, 1992</ref> </p><formula xml:id="formula_15">m = D y ? ? ,<label>(16)</label></formula><formula xml:id="formula_16">? = D y D T y .<label>(17)</label></formula><p>Note, that the matrix ? is constant for each element of the dataset and doesn't require gradients. Note also that ? is a covariance matrix by construction and thus can be used as a parameter of a multivariate normal distribution. Gradients w.r.t. mean vector m can be computed using the following theorem.</p><p>Theorem 4.3. Suppose the scores vector s is distributed</p><formula xml:id="formula_17">according to multivariate normal distribution N (m, ?) in R C?1 and? i is a random vector of all elements in s ex- cept i-th, conditioned on s i = ?m i . Then? i is normally- distributed with some parametersm i and? i and ? ? ? ?+ N (t; m, ?)dt ? ? ? mi = N (0; m i , ? i,i ) ? + N (t;m i ,? i )dt,<label>(18)</label></formula><p>where N (t; ?, ?) denotes multivariate normal PDF and</p><formula xml:id="formula_18">? + : {t ? R C?2 , t i ? 0, i = 1, C ? 2} is an orthant in R C?2 .</formula><p>Same as before, the orthant integral is computed using Genz algorithm <ref type="bibr" target="#b12">(Genz, 1992)</ref>. According to Theorem 4.3 and Equation 16, accuracy of the stochastic model is differentiable w.r.t. both ?(x) and ?(x). The gradient descent method can be applied to minimize the negative value of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Inference</head><p>During inference, EXACT predicts a score vector with maximum density, i.e. ? vector. Variance prediction can be excluded from the model in inference time, leading to zero computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Improvements</head><p>In this section, we analyze corner cases and training with EXACT loss and propose improvements aimed at training stabilization and better final optima.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">MARGIN</head><p>When ? is close to zero, the stochastic model's accuracy reduces to a single-point prediction. In this case EXACT loss resembles the accuracy of the traditional model and suffers from local accuracy optima. To achieve better optimum during training, EXACT has to start with large ?. However, if ? is very large, then the surface of EXACT loss function will interpolate prior class probabilities, as shown in <ref type="figure" target="#fig_2">Figure  4</ref>. In the latter case, EXACT completely smooths optima.</p><p>To make the method more robust to large initial ? values, we incorporate margin to the loss function. EXACT margin is similar to that of hinge loss and affects the computation of the mean vector from Equation 16:</p><formula xml:id="formula_19">m r = min (D y ?, r) ? ,<label>(19)</label></formula><p>where min is applied element-wise and r is a margin value. The obtained m r value is then used instead of m in Equation 16 for loss and gradient computation. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, margin affects training with large ? values, while has almost no effect as ? decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">RATIO AMBIGUITY</head><p>According to Equation 15, ? and ? are used in EXACT loss only as a fraction ? ? . This leads to an ambiguity in the model's prediction because the multiplication of both values by the same number will not affect stochastic accuracy. To decouple the effects of ? and ?, we apply batch normalization <ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015)</ref> to the ? branch of the model. Batch normalization is a part of the loss function and is removed during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3.">VARIANCE SCHEDULER</head><p>While ? prediction subnetwork can be trained simultaneously with other parameters, we found it more beneficial to change ? according to the predefined schedule. One reason behind this is that gradient descent rapidly reduces predicted variance, converging to the closest (usually poor) local optimum. Slow reduction of the variance forces the algorithm to make more steps with large ? and results in higher final accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4.">GRADIENT NORMALIZER</head><p>In practice, EXACT gradient norm largely depends on the output variance. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, cross-entropy and hinge losses achieve plateau during training, while EXACT gradient norm exponentially growth. To better control the update step, we divide EXACT's gradient by running mean statistics. The running mean is computed using exponential smoothing with a factor 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head><p>In this section, we describe data preparation and training details of the proposed method and the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model Architectures</head><p>For tabular data classification, we use linear models. Scores (or logits) of the first class are fixed to zero. Scores of the remaining classes are predicted via linear transform Ax + b of input features x. This setup is equivalent to a binary classification model when the number of classes is equal to 2. We also evaluate logistic regression implementation from Sklearn <ref type="bibr" target="#b33">(Pedregosa et al., 2011)</ref>.</p><p>For image classification datasets we use different neural network models to better match data complexity. For MNIST we use 10-layer plain M3 CNN architecture from the recent work , which achieved top-performing results on this dataset. For SVHN and CIFAR we use Wide ResNet (WRN) architecture <ref type="bibr" target="#b40">(Zagoruyko &amp; Komodakis, 2016)</ref>, which achieved better performance than ResNet <ref type="bibr" target="#b15">(He et al., 2016)</ref>. Particularly, we use WRN-16-8 for SVHN and WRN-28-10 for CIFAR-10 / CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Tabular Data Preparation</head><p>We evaluate linear models using datasets from UCI machine learning repository <ref type="bibr" target="#b9">(Dua &amp; Graff, 2017)</ref>. We choose 10 popular classification datasets with numeric and categorical features. Most UCI datasets have missing values. In order to apply linear models to these datasets, we fill missing  <ref type="table">Table 2</ref>. Test set accuracy (%) of linear models trained with different loss functions on 10 tabular datasets. Mean and STD of 5 runs with different seeds are reported. Sklearn training doesn't depend on the random seed and thus STD is always zero.</p><p>numeric data with mean values and missing categorical data with most frequent elements. We also apply one-hot encoding to categorical data. All features are normalized using per-feature mean and STD statistics. For datasets without original test part, we split 20% randomly selected items for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Image Data Preparation</head><p>We conduct experiments on MNIST (LeCun et al., 1998), SVHN <ref type="bibr" target="#b28">(Netzer et al., 2011)</ref>, CIFAR-10, and CIFAR-100 <ref type="bibr" target="#b21">(Krizhevsky et al., 2009)</ref> datasets. We prefer small and medium-size datasets because hyperparameter tuning on large datasets is practically unattainable.</p><p>For MNIST we use simple data transform consisting of random translation of the image by at most 20% of the original size and random rotation by at most 20 degrees . For SVHN we use auto augmentation <ref type="bibr" target="#b8">(Cubuk et al., 2019)</ref> from PyTorch <ref type="bibr" target="#b30">(Paszke et al., 2019a)</ref> with parameters designed for CIFAR-10. For CIFAR-10 and CIFAR-100 we use CIFAR-10 auto augmentations along with random horizontal flipping.</p><p>All input images are scaled to the fixed size depending on the neural architecture to achieve spatial tensor size 8 ? 8 before the final pooling layer. For M3, used for MNIST, images are scaled to 28 pixels on each side. For Wide ResNet architectures, used for SVHN, CIFAR-10, and CIFAR-100, images are scaled to 32 pixels on each side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Hyperparameters and Training</head><p>The list of hyperparameters includes initial learning rate, gradient clipping threshold, and margin (for hinge loss and EXACT). Hyperparameters are tuned via 50 runs of random search with 20% of the training set reserved for validation. For linear models we also tune L2 regularization coefficient.</p><p>All models, except Sklearn, are trained with batch size equal to 256. Linear models are trained for 8000 steps to balance training for different dataset sizes. The logistic regression from Sklearn is trained using standard L-BFGS solver <ref type="bibr" target="#b4">(Byrd et al., 1995)</ref>. Neural networks are trained for 150 epochs on MNIST and SVHN and for 500 epochs on CIFAR10  and CIFAR100. Models are trained via stochastic gradient descent with momentum equal to 0.9 and weight decay equal to 10 ?4 . Learning rate exponentially decays from initial value to 10 ?4 at last epoch. We also train neural networks with Adam <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2015)</ref> and ASAM <ref type="bibr" target="#b23">(Kwon et al., 2021)</ref> optimizers. EXACT variance scheduler reduces ? from 10 in the first epoch to the final value, which is 0.01 for linear models and 1 for neural networks.</p><p>We use the PyTorch deep learning framework <ref type="bibr">(Paszke et al., 2019b)</ref> for all models except logistic regression from Sklearn. Each experiment is performed using a single NVIDIA V100 GPU.</p><p>EXACT implementation depends on the sample size used in the Genz integration algorithm. We use sample size 16 in all comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this work, we aim to answer the following questions: (1) how does EXACT performance correspond to that of crossentropy and hinge losses, (2) how to choose EXACT sample size, and (3) how big is the computational overhead of the proposed EXACT method? The answers to these questions are summarized in the subsections below.</p><p>6.1. Classification Quality</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">LINEAR MODELS</head><p>We compared linear models trained with Sklearn, crossentropy loss, hinge loss, and the proposed EXACT loss. The comparison on 10 tabular datasets is presented in Tables 1 and 2. On 9 out of 10 datasets EXACT achieves the highest accuracy on the training part of the dataset. It demonstrates the effectiveness of the proposed loss function in accuracy maximization. During testing EXACT achieves the highest accuracy in 6 out of 10 cases. On 3 benchmarks EXACT strictly outperforms all other methods, including Sklearn's logistic regression trained with L-BFGS solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">DEEP IMAGE CLASSIFICATION</head><p>We compared EXACT with surrogate losses on multiple datasets with different optimizers. The results are presented in <ref type="table" target="#tab_3">Table 3</ref>. Hinge loss achieves the highest accuracy on MNIST dataset. On SVHN and CIFAR-10, results are mixed, but EXACT outperforms other methods in most cases. On CIFAR-100 EXACT strictly outperforms all baselines. Generally, EXACT outperforms other methods in 7 out of 12 comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Sample Size</head><p>EXACT depends on the sample size used in the Genz integration algorithm <ref type="bibr" target="#b12">(Genz, 1992)</ref>. Training results for different sample sizes are presented in <ref type="figure">Figure 6</ref>. On MNIST, SVHN, and CIFAR-10, the sample size has minor effects on accuracy. Even training with sample size 1 produces results on par with larger samples. On CIFAR-100, a larger sample size generally leads to higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Computational Complexity</head><p>EXACT memory consumption and computational efficiency depend on the number of classes C and sample size N . The most expensive part of the algorithm is gradient computation, which requires C calls to Genz algorithm with complexity O(N C). Thus the total complexity of EXACT loss is O(N C 2 ) in terms of both operations and memory. Empirical memory usage per data element and computational efficiency (milliseconds per data element) are presented in <ref type="figure" target="#fig_4">Figure 7</ref>. Evaluations for different numbers of classes were made with a sample size equal to 16. Different sample sizes were evaluated with the number of classes equal to 32.</p><p>The relative EXACT overhead largely depends on the num- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>While previous works on deep classification minimized surrogate losses, EXACT directly optimizes the accuracy of the model with stochastic prediction. The benefits of accuracy optimization were illustrated in our toy example, where both cross-entropy and hinge losses failed to solve the problem. The proposed EXACT method treats non-differentiable accuracy metric in a novel way which can potentially be applied to domains beyond classification, such as metric learning, ranking, etc.</p><p>According to our experiments, EXACT leads to competitive results in tabular and image classification tasks. In many cases, EXACT achieves higher classification accuracy with a computational overhead of about 0?6% for neural networks. While computational efficiency is dependent on sample size, we show that EXACT can be trained even with single-point estimation. Extra computational resources can be used to further increase EXACT accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We presented EXACT, a novel approach for optimizing stochastic model accuracy via gradient ascent. Our results show that EXACT achieves higher accuracy than crossentropy and hinge losses in several tabular and image classification tasks, including SVHN, CIFAR10, and CIFAR100. EXACT can be effectively implemented in popular deep A. Proofs of Theorems </p><formula xml:id="formula_20">s i ) = ?+ N (t; D y ? ? , D y D T y )dt,<label>(20)</label></formula><p>where N (t; ?, ?) denotes multivariate normal PDF, D y is a delta matrix of the order C for the label y and ? + :</p><formula xml:id="formula_21">{t ? R C?1 , t i ? 0, i = 1, C ? 1} is an orthant in R C?1 .</formula><p>Proof. Let's rewrite Equation 20:</p><formula xml:id="formula_22">P(s y &gt; max i =y s i ) = P(s y ? max i =y s i &gt; 0) (21) = P(min i =y (s y ? s i ) &gt; 0).<label>(22)</label></formula><p>Due to the definition of the delta matrix D y , vector d = D y s is a (C ? 1)-dimensional vector with elements equal to differences between the score of the ground truth class y and other scores:</p><formula xml:id="formula_23">d i = s y ? s i , i &lt; y s y ? s i+1 , i &gt;= y ,<label>(23)</label></formula><p>then P(s y &gt; max i =y</p><formula xml:id="formula_24">s i ) = P(min i d i &gt; 0) (24) = P(min i d i ? &gt; 0) (25) = P(min i w i &gt; 0),<label>(26)</label></formula><p>where w = Dys ? . Due to the properties of the multivariate normal distribution, random vector w is also normally distributed:</p><formula xml:id="formula_25">w ? N ( D y ? ? , D y D T y ).<label>(27)</label></formula><p>Finally:</p><formula xml:id="formula_26">P(s y &gt; max i =y s i ) = P(w 1 &gt; 0, . . . , w C?1 &gt; 0) (28) = ?+ N (t; D y ? ? , D y D T y )dt.<label>(29)</label></formula><p>Theorem 4.3. Suppose the scores vector s is distributed according to multivariate normal distribution N (m, ?) in R C?1 and? i is a random vector of all elements in s except i-th, conditioned on s i = ?m i . Then? i is normallydistributed with some parametersm i and? i and</p><formula xml:id="formula_27">? ? ? ?+ N (t; m, ?)dt ? ? ? mi = N (0; m i , ? i,i ) ? + N (t;m i ,? i )dt,<label>(30)</label></formula><p>where N (t; ?, ?) denotes multivariate normal PDF and</p><formula xml:id="formula_28">? + : {t ? R C?2 , t i ? 0, i = 1, C ? 2} is an orthant in R C?2 . Proof. ?+ N (t; m, ?)dt (31) = ? 0 ? ? ? ? 0 N (t; m, ?)dt C?1 . . . dt 1 (32) = ? 0 ? ? ? ? 0 N (t ? m; 0, ?)dt C?1 . . . dt 1 (33) = ? ?m1 ? ? ? ? ?m C?1 N (t; 0, ?)dt C?1 . . . dt 1 (34)</formula><p>Assume, without loss of generality, that i = 1. Then we can find the derivative using Leibniz integral rule:</p><formula xml:id="formula_29">? ? ? ?+ N (t; m, ?)dt ? ? ? m1 (35) = ? ?m2 ? ? ? ? ?m C?1 N (t; 0, ?)dt C?1 . . . dt 2 t1=?m1 (36) = ? 0 ? ? ? ? 0 N (t; m, ?)dt C?1 . . . dt 2 t1=0<label>(37)</label></formula><p>Integration region in the last integral is a positive orthant ? + , which is a lower dimension subset of ? + :</p><formula xml:id="formula_30">? ? ? ?+ N (t; m, ?)dt ? ? ? m1 = ? + N (t; m, ?)dt C?1 . . . dt 2 t1=0 .<label>(38)</label></formula><p>We can decompose inner density using properties of the multivariate normal distribution:</p><formula xml:id="formula_31">P s (t) = P s (t 1 , . . . , t C?1 ) (39) = P s (t 1 , t 2:C?1 ) (40) = P s1 (t 1 )P s 2:C?1 (t 2:C?1 |s 1 = t 1 ),<label>(41)</label></formula><formula xml:id="formula_32">so N (t; m, ?) = N (t 1 ; m 1 , ? 1,1 )N (t 2:C?1 ;m 1 ,? 1 ),<label>(42)</label></formula><p>wherem 1 and? 1 are parameters of the conditional distribution from Equation <ref type="formula" target="#formula_0">41</ref>: </p><formula xml:id="formula_33">m 1 = m 2:C?1 + ? 2:C?1,1 ? ?1 1,1 (t 1 ? m 1 ),<label>(43)</label></formula><formula xml:id="formula_34">? 1 = ? 2:C?1,2:C?1 ? ? 2:C?1,1 ? ?1 1,1 ? 1,2:C?1 .<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Notes</head><p>EXACT loss computation algorithm with all improvements is presented in Listing 1.</p><p>Listing 1: Example PyTorch Code for EXACT computation 1 mu = empty(batch_size, dim) # Input. 2 sigma = empty(batch_size) # Input. 3 mu_bn = (mu -mu.mean()) / mu.std() 4 deltas = mu_bn @ Dy.T 5 deltas_margin = deltas.clip(max=r) 6 ratio = deltas_margin / sigma[:, None] 7 loss = 1 -genz_integral(mean=ratio, cov=eye(dim) + 1)</p><p>A considerable boost in EXACT performance can be obtained from analysis of the matrix ? = D y D T y in Equation 20. Matrix D y is a delta matrix for the label y:</p><formula xml:id="formula_35">D y = ? ? ? ? ? ? ? ? ? ? ? ? ?1 0 . . . 0 1 0 . . . 0 0 ?1 . . . 0 1 0 . . . 0 . . . . . . . . . 0 0 . . . ?1 1 0 . . . 0 0 0 . . . 0 1 ?1 . . . 0 . . . . . . . . . 0 0 . . . 0 1 0 . . . ?1 ? ? ? ? ? ? ? ? ? ? ? ? .<label>(45)</label></formula><p>It can be seen, that D y D T y is independent of y:</p><formula xml:id="formula_36">? = D y D T y = ? ? ? ? ? 2 1 . . . 1 1 2 . . . 1 . . . 1 1 . . . 2 ? ? ? ? ? .<label>(46)</label></formula><p>Matrix ? and it's Cholesky decomposition, used in Genz algorithm, can be computed before training. On the other hand, the special form of ? largely simplifies equations for computingm i and? i .</p><p>Cholesky decomposition of the matrix ? has the following form:</p><formula xml:id="formula_37">L = ? ? ? ? ? ? ? ? 1 0 . . . 0 ? 1 ? 2 . . . 0 . . . ? 1 ? 2 . . . 0 ? 1 ? 2 . . . ? C?1 ? ? ? ? ? ? ? .<label>(47)</label></formula><p>Note, that elements in each column below the diagonal are the same. Genz algorithm can be optimized to make use of this property of the matrix L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Backpropagation with Log-derivative Trick</head><p>EXACT loss evaluates expectation of the form:</p><formula xml:id="formula_38">L(x, y, ?) = E s?N (?,?) 1(?(s) = y),<label>(48)</label></formula><p>where ? = ?(x, ?) and ? = ?(x, ?). While EXACT exploits properties of the multivariate normal distribution, there is a more general approach to compute the expectation and its gradients, called REINFORCE <ref type="bibr" target="#b38">(Williams, 1992)</ref>. The expectation itself can be computed using Monte-Carlo integration:</p><formula xml:id="formula_39">L(x, y, ?) ? 1 N N i=1 1(?(s i ) = y),<label>(49)</label></formula><p>where s i are drawn from N (?, ?). Gradient computation can be performed using the log-derivative trick:</p><formula xml:id="formula_40">? ? L(x, y, ?) = E s?N (?,?) 1(?(s) = y)? log N (s; ?, ?),<label>(50)</label></formula><p>where N (s; ?, ?) is a multivariate normal PDF function. The latter expectation can also be evaluated using Monte-Carlo integration.</p><p>We compared the performance of Genz integration algorithm <ref type="bibr" target="#b12">(Genz, 1992)</ref> and EXACT extensions for gradient computation, with Monte-Carlo integration and logderivative trick. For this purpose we estimated the integral from Equation 20 and its gradient from Equation <ref type="formula" target="#formula_2">(30)</ref> ? was set to (1, 2, 0.5, 10, 6, ?3, ?4, 5, 1, 0). Each method was applied 1000 times with different random seeds. After that, we computed root-mean-square error (RMSE) of the integral values and ground truth value, computed with the sample size equal to 1000000. The results are presented in <ref type="figure" target="#fig_6">Figure 8</ref> and <ref type="figure">Figure 9</ref>. It can be seen that EXACT algorithm produces an order of magnitude smaller error than Monte-Carlo with log-derivative trick. For example, Monte-Carlo requires a sample size 256 to achieve the gradient error of EXACT with sample size 1. This study highlights the importance of the proposed EXACT computation algorithm for stable training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Toy Example Derivation</head><p>Cross-entropy loss for the toy example has the following form:</p><formula xml:id="formula_41">L CE (b) = ? log 1 1 + exp(?(0.25 + b)) ? log 1 1 + exp(?b) ? log 1 1 + exp(?(0.25 ? b)</formula><p>) .</p><p>(51) By taking derivative of the loss function w.r.t. b, we get</p><formula xml:id="formula_42">d db L CE (b) = ?3.933e b + 1.284e 3b ? 2.568 (e b + 1) (1.284e b + 1) (1.284 + e b )</formula><p>. <ref type="formula" target="#formula_1">(52)</ref> The derivative equals zero at single point b ? 0.7. As L CE (b) is convex, it has single global minimum at b ? 0.7.</p><p>The same procedure can be applied to minimize Hinge loss: </p><formula xml:id="formula_43">L Hinge (b) = max(0, 1 + (?0.25 ? b)) + max(0, 1 ? b) + max(0, 1 ? (0.25 ? b)) (53) d db L Hinge (b) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?2 b &lt; ? 3 4 ?1 ? 3 4 &lt; b &lt; 3 4 0 3 4 &lt; b &lt; 1 1 b &gt; 1 N/A otherwise<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Toy Example with Learned Weight</head><p>In the toy example from the main paper, we used the model with a single bias parameter, while weight was fixed to 1. Here we demonstrate another example, presented in <ref type="figure">Figure  10</ref>, with a trainable weight parameter. The dataset consists from 5 points in positions -6, -5, -4, 0, and 2. The labels are -1, 1, 1, -1, and 1. We minimized cross-entropy and hinge losses using grid search. Cross-entropy minimum is achieved for the threshold equal to -7.75, hinge loss is minimized at -11. Both cross-entropy and hinge loss lead to an accuracy 60%. Gradient descent optimization of EXACT leads to the threshold -5.35 and accuracy 80%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation Studies</head><p>In this section we compare methods with and without EXACT improvements on UCI datasets. First, we compare EXACT variants without margin, without variance scheduler, and without gradient normalizer with original EXACT method. Results are presented in <ref type="table">Table 4</ref>. According to these results, all improvements generally lead to higher test-time accuracy.</p><p>We also evaluated the effect of gradient normalizer in the context of cross-entropy and hinge loss. Results are presented in <ref type="table" target="#tab_6">Table 5</ref> and <ref type="table" target="#tab_8">Table 6</ref>. According to these results, gradient normalizer has minor effect on training with baseline loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Hyperparameters</head><p>Hyperparameter ranges and values found by 50 iterations of random search for each loss function for image classification datasets are listed in <ref type="table">Table 7</ref>. Ranges and hyperparameters for tabular datasets are presented in <ref type="table" target="#tab_9">Table 8</ref>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Dependency of the expected accuracy on the model parameter in our toy example for different values of ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>EXACT loss dependency on the model parameter with and w/o margin. Margin affects training with large ?, creating a better optimization landscape in early epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Gradient norm during training on CIFAR-100 for different loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>EXACT loss memory consumption (MB per element) and computation speed (ms per element) for different numbers of classes and sample sizes. Performance is compared to the Wide ResNet 16-8 neural network.learning frameworks, including PyTorch, and leads to the computational overhead of about 3% depending on the number of classes and neural model complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>44) Substitution of the Equation 42 to Equation 38 finally proofs the theorem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>. The covariance matrix ? was computed using Equation 46 and Sample Integral value error for Genz and Monte-Carlo algorithms with different sample sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Gradient error for EXACT and Log-derivative trick with different sample sizes. The toy example, which demonstrates importance of accuracy optimization. The model is a linear binary classifier with weight and bias parameters. EXACT achieves 80% accuracy, while cross-entropy and hinge loss minimization leads to 60% accuracy (global optimum).L Hinge (b) reaches minimum at 0.75 ? b ? 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). The Equation 15 represents correct classification probability for a single element of the dataset. Total accuracy is computed according to equation 12 as an average of correct classification probabilities for all elements. Gradient ascent requires gradient computation w.r.t. parameters ? and ?. Both parameters are included only in the mean part of the normal distribution in Equation 15. Let's define</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>? 0.00 85.38 ? 0.02 85.28 ? 0.01 85.47 ? 0.01 annealing train 100.00 ? 0.00 100.00 ? 0.00 100.00 ? 0.00 100.00 ? 0.00 audit-risk train 99.19 ? 0.00 98.23 ? 0.00 100.00 ? 0.00 100.00 ? 0.00 balance-scale train 91.40 ? 0.00 91.40 ? 0.00 91.60 ? 0.00 91.60 ? 0.00 breast-cancer-wisconsin train 98.68 ? 0.00 98.02 ? 0.00 98.90 ? 0.00 99.03 ? 0.11 Train set accuracy (%) of linear models trained with different loss functions on 10 tabular datasets. Mean and STD of 5 runs with different seeds are reported. Sklearn training doesn't depend on the random seed and thus STD is always zero.</figDesc><table><row><cell>Dataset</cell><cell>Split</cell><cell>Sklearn</cell><cell>Cross-entropy</cell><cell>Hinge</cell><cell>EXACT</cell></row><row><cell cols="3">adult 85.33 car train train 94.50 ? 0.00</cell><cell>94.50 ? 0.00</cell><cell>93.63 ? 0.00</cell><cell>96.67 ? 0.00</cell></row><row><cell>cylinder-bands</cell><cell>train</cell><cell>80.05 ? 0.00</cell><cell>80.28 ? 0.00</cell><cell>79.86 ? 0.09</cell><cell>87.42 ? 0.17</cell></row><row><cell>dry-bean</cell><cell>train</cell><cell>92.79 ? 0.00</cell><cell>92.38 ? 0.00</cell><cell>92.44 ? 0.01</cell><cell>92.67 ? 0.02</cell></row><row><cell>mice-protein</cell><cell cols="3">train 100.00 ? 0.00 100.00 ? 0.00</cell><cell>99.44 ? 0.11</cell><cell>100.00 ? 0.00</cell></row><row><cell>wine</cell><cell cols="5">train 100.00 ? 0.00 100.00 ? 0.00 100.00 ? 0.00 100.00 ? 0.00</cell></row><row><cell>Total best</cell><cell></cell><cell>4</cell><cell>3</cell><cell>4</cell><cell>9</cell></row><row><cell>Dataset</cell><cell>Split</cell><cell>Sklearn</cell><cell>Cross-entropy</cell><cell>Hinge</cell><cell>EXACT</cell></row><row><cell>adult</cell><cell>test</cell><cell>85.28 ? 0.00</cell><cell>85.28 ? 0.01</cell><cell>85.26 ? 0.01</cell><cell>85.21 ? 0.01</cell></row><row><cell>annealing</cell><cell cols="5">test 100.00 ? 0.00 100.00 ? 0.00 100.00 ? 0.00 100.00 ? 0.00</cell></row><row><cell>audit-risk</cell><cell>test</cell><cell>98.72 ? 0.00</cell><cell>96.28 ? 0.48</cell><cell>97.05 ? 0.65</cell><cell>96.67 ? 0.75</cell></row><row><cell>balance-scale</cell><cell>test</cell><cell>92.00 ? 0.00</cell><cell>92.00 ? 0.00</cell><cell>92.00 ? 0.00</cell><cell>92.00 ? 0.00</cell></row><row><cell cols="2">breast-cancer-wisconsin test</cell><cell>95.61 ? 0.00</cell><cell>95.96 ? 0.43</cell><cell>96.49 ? 0.00</cell><cell>97.37 ? 0.00</cell></row><row><cell>car</cell><cell>test</cell><cell>92.20 ? 0.00</cell><cell>91.91 ? 0.00</cell><cell>92.77 ? 0.00</cell><cell>95.20 ? 0.14</cell></row><row><cell>cylinder-bands</cell><cell>test</cell><cell>72.22 ? 0.00</cell><cell>72.78 ? 0.45</cell><cell>74.44 ? 0.45</cell><cell>77.22 ? 0.45</cell></row><row><cell>dry-bean</cell><cell>test</cell><cell>93.06 ? 0.00</cell><cell>92.83 ? 0.01</cell><cell>92.93 ? 0.03</cell><cell>92.99 ? 0.07</cell></row><row><cell>mice-protein</cell><cell>test</cell><cell>99.07 ? 0.00</cell><cell>98.89 ? 0.23</cell><cell>97.69 ? 0.00</cell><cell>97.69 ? 0.00</cell></row><row><cell>Total best</cell><cell></cell><cell>7</cell><cell>4</cell><cell>3</cell><cell>6</cell></row></table><note>wine test 100.00 ? 0.00 100.00 ? 0.00 100.00 ? 0.00 100.00 ? 0.00</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Test set accuracy (%) in deep vision tasks for top-performing neural architectures with different optimizers. Mean and STD of 5 runs with different seeds are reported.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Theorem 4.2. Suppose the scores vector s is distributed according to multivariate normal distribution N (?, ? 2 I) in R C . In this case, the probability of the y-th score exceeding other scores can be represented as</figDesc><table><row><cell>P(s y &gt; max i =y</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison of cross-entropy loss with and without gradient normalizer on tabular datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Comparison of hinge loss with and without gradient normalizer on tabular datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Hyperparameters for tabular classification datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An ensemble of simple convolutional neural network models for mnist digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>So</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10400</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attentive interpretable tabular learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Ar?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6679" to="6687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>B?n?dict</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Odijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10566</idno>
		<title level="m">A smooth f1 score surrogate loss for multilabel classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the (non-) existence of convex, calibrated surrogate losses for ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Calauzenes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient auc optimization for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaroszewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on principles of data mining and knowledge discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="42" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data uncertainty learning in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5710" to="5719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-atlas classification of autism spectrum disorder with hinge loss trained deep architectures: Abide i results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Epalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2021.107375.URLhttps:/www.sciencedirect.com/science/article/pii/S1568494621002982</idno>
		<idno>1568- 4946</idno>
		<ptr target="https://doi.org/10.1016/j.asoc.2021.107375.URLhttps://www.sciencedirect.com/science/article/pii/S1568494621002982" />
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107375</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linear hinge loss and average margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Numerical computation of multivariate normal probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Genz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and graphical statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="149" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian neural networks: An introduction and survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Case Studies in Applied Bayesian Data Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="45" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Learning surrogate losses. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with hinge loss trained convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1991" to="2000" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep metric learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">?</forename><surname>Bilge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1066</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Bengio, Y. and LeCun, Y.</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5905" to="5914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="1997" />
			<publisher>McGraw-hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="1997" />
			<publisher>McGraw-hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Levenberg-marquardt multi-classification using hinge loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Ozyildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiran</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2021.07.010.URLhttps:/www.sciencedirect.com/science/article/pii/S0893608021002732</idno>
		<idno>0893- 6080. doi</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2021.07.010.URLhttps://www.sciencedirect.com/science/article/pii/S0893608021002732" />
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>2019b. EXACT: How to Train Your Accuracy</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11179</idno>
		<title level="m">Recall@ k surrogate loss with large batches and similarity mixup</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminative feature selection via employing smooth and robust hinge loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="788" to="802" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Probabilistic face embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6902" to="6911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A comprehensive survey of loss functions in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Data Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="212" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3040" to="3049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
