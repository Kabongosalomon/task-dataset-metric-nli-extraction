<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">All Tokens Matter: Token Labeling for Training Better Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
							<email>zhoudaquan21@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
							<email>shiyujun1016@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Wang</surname></persName>
							<email>anran.wang@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">All Tokens Matter: Token Labeling for Training Better Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present token labeling-a new training objective for training high-performance vision transformers (ViTs). Different from the standard training objective of ViTs that computes the classification loss on an additional trainable class token, our proposed one takes advantage of all the image patch tokens to compute the training loss in a dense manner. Specifically, token labeling reformulates the image classification problem into multiple token-level recognition problems and assigns each patch token with an individual location-specific supervision generated by a machine annotator. Experiments show that token labeling can clearly and consistently improve the performance of various ViT models across a wide spectrum. For a vision transformer with 26M learnable parameters serving as an example, with token labeling, the model can achieve 84.4% Top-1 accuracy on ImageNet. The result can be further increased to 86.4% by slightly scaling the model size up to 150M, delivering the minimal-sized model among previous models (250M+) reaching 86%. We also show that token labeling can clearly improve the generalization capability of the pretrained models on downstream tasks with dense prediction, such as semantic segmentation. Our code and all the training details are publicly available at https://github.com/zihangJiang/TokenLabeling. * Work done as an intern at ByteDance AI Lab.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers <ref type="bibr" target="#b39">[39]</ref> have achieved great performance for almost all the natural language processing (NLP) tasks over the past years <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b25">25]</ref>. Motivated by such success, recently, many researchers attempt to build transformer models for vision tasks, and their encouraging results have shown the great potential of transformer based models for image classification <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b46">46]</ref>, especially the strong benefits of the self-attention mechanism in building long-range dependencies between pairs of input tokens.</p><p>Despite the importance of gathering long-range dependencies, recent work on local data augmentation <ref type="bibr" target="#b57">[57]</ref> has demonstrated that well modeling and leveraging local information for image classification would avoid biasing the model towards skewed and non-generalizable patterns and substantially improve the model performance. However, recent vision transformers normally utilize class tokens <ref type="figure">Figure 1</ref>: Comparison between the proposed LV-ViT and other recent works based on vision transformers, including T2T-ViT <ref type="bibr" target="#b46">[46]</ref>, ConViT <ref type="bibr" target="#b13">[13]</ref>, BoTNet <ref type="bibr" target="#b31">[31]</ref>, DeepViT <ref type="bibr" target="#b59">[59]</ref>, DeiT <ref type="bibr" target="#b36">[36]</ref>, ViT <ref type="bibr" target="#b16">[16]</ref>, Swin Transformer <ref type="bibr" target="#b26">[26]</ref>, LambdaNet <ref type="bibr" target="#b1">[1]</ref>, CvT <ref type="bibr" target="#b43">[43]</ref>, CrossViT <ref type="bibr" target="#b7">[7]</ref>, PVT <ref type="bibr" target="#b40">[40]</ref>, CaiT <ref type="bibr" target="#b37">[37]</ref>. Note that we only show models whose model sizes are under 100M. As can be seen, our LV-ViT achieves the best results using the least amount of learnable parameters. The default test resolution is 224 ? 224 unless specified after @. that aggregate global information to predict the output class while neglecting the role of other patch tokens that encode rich information on their respective local image patches.</p><p>In this paper, we present a new training objective for vision transformers, termed token labeling, that takes advantage of both the patch tokens and the class tokens. Our method takes a K-dimensional score map generated by a machine annotator as supervision to supervise all the tokens in a dense manner, where K is the number of categories for the target dataset. In this way, each patch token is explicitly associated with an individual location-specific supervision indicating the existence of the target objects inside the corresponding image patch, so as to improve the object grounding and recognition capabilities of vision transformers with negligible computation overhead. To the best of our knowledge, this is the first work demonstrating that dense supervision is beneficial to vision transformers in image classification.</p><p>According to our experiments, utilizing the proposed token labeling objective can clearly boost the performance of vision transformers. As shown in <ref type="figure">Figure 1</ref>, our model, named LV-ViT, with 56M parameters, yields 85.4% top-1 accuracy on ImageNet <ref type="bibr" target="#b14">[14]</ref>, behaving better than all the other transformer-based models having no more than 100M parameters. When the model size is scaled up to 150M, the result can be further improved to 86.4%. In addition, we have empirically found that the pretrained models with token labeling are also beneficial to downstream tasks with dense prediction, such as semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Transformers <ref type="bibr" target="#b39">[39]</ref> refer to the models that entirely rely on the self-attention mechanism to build global dependencies, which are originally designed for natural language processing tasks. Due to their strong capability of capturing spatial information, transformers have also been successfully applied to a variety of vision problems, including low-level vision tasks like image enhancement <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b45">45]</ref>, as well as more challenging tasks such as image classification <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b16">16]</ref>, object detection <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b61">61]</ref>, segmentation <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b41">41]</ref> and image generation <ref type="bibr" target="#b29">[29]</ref>. Some works also extend transformers for video and 3D point cloud processing <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b60">60]</ref>.</p><p>Vision Transformer (ViT) is one of the earlier attempts that achieved state-of-the-art performance on ImageNet classification, using pure transformers as basic building blocks. However, ViTs need pretraining on very large datasets, such as ImageNet-22k and JFT-300M, and huge computation resources to achieve comparable performance to ResNet <ref type="bibr" target="#b19">[19]</ref> with a similar model size trained on ImageNet. Later, DeiT <ref type="bibr" target="#b36">[36]</ref> manages to tackle the data-inefficiency problem by simply adjusting the network architecture and adding an additional token along with the class token for Knowledge Distillation <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b47">47]</ref> to improve model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch Tokens</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Token</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Dense score map Output Tokens Token Labeling <ref type="figure">Figure 2</ref>: Pipeline of training vision transformers with token labeling. Other than utilizing the class token (pink rectangle), we also take advantage of all the output patch tokens (orange rounded rectangle) by assigning each patch token an individual location-specific prediction generated by a machine annotator <ref type="bibr" target="#b3">[3]</ref> as supervision (see the part in the red dash rectangle). Our proposed token labeling method can be treated as an auxiliary objective to provide each patch token the local details that aid vision transformers to more accurately locate and recognize the target objects. Note that the traditional vision transformer training does not include the red dash rectangle part.</p><p>Some recent works <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b46">46]</ref> also attempt to introduce the local dependency into vision transformers by modifying the patch embedding block or the transformer block or both, leading to significant performance gains. Moreover, there are also some works <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b40">40]</ref> adopting a pyramid structure to reduce the overall computation while maintaining the model's ability to capture low-level features.</p><p>Unlike most aforementioned works that design new transformer blocks or transformer architectures, we attempt to improve vision transformers by studying the role of patch tokens that embed rich local information inside image patches. We show that by slightly tuning the structure of vision transformers and employing the proposed token labeling objective, we can achieve strong baselines for transformer models at different model size levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Token Labeling Method</head><p>In this section, we first briefly review the structure of the vision transformer <ref type="bibr" target="#b16">[16]</ref> and then describe the proposed training objective-token labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisiting Vision Transformer</head><p>A typical vision transformer <ref type="bibr" target="#b16">[16]</ref> first decomposes a fixed-size input image into a sequence of small patches. Each small patch is mapped to a feature vector, or called a token, by projection with a linear layer. Then, all the tokens combined with an additional learnable class token for classification score prediction are sent into a stack of transformer blocks for feature encoding.</p><p>In loss computing, the class token from the output tokens of the last transformer block is usually selected and sent into a linear layer for the classification score prediction. Mathematically, given an image I, denote the output of the last transformer block as [X cls , X 1 , ..., X N ], where N is the total number of patch tokens, and X cls and X 1 , ..., X N correspond to the class token and the patch tokens, respectively. The classification loss for image I can be written as</p><formula xml:id="formula_0">L cls = H(X cls , y cls ),<label>(1)</label></formula><p>where H(?, ?) is the softmax cross-entropy loss and y cls is the class label. CutMix is operated on the input images. This results in patches containing mixed regions from the two images (see the patches enclosed by red bounding boxes). Differently, MixToken targets at mixing tokens after patch embedding. This enables each token after patch embedding to have clean content as shown in the right part of this figure. The detailed advantage of MixToken can be found in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Token Labeling</head><p>The above classification problem only adopts an image-level label as supervision whereas it neglects the rich information embedded in each image patch. In this subsection, we present a new training objective-token labeling-that takes advantage of the complementary information between the patch tokens and the class tokens.</p><p>Token Labeling: Different from the classification loss as formulated in Eqn. (1) that measures the distance between the single class token (representing the whole input image) and the corresponding image-level label, token labeling emphasizes the importance of all output tokens and advocates that each output token should be associated with an individual location-specific label. Therefore, in our method, the ground truth for an input image involves not only a single K-dimensional vector y cls but also a K ? N matrix or called a K-dimensional score map as represented by [y 1 , ..., y N ], where N is the number of the output patch tokens.</p><p>Specifically, we leverage a dense score map for each training image and use the cross-entropy loss between each output patch token and the corresponding aligned label in the dense score map as an auxiliary loss at the training phase. <ref type="figure">Figure 2</ref> provides an intuitive interpretation. Given the output patch tokens X 1 , ..., X N and the corresponding labels [y 1 , ..., y N ], the token labeling objective can be defined as</p><formula xml:id="formula_1">L tl = 1 N N i=1 H(X i , y i ).<label>(2)</label></formula><p>Recall that H is the cross-entropy loss. Therefore, the total loss function can be written as</p><formula xml:id="formula_2">L total = H(X cls , y cls ) + ? ? L tl ,<label>(3)</label></formula><p>= H(X cls , y cls )</p><formula xml:id="formula_3">+ ? ? 1 N N i=1 H(X i , y i ),<label>(4)</label></formula><p>where ? is a hyper-parameter to balance the two terms. In our experiment, we empirically set it to 0.5.</p><p>Advantages: Our token labeling offers the following advantages. First of all, unlike knowledge distillation methods that require a teacher model to generate supervision labels online, token labeling is a cheap operation. The dense score map can be generated by a pretrained model in advance (e.g., EfficientNet <ref type="bibr" target="#b34">[34]</ref> or NFNet <ref type="bibr" target="#b3">[3]</ref>). During training, we only need to crop the score map and perform interpolation to make it aligned with the cropped image in the spatial coordinate. Thus, the additional computations are negligible. Second, rather than utilizing a single label vector as supervision as done in most classification models and the ReLabel strategy <ref type="bibr" target="#b49">[49]</ref>, we also harness score maps to supervise the models in a dense manner and thereby the label for each patch token provides location-specific information, which can aid the training models to easily discover the target objects and improve the recognition accuracy. Last but not the least, as dense supervision is adopted in training, we found that the pretrained models with token labeling benefit downstream tasks with dense prediction, like semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Token Labeling with MixToken</head><p>While training vision transformer, previous studies <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b46">46]</ref> have shown that augmentation methods, like MixUp <ref type="bibr" target="#b52">[52]</ref> and CutMix <ref type="bibr" target="#b48">[48]</ref>, can effectively boost the performance and robustness of the models. However, vision transformers rely on patch-based tokenization to map each input image to a sequence of tokens and our token labeling strategy also operates on patch-based token labels. If we apply CutMix directly on the raw image, some of the resulting patches may contain content from two images, leading to mixed regions within a small patch as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. When performing token labeling, it is difficult to assign each output token a clean and correct label. Taking this situation into account, we rethink the CutMix augmentation method and present MixToken, which can be viewed as a modified version of CutMix operating on the tokens after patch embedding as illustrated in the right part of <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>To be specific, for two images denoted as I 1 , I 2 and their corresponding token labels</p><formula xml:id="formula_4">Y 1 = [y 1 1 , ..., y N 1 ] as well as Y 2 = [y 1 2 , ..., y N 2 ]</formula><p>, we first feed the two images into the patch embedding module to tokenize each as a sequence of tokens, resulting in</p><formula xml:id="formula_5">T 1 = [t 1 1 , ..., t N 1 ] and T 2 = [t 1 2 , ..., t N 2 ]</formula><p>. Then, we produce a new sequence of tokens by applying MixToken using a binary mask M as follows:</p><formula xml:id="formula_6">T = T 1 M + T 2 (1 ? M ),<label>(5)</label></formula><p>where is element-wise multiplication. We use the same way to generate the mask M as in <ref type="bibr" target="#b48">[48]</ref>.</p><p>For the corresponding token labels, we also mix them using the same mask M :</p><formula xml:id="formula_7">Y = Y 1 M + Y 2 (1 ? M ).<label>(6)</label></formula><p>The label for the class token can be written a?</p><formula xml:id="formula_8">y cls =M y cls 1 + (1 ?M )y cls 2 ,<label>(7)</label></formula><p>whereM is the average of all element values of M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>We evaluate our method on the ImageNet <ref type="bibr" target="#b14">[14]</ref> dataset. All experiments are built and conducted upon PyTorch <ref type="bibr" target="#b30">[30]</ref> and the timm <ref type="bibr" target="#b42">[42]</ref> library. We follow the standard training schedule and train our models on the ImageNet dataset for 300 epochs. Besides normal augmentations like CutOut <ref type="bibr" target="#b57">[57]</ref> and RandAug <ref type="bibr" target="#b11">[11]</ref>, we also explore the effect of applying MixUp <ref type="bibr" target="#b52">[52]</ref> and CutMix <ref type="bibr" target="#b48">[48]</ref> together with our proposed token labeling. Empirically, we have found that using MixUp together with token labeling brings no benefit to the performance, and thus we do not apply it in our experiments.</p><p>For optimization, by default, we use the AdamW optimizer <ref type="bibr" target="#b28">[28]</ref> with a linear learning rate scaling strategy lr = 10 ?3 ? batch_size 640 and 5 ? 10 ?2 weight decay rate. For Dropout regularization, we observe that for small models, using Dropout hurts the performance. This has also been observed in a few other works related to training vision transformers <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b46">46]</ref>. As a result, we do not apply Dropout <ref type="bibr" target="#b32">[32]</ref> and use Stochastic Depth <ref type="bibr" target="#b24">[24]</ref> instead. More details on hyper-parameters and finetuning can be found in our supplementary materials.</p><p>We use the NFNet-F6 <ref type="bibr" target="#b3">[3]</ref> trained on ImageNet with an 86.3% Top-1 accuracy as the machine annotator to generate dense score maps for the ImageNet dataset, yielding a 1000-dimensional score map for each image for training. The score map generation procedure is similar to <ref type="bibr" target="#b49">[49]</ref>, but we limit our experiment setting by training all models from scratch on ImageNet without extra data support, such as JFT-300M and ImageNet-22K. This is different from the original ReLabel paper <ref type="bibr" target="#b49">[49]</ref>, in which the EfficientNet-L2 model pretrained on JFT-300M is used. The input resolution for NFNet-F6 is 576 ? 576, and the dimension of the corresponding output score map for each image is L ? R 18?18?1000 . During training, the target labels for the tokens are generated by applying RoIAlign <ref type="bibr" target="#b18">[18]</ref> on the corresponding score map. In practice, we only store the top-5 score maps for each position in half-precision to save space as storing the entire score maps for all the images results in 2TB storage. In our experiment, we only need 10GB of storage to store all the score maps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Analysis</head><p>Model Settings: The default settings of the proposed LV-ViT are given in <ref type="table" target="#tab_0">Table 1</ref>, where both token labeling and MixToken are used. A slight architecture modification to ViT <ref type="bibr" target="#b16">[16]</ref> is that we replace the patch embedding module with a 4-layer convolution to better tokenize the input image and integrate local information. Detailed ablation about patch embedding can be found in our supplementary materials. As can be seen, our LV-ViT-T with only 8.5M parameters can already achieve a top-1 accuracy of 79.1% on ImageNet. Increasing the embedding dimension and network depth can further boost the performance. More experiments compared to other methods can be found in Sec. 4.3. In the following ablation experiments, we will set our LV-ViT-S as baseline and show the advantages of the proposed token labeling and MixToken methods.</p><p>MixToken: We use MixToken as a substitution for CutMix while applying token labeling. Our experiments show that MixToken performs better than CutMix for token-based transformer models. As shown in <ref type="table" target="#tab_1">Table 2</ref>, when training with the original ImageNet labels, using MixToken is 0.1% higher than using CutMix. When using the ReLabel supervision, we can also see an advantage of 0.2% over the CutMix baseline. Combining with our token labeling, the performance can be further raised to 83.3%.  Data Augmentation: Here, we study the compatibility of MixToken with other augmentation techniques, such as MixUp <ref type="bibr" target="#b52">[52]</ref>, CutOut <ref type="bibr" target="#b57">[57]</ref> and RandAug <ref type="bibr" target="#b11">[11]</ref>. The ablation results are shown in <ref type="table" target="#tab_2">Table 3</ref>. We can see when all the four augmentation methods are used, a top-1 accuracy of 83.1% is achieved. Interestingly, when the MixUp augmentation is removed, the performance can be improved to 83.3%. This may be explained as, using MixToken and MixUp at the same time would bring too much noise in the label, and consequently cause confusion of the model. Moreover, the CutOut augmentation, which randomly erases some parts of the image, is also effective and removing it brings a performance drop of 0.3%. Similarly, the RandAug augmentation also contributes to the performance and using it brings an improvement of 0.5%.</p><p>All Tokens Matter: To show the importance of involving all tokens in our token labeling method, we attempt to randomly drop some tokens and use the remaining ones for computing the token labeling loss. The percentage of the remaining tokens is denoted as Token Participation Rate. As shown in <ref type="figure" target="#fig_1">Figure 4</ref> (Left), we conduct experiments on two models: LV-ViT-S and LV-ViT-M. As can be seen, using only 20% of the tokens to compute the token labeling loss decreases the performance (?0.5% for LV-ViT-S and ?0.4% for LV-ViT-M). Involving more tokens for loss computation consistently leads to better performance. Since involving all tokens brings negligible computation cost and gives the best performance, we always set the token participation rate as 100% in the following experiments. Robustness to Different Annotators: To evaluate the robustness of our token labeling method, we use different pretrained CNNs, including EfficientNet-B3,B4,B5,B6,B7,B8 <ref type="bibr" target="#b34">[34]</ref>, NFNet-F6 <ref type="bibr" target="#b3">[3]</ref> and ResNest269E <ref type="bibr" target="#b51">[51]</ref>, as annotator models to provide dense supervision. Results are shown in the right part of <ref type="figure" target="#fig_1">Figure 4</ref>. We can see that, even if we use an annotator with relatively lower performance, such as EfficientNet-B3 whose Top-1 accuracy is 81.6%, it can still provide multi-label location-specific supervision and help improve the performance of our LV-ViT-S model. Meanwhile, annotator models with better performance can provide more accurate supervision, bringing even better performance, as stronger annotator models can generate better token-level labels. The largest annotator NFNet-F6 <ref type="bibr" target="#b3">[3]</ref>, which has the best performance of 86.3%, allows us to achieve the best result for LV-ViT-S, which is 83.3%. In addition, we also attempt to use a better model, EfficientNet-L2 pretrained on JFT-300M as described in <ref type="bibr" target="#b49">[49]</ref> which has 88.2% Top-1 ImageNet accuracy, as our annotator. The performance of LV-ViT-S can be further improved to 83.5%. However, to fairly compare with the models without extra training data, we only report results based on dense supervision produced by NFNet-F6 [3] that uses only ImageNet training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to Different ViT Variants:</head><p>To further evaluate the robustness of our token labeling, we train different transformer-based networks, including DeiT <ref type="bibr" target="#b36">[36]</ref>, T2T-ViT <ref type="bibr" target="#b3">[3]</ref> and our model LV-ViT, with the proposed training objective. Results are shown in <ref type="figure" target="#fig_2">Figure 5</ref>. It can be found that, all the models trained with token labeling consistently outperform their vanilla counterparts, demonstrating the robustness of token labeling with respect to different variants of patch-based vision transformers. Meanwhile, for different scales of the models, the improvement is also consistent. Interestingly, we observe larger improvements for larger models. These indicate that our proposed token labeling method is widely applicable to a large range of patch-based vision transformer variants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to Other Methods</head><p>We compare our proposed model LV-ViT with other state-of-the-art methods in <ref type="table" target="#tab_3">Table 4</ref>. For smallsized models, when the test resolution is set to 224 ? 224, we achieve an 83.3% accuracy on ImageNet with only 26M parameters, which is 3.4% higher than the strong baseline DeiT-S <ref type="bibr" target="#b36">[36]</ref>. For medium-sized models, when the test resolution is set to 384 ? 384 we achieve the performance of 85.4%, the same as CaiT-S36 <ref type="bibr" target="#b37">[37]</ref>, but with much less computational cost and parameters. Note that both DeiT and CaiT use knowledge distillation to improve their models, which introduce much more computations in training. However, we do not require any extra computations in training and only have to compute and store the dense score maps in advance. For large-sized models, our LV-ViT-L with a test resolution of 512 ? 512 achieves an 86.4% top-1 accuracy, which is better than CaiT-M36 <ref type="bibr" target="#b37">[37]</ref> but with far fewer parameters and FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Semantic Segmentation on ADE20K</head><p>It has been shown in <ref type="bibr" target="#b20">[20]</ref> that different training techniques for pretrained models have different impacts on downstream tasks with dense prediction, like semantic segmentation. To demonstrate the advantage of the proposed token labeling objective on tasks with dense prediction, we apply our pretrained LV-ViT with token labeling to the semantic segmentation task.</p><p>Similar to previous work <ref type="bibr" target="#b26">[26]</ref>, we run experiments on the widely-used ADE20K <ref type="bibr" target="#b58">[58]</ref> dataset. ADE20K contains 25K images in total, including 20K images for training, 2K images for validation and 3K images for test, and covering 150 different foreground categories. We take both FCN <ref type="bibr" target="#b27">[27]</ref> and UperNet <ref type="bibr" target="#b44">[44]</ref> as our segmentation frameworks and use the mmseg toolbox to implement. During training, following <ref type="bibr" target="#b26">[26]</ref>, we use the AdamW optimizer with an initial learning rate of 6e-5 and a weight decay of 0.01. We also use a linear learning schedule with a minimum learning rate of 5e-6. All models are trained on 8 GPUs and with a batch size of 16 (i.e., 2 images on each GPU). The input resolution is set to 512 ? 512. In inference, a multi-scale test with interpolation rates of [0.75, 1.0, 1.25, 1.5, 1.75] is used. As suggested by <ref type="bibr" target="#b58">[58]</ref>, we report results in terms of both mean intersection-over-union (mIoU) and the average pixel accuracy (Pixel Acc.).</p><p>In <ref type="table" target="#tab_4">Table 5</ref>, we test the performance of token labeling on both FCN and UperNet frameworks. The FCN framework has a light convolutional head and can directly reflect the performance of the pretrained models in terms of transferable capability. As can be seen, pretrained models with token labeling perform better than those without token labeling. This indicates token labeling is indeed beneficial to semantic segmentation.</p><p>We also compare our segmentation results with previous state-of-the-art segmentation methods in <ref type="table" target="#tab_5">Table 6</ref>. Without pretraining on large-scale datasets such as ImageNet-22K, our LV-ViT-M with the UperNet segmentation architecture achieves an mIoU score of 50.6 with only 77M parameters. This result is much better than the previous CNN-based and transformer-based models. Furthermore, using our LV-ViT-L as the pretrained model yields a better result of 51.8 in terms of mIoU. As far as we know, this is the best result reported on ADE20K with no pretraining on ImageNet-22K or other large-scale datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Discussion</head><p>In this paper, we introduce a new token labeling method to help improve the performance of vision transformers. We also analyze the effectiveness and robustness of our token labeling with respect to different annotators and different variants of patch-based vision transformers. By applying token labeling, our proposed LV-ViT achieves 84.4% Top-1 accuracy with only 26M parameters and 86.4% Top-1 accuracy with 150M parameters on ImageNet-1K benchmark.</p><p>Despite the effectiveness, token labeling has a limitation of requiring a pretrained model as the machine annotator. Fortunately, the machine annotating procedure can be done in advance to avoid introducing extra computational cost in training. This makes our method quite different from knowledge distillation methods that rely on online teaching. For users with limited machine resources on hand, our token labeling provides a promising training technique to improve the performance of vision transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More Experiment Details</head><p>We show the default hyper-parameters for our ImageNet classification experiments in <ref type="table" target="#tab_6">Table 7</ref>. In addition, for fine-tuning on larger image resolution, we set batch size to 512, learning rate to 5e-6, weight decay to 1e-8 and fine-tune 30 epochs. Other hyper-parameters are set the same as default.</p><p>During training, a machine node with 8 NVIDIA V100 GPUs (32G memory) is required. When fine-tuning our large model with image resolution of 448 ? 448, we need 4 machine nodes with the same GPU settings as above. We present a summary of our modification and proposed token labeling method to improve vision transformer models in <ref type="table" target="#tab_7">Table 8</ref>. We take the DeiT-Small <ref type="bibr" target="#b36">[36]</ref> model as our baseline and show the performance increment as more training techniqeus are added. In this subsection, we will ablate the proposed modifications and evaluate the effectiveness of them. Explicit inductive bias for patch embedding: Ablation analysis of patch embedding is presented in <ref type="table" target="#tab_8">Table 9</ref>. The baseline is set to the same as the setting as presented in the third row of <ref type="table" target="#tab_7">Table 8</ref>. Clearly, by adding more convolutional layers and narrow the kernel size in the patch embedding, we can see a consistent increase in the performance comparing to the original single-layer patch embedding. However, when further increasing the number of convolutional layer in patch embedding to 6, we do not observe any performance gain. This indicates that using 4-layer convolutions in patch embedding is enough. Meanwhile, if we use a larger stride to reduce the size of the feature map, we can largely reduce the computation cost, but the performance also drops. Thus, we only apply a convolution of stride 2 and kernel size 7 at the beginning of the patch embedding module, followed by two convolutional layers with stride 1 and kernel size 3. The feature map is finally tokenized to a sequence of tokens using a convolutional layer of stride 8 and kernel size 8 (see the fifth line in <ref type="table" target="#tab_8">Table 9</ref>). Enhanced residual connection: We found that introducing a residual scaling factor can also bring benefit as shown in <ref type="table" target="#tab_0">Table 10</ref>. We found that using smaller scaling factor can lead to better performance and faster convergence. Part of the reason is that more information can be preserved in the main branch, leading to less information loss and better performance. </p><formula xml:id="formula_9">X ?? X + F (X) 26M 82.2 X ?? X + F (X)/2 26M 82.4 X ?? X + F (X)/3 26M 82.4</formula><p>Larger input resolution: To adapt our model to larger input image, we interpolate the positional encoding and fine-tune the model on larger image resolution for a few epochs. Token labeling objective as well as MixToken are also used during fine-tuning. As can be seen from <ref type="table" target="#tab_7">Table 8</ref>, fine-tuning on larger input resolution of 384 ? 384 can improve the performance by 1.1% for our LV-ViT-S model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Beyond Vision Transformers: Performance on MLP-Based and CNN-Based Models</head><p>We further explore the performance of token labeling on other CNN-based and MLP-based models. Results are shown in <ref type="table" target="#tab_0">Table 11</ref>. Besides our re-implementation with more data augmentation and regularization technique, we also list the results from the original papers. It shows that for both MLP-based and CNN-based models, token labeling objective can still improve the performance over the strong baselines by providing the location-specific dense supervision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Comparison with CaiT</head><p>CaiT <ref type="bibr" target="#b37">[37]</ref> is currently the best transformer-based model. We list the comparison of training hyperparameters and model configuration with CaiT in <ref type="table" target="#tab_0">Table 12</ref>. It can be seen that using less training techniques, computations, and smaller model size, our LV-ViT achieves identical result to the stateof-the-art CaiT model.  <ref type="bibr" target="#b24">[24]</ref> 0.2 (Linear) 0.2 (Fixed) Rand Augmentation <ref type="bibr" target="#b11">[11]</ref> CutMix Augmentation <ref type="bibr" target="#b48">[48]</ref> MixUp Augmentation <ref type="bibr" target="#b52">[52]</ref> LayerScaling <ref type="bibr" target="#b37">[37]</ref> Class Attention <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Visualization</head><p>We apply the method proposed in <ref type="bibr" target="#b6">[6]</ref> to visualize both DeiT-base and our LV-ViT-S. Results are shown in <ref type="figure" target="#fig_3">Figure 6</ref> and <ref type="figure" target="#fig_4">Figure 7</ref>. In <ref type="figure" target="#fig_3">Figure 6</ref>, we can observe that our LV-ViT-S model performs better in locating the target objects and hence yields better classification performance with high confidence. In <ref type="figure" target="#fig_4">Figure 7</ref>, we visualize the top-2 classes predicted by the two models. Noted that we follow <ref type="bibr" target="#b6">[6]</ref> to select images with at least 2 classes existing. It can be seen that our LV-ViT-S trained with token labeling can accurately locate both classes while the DeiT-base sometimes fails in locating the entire target object for a certain class. This demonstrates that our token labeling objective does help in improving models' visual grounding capability because of the location-specific token-level information.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Comparison between CutMix<ref type="bibr" target="#b48">[48]</ref> (Left) and our proposed MixToken (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Left: LV-ViT ImageNet Top-1 Accuracy w.r.t. the token participation rate while applying token labeling. Token participation rate indicates the percentage of patch tokens involved in computing the token labeling loss. This experiment reflects that all tokens matter for vision transformers. Right: LV-ViT-S ImageNet Top-1 Accuracy w.r.t. different annotator models. The point size indicates the parameter number of the annotator model. Clearly, our token labeling objective is robust to different annotator models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Performance of the proposed token labeling objective on three different vision transformers: DeiT<ref type="bibr" target="#b36">[36]</ref> (Left), T2T-ViT<ref type="bibr" target="#b46">[46]</ref> (Middle), and LV-ViT (Right). Our method has a consistent improvement on all 7 different ViT models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Visual comparisons between DeiT-base and LV-ViT-S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Visual comparisons between DeiT-base and LV-ViT-S for the top-2 predicted classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of the proposed LV-ViT with different model sizes. Here, 'depth' denotes the number of transformer blocks used in different models. By default, the test resolution is set to 224 ? 224 except the last one which is 288 ? 288. Depth Embed dim. MLP Ratio #Heads #Parameters Resolution Top-1 Acc. (%)</figDesc><table><row><cell>Name</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LV-ViT-T</cell><cell>12</cell><cell>240</cell><cell>3.0</cell><cell>4</cell><cell>8.5M</cell><cell>224x224</cell><cell>79.1</cell></row><row><cell>LV-ViT-S</cell><cell>16</cell><cell>384</cell><cell>3.0</cell><cell>6</cell><cell>26M</cell><cell>224x224</cell><cell>83.3</cell></row><row><cell>LV-ViT-M</cell><cell>20</cell><cell>512</cell><cell>3.0</cell><cell>8</cell><cell>56M</cell><cell>224x224</cell><cell>84.1</cell></row><row><cell>LV-ViT-L</cell><cell>24</cell><cell>768</cell><cell>3.0</cell><cell>12</cell><cell>150M</cell><cell>288x288</cell><cell>85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation on the proposed MixToken and token labeling augmentations. We also show results with either the ImageNet hard label and the ReLabel<ref type="bibr" target="#b49">[49]</ref> as supervision.</figDesc><table><row><cell>Aug. Method</cell><cell>Supervision</cell><cell>Top-1 Acc.</cell></row><row><cell>MixToken</cell><cell>Token labeling</cell><cell>83.3</cell></row><row><cell>MixToken</cell><cell>ReLabel</cell><cell>83.0</cell></row><row><cell>CutMix</cell><cell>ReLabel</cell><cell>82.8</cell></row><row><cell>Mixtoken</cell><cell>ImageNet Label</cell><cell>82.5</cell></row><row><cell>CutMix</cell><cell>ImageNet Label</cell><cell>82.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>83.3</cell></row><row><cell>81.3</cell></row><row><cell>83.1</cell></row><row><cell>83.0</cell></row><row><cell>82.8</cell></row></table><note>Ablation on different widely-used data aug- mentations. We have empirically found our proposed MixToken performs even better than the combination of MixUp and CutMix in vision transformers.MixToken MixUp CutOut RandAug Top-1 Acc.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Top-1 accuracy comparison with other methods on ImageNet<ref type="bibr" target="#b14">[14]</ref> and ImageNet Real<ref type="bibr" target="#b2">[2]</ref>. All models are trained without external data. With the same computation and parameter constraint, our model consistently outperforms other CNN-based and transformer-based counterparts. The results of CNNs and ViT are referenced from<ref type="bibr" target="#b37">[37]</ref>.</figDesc><table><row><cell></cell><cell>Network</cell><cell cols="6">Params FLOPs Train size Test size Top-1(%) Real Top-1 (%)</cell></row><row><cell></cell><cell>EfficientNet-B5 [34]</cell><cell>30M</cell><cell>9.9B</cell><cell>456</cell><cell>456</cell><cell>83.6</cell><cell>88.3</cell></row><row><cell>CNNs</cell><cell>EfficientNet-B7 [34] Fix-EfficientNet-B8 [34, 38] NFNet-F3 [3]</cell><cell cols="2">66M 87M 255M 114.8B 37.0B 89.5B</cell><cell>600 672 320</cell><cell>600 800 416</cell><cell>84.3 85.7 85.7</cell><cell>_ 90.0 89.4</cell></row><row><cell></cell><cell>NFNet-F4 [3]</cell><cell cols="2">316M 215.3B</cell><cell>384</cell><cell>512</cell><cell>85.9</cell><cell>89.4</cell></row><row><cell></cell><cell>NFNet-F5 [3]</cell><cell cols="2">377M 289.8B</cell><cell>416</cell><cell>544</cell><cell>86.0</cell><cell>89.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>16.0B</cell><cell>224</cell><cell>224</cell><cell>84.1</cell><cell>88.4</cell></row><row><cell></cell><cell>LV-ViT-M?384</cell><cell>56M</cell><cell>42.2B</cell><cell>224</cell><cell>384</cell><cell>85.4</cell><cell>89.5</cell></row><row><cell></cell><cell>LV-ViT-L</cell><cell>150M</cell><cell>59.0B</cell><cell>288</cell><cell>288</cell><cell>85.3</cell><cell>89.3</cell></row><row><cell></cell><cell>LV-ViT-L?448</cell><cell cols="2">150M 157.2B</cell><cell>288</cell><cell>448</cell><cell>85.9</cell><cell>89.7</cell></row><row><cell></cell><cell>LV-ViT-L?448</cell><cell cols="2">150M 157.2B</cell><cell>448</cell><cell>448</cell><cell>86.2</cell><cell>89.9</cell></row><row><cell></cell><cell>LV-ViT-L?512</cell><cell cols="2">151M 214.8B</cell><cell>448</cell><cell>512</cell><cell>86.4</cell><cell>90.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Transfer performance of the proposed LV-ViT in semantic segmentation. We take two classic methods, FCN and UperNet, as segmentation architectures and show both single-scale (SS) and multi-scale (MS) results on the validation set.</figDesc><table><row><cell>Method</cell><cell cols="5">Token Labeling Model Size mIoU (SS) P. Acc. (SS) mIoU (MS) P. Acc. (MS)</cell></row><row><cell>LV-ViT-S + FCN</cell><cell>30M</cell><cell>46.1</cell><cell>81.9</cell><cell>47.3</cell><cell>82.6</cell></row><row><cell>LV-ViT-S + FCN</cell><cell>30M</cell><cell>47.2</cell><cell>82.4</cell><cell>48.4</cell><cell>83.0</cell></row><row><cell>LV-ViT-S + UperNet</cell><cell>44M</cell><cell>46.5</cell><cell>82.1</cell><cell>47.6</cell><cell>82.7</cell></row><row><cell>LV-ViT-S + UperNet</cell><cell>44M</cell><cell>47.9</cell><cell>82.6</cell><cell>48.6</cell><cell>83.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison with previous work on ADE20K validation set. As far as we know, our LV-ViT-L + UperNet achieves the best result on ADE20K with only ImageNet-1K as training data in pretraining.</figDesc><table><row><cell></cell><cell>Backbone</cell><cell>Segmentation Architecture</cell><cell>Model Size</cell><cell>mIoU (MS)</cell><cell>Pixel Acc. (MS)</cell></row><row><cell>CNNs</cell><cell>ResNet-269 ResNet-101 ResNet-101</cell><cell>PSPNet [54] UperNet [44] Strip Pooling [23]</cell><cell>-86M -</cell><cell>44.9 44.9 45.6</cell><cell>81.7 -82.1</cell></row><row><cell></cell><cell>ResNeSt200</cell><cell>DeepLabV3+ [9]</cell><cell>88M</cell><cell>48.4</cell><cell>-</cell></row><row><cell>Transformers</cell><cell>DeiT-S ViT-Large  ? Swin-T [26] Swin-S [26] Swin-B [26] Swin-B  ? [26]</cell><cell>UperNet SETR [56] UperNet UperNet UperNet UperNet</cell><cell>52M 308M 60M 81M 121M 121M</cell><cell>44.0 50.3 46.1 49.3 49.7 51.6</cell><cell>-83.5 ----</cell></row><row><cell>LV-ViT</cell><cell>LV-ViT-S LV-ViT-S LV-ViT-M LV-ViT-L</cell><cell>FCN UperNet UperNet UperNet</cell><cell>30M 44M 77M 209M</cell><cell>48.4 48.6 50.6 51.8</cell><cell>83.0 83.1 83.5 84.1</cell></row></table><note>? Pretrained on ImageNet-22K.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Default hyper-parameters for our experiments. Note that we do not use the MixUp augmentation method when ReLabel or token labeling is used.</figDesc><table><row><cell>Supervision</cell><cell>Standard</cell><cell>ReLabel</cell><cell>Token labeling</cell></row><row><cell>Epoch</cell><cell>300</cell><cell>300</cell><cell>300</cell></row><row><cell>Batch size</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell></row><row><cell>LR LR decay</cell><cell>1e-3 ? batch_size 1024 cosine</cell><cell>1e-3? batch_size 1024 cosine</cell><cell>1e-3? batch_size 640 cosine</cell></row><row><cell>Weight decay</cell><cell>0.05</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>Warmup epochs</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>Dropout</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Stoch. Depth</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>MixUp alpha</cell><cell>0.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Erasing prob.</cell><cell>0.25</cell><cell>0.25</cell><cell>0.25</cell></row><row><cell>RandAug</cell><cell>9/0.5</cell><cell>9/0.5</cell><cell>9/0.5</cell></row><row><cell>B More Experiments</cell><cell></cell><cell></cell><cell></cell></row><row><cell>B.1 Training Technique Analysis</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Ablation path from the DeiT-Small<ref type="bibr" target="#b36">[36]</ref> baseline to our LV-ViT-S. All experiments expect for larger input resolution can be finished within 3 days using a single server node with 8 V100 GPUs. Clearly, with only 26M learnable parameters, the performance can be boosted from 79.9 to 84.4 (+4.5) using the proposed Token Labeling and other proposed training techniques.</figDesc><table><row><cell>Training techniques</cell><cell cols="2">#Param. Top-1 Acc. (%)</cell></row><row><cell>Baseline (DeiT-Small [36])</cell><cell>22M</cell><cell>79.9</cell></row><row><cell>+ More transformers (12 ? 16)</cell><cell>28M</cell><cell>81.2 (+1.2)</cell></row><row><cell cols="2">+ Less MLP expansion ratio (4 ? 3) 25M</cell><cell>81.1 (+1.1)</cell></row><row><cell>+ More convs for patch embedding</cell><cell>26M</cell><cell>82.2 (+2.3)</cell></row><row><cell>+ Enhanced residual connection</cell><cell>26M</cell><cell>82.4 (+2.5)</cell></row><row><cell>+ Token labeling with MixToken</cell><cell>26M</cell><cell>83.3 (+3.4)</cell></row><row><cell>+ Input resolution (224 ? 384)</cell><cell>26M</cell><cell>84.4 (+4.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Ablation on patch embedding. Baseline is set as 16 layer ViT with embedding size 384 and MLP expansion ratio of 3. All convolutional layers except the last block have 64 filters. #Convs indicatie the total number of convolutions for patch embedding, while the kernel size and stride correspond to each layer are shown as a list in the table.</figDesc><table><row><cell cols="2">#Convs Kerenl size</cell><cell>Stride</cell><cell cols="2">Params Top-1 Acc. (%)</cell></row><row><cell>1</cell><cell>[16]</cell><cell>[16]</cell><cell>25M</cell><cell>81.1</cell></row><row><cell>2</cell><cell>[7,8]</cell><cell>[2,8]</cell><cell>25M</cell><cell>81.4</cell></row><row><cell>3</cell><cell>[7,3,8]</cell><cell>[2,2,4]</cell><cell>25M</cell><cell>81.4</cell></row><row><cell>3</cell><cell>[7,3,8]</cell><cell>[2,1,8]</cell><cell>26M</cell><cell>81.9</cell></row><row><cell>4</cell><cell>[7,3,3,8]</cell><cell>[2,1,1,8]</cell><cell>26M</cell><cell>82.2</cell></row><row><cell>6</cell><cell cols="3">[7,3,3,3,3,8] [2,1,1,1,1,8] 26M</cell><cell>82.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Ablation on enhancing residual connection by applying a scaling factor. Baseline is a 16-layer vision transformer with 4-layer convolutional patch embedding. Here, function F represents either self-attention (SA) or feed forward (FF).</figDesc><table><row><cell>Forward Function</cell><cell>#Parameters</cell><cell>Top-1 Acc. (%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Performance of the proposed token labeling objective on representative CNN-based (ResNeSt) and MLP-based (Mixer-MLP) models. Our method has a consistent improvement on all different models. Here ? indicates results reported in original papers. Acc. (%) 73.8 ? 75.6 76.1 76.4 ? 78.3 79.5 71.6 ? 77.7 80.1 81.1 ? 80.9 81.5</figDesc><table><row><cell>Model</cell><cell>Mixer-S/16 [35]</cell><cell>Mixer-B/16 [35]</cell><cell>Mixer-L/16 [35]</cell><cell>ResNeSt-50 [51]</cell></row><row><cell>Token Labeling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Parameters</cell><cell cols="4">18M 18M 18M 59M 59M 59M 207M 207M 207M 27M 27M 27M</cell></row><row><cell>Top-1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Comparison with CaiT<ref type="bibr" target="#b37">[37]</ref>. Our model exploits less training techniques, model size, and computations but achieve identical result to CaiT.</figDesc><table><row><cell>Settings</cell><cell>LV-ViT (Ours)</cell><cell>CaiT [37]</cell></row><row><cell>Transformer Blocks</cell><cell>20</cell><cell>36</cell></row><row><cell>#Head in Self-attention</cell><cell>8</cell><cell>12</cell></row><row><cell>MLP Expansion Ratio</cell><cell>3</cell><cell>4</cell></row><row><cell>Embedding Dimension</cell><cell>512</cell><cell>384</cell></row><row><cell>Stochastic Depth</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lv-Vit Lv-Vit-S</forename><surname>Ours</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>26m</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<title level="m">Modeling long-range interactions without attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Xiaohua Zhai, and A?ron van den Oord. Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transformer interpretability beyond attention visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shir</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09838</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14899</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16302</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Strip pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10881</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06423</idno>
		<title level="m">Matthijs Douze, and Herv? J?gou. Fixing the train-test resolution discrepancy</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05022</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="528" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Point transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">End-to-end object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09315</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>Original Original DeiT-Base DeiT-Base LV-ViT-S LV-ViT-S</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
