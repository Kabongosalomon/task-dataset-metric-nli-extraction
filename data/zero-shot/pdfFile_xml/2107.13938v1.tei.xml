<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Why You Should Try the Real Data for the Scene Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loginov</forename><surname>Vladimir</surname></persName>
							<email>vladimir.loginov@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Why You Should Try the Real Data for the Scene Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Domain Shift ? Open Images ? Scene Text Recognition</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works in the text recognition area have pushed forward the recognition results to the new horizons. But for a long time a lack of large human-labeled natural text recognition datasets has been forcing researchers to use synthetic data for training text recognition models. Even though synthetic datasets are very large (MJSynth and SynthText, two most famous synthetic datasets, have several million images each), their diversity could be insufficient, compared to natural datasets like ICDAR and others. Fortunately, the recently released text recognition annotation for OpenImages V5 dataset has comparable with synthetic dataset number of instances and more diverse examples. We have used this annotation with a Text Recognition head architecture from the Yet Another Mask Text Spotter and got comparable to the SOTA results. On some datasets we have even outperformed previous SOTA models. In this paper we also introduce a text recognition model. The model's code is available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The text recognition plays an important role in people's lives. It can be used for digitalizing old documents and helping blind people read. Also, the text recognition could be utilized in a variety of automation algorithms; one of the most common use-cases is the license plate recognition.</p><p>Since the release of the synthetic datasets <ref type="bibr">(MJSynthJaderberg et al. (2014)</ref>, <ref type="bibr">SynthTextGupta et al. (2016)</ref>), a common approach to the model training was the following: use synthetic data for training and real data for evaluation. This was caused by a lack of labeled large text recognition datasets. From the very first deep learning text recognition architectures such as <ref type="bibr" target="#b24">Shi et al. (2015)</ref> to the modern works like <ref type="bibr" target="#b5">Cai et al. (2021)</ref>, <ref type="bibr" target="#b7">Cui et al. (2021)</ref> researchers have followed this pattern. Even though real datasets have train splits, their size is significantly less than the size of synthetic datasets.</p><p>Due to the lack of real data for training, previous works aimed mainly at tuning an architecture of the model rather than using different data. <ref type="bibr" target="#b1">Baek et al. (2019)</ref> suggested the idea of differentiating a text recognition model into 4 stages: transformation, feature extraction, sequence modelling and prediction. Many of the existing text recognition model architectures fit into this scheme. We follow the same approach: in our work we use <ref type="bibr">TPS Jaderberg et al. (2015)</ref> for transformation, ResNeXt <ref type="bibr" target="#b28">Xie et al. (2016)</ref> for feature extraction, convolutional encoder-recurrent decoder from <ref type="bibr">YAMTS Krylov et al. (2021)</ref> with 2D attention mechanism the sequence modelling and the prediction.</p><p>The rest of the paper is organized as follows:</p><p>-In the Section 2 we briefly discuss other works in this field.</p><p>-In the Section 3 we describe in details the suggested model architecture.</p><p>-In the Section 4 we show the results of our experiments.</p><p>-In the Section 5 we make conclusions and do some call to action for other researchers.</p><p>Our main contributions are as follows:</p><p>-Developing the idea of using real data together with synthetic data for training text recognition models.</p><p>-Suggesting text recognition model which performs on par with other models and even outperforms current state-of-the-art approaches on some datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>MJSynthJaderberg et al. <ref type="formula">(2014)</ref>, <ref type="bibr">SynthTextGupta et al. (2016)</ref> are two large (each has several million images with text) text recognition datasets. These two datasets contain images generated in the certain pipeline. In MJSynth, image generation process is the following: first, certain font is picked and text in this font is rendered; second, using the image from the previous step, borders and shadows are colored and composed together; third, projective distortion is applied to the image; and the last step is natural image blending. See <ref type="bibr" target="#b10">Jaderberg et al. (2014)</ref> for details. SynthText's pipeline takes into account depth of the image regions and uses segmentation network to predict suitable positions to put the text. See the corresponding publication <ref type="bibr" target="#b8">Gupta et al. (2016)</ref> for details. These two works play an important role in text recognition area, and their contribution could hardly be overestimated. However, even though images from MJSynth and SynthText look like real, in some cases it is clear that the specific image is manually generated. Moreover, real world images are sometimes so various and rich in texture, curvature, color and other visual features that it is hard to generate them using a set of predefined rules. Thus, there is a problem: there are datasets used for training with great number of examples, but these examples are not diverse enough; and there are testing datasets with more complex, real images on which the model is evaluated. This causes a misalignment between training and testing samples. The problem is also known as domain shift <ref type="bibr" target="#b19">Michieli et al. (2020)</ref>; <ref type="bibr" target="#b29">Zhang et al. (2019)</ref>. Model fits to the training images and, without augmentations, shows poorer performance on the testing ones. If a researcher decides to use augmentation to deal with the overfitting problem, model spends its capacity to learn augmented training samples which are still different from testing. This results in model performance drop on the testing dataset. <ref type="bibr" target="#b2">Baek et al. (2021)</ref> was one of the first works which suggested the idea of using real datasets for training text recognition models, but in unsupervised manner, since large annotated datasets were not available then. Recently released text spotting annotation for OpenImagesV5 <ref type="bibr" target="#b15">Krylov et al. (2021)</ref> and <ref type="bibr">TextOCR Singh et al. (2021)</ref> made training text recognition models on real data in a supervised manner possible. <ref type="bibr">ASTER Shi et al. (2019)</ref> is one of the works which utilizes Thin Plate Spline transformation for automatic image rectification. Experiments from <ref type="bibr" target="#b25">Shi et al. (2019)</ref> show that this technique helps to boost the recognition accuracy.</p><p>ResNeXt <ref type="bibr" target="#b28">Xie et al. (2016)</ref> and ResNet-like <ref type="bibr" target="#b9">He et al. (2016)</ref> backbones are widely used as feature extractors for many text recognition models (Shi et al.  <ref type="formula">2019)</ref>). We have experimented extensively with the configuration of the backbone and found that standard configuration without the last stage pretrained on the ImageNetRussakovsky et al. (2014) performs the best.</p><p>Attention mechanism <ref type="bibr" target="#b3">Bahdanau et al. (2016)</ref> was first introduced as a novel technique for the machine translation. Recently, the attention mechanism have become a standard for text recognition models.</p><p>Alongside with annotation for text spotting we also use the text recognition head from <ref type="bibr" target="#b15">Krylov et al. (2021)</ref>. It performs well for text spotting task and our experiments show that it also performs well for text recognition task.</p><p>3 Model Architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>We follow the same approach as in <ref type="bibr" target="#b1">Baek et al. (2019)</ref> but do not divide the last two stages. Thus, the text recognition head consists of sequence modelling and final prediction. We utilize <ref type="bibr">TPS Jaderberg et al. (2015)</ref> as in <ref type="bibr">ASTER Shi et al. (2019)</ref> for image rectification, since this stage has become one of the standard building blocks for modern text recognition models. We use ResNeXt <ref type="bibr" target="#b28">Xie et al. (2016)</ref> for feature extraction due to its good performance/accuracy trade-off. In our work we have utilized the same text recognition head as in <ref type="bibr" target="#b15">Krylov et al. (2021)</ref>. You can see the full model in the <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Thin Plate Spline</head><p>Thin Plate Spline (TPS) <ref type="bibr" target="#b11">Jaderberg et al. (2015)</ref> was first proposed for image recognition in the rectification task. Later, this module was utilized in some text recognition models <ref type="bibr" target="#b25">Shi et al. (2019)</ref>; <ref type="bibr" target="#b22">Qiao et al. (2020)</ref>.</p><p>The spatial transformer mechanism is split into three parts. In order of computation, first a localization network takes the image and generates parameters for spatial transformation. Using the predicted parameters, a sampling grid is generated, which is a set of points where the input map should be sampled to produce the transformed output. This is done by the grid generator. Finally, the sampler generates output image using the grid and input image.</p><p>Thin plate spline is designed in such way that it does not require letter level annotation. Thus, it can be utilized as is in many text recognition architectures and its parameters can be learned in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Backbone</head><p>Since the release of the ResNet <ref type="bibr" target="#b9">He et al. (2016)</ref> this network has become popular for many computer vision tasks due to its simplicity of learning and capacity. During the past years, ResNet-like networks showed their effectiveness and were used in many other text recognition models: <ref type="bibr">ASTER Shi et al. (2019)</ref>, <ref type="bibr">CSTR Cai et al. (2021)</ref>, <ref type="bibr">SEED Qiao et al. (2020)</ref>, <ref type="bibr" target="#b1">Baek et al. (2019)</ref>. The ResNeXt-101 consists of 5 stages. We use ResNeXt-101 without the last stage. We find this configuration to have a good accuracy/performance trade-off. In our model we utilize pretrained on ImageNet <ref type="bibr" target="#b23">Russakovsky et al. (2014)</ref> weights as initial for the ResNeXt-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Text Recognition Head</head><p>Our network heavily utilizes the text recognition head from <ref type="bibr" target="#b15">Krylov et al. (2021)</ref> with minor changes. First, we increase the number of channels to 1024 in the convolutional encoder since this is the exact number of the output channels from the ResNext-101 (without the last stage). Convolutional encoder of the head also produces 1024 output channels. Second, for the same reason, we have changed the size of the feature map to 3 ? 12. For the rest of the parameters, we use the same architecture, including the depth of the encoder part and the type of the recurrent decoder (GRU <ref type="bibr" target="#b6">Cho et al. (2014)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Training Datasets There are two large synthetic datasets:</p><p>-MJSynth is a synthetically generated text recognition dataset <ref type="bibr" target="#b10">Jaderberg et al. (2014)</ref>. It has about 9M images. -SynthText is also a synthetic dataset, but it has an annotation for text detection and text recognition. It consists of about 800k images with approximately 8M word instances.</p><p>Recently there were released two large natural text recognition datasets: Tex-tOCR <ref type="bibr" target="#b26">Singh et al. (2021)</ref> and OpenImages V5 text spotting <ref type="bibr" target="#b15">Krylov et al. (2021)</ref>.</p><p>-TextOCR has about 900k instances of annotated words.</p><p>-OpenImages V5 text spotting dataset has about 2M text instances.</p><p>In our work we have used a combination of synthetic and real data: MJSynth and OpenImages V5 text spotting. We did not use any specific technique to balance samples from different datasets in the batch.</p><p>Testing Datasets In the text recognition task, datasets could be divided into two groups: Regular and Irregular datasets. In the regular datasets, like IC-DAR03, ICDAR13, IIIT5k, SVT, text is mainly horizontally oriented without curvatures and perspective distortions. Irregular datasets, like ICDAR15, SVT-P, CUTE, mostly consist of curved texts, which could be with some perspective distortions. Images from irregular datasets are usually more challenging for text recognition models to make the prediction.</p><p>We perform evaluation of our model on the testing parts of the following datasets:</p><p>Regular datasets:</p><p>- <ref type="bibr">ICDAR2003 Lucas et al. (2003</ref> consists of 1156 images for training and 1110 images for evaluation. A common practice for text recognition models is to ignore all images which contain non-alphanumeric symbols and are less than 3 symbols in length. This results in 867 images for evaluation. Both training and evaluation of the model were performed in case-insensitive mode. The vocabulary of our experiments consists of 10 digits, 26 letters and 4 special symbols: start token, end token, pad token and unknown token. Pad token is used in training phase for numeric representations of the texts to be equal length and further concatenate them into one batch. Unknown token is used to represent symbol that is not in the vocabulary (e.g., question mark). Thus, the final stage of our model predicts every character among 40 classes. We do not use any lexicon or beam search to boost the recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Preprocessing</head><p>Image preprocessing and augmentation plays an important role in many deep learning algorithms. We resize all input images to 64 ? 256 fixed resolution. TPS preprocessing of our network produces image of fixed size 48 ? 192. We also use color jittering (from torchvision <ref type="bibr" target="#b18">Marcel and Rodriguez (2010)</ref>) and Gaussian blurring (from opencv <ref type="bibr" target="#b4">Bradski (2000)</ref>) as augmentations. In the end of the augmentation pipeline we convert all images to grayscale and scale images to be in [0;1] interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss Function</head><p>Standard Negative Loglikelihood loss function is used as cost function. We use Adam <ref type="bibr" target="#b14">Kingma and Ba (2017)</ref> optimizer with a constant learning rate 1e-4. Other parameters are default for PyTorch's Adam optimizer, including zero weight decay. The model was trained for 15 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Environment</head><p>All experiments were performed on a server with 8 NVIDIA? Tesla P100 GPUs with 16 GB VRAM. We use PyTorch 1.8.1 as our training framework. Batch size was set to 48.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Results</head><p>We compare our model with current state-of-the art approaches in the <ref type="table" target="#tab_1">Table 1</ref>. Standard text recognition accuracy is used as a metric of quality.  It is clear from the table, that having only synthetic data is insufficient to train a robust text recognition model. It is also interesting that using only real data drops recognition accuracy a bit. Possible explanation for this fact is that since the training examples are more difficult, the model should be trained on a longer schedule (possibly with lower learning rate) like in case of strong augmentations.</p><p>Influence of Case Sensitive Learning As it was stated in the previous section, our first models were trained in case-sensitive mode while evaluation was performed in case-insensitive mode, like in <ref type="bibr" target="#b16">Lee et al. (2020)</ref>. This was caused by the overfitting problem, and case-sensitive training could be thought as a special augmentation type. Since the OpenImagesV5 dataset is more challenging than synthetic dataset, augmentation could be reduced, allowing model to spend its capacity on learning features of the target dataset, and not augmentations. We have compared two training setups, the only difference is case mode (and, thus, number of output classes). The results are reported in the <ref type="table" target="#tab_3">Table 3</ref>. Note, these two experiments are performed without TPS module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Experiments with ViTSTR</head><p>Transformer models have become very popular during the past months for many deep learning problems, and scene text recognition is not an exception. We have tried to train ViTSTR Atienza <ref type="formula">(2021)</ref>   The intention of this experiment was not to train the best model, but to show that the model actually have higher capacity and could perform better on the validation datasets, if trained on the real data. The more difficult is the dataset (e.g. IC15 and SVT-P), the higher is the accuracy gain. It is clear from the table that even though large augmentation was applied to train the baseline model, not full capacity of the model was utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we have shown that training on natural text recognition datasets could be more efficient than on synthetically generated images. Our experiments have proven the fact that training with more challenging data and less augmentations is more efficient in terms of accuracy on the testing dataset. We have also introduced a text recognition model which achieves state-of-the-art results on some common text recognition benchmarks. We encourage other researchers to try to use the real data to train their models, because as we have shown in the Section 4, this can help to boost the recognition accuracy, especially, for the attention-based and transformer models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(2019); Cai et al. (2021); Qiao et al. (2020); Baek et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Proposed text recognition model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ICDAR2013 Karatzas et al. (2013 is inherited from ICDAR2003. Its train and test split have 848 and 1095 images, respectively. Ignoring non-alphanumeric words results in the dataset of size 1015. -IIIT5k Mishra et al. (2012) is collected from Google image search. It has 2k images for training and 3k for testing. -SVT Wang et al. (2011) has 257 images for training and 647 images for evaluation. Its name stands for Street View Text and consists of outdoor street images. ICDAR2015 Karatzas et al. (2015) was created for the ICDAR 2015 Robust Reading competition and contains 4468 images for training and 2077 images for evaluation. All evaluation subset is used for the evaluation, even though it consists of some examples with non-alphanumeric instances. Unlike the previous datasets, this has examples with irregular text. -SVT-P Phan et al. (2013) is also collected from Google Street View and has many examples with perspective distortions. Dataset size is 645.</figDesc><table><row><cell>-</cell></row><row><cell>Irregular datasets:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results and comparison with other models. Results in bold are the best in every dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">IC03 IC13 IC15 IIIT5K SVT SVT-P</cell></row><row><cell>CSTR Cai et al. (2021)</cell><cell cols="2">94.8 93.2 81.6 90.1</cell><cell>93.7 85.0</cell></row><row><cell>ASTER Shi et al. (2019)</cell><cell cols="2">94.8 93.2 81.6 90.1</cell><cell>93.7 85.0</cell></row><row><cell>SEED Qiao et al. (2020)</cell><cell>-</cell><cell>92.8 80.0 93.8</cell><cell>89.6 81.4</cell></row><row><cell>RCEED Cui et al. (2021)</cell><cell>-</cell><cell cols="2">94.7 82.2 94.9 91.8 83.6</cell></row><row><cell>SATRN Lee et al. (2020)</cell><cell cols="2">96.7 94.1 79.0 92.8</cell><cell>91.3 86.5</cell></row><row><cell cols="3">Baek et al. Baek et al. (2019) 94.4 92.3 71.8 87.9</cell><cell>87.9 79.2</cell></row><row><cell>Our model</cell><cell cols="2">97.1 96.8 80.2 93.5</cell><cell>94.7 89.9</cell></row><row><cell>4.6 Ablation Study</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Results Using Different Train Datasets During our research, we have ex-</cell></row><row><cell cols="4">perimented with different datasets used in training. You can see experiments'</cell></row><row><cell cols="4">results in the Table 2. All experiments were performed under the same condi-</cell></row><row><cell cols="4">tions: ResNeXt-101 backbone, YAMTS text recognition head, case-insensitive</cell></row><row><cell cols="4">training, case-insensitive evaluation. We did not use TPS module for this exper-</cell></row><row><cell>iment.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the recognition accuracy using different training datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">IC03 IC13 IC15 IIIT5K SVT SVT-P</cell></row><row><cell>MJSynth</cell><cell>92.7 88.9 64.2 84.2</cell><cell>86.2 78.1</cell></row><row><cell>MJSynth + SynthText</cell><cell>95.2 93.5 69.3 87.5</cell><cell>88.1 82.9</cell></row><row><cell cols="2">MJSynth + OpenImagesV5 95.6 95.2 75.3 92</cell><cell>91.8 87.1</cell></row><row><cell>OpenImages V5</cell><cell>94.1 94.2 73.2 91.1</cell><cell>89.0 82.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of the recognition accuracy using different case mode.</figDesc><table><row><cell>Case Mode</cell><cell cols="2">IC03 IC13 IC15 IIIT5K SVT SVT-P</cell></row><row><cell cols="2">Case-Sensitive 95.6 95.2 75.3 92.0</cell><cell>91.8 87.1</cell></row><row><cell cols="2">Case-Insensitive 97.0 96.5 78.2 93.5</cell><cell>93.5 91.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>using the OpenImagesV5 annotation. The results can be seen in the table 4. We have trained the models in the same framework, but, for an honest comparison, we have disabled filtering nonalphanumeric symbols during the test. Numbers near the name of the datasets mean portion of the dataset in the batch. Thus, 0.5 ST + 0.5 MJ means balanced batch containing examples from both MJSynth and SynthText. Baseline model is ViTSTR-Tiny, both models were trained using random data augmentation from the source framework.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison of the recognition accuracy using different training datasets for ViTSTR-Tiny+Aug.</figDesc><table><row><cell>Training Dataset</cell><cell cols="2">IC03 IC13 IC15 IIIT5K SVT SVT-P</cell></row><row><cell>0.5MJ + 0.5 ST (baseline)</cell><cell>93.7 89.8 66.6 85.1</cell><cell>85.9 78.5</cell></row><row><cell cols="2">0.4MJ + 0.2 OpenImagesV5 + 0.4 ST 95.6 94.2 77.0 91.7</cell><cell>90.7 86.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vision transformer for fast and efficient scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Atienza</surname></persName>
		</author>
		<idno>abs/2105.08582</idno>
		<ptr target="https://arxiv.org/abs/2105.08582" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What if we only use real datasets for scene text recognition? toward scene text recognition with fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">CSTR: A classification perspective on scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<idno>abs/2102.10884</idno>
		<ptr target="https://arxiv.org/abs/2102.10884" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Representation and correlation enhanced encoder-decoder framework for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning, NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2015/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2013.221</idno>
		<ptr target="https://doi.org/10.1109/ICDAR.2013.221" />
		<title level="m">ICDAR 2013 robust reading competition</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2015.7333942</idno>
		<ptr target="https://doi.org/10.1109/ICDAR.2015.7333942" />
		<title level="m">13th International Conference on Document Analysis and Recognition (IC-DAR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>ICDAR 2015 competition on Robust Reading</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nosov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sovrasov</surname></persName>
		</author>
		<title level="m">Open images v5 text annotation and yet another mask text spotter</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On recognizing texts of arbitrary shapes with 2d self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop on Text and Documents in the Deep Learning Era WTDDLE</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop on Text and Documents in the Deep Learning Era WTDDLE</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2003.1227749</idno>
		<idno>ISBN 0- 7695-1960-1</idno>
		<ptr target="https://doi.org/10.1109/ICDAR.2003.1227749" />
		<title level="m">ICDAR 2003 robust reading competitions</title>
		<imprint>
			<date type="published" when="2003-09" />
			<biblScope unit="volume">682</biblScope>
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Torchvision the machine-vision package of torch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rodriguez</surname></persName>
		</author>
		<idno type="DOI">10.1145/1873951.1874254</idno>
		<ptr target="https://doi.org/10.1145/1873951.1874254" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimedia</title>
		<meeting>the 18th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1485" to="1488" />
		</imprint>
	</monogr>
	<note>ISBN 9781605589336</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adversarial learning and self-teaching techniques for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biasetton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Seed: Semantics enhanced encoder-decoder framework for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1409.0575</idno>
		<ptr target="http://arxiv.org/abs/1409.0575" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An end-to-end trainable neural network for imagebased sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<idno>abs/1507.05717</idno>
		<ptr target="http://arxiv.org/abs/1507.05717" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2848939</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2848939" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<idno>abs/2105.05486</idno>
		<ptr target="https://arxiv.org/abs/2105.05486" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>CoRR abs/1611.05431</idno>
		<ptr target="http://arxiv.org/abs/1611.05431" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence domain adaptation network for robust text image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
