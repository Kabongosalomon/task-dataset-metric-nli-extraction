<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MVTN: Multi-View Transformation Network for 3D Shape Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Hamdi</surname></persName>
							<email>abdullah.hamdi@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
							<email>silvio.giancola@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MVTN: Multi-View Transformation Network for 3D Shape Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-view projection methods have demonstrated their ability to reach state-of-the-art performance on 3D shape recognition. Those methods learn different ways to aggregate information from multiple views. However, the camera view-points for those views tend to be heuristically set and fixed for all shapes. To circumvent the lack of dynamism of current multi-view methods, we propose to learn those viewpoints. In particular, we introduce the Multi-View Transformation Network (MVTN) that regresses optimal view-points for 3D shape recognition, building upon advances in differentiable rendering. As a result, MVTN can be trained end-to-end along with any multi-view network for 3D shape classification. We integrate MVTN in a novel adaptive multiview pipeline that can render either 3D meshes or point clouds. MVTN exhibits clear performance gains in the tasks of 3D shape classification and 3D shape retrieval without the need for extra training supervision. In these tasks, MVTN achieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the most recent and realistic ScanOb-jectNN dataset (up to 6% improvement). Interestingly, we also show that MVTN can provide network robustness against rotation and occlusion in the 3D domain. The code is available at https://github.com/ajhamdi/MVTN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given its success in the 2D realm, deep learning naturally expanded to the 3D vision domain. In 3D, deep networks achieve impressive results in classification, segmentation, and detection. 3D deep learning pipelines operate directly on 3D data, commonly represented as point clouds <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b73">74]</ref>, meshes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref>, or voxels <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. However, other methods choose to represent 3D information by rendering multiple 2D views of objects or scenes <ref type="bibr" target="#b68">[69]</ref>. Such multi-view methods are more similar to a human approach, where the human visual system is fed with streams of rendered images instead of more elaborate 3D representations.</p><p>Recent developments in multi-view methods show im- <ref type="figure">Figure 1</ref>. Multi-View Transformation Network (MVTN). We propose a differentiable module that predicts the best view-points for a task-specific multi-view network. MVTN is trained jointly with this network without any extra training supervision, while improving the performance on 3D classification and shape retrieval.</p><p>pressive performance, and in many instances, achieve stateof-the-art results in 3D shape classification and segmentation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b15">16]</ref>. Multi-view approaches bridge the gap between 2D and 3D learning by solving a 3D task using 2D convolutional architectures. These methods render several views for a given 3D shape and leverage the rendered images to solve the end task. As a result, they build upon the recent advances in 2D grid-based deep learning and leverage larger image datasets for pre-training (e.g. ImageNet <ref type="bibr" target="#b66">[67]</ref>) to compensate for the general scarcity of labeled 3D datasets. However, the manner of choosing the rendering view-points for such methods remains mostly unexplored. Current methods rely on heuristics like random sampling in scenes <ref type="bibr" target="#b44">[45]</ref> or predefined canonical view-points in oriented datasets <ref type="bibr" target="#b74">[75]</ref>.</p><p>There is no evidence suggesting that such heuristics are empirically the best choice. To address this shortcoming, we propose to learn better view-points by introducing a Multi-View Transformation Network (MVTN). As shown in <ref type="figure">Fig. 1</ref>, MVTN learns to regress view-points, renders those views with a differentiable renderer, and trains the downstream task-specific network in an end-to-end fashion, thus leading to the most suitable views for the task. MVTN is inspired by the Spatial Transformer Network (STN) <ref type="bibr" target="#b33">[34]</ref>, which was developed for the 2D image domain. Both MVTN and STN learn spatial transformations for the input without leveraging any extra supervision nor adjusting the learning process. The paradigm of perception by predicting the best environment parameters that generated the image is called Vision as Inverse Graphics (VIG) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b84">85]</ref>. One approach to VIG is to make the rendering process invertible or differentiable <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b50">51]</ref>. In this paper, MVTN takes advantage of differentiable rendering <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b65">66]</ref>. With such a renderer, models can be trained end-to-end for a specific target 3D vision task, with the view-points (i.e. camera poses) being inferred by MVTN in the same forward pass. To the best of our knowledge, we are the first to integrate a learnable approach to view-point prediction in multi-view methods by using a differentiable renderer and establishing an end-to-end pipeline that works for both mesh and 3D point cloud classification and retrieval. Contributions: (i) We propose a Multi-View Transformation Network (MVTN) that regresses better view-points for multi-view methods. Our MVTN leverages a differentiable renderer that enables end-to-end training for 3D shape recognition tasks. (ii) Combining MVTN with multi-view approaches leads to state-of-the-art results in 3D classification and shape retrieval on standard benchmarks ModelNet40 <ref type="bibr" target="#b78">[79]</ref>, ShapeNet Core55 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b67">68]</ref>, and ScanObjectNN <ref type="bibr" target="#b71">[72]</ref>.</p><p>(iii) Additional analysis shows that MVTN improves the robustness of multi-view approaches to rotation and occlusion, making MVTN more practical for realistic scenarios, where 3D models are not perfectly aligned or partially cropped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Learning on 3D Data. PointNet <ref type="bibr" target="#b61">[62]</ref> paved the way as the first deep learning algorithm to operate directly on 3D point clouds. PointNet computes point features independently and aggregates them using an order invariant function like max-pooling. Subsequent works focused on finding neighborhoods of points to define point convolutional operations <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b72">73]</ref>. Voxel-based deep networks allow for 3D CNNs yet suffer from cubic memory complexity <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. Several recent works combine point cloud representations with other 3D modalities like voxels <ref type="bibr" target="#b55">[56]</ref> or multi-view images <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b34">35]</ref>. In this paper, we leverage a point encoder to predict the optimal view-points, from which images are rendered and fed to a multi-view network. Multi-View 3D Shape Classification. The first work on using 2D images to recognize 3D objects was proposed by Bradski et al. <ref type="bibr" target="#b4">[5]</ref>. Twenty years later and after the success of deep learning in 2D vision tasks, MVCNN <ref type="bibr" target="#b68">[69]</ref> emerged as the first use of deep 2D CNNs for 3D object recognition. The original MVCNN uses max pooling to aggregate features from different views. Several follow-up works propose different strategies to assign weights to views to perform weighted average pooling of view-specific features <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref>. RotationNet <ref type="bibr" target="#b39">[40]</ref> classifies the views and the object jointly. Equivariant MV-Network <ref type="bibr" target="#b17">[18]</ref> uses a rotation equivariant convolution operation on multi-views by utilizing rotation group convolutions <ref type="bibr" target="#b14">[15]</ref>. The more recent work of ViewGCN <ref type="bibr" target="#b74">[75]</ref> utilizes dynamic graph convolution operations to adaptively pool features from different fixed views for the task of 3D shape classification. All these previous methods rely on fixed rendered datasets of 3D objects. The work of <ref type="bibr" target="#b11">[12]</ref> attempts to select views adaptively through reinforcement learning and RNNs, but this comes with limited success and an elaborate training process. In this paper, we propose a novel MVTN framework for predicting optimal view-points in a multi-view setup. This is done by jointly training MVTN with a multi-view task-specific network, without the need for any extra supervision nor adjustment to the learning process.</p><p>3D Shape Retrieval. Early methods in the literature compare the distribution of hand-crafted descriptors to retrieve similar 3D shapes. Those shape signatures could either represent geometric <ref type="bibr" target="#b59">[60]</ref> or visual <ref type="bibr" target="#b9">[10]</ref> cues. Traditional geometric methods would estimate distributions of certain characteristics (e.g. distances, angles, areas, or volumes) to measure the similarity between shapes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref>. Gao et al. <ref type="bibr" target="#b21">[22]</ref> use multiple camera projections, and Wu et al. <ref type="bibr" target="#b79">[80]</ref> use a voxel grid to extract analogous model-based signatures. Su et al. <ref type="bibr" target="#b68">[69]</ref> introduce a deep learning pipeline for multiview classification, with aggregated features achieving high retrieval performance. They use a low-rank Mahalanobis metric atop extracted multi-view features to improve retrieval performance. This seminal work on multi-view learning is extended for retrieval with volumetric-based descriptors <ref type="bibr" target="#b62">[63]</ref>, hierarchical view-group architectures <ref type="bibr" target="#b19">[20]</ref>, and triplet-center loss <ref type="bibr" target="#b31">[32]</ref>. Jiang et al. <ref type="bibr" target="#b36">[37]</ref> investigate better views for retrieval using many loops of circular cameras around the three principal axes. However, these approaches consider fixed camera view-points compared to MVTN's learnable ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision as Inverse Graphics (VIG).</head><p>A key issue in VIG is the non-differentiability of the classical graphics pipeline. Recent VIG approaches focus on making the graphics operations differentiable, allowing gradients to flow from the image to the rendering parameters directly <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b26">27]</ref>. NMR <ref type="bibr" target="#b40">[41]</ref> approximates non-differentiable rasterization by smoothing the edge rendering, where SoftRas <ref type="bibr" target="#b52">[53]</ref> assigns a probability for all mesh triangles to every pixel in the image. Synsin <ref type="bibr" target="#b75">[76]</ref> proposes an alpha-blending mechanism for differentiable point cloud rendering. The Pytorch3D <ref type="bibr" target="#b65">[66]</ref> renderer improves the speed and modularity of SoftRas and Synsin and allows for customized shaders and point cloud rendering. MVTN harnesses advances in differentiable rendering to train jointly with the multi-view network in an end-to-end fashion. Using both mesh and point cloud differentiable rendering enables MVTN to work on 3D CAD models and the more accessible 3D point cloud data. <ref type="figure">Figure 2</ref>. End-to-End Learning Pipeline for Multi-View Recognition. To learn adaptive scene parameters u that maximize the performance of a multi-view network C for every 3D object shape S, we use a differentiable renderer R. MVTN extracts coarse features from S by a point encoder and regresses the adaptive scene parameters for that object. In this example, the parameters u are the azimuth and elevation angles of cameras pointing towards the center of the object. The MVTN pipeline is optimized end-to-end for the task loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We illustrate our proposed multi-view pipeline using MVTN in <ref type="figure">Fig. 2</ref>. MVTN is a generic module that learns camera view-point transformations for specific 3D multi-view tasks, e.g. 3D shape classification. In this section, we review a generic framework for common multi-view pipelines, introduce MVTN details, and present an integration of MVTN for 3D shape classification and retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of Multi-View 3D Recognition</head><p>3D multi-view recognition defines M different images {x i } M i=1 rendered from multiple view-points of the same shape S. The views are fed into the same backbone network f that extracts discriminative features per view. These features are then aggregated among views to describe the entire shape and used for downstream tasks such as classification or retrieval. Specifically, a multi-view network C with parameters ? C operates on an input set of images X ? R M ?h?w?c to obtain a softmax probability vector for the shape S. Training Multi-View Networks. The simplest deep multiview classifier is MVCNN, where C = MLP (max i f (x i )) with f : R h?w?c ? R d being a 2D CNN backbone (e.g. ResNet <ref type="bibr" target="#b30">[31]</ref>) applied individually on each rendered image. A more recent method like ViewGCN would be described as C = MLP (cat GCN (f (x i ))), where cat GCN is an aggregation of views' features learned from a graph convolutional network. In general, learning a task-specific multi-view network on a labeled 3D dataset is formulated as:</p><formula xml:id="formula_0">arg min ? C N n L C(X n ) , y n = arg min ? C N n L C R(S n , u 0 ) , y n ,<label>(1)</label></formula><p>where L is a task-specific loss defined over N 3D shapes in the dataset, y n is the label for the n th 3D shape S n , and u 0 ? R ? is a set of ? fixed scene parameters for the entire dataset. These parameters represent properties that affect the rendered image, including camera view-point, light, object color, and background. R is the renderer that takes as input a shape S n and the parameters u 0 to produce M multi-view images X n per shape. In our experiments, we choose the scene parameters u to be the azimuth and elevation angles of the camera view-points pointing towards the object center, thus setting ? = 2M . Canonical Views. Previous multi-view methods rely on scene parameters u 0 that are pre-defined for the entire 3D dataset. In particular, the fixed camera view-points are usually selected based on the alignment of the 3D models in the dataset. The most common view configurations are circular that aligns view-points on a circle around the object <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b83">84]</ref> and spherical that aligns equally spaced view-points on a sphere surrounding the object <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b39">40]</ref>. Fixing those canonical views for all 3D objects can be misleading for some classes. For example, looking at a bed from the bottom could confuse a 3D classifier. In contrast, MVTN learns to regress per-shape view-points, as illustrated in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-View Transformation Network (MVTN)</head><p>Previous multi-view methods take the multi-view image X as the only representation for the 3D shape, where X is rendered using fixed scene parameters u 0 . In contrast, we consider a more general case, where u is variable yet within bounds ?u bound . Here, u bound is positive and it defines the permissible range for the scene parameters. We set u bound to 180 ? and 90 ? for each azimuth and elevation angle. Differentiable Renderer. A renderer R takes a 3D shape S (mesh or point cloud) and scene parameters u as inputs, and outputs the corresponding M rendered images</p><formula xml:id="formula_1">{x i } M i=1 . Since R is differentiable, gradients ? xi</formula><p>?u can propagate backward from each rendered image to the scene parameters, thus establishing a framework that suits end-to-end deep learning pipelines. When S is represented as a 3D mesh, R has two components: a rasterizer and a shader. First, the rasterizer transforms meshes from the world to view coordinates given Circular Spherical MVTN <ref type="figure">Figure 3</ref>. Multi-View Camera Configurations: The view setups commonly used in the multi-view literature are circular <ref type="bibr" target="#b68">[69]</ref> or spherical <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b39">40]</ref>. Our MVTN learns to predict specific viewpoints for each object shape at inference time. The shape's center is shown as a red dot, and the view-points as blue cameras with their mesh renderings shown at the bottom.</p><p>the camera view-point and assigns faces to pixels. Using these face assignments, the shader creates multiple values for each pixel then blends them. On the other hand, if S is represented by a 3D point cloud, R would use an alpha-blending mechanism instead <ref type="bibr" target="#b75">[76]</ref>. <ref type="figure">Fig. 3</ref> and <ref type="figure" target="#fig_0">Fig. 4</ref> illustrate examples of mesh and point cloud renderings used in MVTN. View-Points Conditioned on 3D Shape. We design u to be a function of the 3D shape by learning a Multi-View Transformation Network (MVTN), denoted as G ? R P ?3 ? R ? and parameterized by ? G , where P is the number of points sampled from shape S. Unlike Eq (1) that relies on constant rendering parameters, MVTN predicts u adaptively for each object shape S and is optimized along with the classifier C. The pipeline is trained end-to-end to minimize the following loss on a dataset of N objects:</p><p>arg min</p><formula xml:id="formula_2">? C ,? G N n L C R(S n , u n ) , y n , s. t. u n = u bound .tanh G(S n )<label>(2)</label></formula><p>Here, G encodes a 3D shape to predict its optimal viewpoints for the task-specific multi-view network C. Since the goal of G is only to predict view-points and not classify objects (as opposed to C), its architecture is designed to be simple and light-weight. As such, we use a simple point encoder (e.g. shared MLP as in PointNet <ref type="bibr" target="#b61">[62]</ref>) that processes P points from S and produces coarse shape features of dimension b. Then, a shallow MLP regresses the scene parameters u n from the global shape features. To force the predicted parameters u to be within a permissible range ?u bound , we use a hyperbolic tangent function scaled by u bound . MVTN for 3D Shape Classification. To train MVTN for 3D shape classification, we define a cross-entropy loss in Eq (2), yet other losses and regularizers can be used here as well. The multi-view network (C) and the MVTN (G) are trained jointly on the same loss. One merit of our multi-view pipeline is its ability to seamlessly handle 3D point clouds, which is absent in previous multi-view methods. When S is a 3D point cloud, we simply define R as a differentiable point cloud renderer. MVTN for 3D Shape Retrieval. The shape retrieval task is defined as follows: given a query shape S q , find the most similar shapes in a broader set of size N . For this task, we follow the retrieval setup of MVCNN <ref type="bibr" target="#b68">[69]</ref>. In particular, we consider the deep feature representation of the last layer before the classifier in C. We project those features into a more expressive space using LFDA reduction <ref type="bibr" target="#b69">[70]</ref> and consider the reduced feature as the signature to describe a shape. At test time, shape signatures are used to retrieve (in order) the most similar shapes in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate MVTN for the tasks of 3D shape classification and retrieval on ModelNet40 <ref type="bibr" target="#b78">[79]</ref>, ShapeNet Core55 <ref type="bibr" target="#b7">[8]</ref>, and the more realistic ScanObjectNN <ref type="bibr" target="#b71">[72]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ModelNet40. ModelNet40 <ref type="bibr" target="#b78">[79]</ref> is composed of 12,311 3D objects (9,843/2,468 in training/testing) labelled with 40 object classes. Since we render 3D models in the forward pass, we limit the number of triangles in the meshes due to hardware constraints. In particular, we simplify the meshes to 20k vertices using the official Blender API <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>. ShapeNet Core55. ShapeNet Core55 is a subset of ShapeNet <ref type="bibr" target="#b7">[8]</ref> comprising 51,162 3D mesh objects labelled with 55 object classes. The training, validation, and test sets consist of 35764, 5133, and 10265 shapes, respectively. It is designed for the shape retrieval challenge SHREK <ref type="bibr" target="#b67">[68]</ref>. ScanObjectNN. ScanObjectNN <ref type="bibr" target="#b71">[72]</ref> is a recently released point cloud dataset for 3D classification that is more realistic and challenging than ModelNet40, since it includes background and considers occlusions. The dataset is composed of 2902 point clouds divided into 15 object categories. We consider its three main variants: object only, object with background, and the hardest perturbed variant (PB_T50_RS variant). These variants are used in the 3D Scene Understanding Benchmark associated with the ScanObjectNN dataset. This dataset offers a more challenging setup than Model-Net40 and tests the generalization capability of 3D deep learning model in more realistic scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>Classification Accuracy. The standard evaluation metric in 3D classification is accuracy. We report overall accuracy (percentage of correctly classified test samples) and average per-class accuracy (mean of all true class accuracies). Retrieval mAP. Shape retrieval is evaluated by mean Average Precision (mAP) over test queries. For every query shape S q from the test set, AP is defined as</p><formula xml:id="formula_3">AP = 1 GTP N n 1(Sn) n ,</formula><p>where GT P is the number of ground truth positives, N is the size of the ordered training set, and 1(S n ) = 1 if the shape S n is from the same class label of query S q . We average the retrieval AP over the test set to measure retrieval mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baselines</head><p>Voxel Networks. We choose VoxNet <ref type="bibr" target="#b58">[59]</ref>, DLAN <ref type="bibr" target="#b20">[21]</ref>, and 3DShapeNets <ref type="bibr" target="#b78">[79]</ref> as baselines that use voxels. Point Cloud Networks. We select PointNet <ref type="bibr" target="#b61">[62]</ref>, Point-Net++ <ref type="bibr" target="#b64">[65]</ref>, DGCNN <ref type="bibr" target="#b73">[74]</ref>, PVNet <ref type="bibr" target="#b82">[83]</ref>, and KPConv <ref type="bibr" target="#b70">[71]</ref> as baselines that use point clouds. These methods leverage different convolution operators on point clouds by aggregating local and global point information.</p><p>Multi-view Networks. We compare against MVCNN <ref type="bibr" target="#b68">[69]</ref>, RotationNet <ref type="bibr" target="#b39">[40]</ref>, GVCNN <ref type="bibr" target="#b19">[20]</ref> and ViewGCN <ref type="bibr" target="#b74">[75]</ref> as representative multi-view methods. These methods are limited to meshes, pre-rendered from canonical view-points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">MVTN Details</head><p>Rendering. We choose the differentiable mesh and point cloud renderers R from Pytorch3D <ref type="bibr" target="#b65">[66]</ref> in our pipeline for their speed and compatibility with Pytorch libraries <ref type="bibr" target="#b60">[61]</ref>. We show examples of the rendered images for meshes ( <ref type="figure">Fig. 3</ref>) and point clouds ( <ref type="figure" target="#fig_0">Fig. 4</ref>). Each rendered image has a size of 224?224. For ModelNet40, we use the differentiable mesh renderer. We direct the light randomly and assign a random color for the object for augmentation purposes in training. In testing, we keep a fixed light pointing towards the object center and color the object white for stable performance. For ShapeNet Core55 and ScanObjectNN, we use the differentiable point cloud renderer using 2048 and 5000 points, respectively. Point cloud rendering offers a light alternative to mesh rendering when the mesh contains a large number of faces that hinders training the MVTN pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Accuracy Method</head><p>Data Type (Per-Class) (Overall) View-Point Prediction. As shown in Eq (2), the MVTN G network learns to predict the view-points directly (MVTNdirect). Alternatively, MVTN can learn relative offsets w.r.t. initial parameters u 0 . In this case, we concatenate the point features extracted in G with u 0 to predict the offsets to apply on u 0 . The learned view-points u n in Eq (2) are defined as:</p><formula xml:id="formula_4">u n = u 0 + u bound .tanh G(u 0 , S n )</formula><p>. We take u 0 to be the circular or spherical configurations commonly used in multiview classification pipelines <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b74">75]</ref>. We refer to these learnable variants as MVTN-circular and MVTN-spherical,  <ref type="table">Table 3</ref>. 3D Shape Retrieval. We benchmark the shape retrieval mAP of MVTN on ModelNet40 <ref type="bibr" target="#b78">[79]</ref> and ShapeNet Core55 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b67">68]</ref>. MVTN achieves the best retrieval performance among recent stateof-the-art methods on both datasets with only 12 views.</p><p>accordingly. For MVTN-circular, the initial elevations for the views are 30 ? , and the azimuth angles are equally distributed over 360 ? following <ref type="bibr" target="#b68">[69]</ref>. For MVTN-spherical, we follow the method from <ref type="bibr" target="#b16">[17]</ref> that places equally-spaced viewpoints on a sphere for an arbitrary number of views, which is similar to the "dodecahedral" configuration in ViewGCN.</p><p>Architecture. We select MVCNN <ref type="bibr" target="#b68">[69]</ref>, RotationNet <ref type="bibr" target="#b39">[40]</ref>, and the more recent ViewGCN <ref type="bibr" target="#b74">[75]</ref> as our multi-view networks of choice in the MVTN pipeline. In our experiments, we select PointNet <ref type="bibr" target="#b61">[62]</ref> as the 3D point encoder network G and experiment with DGCNN in Section 6.1. We sample P = 2048 points from each mesh as input to the point encoder and use a 5-layer MLP for the regression network, which takes as input the point features extracted by the point encoder of size b = 40. All MVTN variants and the baseline multi-view networks use ResNet-18 <ref type="bibr" target="#b30">[31]</ref> pre-trained on Im-ageNet <ref type="bibr" target="#b66">[67]</ref> for the multi-view backbone in C, with output features of size d = 1024. The main classification and retrieval results are based on MVTN-spherical with ViewGCN <ref type="bibr" target="#b74">[75]</ref> as the multi-view network C, unless otherwise specified as in Section 5.3 and Section 6.1. Training Setup. To avoid gradient instability introduced by the renderer, we use gradient clipping in the MVTN network G. We clip the gradient updates such that the 2 norm of the gradients does not exceed 30. We use a learning rate of 0.001 but refrain from fine-tuning the hyper-parameters introduced in MVCNN <ref type="bibr" target="#b68">[69]</ref> and View-GCN <ref type="bibr" target="#b74">[75]</ref>. More details about the training procedure are in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>The main results of MVTN are summarized in Tables 1, 2, 3 and 4. We achieve state-of-the-art performance in 3D classification on ScanObjectNN by a large margin (up to 6%) and achieve a competitive test accuracy of 93.8% on ModelNet40. On shape retrieval, we achieve state-of-theart performance on both ShapeNet Core55 (82.9 mAP) and ModelNet40 (92.9 mAP). Following the common practice, we report the best results out of four runs in benchmark tables, but detailed results are in Appendix. <ref type="table">Table 1</ref> compares the performance of MVTN against other methods on ModelNet40 <ref type="bibr" target="#b78">[79]</ref>. Our MVTN achieves a competitive test accuracy of 93.8% compared to all previous methods. ViewGCN <ref type="bibr" target="#b74">[75]</ref> achieves higher classification performance by relying on higher quality images from a more advanced yet non-differentiable OpenGL <ref type="bibr" target="#b76">[77]</ref> renderer. For a fair comparison, we report with an * the performance of ViewGCN using images generated by the renderer used in MVTN. Using the same rendering process, regressing views with MVTN improves the classification performance of the baseline ViewGCN at 12 and 20 views. We believe future advances in differentiable rendering would bridge the gap between our rendered images and the original high-quality pre-rendered ones.  <ref type="table">Table 3</ref> reports the retrieval mAP of MVTN compared with recent methods on ModelNet40 <ref type="bibr" target="#b78">[79]</ref> and ShapeNet Core55 <ref type="bibr" target="#b7">[8]</ref>. The results of the latter methods are taken from <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b82">83]</ref>. MVTN achieves state-of-the-art retrieval performance (92.9% mAP) on ModelNet40. It also improves the state-of-the-art by a large margin in ShapeNet, while only using 12 views. It is important to note that the baselines in <ref type="table">Table 3</ref> include strong and recent methods trained specifically for retrieval, such as MLVCNN <ref type="bibr" target="#b36">[37]</ref>. <ref type="figure" target="#fig_1">Fig. 5</ref> shows qualitative examples of objects retrieved using MVTN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">3D Shape Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">3D Shape Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Rotation Robustness</head><p>A common practice in 3D shape classification literature is to test the robustness of trained models to perturbations at test time. Following the same setup as <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b27">28]</ref>, we perturb the shapes with random rotations around the Y-axis (gravity-axis) contained within ?90 ? and ?180 ? . We repeat the inference ten times for each setup and report the average performance in <ref type="table">Table 4</ref>. The MVTN-circular variant (with MVCNN) reaches state-of-the-art performance in rotation robustness (91.2% test accuracy) compared to more advanced methods trained in the same setup. The baseline RSCNN <ref type="bibr" target="#b54">[55]</ref> is a strong baseline designed to be invariant to translation and rotation. In contrast, MVTN is learned in a simple setup with MVCNN without targeting rotation invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Occlusion Robustness</head><p>To test the usefulness of MVTN in a realistic scenario, we investigate the common problem of occlusion in 3D computer vision, especially in 3D point cloud scans. Various factors lead to occlusion, including the view angle to the object, the sensor's sampling density (e.g. LiDAR), or the presence of noise in the sensor. In such realistic scenarios, deep learning models typically fail. To quantify this occlusion effect due to the viewing angle of the 3D sensor in our setup of 3D classification, we simulate realistic occlusion by cropping the object from canonical directions. We train PointNet <ref type="bibr" target="#b61">[62]</ref>, DGCNN <ref type="bibr" target="#b73">[74]</ref>, and MVTN on the  ModelNet40 point cloud dataset. Then, at test time, we crop a portion of the object (from 0% occlusion ratio to 100%) along the ?X, ?Y, and ?Z directions. <ref type="figure">Fig. 6</ref> shows examples of this occlusion effect with different occlusion ratios. In all robustness experiments, the studied transformations (rotation or occlusion) happen only in test time. All the methods compared, including MVTN, are trained naturally without any augmentation by those transformations. We report the average test accuracy of the six cropping directions for the baselines and MVTN in  <ref type="table">Table 6</ref>. Ablation Study. We analyze the effect of ablating different MVTN components on test accuracy in ModelNet40. Namely, we observe that using deeper backbone CNNs or a more complex point encoder do not increase the test accuracy. average test accuracies with confidence intervals are shown in <ref type="figure">Fig. 7</ref>. The plots show how learned MVTN-spherical achieves consistently superior performance across a different number of views. Choice of Backbone and Point Encoders. In all of our main MVTN experiments, we use ResNet-18 as our backbone and PointNet as the point feature extractor. However, different choices could be made for both. We explore using DGCNN <ref type="bibr" target="#b73">[74]</ref> as an alternative point encoder and ResNet-34 as an alternative 2D backbone in ViewGCN. We report all MVTN ablation results in <ref type="table">Table 6</ref>. We observe diminishing returns for making the CNN backbone and the shape feature extractor more complex in the MVTN setup, which justifies using the simpler combination in our main experiments Choice of Multi-View Network.</p><p>MVTN integrates smoothly with different multi-view networks and always leads to performance boost. In  <ref type="table">Table 8</ref>. Time and Memory Requirements. We assess the contribution of the MVTN module to the time and memory requirements in the multi-view pipeline. We note that the MVTN's time and memory requirements are negligible.</p><p>with different multi-view networks.</p><p>Other Factors Affecting MVTN. We study the effect of the light direction in the renderer, the camera's distance to the object, and the object's color. We also study the transferability of the learned views from one multi-view network to another, and the performance of MVTN variants. More details are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Time and Memory Requirements</head><p>We compare the time and memory requirements of different parts of our 3D recognition pipeline. We record the number of floating-point operations (GFLOPs) and the time of a forward pass for a single input sample. In <ref type="table">Table 8</ref>, MVTN contributes negligibly to the time and memory requirements of the multi-view networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Future Work</head><p>Current multi-view methods rely on fixed views aligned with the dataset. We propose MVTN that learns to regress view-points for any multi-view network in a fully differentiable pipeline. MVTN harnesses recent developments in differentiable rendering and does not require any extra training supervision. Empirical results highlight the benefits of MVTN in 3D classification and 3D shape retrieval. Some possible future works for MVTN include extending it to other 3D vision tasks such as shape and scene segmentation. Furthermore, MVTN can include more intricate scene parameters different from the camera view-points, such as light and textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Datasets</head><p>ModelNet40. We show in <ref type="figure" target="#fig_3">Fig. 8</ref> examples of the mesh renderings of ModelNet40 used in training our MVTN. Note that the color of the object and the light direction are randomized in training for augmentation but are fixed in testing for stable performance. ShapeNet Core55. In <ref type="figure">Fig. 9, we</ref> show examples of the point cloud renderings of ShapeNet Core55 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b67">68]</ref> used in training MVTN. Note how point cloud renderings offer more information about content hidden from the camera view-point, which can be useful for recognition. White color is used in training and testing for all point cloud renderings. For visualization purposes, colors are inverted in the main paper examples <ref type="figure" target="#fig_0">(Fig. 4 in the main paper)</ref>. ScanObjectNN. ScanObjectNN <ref type="bibr" target="#b71">[72]</ref> has three main variants: object only, object with background, and the PB_T50_RS variant (hardest perturbed variant). <ref type="figure">Fig. 10</ref> show examples of multi-view renderings of different samples of the dataset from its three variants. Note that adding the background points to the rendering gives some clues to our MVTN about the object, which explains why adding background improves the performance of MVTN in <ref type="table">Table  10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. MVTN Details</head><p>MVTN Rendering. Point cloud rendering offers a light alternative to mesh rendering in ShapeNet because its meshes contain large numbers of faces that hinders training the MVTN pipeline. Simplifying theses "high-poly" meshes (similar to ModelNet40) results in corrupted shapes that lose their main visual clues. Therefore, we use point cloud rendering for ShapeNet, allowing to process all shapes with equal memory requirements. Another benefit of point cloud rendering is making it possible to train MVTN with a large batch size on the same GPU (bath size of 30 on V100 GPU). MVTN Architecture. We incorporate our MVTN into MVCNN <ref type="bibr" target="#b68">[69]</ref> and ViewGCN <ref type="bibr" target="#b74">[75]</ref>. In our experiments, we select PointNet <ref type="bibr" target="#b61">[62]</ref> as the default point encoder of MVTN. All MVTNs and their baseline multi-view networks use ResNet18 <ref type="bibr" target="#b30">[31]</ref> as backbone in our main experiments with output feature size d = 1024. The azimuth angle maximum range (u bound ) is 180 ? M for MVTN-circular and MVTNspherical, while it is 180 ? for MVTN-direct. On the other hand, the elevation angle maximum range (u bound ) is 90 ? . We use a 4-layer MLP for MVTN's regression network G.  <ref type="figure">Fig. 11</ref>.MVTN concatenates all of its inputs, and the MLP outputs the offsets to the initial 2 ? M azimuth and elevation angles. The size of the MVTN network (with b = 40) is 14M 2 + 211M + 3320 parameters, where M is the number of views. It is a shallow network of only around 9K parameters when M = 12. View-Points. In <ref type="figure">Fig. 12</ref>, we show the basic views configurations for M views previously used in the literature: circular, spherical, and random. MVTN's learned views are shown later in C.1 Since ViewGCN uses view sampling as a core operation, it requires the number of views to be at least 12, and hence, our MVTN with ViewGCN follows accordingly. Training MVTN. We use AdamW <ref type="bibr" target="#b57">[58]</ref> for our MVTN networks with a learning rate of 0.001. For other training details (e.g. training epochs and optimization), we follow the previous works <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b68">69]</ref> for a fair comparison. The training of MVTN with MVCNN is done in 100 epochs and a batch size of 20, while the MVTN with ViewGCN is performed in two stages as proposed in the official code of the paper <ref type="bibr" target="#b74">[75]</ref>. The first stage is 50 epochs of training the backbone CNN on the single view images, while the second stage is 35 epochs on the multi-view network on the M views of the 3D object. We use learning rates of 0.0003 for MVCNN and 0.001 for ViewGCN, and a ResNet-18 <ref type="bibr" target="#b30">[31]</ref> as the backbone CNN for both baselines and our MVTN-based networks. A weight decay of 0.01 is applied for both the multi-view network and in the MVTN networks. Due to gradient instability from the renderer, we introduce gradient clipping in the MVTN to limit the 2 norm of gradient updates to 30 forG. The code is available at https://github.com/ajhamdi/MVTN.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Classification and Retrieval Benchmarks</head><p>We provide in <ref type="table" target="#tab_7">Tables 9,10</ref>, and 11 comprehensive benchmarks of 3D classifications and 3D shape retrieval methods on ModelNet40 <ref type="bibr" target="#b78">[79]</ref>, ScanObjectNN <ref type="bibr" target="#b71">[72]</ref>, and ShapeNet Core55 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b67">68]</ref>. These tables include methods that use points as representations as well as other modalities like multi-view and volumetric representations. Our reported results of four runs are presented in each table as "max (avg ? std)". Note in <ref type="table" target="#tab_7">Table 9</ref> how our MVTN improves the previous state-ofthe-art in classification (ViewGCN <ref type="bibr" target="#b74">[75]</ref>) when tested on the same setup. Our implementations (highlighted using * ) slightly differ from the reported results in their original paper. This can be attributed to the specific differentiable renderer of Pytorch3D <ref type="bibr" target="#b65">[66]</ref> that we are using, which might not have the same quality of the non-differentiable OpenGL renderings <ref type="bibr" target="#b76">[77]</ref> used in their setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Rotation Robustness</head><p>A common practice in the literature in 3D shape classification is to test the robustness of models trained on the aligned dataset by injecting perturbations during test time <ref type="bibr" target="#b54">[55]</ref>. We follow the same setup as <ref type="bibr" target="#b54">[55]</ref> by introducing random rotations during test time around the Y-axis (gravity-axis). We also investigate the effect of varying rotation perturbations on the accuracy of circular MVCNN when M = 6 and M = 12. We note from <ref type="figure" target="#fig_5">Fig. 13</ref> that using less views leads to higher sensitivity to rotations in general. Furthermore, we note that our MVTN helps in stabilizing the performance on increasing thresholds of rotation perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Occlusion Robustness</head><p>To quantify the occlusion effect due to the viewing angle of the 3D sensor in our setup of 3D classification, we simulate realistic occlusion by cropping the object from canonical directions. We train PointNet <ref type="bibr" target="#b61">[62]</ref>, DGCNN <ref type="bibr" target="#b73">[74]</ref>, and MVTN on the ModelNet40 point cloud dataset. Then, at test time, we crop a portion of the object (from 0% occlusion ratio to 75%) along the ?X, ?Y, and ?Z directions independently. <ref type="figure" target="#fig_1">Fig. 15</ref> shows examples of this occlusion effect with different occlusion ratios. We report the average test accuracy (on all the test set) of the six cropping directions for the baselines and MVTN in <ref type="figure" target="#fig_0">Fig. 14.</ref> Note how MVTN achieves high test accuracy even when large portions of the object are cropped. Interestingly, MVTN outperforms PointNet <ref type="bibr" target="#b61">[62]</ref> by 13% in test accuracy when half of the object is occluded. This result is significant, given that PointNet is well-known for its robustness <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b28">29]</ref>.   <ref type="table">Table 11</ref>. 3D Shape Retrieval. We benchmark the shape retrieval capability of MVTN on ModelNet40 <ref type="bibr" target="#b78">[79]</ref> and ShapeNet Core55 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b67">68]</ref>. MVTN achieves the best retrieval performance among recent state-of-the-art methods on both datasets with only 12 views. In brackets, we report the average and standard deviation of four runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis and Insights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Ablation Study</head><p>This section introduces a comprehensive ablation study on the different components of MVTN, and their effect on test accuracy on the standard ModelNet40 <ref type="bibr" target="#b78">[79]</ref>. MVTN Variants. We study the effect of the number of views M on the performance of different MVTN variants (direct, circular, spherical). The experiments are repeated four times, and the average test accuracies with confidence intervals are shown in <ref type="figure">Fig. 16</ref>. The plots show how learned MVTN-spherical achieves consistently superior performance across a different number of views. Also, note that MVTNdirect suffers from over-fitting when the number of views is larger than four (i.e. it gets perfect training accuracy but deteriorates in test accuracy). This can be explained by observing that the predicted view-points tend to be similar to each other for MVTN-direct when the number of views is large. The similarity in views leads the multi-view network to memorize the training but to suffer in testing. Backbone. In the main manuscript <ref type="table">(Table 6</ref>), we study MVTN with ViewGCN as the multi-view network. Here, we study the backbone effect on MVTN with MVCNN as the multi-view network and report all results in <ref type="table">Table 14</ref>. The study includes the backbone choice, and the point encoder choice. Note that including more sophisticated backbones does not improve the accuracy Late Fusion. In the MVTN pipeline, we use a point encoder and a multi-view network. One can argue that an easy way to combine them would be to fuse them later in the architecture. For example, PointNet <ref type="bibr" target="#b61">[62]</ref> and MVCNN <ref type="bibr" target="#b68">[69]</ref> can be max pooled together at the last layers and trained jointly. We train such a setup and compare it to MVTN. We observe that MVTN achieves 91.8% compared to 88.4% by late fusion. More results are reported in <ref type="table">Table 14</ref> Light Direction Effect. We study the effect of light's direction on the performance of multi-view networks. We note that picking a random light in training helps the network generalize to the test set. Please see <ref type="figure">Fig. 17</ref> for the results on circular MVTN with MVCNN when comparing this strategy to fixed light from the top or from camera (relative). Note that we use relative light in test time to stabilize the performance. Effect of Object Color. Our main experiments used random colors for the objects during training and fixed them to white in testing. We tried different coloring approaches, like using a fixed color during training and test. The results are illustrated in <ref type="table" target="#tab_0">Table 12</ref>. Image size and number of points. We study the effect of rendered image size and the number of points sampled in a 4-view MVTN trained on ModelNet40 and report the overall accuracies (averaged over four runs) as follows. For image sizes 160?160, 224?224, and 280?280, the results Learning Distance to the Object. One possible ablation to the MVTN is to learn the distance to the object. This feature should allow the cameras to get closer to details that might be important to the classifier to understand the object properly. However, we observe that MVTN generally performs worse or does not improve with this setup, and hence, we refrain from learning it. In all of our main experiments, we fixed the distance to 2.2 units, which is a good middle ground providing best accuracy. Please see <ref type="figure" target="#fig_3">Fig. 18</ref> for the effect of picking a fixed distance in training spherical ViewGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Time and Memory of MVTN</head><p>We compare the time and memory requirements of different parts of our pipeline to assess the MVTN module's contribution. We record FLOPs and MACs to count each module's operations and record the time of a forward pass for a single input sample and the number of parameters for each module. We find in <ref type="table">Table 13</ref> that MVTN contributes negligibly to the time and memory requirements of the multi-view networks and the 3D point encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Transferability of MVTN View-Points</head><p>We hypothesize that the views learned by MVTN are transferable across multi-view classifiers. Looking at results in <ref type="figure" target="#fig_9">Fig. 20, 21</ref>, we believe MVTN picks the best views based on the actual shape and is less influenced by the multi-view network. This means that MVTN learns views that are more representative of the object, making it easier for any multiview network to recognize it. As such, we ask the following: can we transfer the views MVTN learns under one setting to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. MVTN Predicted Views</head><p>We visualize the distribution of predicted views by MVTN for specific classes in <ref type="figure">Fig. 19</ref>. This is done to ensure that MVTN is learning per-instance views and regressing the same views for the entire class (collapse scenario). We can see that the MVTN distribution of the views varies from one class to another, and the views themselves on the same class have some variance from one instance to another. We also show specific examples for predicted views in <ref type="figure" target="#fig_9">Fig. 20, 21</ref>.</p><p>Here, we show both the predicted camera view-points and the renderings from these cameras. Note how MVTN shifts every view to better show the discriminative details about the 3D object. To test that these views are per-instance, we average all the views predicted by our 4-view MVTN for every class and test the trained MVCNN on these fixed per-class views. In this setup, MVTN achieves 90.6% on ModelNet40, as compared to 91.0% for the per-instance views and 89% for the fixed views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Shape Retrieval Examples</head><p>We show qualitative examples of our retrieval results using the MVTN-spherical with ViewGCN in <ref type="figure" target="#fig_12">Fig. 22</ref>. Note that the top ten retrieved objects for all these queries are positive (from the same classes of the queries).  <ref type="table">Table 14</ref>. Ablation Study. We study the effect of ablating different components of MVTN on the test accuracy on ModelNet40. Namely, we observe that using more complex backbone CNNs (like ResNet50 <ref type="bibr" target="#b30">[31]</ref>) or a more complex features extractor (like DGCNN <ref type="bibr" target="#b73">[74]</ref>) does not increase the performance significantly compared to ResNet18 and PointNet <ref type="bibr" target="#b61">[62]</ref> respectively. Furthermore, combining the shape features extractor with the MVCNN <ref type="bibr" target="#b68">[69]</ref> in late fusion does not work as well as MVTN with the same architectures. All the reported results are using MVCNN <ref type="bibr" target="#b68">[69]</ref> as multi-view network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Views</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of Azimuth Angles</head><p>Airplane Bottle Desk <ref type="figure">Figure 19</ref>. Visualizing MVTN learned Views. We visualize the distribution of azimuth and elevation angles predicted by the MVTN for three different classes. Note that MVTN learns interclass variations (between different classes) and intra-class variations (on the same class).  The view setups commonly followed in the multi-view literature are circular <ref type="bibr" target="#b68">[69]</ref> or spherical <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b39">40]</ref>. The red dot is the center of the object. MVTN-circular/MVTN-spherical are trained to predict the views as offsets to these common configurations. Note that MVTN adjust the original views to make the 3D object better represented by the multi-view images.  The view setups commonly followed in the multi-view literature are circular <ref type="bibr" target="#b68">[69]</ref> or spherical <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b39">40]</ref>. The red dot is the center of the object. MVTN-circular/MVTN-spherical are trained to predict the views as offsets to these common configurations. Note that MVTN adjust the original views to make the 3D object better represented by the multi-view images. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Multi-View Point Cloud Renderings. We show some examples of point cloud renderings used in our pipeline. Note how point cloud renderings offer more information about content hidden from the camera view-point (e.g. car wheels from the occluded side), which can be useful for recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative Examples for Object Retrieval: (left): we show some query objects from the test set. (right): we show top five retrieved objects by our MVTN from the training set. Images of negative retrieved objects are framed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For MVTN-spherical/MVTN-spherical, the regression network takes as input M azimuth angles, M elevation angles, and the point features of shape S of size b = 40. The widths of the MVTN networks are illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Training Data with Randomized Color and Lighting. We show examples of mesh renderings of ModelNet40 used in training our MVTN. The color of the object and the light's direction are randomized during training for augmentation purposes and fixed in testing for stable performance. For this figure, eight circular views are shown for each 3D shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .Figure 10 .Figure 11 .Figure 12 .</head><label>9101112</label><figDesc>ShapeNet Core55. We show some examples of point cloud renderings of ShapeNet Core55 [8] used in training MVTN. Note how point cloud renderings offer more information about content hidden from the camera view-point (e.g. car wheels from the occluded side), which can be useful for recognition. For this figure, 12 spherical views are shown for each 3D shape. ScanObjectNN Variants. We show examples of point cloud renderings of different variants of the ScanObjectNN [72] point cloud dataset used to train MVTN. The variants are: object only, object with background, and the hardest perturbed variant (with rotation and translation). For this figure, six circular views are shown for each 3D shape. MVTN Network Architecture. We show a schematic and a code snippet for MVTN-spherical/MVTN-circular regression architectures used, where b is the size of the point features extracted by the point encoder of MVTN and M is the number of views learned. In most of our experiments, b = 40, while the output is the azimuth and elevation angles for all the M views used. The network is drawn using [48] Views Configurations. We show some possible views configurations that can be used with a varying number of views. (a): circular, (b): spherical, (c): random</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 13 .</head><label>13</label><figDesc>Robustness on a Varying Y-Rotation. We study the effect of varying the maximum rotation perturbation on the classification accuracies on ModelNet40. We compare the performance of circular MVCNN<ref type="bibr" target="#b68">[69]</ref> to our circular-MVTN when it equips MVCNN when the number of views is 6 and 12. Note how MVTN stabilizes the drop in performance for larger Y-rotation perturbations, and the improvement is more significant for the smaller number of views M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 15 .</head><label>15</label><figDesc>Occlusion of 3D Objects: We simulate realistic occlusion scenarios in 3D point clouds by cropping a percentage of the object along canonical directions. Here, we show an object occluded with different ratios and from different directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 20 .</head><label>20</label><figDesc>Qualitative Examples for MVTN predicted views (I):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 21 .</head><label>21</label><figDesc>Qualitative Examples for MVTN predicted views (II):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 22 .</head><label>22</label><figDesc>Qualitative Examples for Object Retrieval: (left): we show some query objects from the test set. (right): we show top ten retrieved objects by our MVTN from the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>3D Shape Classification on ModelNet40. We compare MVTN against other methods in 3D classification on ModelNet40<ref type="bibr" target="#b78">[79]</ref>. * indicates results from our rendering setup (differentiable pipeline), while other multi-view results are reported from prerendered views. Bold denotes the best result in its setup. 3D Point Cloud Classification on ScanObjectNN. We compare the performance of MVTN in 3D point cloud classification on three different variants of ScanObjectNN<ref type="bibr" target="#b71">[72]</ref>. The variants include object with background, object only, and the hardest variant.</figDesc><table><row><cell cols="2">VoxNet [59] Voxels</cell><cell>83.0</cell><cell>85.9</cell></row><row><cell cols="2">PointNet [62] Points</cell><cell>86.2</cell><cell>89.2</cell></row><row><cell cols="2">PointNet++ [65] Points</cell><cell>-</cell><cell>91.9</cell></row><row><cell cols="2">PointCNN [52] Points</cell><cell>88.1</cell><cell>91.8</cell></row><row><cell cols="2">DGCNN [74] Points</cell><cell>90.2</cell><cell>92.2</cell></row><row><cell cols="2">SGAS [50] Points</cell><cell>-</cell><cell>93.2</cell></row><row><cell cols="2">KPConv[71] Points</cell><cell>-</cell><cell>92.9</cell></row><row><cell cols="2">PTransformer[86] Points</cell><cell>90.6</cell><cell>93.7</cell></row><row><cell cols="2">MVCNN [69] 12 Views</cell><cell>90.1</cell><cell>90.1</cell></row><row><cell cols="2">GVCNN [20] 12 Views</cell><cell>90.7</cell><cell>93.1</cell></row><row><cell cols="2">ViewGCN [75] 20 Views</cell><cell>96.5</cell><cell>97.6</cell></row><row><cell cols="2">ViewGCN [75]  *  12 views</cell><cell>90.7</cell><cell>93.0</cell></row><row><cell cols="2">ViewGCN [75]  *  20 views</cell><cell>91.3</cell><cell>93.3</cell></row><row><cell cols="2">MVTN (ours)  *  12 Views</cell><cell>92.0</cell><cell>93.8</cell></row><row><cell cols="2">MVTN (ours)  *  20 Views</cell><cell>92.2</cell><cell>93.5</cell></row><row><cell></cell><cell cols="3">Classification Overall Accuracy</cell></row><row><cell>Method</cell><cell cols="3">OBJ_BG OBJ_ONLY Hardest</cell></row><row><cell>3DMFV [3]</cell><cell>68.2</cell><cell>73.8</cell><cell>63.0</cell></row><row><cell>PointNet [62]</cell><cell>73.3</cell><cell>79.2</cell><cell>68.0</cell></row><row><cell>SpiderCNN [81]</cell><cell>77.1</cell><cell>79.5</cell><cell>73.7</cell></row><row><cell>PointNet ++ [65]</cell><cell>82.3</cell><cell>84.3</cell><cell>77.9</cell></row><row><cell>PointCNN [52]</cell><cell>86.1</cell><cell>85.5</cell><cell>78.5</cell></row><row><cell>DGCNN [74]</cell><cell>82.8</cell><cell>86.2</cell><cell>78.1</cell></row><row><cell>SimpleView [24]</cell><cell>-</cell><cell>-</cell><cell>79.5</cell></row><row><cell>BGA-DGCNN [72]</cell><cell>-</cell><cell>-</cell><cell>79.7</cell></row><row><cell>BGA-PN++ [72]</cell><cell>-</cell><cell>-</cell><cell>80.2</cell></row><row><cell>MVTN (ours)</cell><cell>92.6</cell><cell>92.3</cell><cell>82.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 reportsTable 4 .</head><label>24</label><figDesc>the classification accuracy of a 12 view MVTN on the realistic ScanObjectNN benchmark<ref type="bibr" target="#b71">[72]</ref>. MVTN improves performance on different variants of the dataset. The most difficult variant of ScanObjectNN (PB_T50_RS) includes challenging scenarios of objects undergoing translation and rotation. Our MVTN achieves stateof-the-art results (+2.6%) on this variant, highlighting the merits of MVTN for realistic 3D point cloud scans. Also, note how adding background points (in OBJ_BG) does not hurt MVTN, contrary to most other classifiers. . Rotation Robustness on ModelNet40. At test time, we randomly rotate objects in ModelNet40 around the Y-axis (gravity) with different ranges and report the overall accuracy. MVTN displays strong robustness to such Y-rotations.</figDesc><table><row><cell></cell><cell cols="3">Rotation Perturbations Range</cell></row><row><cell>Method</cell><cell>0 ?</cell><cell>?90 ?</cell><cell>?180 ?</cell></row><row><cell>PointNet [62]</cell><cell>88.7</cell><cell>42.5</cell><cell>38.6</cell></row><row><cell>PointNet ++ [65]</cell><cell>88.2</cell><cell>47.9</cell><cell>39.7</cell></row><row><cell>RSCNN [55]</cell><cell>90.3</cell><cell>90.3</cell><cell>90.3</cell></row><row><cell>MVTN (ours)</cell><cell>91.7</cell><cell>90.8</cell><cell>91.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Occlusion Robustness of 3D Methods. We report the test accuracy on point cloud ModelNet40 for different occlusion ratios of the data to measure occlusion robustness of different 3D methods. MVTN achieves 13% better accuracy than PointNet (a robust network) when half of the object is occluded.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Note how MVTN achieves high test accuracy even when large portions of the object are cropped. Interestingly, MVTN outperforms Point-Net<ref type="bibr" target="#b61">[62]</ref> by 13% in test accuracy when half of the object is occluded. This result is significant, given that PointNet is well-known for its robustness<ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b28">29]</ref>. Number of Views. We study the effect of the number of views M on the performance of MVCNN when using fixed views (circular/spherical), learned views (MVTN), and random views. The experiments are repeated four times, and the</figDesc><table><row><cell>6. Analysis and Insights</cell></row><row><cell>6.1. Ablation Study</cell></row><row><cell>This section performs a comprehensive ablation study on</cell></row><row><cell>the different components of MVTN and their effect on the</cell></row><row><cell>overall test accuracy on ModelNet40 [79].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .Table 7 .</head><label>77</label><figDesc>we show the overall accuracies (averaged over four runs) on ModelNet40 of 12 views when fixed views are used versus when MVTN is used Integrating MVTN with Multi-View Networks. We show overall classification accuracies on ModelNet40 with 12 views on different multi-view networks when fixed views are used versus when MVTN is used.</figDesc><table><row><cell>View Selection</cell><cell></cell><cell cols="2">Multi-View Networks</cell></row><row><cell></cell><cell cols="3">MVCNN[69] RotNet[40] ViewGCN[75]</cell></row><row><cell>fixed views</cell><cell>90.4</cell><cell>91.6</cell><cell>93.0</cell></row><row><cell>with MVTN</cell><cell>92.6</cell><cell>93.2</cell><cell>93.8</cell></row><row><cell>Network</cell><cell cols="3">GFLOPs Time (ms) Parameters # (M)</cell></row><row><cell cols="2">MVCNN [69] 43.72</cell><cell>39.89</cell><cell>11.20</cell></row><row><cell cols="2">ViewGCN [75] 44.19</cell><cell>26.06</cell><cell>23.56</cell></row><row><cell>MVTN module</cell><cell>1.78</cell><cell>4.24</cell><cell>3.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Figure 14. Occlusion Robustness of 3D Methods. We plot test accuracy vs. the Occlusion Ratio of the data to simulate the occlusion robustness of different 3D methods: PointNet<ref type="bibr" target="#b61">[62]</ref>, DGCNN<ref type="bibr" target="#b73">[74]</ref>, and MVTN. Our MVTN achieves close to 13% better than PointNet when half of the object is occluded. MVTN 1 refers to MVTN with MVCNN as the multi-view network while MVTN 2 refers to MVTN with View-GCN as the multi-view network. 3D Shape Classification on ModelNet40. We compare MVTN against other methods in 3D classification on ModelNet40<ref type="bibr" target="#b78">[79]</ref>. * indicates results from our rendering setup (differentiable pipeline), while other multi-view results are reported from pre-rendered views. Bold denotes the best result in its setup. In brackets, we report the average and standard deviation of four runs</figDesc><table><row><cell></cell><cell>Accuracy under Data Occlusion</cell></row><row><cell>Test Accuracy (%)</cell><cell>PointNet DGCNN MVTN 1 MVTN 2</cell></row></table><note>Table 10. 3D Point Cloud Classification on ScanObjectNN. We compare the performance of MVTN in 3D point cloud classification on three different variants of ScanObjectNN [72]. The variants include object with background, object only, and the hardest variant.* indicates results from our rendering setup (differentiable pipeline), and we report the average and standard deviation of four runs in brackets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Variants of MVTN. We plot test accuracy vs. the number of views used in training different variants of our MVTN. Note how MVTN-spherical is generally more stable in achieving better performance on ModelNet40. 95% confidence interval is also plotted on each setup (repeated four times). are 91.0%, 91.6%, and 91.9% respectively. For the number of randomly sampled points P = 512, 1024, and 2048, the results are 91.2% 91.6% , and 91.6% respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>MVTN Variants</cell></row><row><cell></cell><cell>92</cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>84 86 88 90</cell><cell>1 2</cell><cell>4 Number of Views (M) 6 8 MVTN-direct-10 MVTN-circular 12 MVTN-spherical</cell></row><row><cell cols="3">Figure 16.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 .Table 13 .</head><label>1213</label><figDesc>Light Direction Effect. We study the effect of light direction in the performance of the MVTN-circular. We note that randomizing the light direction in training reduce overfitting for larger number of views and leads to better generalization. Effect of Color Selection. We ablate selecting the color of the object in training our MVTN and when views are fixed in the spherical configuration. Fixed white color is compared to random colors in training. Note how randomizing the color helps in improving the test accuracy on ModelNet40 a little bit. Effect of Distance to 3D Object. We study the effect of changing the distance on training a spherical ViewGCN. We show that the distance of 2.2 units to the center is in between far and close it and gives the best accuracy. G 21.<ref type="bibr" target="#b85">86</ref> G 11.20 M 39.89 ms ViewGCN 44.19 G 22.09 G 23.56 M 26.06 ms MVTN Time and Memory Requirements. We assess the contribution of the MVTN module to time and memory requirements in the multi-view pipeline. MVTN * refers to MVTN's regressor excluding the point encoder, while MVTN ? refers to the full MVTN module including PointNet as a point encoder.module and only train ViewGCN on these learned but fixed views. ViewGCN with transferred MVTN views reaches 93.1% accuracy in classification. It corresponds to a boost of 0.7% from the 92.4% of the original ViewGCN. Although this result is lower than fully trained MVTN(?0.3%), we observe a decent transferability between both multi-view architectures.</figDesc><table><row><cell></cell><cell cols="7">Light Effect on ModelNet40 Classification Accuracies 92</cell><cell>Network PointNet DGCNN</cell><cell>FLOPs 1.78 G 10.42 G 5.21 G MACs Params. # 0.89 G 3.49 M 0.95 M</cell><cell>Time 3.34 ms 16.35 ms</cell></row><row><cell cols="2">Accuracy (%)</cell><cell>82 84 86 88 90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>relative top light random light</cell><cell>MVCNN MVTN ?</cell><cell>43.72 18.52 K 9.26 K 1.78 G 0.89 G</cell><cell>9.09 K 4.24 M</cell><cell>0.9 ms 3.50 ms</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 2</cell><cell></cell><cell cols="2">4 Number of views (M) 6 8</cell><cell>10</cell><cell>12</cell></row><row><cell cols="7">Figure 17. Object Color</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell>White</cell><cell cols="2">Random</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Fixed views</cell><cell cols="3">92.8 ? 0.1 92.8 ? 0.1</cell></row><row><cell></cell><cell></cell><cell cols="6">MVTN (learned) 93.3 ? 0.1 93.4 ? 0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Effect of Distance ModelNet40 Classification Accuracies</cell></row><row><cell></cell><cell cols="2">93.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell cols="2">91.5 92.0 92.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">91.0</cell><cell>1.5</cell><cell>1.8</cell><cell>2.2 Distance to the Object 2.5</cell><cell></cell><cell>3.0</cell></row><row><cell cols="6">Figure 18. a different multi-view network?</cell><cell></cell></row><row><cell cols="8">To test our hypothesis, we take a 12-view MVTN-</cell></row><row><cell cols="8">spherical module trained with MVCNN as a multi-view</cell></row><row><cell cols="8">network and transfer the predicted views to a ViewGCN</cell></row><row><cell cols="8">multi-view network. In this case, we freeze the MVTN</cell></row></table><note>*</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d model retrieval using probability densitybased shape descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyhun</forename><surname>Burak Akg?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B?lent</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y?cel</forename><surname>Yemez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1117" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gift: A real-time and scalable 3d shape search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longin Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in realtime using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhak</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Blender -a 3D modelling and rendering package. Blender Foundation, Blender Institute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blender Online Community</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognition of 3-d objects from multiple 2-d views by a self-organizing neural architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Statistics to Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="349" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shape google: Geometric words and expressions for invariant shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data-driven suggestions for creativity support in 3d modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH Asia 2010 papers</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On visual similarity based 3d model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Pei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Te</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On visual similarity based 3d model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Pei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Te</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Veram: View-enhanced recurrent attention model for 3d shape classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songle</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3244" to="3257" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to predict 3d objects with an interpolation-based differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9609" to="9619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="452" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">How to generate equidistributed points on the surface of a sphere. If Polymerforshung</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Deserno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Equivariant multi-view networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinshuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1568" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meshnet: Mesh neural network for 3d shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8279" to="8286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gvcnn: Group-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep aggregation of local 3d geometric features for 3d model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiko</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutarou</forename><surname>Ohbuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Camera constraint-free view-based 3-d object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2269" to="2281" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Surface simplification using quadric error metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heckbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 24th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting point cloud shape classification with a simple and effective baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pattern analysis: lectures in pattern theory, volume I</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Grenander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards analyzing semantic robustness of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="22" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SADA: semantic adversarial diagnostic attacks for autonomous applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Advpc: Transferable adversarial perturbations on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>Hanocka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Fish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachar</forename><surname>Fleishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Triplet-center loss for multi-view 3d object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fusionnet: 3d object classification using multiple data representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishakh</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Zadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05695</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-view pointnet for 3d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep multi-view learning using neuron-wise correlationmaximizing regularizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiehong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mlvcnn: Multi-loop-view convolutional neural network for 3d shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors</editor>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4996" to="5004" />
			<date type="published" when="2016" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d shape segmentation with projective convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melinos</forename><surname>Averkiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3779" to="3788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshifumi</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rotation invariant spherical harmonic representation of 3 d shape descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on geometry processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2539" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Virtual multi-view fusion for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13138</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Point cloud oversegmentation with graph-structured deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Boussaha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7440" to="7449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Alex Lenail. Nn-svg</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">3d model retrieval using hybrid features and class information. Multimedia tools and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Johan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1620" to="1630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Differentiable monte carlo ray tracing through edge sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Mao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2018 Technical Papers</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">222</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Soft rasterizer: A differentiable renderer for image-based 3d reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7708" to="7717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pointvoxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Opendr: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Shape distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Osada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dobkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="807" to="832" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08501</idno>
		<title level="m">Accelerating 3d deep learning with pytorch3d</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. 1, 6</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Exploiting the PANORAMA Representation for Convolutional Neural Network Classification and Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Sfikas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval</title>
		<editor>Ioannis Pratikakis, Florent Dupont, and Maks Ovsjanikov</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note>The Eurographics Association</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dimensionality reduction of multimodal labeled data by local fisher discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1027" to="1061" />
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<editor>Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">View-gcn: View-based graph convolutional network for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Synsin: End-to-end view synthesis from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">OpenGL Programming Guide: The Official Guide to Learning OpenGL, Release 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Shreiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Addison-wesley</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Neural Scene De-rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning relationships for multiview 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7505" to="7514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Pvnet: A joint convolutional network of point cloud and multi-view for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Multi-view harmonized bilinear network for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12328</idno>
		<title level="m">Inverserendernet: Learning single image inverse rendering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Point transformer. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
