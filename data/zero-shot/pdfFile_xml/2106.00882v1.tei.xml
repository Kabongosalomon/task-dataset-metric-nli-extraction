<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Passage Retrieval with Hashing for Open-domain Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<email>hannaneh@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename><surname>Studio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousia</forename><forename type="middle">?</forename><surname>Riken</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Passage Retrieval with Hashing for Open-domain Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most state-of-the-art open-domain question answering systems use a neural retrieval model to encode passages into continuous vectors and extract them from a knowledge source. However, such retrieval models often require large memory to run because of the massive size of their passage index. In this paper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural retrieval model that integrates a learning-to-hash technique into the state-of-the-art Dense Passage Retriever (DPR) <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref> to represent the passage index using compact binary codes rather than continuous vectors. BPR is trained with a multi-task objective over two tasks: efficient candidate generation based on binary codes and accurate reranking based on continuous vectors. Compared with DPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss of accuracy on two standard open-domain question answering benchmarks: Natural Questions and TriviaQA. Our code and trained models are available at https://github.com/studio-ousia/ bpr. 1  The passage index of the off-the-shelf DPR model <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref> requires 65GB for indexing the 21M English Wikipedia passages, which have 13GB storage size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open-domain question answering (QA) is the task of answering arbitrary factoid questions based on a knowledge source (e.g., Wikipedia). Recent stateof-the-art QA models are typically based on a twostage retriever-reader approach <ref type="bibr" target="#b2">(Chen et al., 2017)</ref> using a retriever that obtains a small number of relevant passages from a large knowledge source and a reader that processes these passages to produce an answer. Most recent successful retrievers encode questions and passages into a common continuous embedding space using two independent encoders <ref type="bibr" target="#b12">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b6">Guu et al., 2020)</ref>. Relevant passages are retrieved using a nearest neighbor search on the index con-  taining the passage embeddings with a question embedding as a query. These retrievers often outperform classical methods (e.g., BM25), but they incur a large memory cost due to the massive size of their passage index, which must be stored entirely in memory at runtime. For example, the index of a common knowledge source (e.g., Wikipedia) requires dozens of gigabytes. 1 A reduction in the index size is critical for real-world QA that requires large knowledge sources such as scientific databases (e.g., PubMed) and web-scale corpora (e.g., Common Crawl).</p><p>In this paper, we introduce Binary Passage Retriever (BPR), which learns to hash continuous vectors into compact binary codes using a multitask objective that simultaneously trains the encoders and hash functions in an end-to-end manner (see <ref type="figure" target="#fig_0">Figure 1</ref>). In particular, BPR integrates our learning-to-hash technique into the state-ofthe-art Dense Passage Retriever (DPR) <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref> to drastically reduce the size of the passage index by storing it as binary codes. BPR computes binary codes by applying the sign function to continuous vectors. As the sign function is not compatible with back-propagation, we approximate it using the scaled tanh function during training. To improve search-time efficiency while maintaining accuracy, BPR is trained to obtain both binary codes and continuous embeddings for questions with multi-task learning over two tasks: (1) candidate generation based on the Hamming distance using the binary code of the question and (2) reranking based on the inner product using the continuous embedding of the question. The former task aims to detect a small number of candidate passages efficiently from the entire passages and the latter aims to rerank the candidate passages accurately.</p><p>We conduct experiments using the Natural Questions (NQ) <ref type="bibr" target="#b13">(Kwiatkowski et al., 2019)</ref> and Triv-iaQA (TQA) <ref type="bibr" target="#b11">(Joshi et al., 2017)</ref> datasets. Compared with DPR, our BPR achieves similar QA accuracy and competitive retrieval performance with a substantially reduced memory cost from 65GB to 2GB. Furthermore, using an improved reader, we achieve results that are competitive with those of the current state of the art in open-domain QA. Our code and trained models are available at https://github.com/studio-ousia/bpr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Retrieval for Open-domain QA Many recent open-domain QA models depend on the retriever to select relevant passages from a knowledge source. Early works involved the adoption of sparse representations <ref type="bibr" target="#b2">(Chen et al., 2017)</ref> for the retriever, whereas recent works <ref type="bibr" target="#b6">Guu et al., 2020;</ref><ref type="bibr" target="#b12">Karpukhin et al., 2020)</ref> have often adopted dense representations based on neural networks. Our work is an extension of DPR <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref>, which has been used in recent state-of-theart QA models <ref type="bibr" target="#b12">(Lewis et al., 2020;</ref>. Concurrent with our work,  attempted to reduce the memory cost of DPR using post-hoc product quantization with dimension reduction and filtering of passages. However, they observed a significant degradation in the QA accuracy compared with their full model. We adopt the learning-to-hash method with our multi-task objective and substantially compress the index without losing accuracy.</p><p>Learning to Hash The objective of hashing is to reduce the memory and search-time cost of the nearest neighbor search by representing data points using compact binary codes. Learning to hash ) is a method for learning hash functions in a data-dependent manner. Recently, many deep-learning-to-hash methods have been proposed <ref type="bibr" target="#b14">(Lai et al., 2015;</ref><ref type="bibr" target="#b23">Zhu et al., 2016;</ref><ref type="bibr" target="#b17">Li et al., 2016;</ref><ref type="bibr" target="#b1">Cao et al., 2017</ref><ref type="bibr" target="#b0">Cao et al., , 2018</ref> to jointly learn feature representations and hash functions in an end-to-end manner. We follow <ref type="bibr" target="#b1">Cao et al. (2017)</ref> to implement our hash functions. Similar to our work, <ref type="bibr" target="#b22">Xu and Li (2020)</ref> used the learning-to-hash method to reduce the computational cost of the answer sentence selection task, the objective of which is to select an answer sentence from a limited number of candidates (up to 500 in their experiments). Our work is different from the aforementioned work because we focus on efficient and scalable passage retrieval from a large knowledge source (21M Wikipedia passages in our experiments) using an effective multi-task approach. In addition to hashingbased methods, improving approximate neighbor search has been actively studied <ref type="bibr" target="#b9">(J?gou et al., 2011;</ref><ref type="bibr" target="#b18">Malkov and Yashunin, 2020;</ref><ref type="bibr" target="#b5">Guo et al., 2020)</ref>. We use <ref type="bibr" target="#b9">J?gou et al. (2011)</ref> and <ref type="bibr" target="#b18">Malkov and Yashunin (2020)</ref> as baselines in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Given a question and large-scale passage collection such as Wikipedia, a retriever finds relevant passages that are subsequently processed by a reader. Our retriever is built on DPR <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref>, which is a retriever based on BERT . In this section, we first introduce DPR and then explain our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dense Passage Retriever (DPR)</head><p>DPR uses two independent BERT encoders to encode question q and passage p into d-dimensional continuous embeddings:</p><formula xml:id="formula_0">e q = BERT q (q), e p = BERT p (p), (1)</formula><p>where e q ? R d and e p ? R d . We use the uncased version of BERT-base; therefore, d = 768. The output representation of the [CLS] token is obtained from the encoder. To create passage p, the passage title and text are concatenated ([CLS] title [SEP] passage [SEP]). The relevance score of passage p, given question q, is computed using the inner product of the corresponding vectors, e q , e p .</p><formula xml:id="formula_1">Training Let D = { q i , p + i , p ? i,1 , ? ? ? , p ? i,n } m i=1</formula><p>be m training instances consisting of a question q i , a passage that answers the question (positive passage), p + i , and n passages that are irrelevant for the question (negative passages), p ? i,j . The model is trained by minimizing the negative log-likelihood of the positive passage:</p><formula xml:id="formula_2">L dpr = ? log exp( e q i , e p + i ) exp( e q i , e p + i )+ n j=1 exp( e q i , e p ? i,j ) . (2)</formula><p>Inference DPR creates a passage index by applying the passage encoder to each passage in the knowledge source. At runtime, it retrieves the top-k passages employing maximum inner product search with the question embedding as a query. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture of BPR. BPR builds a passage index by computing a binary code for each passage in the knowledge source. To compute the binary codes for questions and passages, we add a hash layer on top of the question and passage encoders of DPR. Given embedding e ? R d computed by an encoder, the hash layer computes its binary code,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><formula xml:id="formula_3">h ? {?1, 1} d , as h = sign(e),<label>(3)</label></formula><p>where sign(?) is the sign function such that for i = 1, ..., d, sign(h i ) = 1 if h i &gt; 0; otherwise, sign(h i ) = ?1. However, the sign function is incompatible with back-propagation because its gradient is zero for all non-zero inputs and is illdefined at zero. Inspired by <ref type="bibr" target="#b1">Cao et al. (2017)</ref>, we address this by approximating the sign function using the scaled tanh function during the training:</p><formula xml:id="formula_4">h = tanh(?e),<label>(4)</label></formula><p>where ? is a scaling parameter. When ? increases, the function gradually becomes non-smooth, and as ? ? ?, it converges to the sign function. At each training step, we increase ? by setting ? = ? ? ? step + 1, where step is the number of finished training steps. We set ? = 0.1 and explain the effects of changing it in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Two-stage Approach</head><p>To reduce the computational cost without losing accuracy, BPR splits the task into candidate generation and reranking stages. At the candidate generation stage, we efficiently obtain the top-l candidate passages using the Hamming distance between the binary code of question h q and that of each passage, h p . We then rerank the l candidate passages using the inner product between the continuous embedding of question e q and h p and select the top-k passages from the reranked candidates. We perform candidate generation using binary code h q for search-time efficiency, and reranking using expressive continuous embedding e q for accuracy. We set l = 1000 and describe the effects of using different l values in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>To compute effective representations for both the candidate generation and reranking stages, we combine the loss functions of the two tasks:</p><formula xml:id="formula_5">L = L cand + L rerank .<label>(5)</label></formula><p>Task #1 for Candidate Generation The objective of this task is to improve candidate generation using the ranking loss with the approximated hash code of questionh q and that of passageh p :</p><formula xml:id="formula_6">L cand = n j=1 max(0, ?( h q i ,h p + i + h q i ,h p ? i,j ) + ?). (6)</formula><p>We set ? = 2 and investigate the effects of selecting different ? values and using the cross-entropy loss instead of the ranking loss in Appendix D. Note that the retrieval performance based on the Hamming distance can be optimized using this loss function because the Hamming distance and inner product can be used interchangeably for binary codes. 2</p><p>Task #2 for Reranking We improve the reranking stage using the following loss function:</p><formula xml:id="formula_7">L rerank = ? log exp( e q i ,h p + i ) exp( e q i ,h p + i )+ n j=1 exp( e q i ,h p ? i,j</formula><p>) . <ref type="formula">(7)</ref> This function is equivalent to L dpr , with the exception thath p is used instead of e p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Algorithms for Candidate Generation</head><p>To perform candidate generation, we test two standard algorithms: <ref type="formula">(1)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets We conduct experiments using the NQ and TQA datasets and English Wikipedia as the knowledge source. We use the following preprocessed data available on the DPR website: 4 Wikipedia corpus containing 21M passages and the training/validation datasets for the retriever containing multiple positive, random negative, and hard negative passages for each question.</p><p>Baselines We compare our BPR with DPR with linear scan and DPR with Hierarchical Navigable Small World (HSNW) graphs <ref type="bibr" target="#b18">(Malkov and Yashunin, 2020</ref>) -which builds a multi-layer structure consisting of a hierarchical set of proximity graphs, following <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref> -for our primary baselines. We also apply two popular post-hoc quantization algorithms to the DPR passage index: simple locality sensitive hashing (LSH) <ref type="bibr" target="#b19">(Neyshabur and Srebro, 2015)</ref> and product quantization (PQ) <ref type="bibr" target="#b9">(J?gou et al., 2011)</ref>. We configure these algorithms such that their passage representations have the same size as that of BPR: the number of bits per passage of the LSH is set as 768, and the number of centroids and the code size of the PQ are configured as 96 and 8 bits, respectively.</p><p>Experimental settings Our experimental setup follows <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>. We evaluate our model based on its top-k recall (the percentage of positive passages in the top-k passages), retrieval 4 https://github.com/facebookresearch/ DPR efficiency (the index size and query time), and exact match (EM) QA accuracy measured by combining our model with a reader. We use the same BERTbased reader as that used by DPR. Our model is trained using the same method as DPR. We conduct experiments on servers with two Intel Xeon E5-2698 v4 CPUs and eight Nvidia V100 GPUs. The passage index are built using Faiss <ref type="bibr" target="#b10">(Johnson et al., 2019)</ref>. Further details are provided in Appendix A. <ref type="table" target="#tab_1">Table 1</ref> presents the top-k recall (for k ? {1, 20, 100}), EM QA accuracy, index size, and query time achieved by BPR and baselines on the NQ and TQA test sets. BPR achieves similar or even better performance than DPR in both retrieval with k ? 20 and EM accuracy with a substantially reduced index size from 65GB to 2GB. We observe that BPR performs worse than DPR for k = 1, but usually the recall in small k is less important because the reader usually produces an answer based on k ? 20 passages. BPR significantly outperforms all quantization baselines. The query time of BPR is substantially shorter than that of DPR. Hash table lookup is faster than linear scan but requires slightly more storage. DPR+HNSW is faster than BPR; however, it requires 151GB of storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main results</head><p>Ablations <ref type="table" target="#tab_2">Table 2</ref> shows the results of our ablation study. Disabling the reranking clearly degrades performance, demonstrating the effectiveness of our two-stage approach. Disabling the can-Model Pretrained model #params NQ TQA RAG <ref type="bibr" target="#b12">(Lewis et al., 2020)</ref> BART-large 406M 44.5 56.1 FiD (base)  T5-base 220M 48.2 65.0 FiD (large)  T5  didate generation (treating all passages as candidates) results in the same performance as using only top-1000 candidates, but significantly increases the query time due to the expensive inner product computation over all passage embeddings.</p><p>Comparison with State of the Art <ref type="table" target="#tab_4">Table 3</ref> presents the EM QA accuracy of BPR combined with state-of-the-art reader models. Here, we also report the results of our model using an improved reader based on ELECTRA-large <ref type="bibr" target="#b3">(Clark et al., 2020)</ref> instead of BERT-base. Our improved model outperforms all models except the large model of Fusion-in-Decoder (FiD), which contains more than twice as many parameters as our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce BPR, which is an extension of DPR, based on a learning-to-hash technique and a novel two-stage approach. It reduces the computational cost of open-domain QA without a loss in accuracy.</p><p>Appendix for "Efficient Passage Retrieval with Hashing for Open-domain Question Answering"</p><p>A Details of Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Knowledge Source</head><p>As the knowledge source, we use the preprocessed Wikipedia corpus consisting of 21,015,324 Wikipedia passages available on the website of <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>. The corpus is based on the December 20, 2018 version of the English Wikipedia and created by filtering out semistructured data (i.e., tables, infoboxes, lists, and disambiguation pages) and splitting the remaining Wikipedia articles into multiple, disjointed text blocks of 100 words each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Question Answering Datasets</head><p>We conduct experiments using the NQ and TQA datasets with the training, development, and test sets as in ; <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>. A brief description of these datasets is provided as follows:</p><p>? NQ is a QA dataset for which questions are obtained from Google queries and answers comprise the spans of English Wikipedia articles.</p><p>? TQA consists of trivia questions and their answers retrieved from the Web. We use the preprocessed datasets available on the website of <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>. <ref type="bibr">5</ref> The numbers of questions contained in these datasets are listed in <ref type="table" target="#tab_6">Table 4</ref>. For each question, the dataset contains three types of passages: (1) positive passages selected based on gold-standard human annotations or distant supervision, (2) random negative passages selected randomly from all the passages, and (3) hard negative passages selected based on the BM25 scores between the question and all the passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Details of BPR</head><p>Our training configuration follows that of <ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref>. In particular, for each question, we use one positive and one hard negative passage and create a mini-batch comprising 128 questions. We use the method of inbatch-negatives, wherein each positive passage in a mini-batch is treated as the negative passage of each question 5 https://github.com/facebookresearch/ DPR   in the mini-batch if it does not correspond to the question. Our model contains 220 million parameters, and is trained for up to 40 epochs using Adam.</p><p>Regarding the hyperparameter search, we select the learning rate from the search range {1e-5, 2e-5, 3e-5, 5e-5} based on the top-100 recall on the validation set of the NQ dataset. Therefore, the number of hyperparameter search trials is 4. The detailed hyperparameters are listed in <ref type="table" target="#tab_7">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Details of Reader</head><p>For each passage in the top-k passages retrieved by the retriever, the reader assigns a relevance score to the passage and selects the best answer span in the passage. The final answer is the selected span from the passage with the highest relevance score. Let P i ? R q?d (1 ? i ? k) be a BERT output representation for the i-th passage, where q is the maximum token length of the passage, and d is the dimension size of the output representation. The probabilities of a passage being selected and a token being the start or end positions of an answer is computed as P score (i) = softmax P w score i , (8) P start,i (s) = softmax P i w start s , (9) P end,i (t) = softmax P i w end t , (10)   The passage selection score of the i-th passage is given as P score (i), and the score of the s-th to t-th tokens from the i-th passage is given as P start,i (s) ? P end,i (t).</p><formula xml:id="formula_8">whereP = [P [CLS] 1 , . . . , P [CLS] k ] ? R d?k , w score ? R d , w start ? R d , and w end ? R d .</formula><p>During the training, we sample one positive and multiple negative passages from the passages returned by the retriever. The model is trained to maximize the log-likelihood of the correct answer span in the positive passage, combined with the loglikelihood of the positive passage being selected. We use the BERT-base or ELECTRA-large as our pretrained model. Regarding the hyperparameter search, we select the learning rate from {1e-5, 2e-5, 3e-5, 5e-5} based on its EM accuracy on the validation set of the NQ dataset. Therefore, the number of hyperparameter search trials is 4. Detailed hyperparameters are listed in <ref type="table" target="#tab_9">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Effects of Scaling Parameter</head><p>To investigate how the scaling parameter, ?, affects the performance, we test the performance of our model using various ? values, where ? ? {0.025, 0.05, 0.1, 0.2}. The retrieval performance on the validation set of the NQ dataset is shown in <ref type="table" target="#tab_10">Table 7</ref>. Overall, the scaling parameter has a minor impact on the performance. We select ? = 0.1 because of its enhanced performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Effects of Number of Candidate Passages</head><p>We report the performance of our model with the varied number of candidate passages l in <ref type="table" target="#tab_12">Table 8</ref>. Overall, BPR achieves similar performance in all settings. Increasing the number of candidate passages slightly improves the top-100 performance until it reaches l = 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Effects of Loss of Task #1 with Various Settings</head><p>We investigate the effects of using various settings of the loss function L cand in Eq.(6). Instead of using the ranking loss, we test the performance with the cross-entropy loss, similar to Eq.(2), andh q andh p are used instead of e q and e p , respectively. Furthermore, we also test how the parameter ? affects the performance. As shown in <ref type="table" target="#tab_13">Table 9</ref>, the cross-entropy loss clearly performs worse than the ranking loss. Furthermore, a change in the parameter ? has a minor impact on the performance. Here, we select the ranking loss with ? = 2.0 because of its enhanced performance on the top-20 and top-100 performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of BPR, a BERT-based model generating compact binary codes for questions and passages. The passages are retrieved in two stages: (1) efficient candidate generation based on the Hamming distance using the binary code of the question and (2) accurate reranking based on the inner product using the continuous embedding of the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>linear scan based on efficient Hamming distance computation, 3 and (2) hash table lookup implemented by building a hash table that maps each binary code to the corresponding passages and querying it multiple times by increasing the Hamming radius until we obtain l passages. Top k recall and exact match (EM) QA accuracy on test sets with the index size and query time of BPR and baselines. All models use the same reader based on BERT-base to evaluate the QA accuracy.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Top 1</cell><cell cols="2">Top 20</cell><cell cols="2">Top 100</cell><cell cols="3">QA Acc. (EM)</cell><cell>Index</cell><cell>Query</cell></row><row><cell></cell><cell cols="7">NQ TQA NQ TQA NQ TQA NQ</cell><cell cols="2">TQA</cell><cell>size</cell><cell>time</cell></row><row><cell>DPR</cell><cell cols="7">46.0 53.5 78.4 79.4 85.4 85.0 41.5</cell><cell cols="2">56.8</cell><cell cols="2">64.6GB 456.9ms</cell></row><row><cell>DPR + HNSW</cell><cell cols="7">45.7 53.2 78.8 78.8 85.2 84.2 41.2</cell><cell cols="2">56.6</cell><cell>151.0GB</cell><cell>1.8ms</cell></row><row><cell>DPR + Simple LSH</cell><cell cols="7">21.5 28.4 63.9 65.2 77.2 76.9 35.8</cell><cell cols="2">48.1</cell><cell>2.0GB</cell><cell>28.8ms</cell></row><row><cell>DPR + PQ</cell><cell cols="7">32.5 42.8 72.2 73.2 81.2 80.4 38.4</cell><cell cols="2">52.0</cell><cell>2.0GB</cell><cell>44.0ms</cell></row><row><cell>BPR (linear scan; l = 1000)</cell><cell cols="7">41.1 49.7 77.9 77.9 85.7 84.5 41.6</cell><cell cols="2">56.8</cell><cell>2.0GB</cell><cell>85.3ms</cell></row><row><cell>BPR (hash table lookup; l = 1000)</cell><cell>"</cell><cell>"</cell><cell>"</cell><cell>"</cell><cell>"</cell><cell>"</cell><cell>"</cell><cell>"</cell><cell></cell><cell>2.2GB</cell><cell>38.1ms</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell>Top 1</cell><cell></cell><cell>Top 20</cell><cell></cell><cell>Top 100</cell><cell></cell><cell cols="2">Query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">NQ TQA NQ TQA NQ TQA</cell><cell cols="2">time</cell></row><row><cell>BPR (l = 1000)</cell><cell></cell><cell></cell><cell cols="6">41.1 49.7 77.9 77.9 85.7 84.5</cell><cell cols="2">38.1ms</cell></row><row><cell>BPR w/o reranking</cell><cell></cell><cell></cell><cell cols="6">38.0 46.1 76.5 75.9 84.9 83.4</cell><cell cols="2">37.9ms</cell></row><row><cell cols="11">BPR w/o candidate generation 41.1 49.7 77.9 77.9 85.7 84.5 457.8ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of our ablation study. Hash table lookup is used to implement candidate generation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Exact match QA accuracy of BPR and state of the art models. BPR achieves performance close to FiD (large) with almost half of the parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Number of questions in the preprocessed dataset used in our experiments.</figDesc><table><row><cell>Name</cell><cell>Value</cell></row><row><cell>Batch size</cell><cell>128</cell></row><row><cell>Maximum question length</cell><cell>256</cell></row><row><cell>Maximum passage length</cell><cell>256</cell></row><row><cell>Maximum training epochs</cell><cell>40</cell></row><row><cell>Peak learning rate</cell><cell>2e-5</cell></row><row><cell>Learning rate decay</cell><cell>linear</cell></row><row><cell>Warmup ratio</cell><cell>0.06</cell></row><row><cell>Dropout</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>0.0</cell></row><row><cell>Adam ?1</cell><cell>0.9</cell></row><row><cell>Adam ?2</cell><cell>0.999</cell></row><row><cell>Adam</cell><cell>1e-6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters used to train BPR.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters used to train the reader based on BERT-base and that based on ELECTRA-large.</figDesc><table><row><cell cols="4">Configuration Top 1 Top 20 Top 100</cell></row><row><cell>? = 0.025</cell><cell>39.4</cell><cell>76.7</cell><cell>83.8</cell></row><row><cell>? = 0.05</cell><cell>39.5</cell><cell>76.5</cell><cell>84.0</cell></row><row><cell>? = 0.1</cell><cell>39.8</cell><cell>76.7</cell><cell>84.1</cell></row><row><cell>? = 0.2</cell><cell>39.6</cell><cell>76.3</cell><cell>83.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Top-1, top-20, and top-100 recall of our model</cell></row><row><cell>with ? ? {0.025, 0.05, 0.1, 0.2} on the validation set of</cell></row><row><cell>the NQ dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>NQ TQA NQ TQA NQ TQA l = 20041.1 49.7 77.9 77.9 85.4 84.0 l = 500 41.1 49.7 77.9 77.9 85.6 84.4 l = 1000 41.1 49.7 77.9 77.9 85.7 84.5 l = 2000 41.1 49.7 77.9 77.9 85.7 84.5</figDesc><table><row><cell>#candidates</cell><cell>Top 1</cell><cell>Top 20</cell><cell>Top 100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>, and top-100 recall of our model with l ? {200, 500, 1000} on test sets.</figDesc><table><row><cell>Configuration</cell><cell cols="3">Top 1 Top 20 Top 100</cell></row><row><cell>Cross entropy loss</cell><cell>28.6</cell><cell>67.8</cell><cell>79.8</cell></row><row><cell>Ranking loss ? = 0.0</cell><cell>39.8</cell><cell>76.4</cell><cell>84.0</cell></row><row><cell>Ranking loss ? = 1.0</cell><cell>40.0</cell><cell>76.5</cell><cell>84.0</cell></row><row><cell>Ranking loss ? = 2.0</cell><cell>39.8</cell><cell>76.7</cell><cell>84.1</cell></row><row><cell>Ranking loss ? = 4.0</cell><cell>40.3</cell><cell>76.7</cell><cell>84.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Top-1, top-20, and top-100 recall of our model with the various settings of the loss function L cand evaluated on the validation set of the NQ dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Given two binary codes, hi and hj, there exists a relationship between their Hamming distance, distH (?, ?), and inner product, ?, ? : distH (hi, hj) = 1 2 (const ? hi, hj ). 3 The Hamming distance can be computed more efficiently than the inner product using the POPCNT CPU instruction.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We are grateful for the feedback and suggestions from the anonymous reviewers and the members of the UW NLP group. This research was supported by Allen Distinguished investigator award, a gift from Facebook, and the Nakajima Foundation Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">HashGAN: Deep Learning to Hash With Pair Conditional Wasserstein GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00140</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1287" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">HashNet: Deep Learning to Hash by Continuation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.598</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5609" to="5618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to Answer Open-Domain Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accelerating Large-Scale Inference with Anisotropic Vector Quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retrieval Augmented Language Model Pre-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingwei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A Memory Efficient Baseline for Open Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15156</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Product Quantization for Nearest Neighbor Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.57</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Billion-Scale Similarity Search with GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?gou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBDATA.2019.2921572</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense Passage Retrieval for Open-Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural Questions: A Benchmark for Question Answering Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl{_}a{_}00276</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous Feature Learning and Hash Coding With Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298947</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent Retrieval for Weakly Supervised Open Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tim Rockt?schel, and others. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature Learning Based Deep Supervised Hashing with Pairwise Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu-Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1711" to="1717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D A</forename><surname>Yashunin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2889473</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On Symmetric and Asymmetric LSHs for Inner Product Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1926" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to Hash for Indexing Big Data-A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2015.2487976</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="34" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Survey on Learning to Hash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H T</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2699960</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="769" to="790" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hashing Based Answer Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu-Jun</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6473</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9330" to="9337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Hashing Network for Efficient Similarity Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2415" to="2421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
