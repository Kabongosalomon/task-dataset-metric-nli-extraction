<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-02-13">13 Feb 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
							<email>suzuki.jun@lab.ntt.co.jp</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
							<email>nagata.masaaki@lab.ntt.co.jp</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-02-13">13 Feb 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The RNN-based encoder-decoder (EncDec) approach has recently been providing significant progress in various natural language generation (NLG) tasks, i.e., machine translation (MT) <ref type="bibr" target="#b0">Sutskever et al. (2014)</ref>;  and abstractive summarization (ABS) <ref type="bibr" target="#b2">Rush et al. (2015)</ref>. Since a scheme in this approach can be interpreted as a conditional language model, it is suitable for NLG tasks. However, one potential weakness is that it sometimes repeatedly generates the same phrase (or word).</p><p>This issue has been discussed in the neural MT (NMT) literature as a part of a coverage problem <ref type="bibr">Tu et al. (2016)</ref>; <ref type="bibr">Mi et al. (2016)</ref>. Such repeating generation behavior can become more severe in some NLG tasks than in MT. The very short ABS task in <ref type="bibr">DUC-2003</ref><ref type="bibr" target="#b20">and 2004</ref><ref type="bibr" target="#b5">Over et al. (2007</ref> is a typical example because it requires the generation of a summary in a pre-defined limited output space, such as ten words or 75 bytes. Thus, the repeated output consumes precious limited output space. Unfortunately, the coverage approach cannot be directly applied to ABS tasks 1 This is a draft version of <ref type="bibr">EACL-2017</ref> since they require us to optimally find salient ideas from the input in a lossy compression manner, and thus the summary (output) length hardly depends on the input length; an MT task is mainly loss-less generation and nearly one-to-one correspondence between input and output <ref type="bibr" target="#b6">Nallapati et al. (2016a)</ref>.</p><p>From this background, this paper tackles this issue and proposes a method to overcome it in ABS tasks. The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by <ref type="bibr" target="#b2">Rush et al. Rush et al. (2015)</ref>, and evaluated in <ref type="bibr" target="#b7">Chopra et al. (2016);</ref><ref type="bibr">Nallapati et al. (2016b)</ref>; <ref type="bibr" target="#b9">Kikuchi et al. (2016);</ref><ref type="bibr" target="#b10">Takase et al. (2016)</ref>; <ref type="bibr" target="#b11">Ayana et al. (2016)</ref>; <ref type="bibr">Gulcehre et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline RNN-based EncDec Model</head><p>The baseline of our proposal is an RNN-based EncDec model with an attention mechanism <ref type="bibr" target="#b13">Luong et al. (2015)</ref>. In fact, this model has already been used as a strong baseline for ABS tasks <ref type="bibr" target="#b7">Chopra et al. (2016)</ref>; <ref type="bibr" target="#b9">Kikuchi et al. (2016)</ref> as well as in the NMT literature. More specifically, as a case study we employ a 2-layer bidirectional LSTM encoder and a 2-layer LSTM decoder with a global attention . We omit a detailed review of the descriptions due to space limitations. The following are the necessary parts for explaining our proposed method.</p><p>Input: H s = (h s i ) I i=1 ? list of hidden states generated by encoder Initialize: s ? 0 ? s: cumulative log-likelihood Y ? 'BOS ? ?? : list of generated words H t ? H s ? H t : hidden states to process decoder 1: h ? (s,? , H t ) ? triplet of (minimal) info for decoding process 2: Qw ? push(Qw, h) ? set initial triplet h to priority queue Qw 3: Qc ? {} ? prepare queue to store complete sentences 4: Repeat 5:? ? () ? prepare empty list 6: Repeat 7:</p><p>h ? pop(Qw) ? pop a candidate history 8:? ? calcLL(h) ? see Eq. 2 9:? ? append(?,?) ? append likelihood vector 10: Let X = (x i ) I i=1 and Y = (y j ) J j=1 be input and output sequences, respectively, where x i and y j are one-hot vectors, which correspond to the i-th word in the input and the j-th word in the output. Let V t denote the vocabulary (set of words) of output. For simplification, this paper uses the following four notation rules:</p><formula xml:id="formula_0">Until Qw = ? ? repeat until Qw is empty 11: {(m,k)z} K?C z=1 ? findKBest(?) 12: {hz} K?C z=1 ? makeTriplet({(m,k)z} K?C z=1 ) 13: Q ? ? selectTopK(Qc, {hz} K?C z=1 ) 14: (Qw, Qc) ? SepComp(Q ? ) ? separate Q ? into Qc or Qw 15: Until Qw = ? ? finish if Qw is empty Output: Qc</formula><p>1. (x i ) I i=1 is a short notation for representing a list of (column) vectors, i.e., (x 1 , . . . ,</p><formula xml:id="formula_1">x I ) = (x i ) I i=1 .</formula><p>2. v(a, D) represents a D-dimensional (column) vector whose elements are all a, i.e., v(1, 3) = (1, 1, 1) ? .</p><p>3. x[i] represents the i-th element of x, i.e., x = (0.1, 0.2, 0.3) ? , then x[2] = 0.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder:</head><p>Let ? s (?) denote the overall process of our 2-layer bidirectional LSTM encoder. The encoder receives input X and returns a list of final hidden states H s = (h s i ) I i=1 :</p><formula xml:id="formula_2">H s = ? s (X).<label>(1)</label></formula><p>Decoder: We employ a K-best beam-search decoder to find the (approximated) best output? given input X. <ref type="figure" target="#fig_0">Figure 1</ref> shows a typical Kbest beam search algorithm used in the decoder of EncDec approach. We define the (minimal) required information h shown in <ref type="figure" target="#fig_0">Figure 1</ref> for the jth decoding process is the following triplet, h = (s j?1 ,? j?1 , H t j?1 ), where s j?1 is the cumulative log-likelihood from step 0 to j ? 1,? j?1 is a (candidate of) output word sequence generated so far from step 0 to j ? 1, that is,? j?1 = (y 0 , . . . , y j?1 ) and H t j?1 is the all the hidden states for calculating the j-th decoding process. Then, the function calcLL in Line 8 can be written as follows:</p><formula xml:id="formula_3">o j = v s j?1 , M + log Softmax(o j ) o j = ? t H s , H t j?1 ,? j?1 ,<label>(2)</label></formula><p>where Softmax(?) is the softmax function for a given vector and ? t (?) represents the overall process of a single decoding step.</p><formula xml:id="formula_4">Moreover,? in Line 11 is a (M ? (K ? C))- matrix, where C is the number of complete sen- tences in Q c . The (m, k)-element of? repre- sents a likelihood of the m-th word, namely? j [m]</formula><p>, that is calculated using the k-th candidate in Q w at the (j ? 1)-th step. In Line 12, the function makeTriplet constructs a set of triplets based on the information of index (m,k). Then, in Line 13, the function selectTopK selects the top-K candidates from union of a set of generated triplets at current step {h z } K?C z=1 and a set of triplets of complete sentences in Q c . Finally, the function sepComp in Line 13 divides a set of triplets Q ? in two distinct sets whether they are complete sentences, Q c , or not, Q w . If the elements in Q ? are all complete sentences, namely, Q c = Q ? and Q w = ?, then the algorithm stops according to the evaluation of Line 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word Frequency Estimation</head><p>This section describes our proposed method, which roughly consists of two parts:</p><p>1. a sub-model that estimates the upper-bound frequencies of the target vocabulary words in the output, and 2. architecture for controlling the output words in the decoder using estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition</head><p>Let? denote a vector representation of the frequency estimation. ? denotes element-wise prod-uct.? is calculated by:</p><formula xml:id="formula_5">a =r ?? r = ReLU(r),? = Sigmoid(g),<label>(3)</label></formula><p>where Sigmoid(?) and ReLu(?) represent the element-wise sigmoid and ReLU Glorot et al.</p><formula xml:id="formula_6">(2011), respectively. Thus,r ? [0, +?] M ,? ? [0, 1] M , and? ? [0, +?] M .</formula><p>We incorporate two separated components,r and?, to improve the frequency fitting. The purpose of? is to distinguish whether the target words occur or not, regardless of their frequency. Thus, g can be interpreted as a gate function that resembles estimating the fertility in the coverage <ref type="bibr">Tu et al. (2016)</ref> and a switch probability in the copy mechanism <ref type="bibr">Gulcehre et al. (2016)</ref>. These ideas originated from such gated recurrent networks as <ref type="bibr">LSTM Hochreiter and Schmidhuber (1997)</ref> and <ref type="bibr">GRU Chung et al. (2014)</ref>. Then,r can much focus on to model frequency equal to or larger than 1. This separation can be expected sincer <ref type="bibr">[m]</ref> has no influence if?[m] = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effective usage</head><p>The technical challenge of our method is effectively leveraging WFE?. Among several possible choices, we selected to integrate it as prior knowledge in the decoder. To do so, we re-define? j in Eq. 2 as:</p><formula xml:id="formula_7">o j = v s j?1 , M + log (Softmax(o j )) +? j .</formula><p>The difference is the additional term of? j , which is an adjusted likelihood for the j-th step originally calculated from?. We define? j as:</p><formula xml:id="formula_8">a j = log (ClipReLU 1 (r j ) ??) .<label>(4)</label></formula><p>ClipReLU 1 (?) is a function that receives a vector and performs an element-wise calculation:</p><formula xml:id="formula_9">x ? [m] = max (0, min(1, x[m])</formula><p>) for all m if it receives x. We define the relation betweenr j in Eq. 4 andr in Eq. 3 as follows:</p><formula xml:id="formula_10">r j = r if j = 1 r j?1 ?? j?1 otherwise .<label>(5)</label></formula><p>Eq. 5 is updated fromr j?1 tor j with the estimated output of previous step? j?1 . Since? j ? {0, 1} M for all j, all of the elements inr j are monotonically non-increasing.</p><formula xml:id="formula_11">Ifr j ? [m] ? 0 at j ? , the? o j ? [m] = ?? regardless of o[m]</formula><p>. This means that Input: H s = (h s i ) I i=1 ? list of hidden states generated by encoder Parameters:</p><formula xml:id="formula_12">W r 1 , W g 1 ? R H?H , W r 2 ? R M ?H , W g 2 ? R M ?2H , 1: H r 1 ? W r 1 H s ? linear transformation for frequency model 2: h r 1 ? H r 1 v(1, M ) ? h r 1 ? R H , H r 1 ? R H?I 3: r ? W r 2 h r 1 ? frequency estimation 4: H g 1 ? W g 1 H s ? linear transformation for occurrence model 5: h g+ 2 ? RowMax(H g 1 ) ? h g+ 2 ? R H , and H g 1 ? R H?I 6: h g? 2 ? RowMin(H g 1 ) ? h g? 2 ? R H , and H g 1 ? R H?I 7: g ? W g 2 concat(h g+ 2 , h g? 2 )</formula><p>? occurrence estimation Output: (g, r) <ref type="figure">Figure 2</ref>: Procedure for calculating the components of our WFE sub-model. the m-th word will never be selected any more at step j ? ? j for all j. Thus, the interpretation of r j is that it directly manages the upper-bound frequency of each target word that can occur in the current and future decoding time steps. As a result, decoding with our method never generates words that exceed the estimationr, and thus we expect to reduce the redundant repeating generation.</p><p>Note here that our method never requires r j [m] ? 0 (orr j [m] = 0) for all m at the last decoding time step j, as is generally required in the coverage <ref type="bibr">Tu et al. (2016)</ref>; <ref type="bibr">Mi et al. (2016)</ref>; . This is why we say upper-bound frequency estimation, not just (exact) frequency. <ref type="figure">Figure 2</ref> shows the detailed procedure for calculating g and r in Eq. 3. For r, we sum up all of the features of the input given by the encoder (Line 2) and estimate the frequency. In contrast, for g, we expect Lines 5 and 6 to work as a kind of voting for both positive and negative directions since g needs just occurrence information, not frequency. For example, g may take large positive or negative values if a certain input word (feature) has a strong influence for occurring or not occurring specific target word(s) in the output. This idea is borrowed from the Max-pooling layer <ref type="bibr" target="#b19">Goodfellow et al. (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Calculation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter estimation (Training)</head><p>Given the training data, let a * ? P M be a vector representation of the true frequency of the target words given the input, where P = {0, 1, . . . , +?}. Clearly a * can be obtained by counting the words in the corresponding output. We define loss function ? wfe for estimating our </p><formula xml:id="formula_13">? wfe X, a * , W = d ? v(1, M ) (6) d = c 1 max v(0, M ),? ? a * ? v(?, M ) b + c 2 max v(0, M ), a * ?? ? v(?, M ) b ,</formula><p>where W represents the overall parameters. The form of ? wfe (?) is closely related to that used in support vector regression (SVR) <ref type="bibr" target="#b20">Smola and Sch?lkopf (2004)</ref>. We allow estimation?[m] for all m to take a value in the range of [a * [m] ? ?, a * [m] + ?] with no penalty (the loss is zero). In our case, we select ? = 0.25 since all the elements of a * are an integer. The remaining 0.25 for both the positive and negative sides denotes the margin between every integer. We select b = 2 to penalize larger for more distant error, and c 1 &lt; c 2 , i.e., c 1 = 0.2, c 2 = 1, since we aim to obtain upper-bound estimation and to penalize the underestimation below the true frequency a * . Finally, we minimize Eq. 6 with a standard negative log-likelihood objective function to estimate the baseline EncDec model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We investigated the effectiveness of our method on ABS experiments, which were first performed by <ref type="bibr" target="#b2">Rush et al., Rush et al. (2015)</ref>. The data consist of approximately 3.8 million training, 400,000 validation and 400,000 test data, respectively 3 . Generally, 1951 test data, randomly extracted from the test data section, are used for evaluation 4 . Ad-3 The data can be created by the data construction scripts in the author's code: https://github.com/facebook/NAMAS. <ref type="bibr">4</ref> As previously described <ref type="bibr" target="#b7">Chopra et al. (2016)</ref> we removed the ill-formed (empty) data for Gigaword.   <ref type="bibr" target="#b5">Over et al. (2007)</ref>  <ref type="bibr">5</ref> were also evaluated by the identical models trained on the above Gigaword data. We strictly followed the instructions of the evaluation setting used in previous studies for a fair comparison. <ref type="table">Table 1</ref> summarizes the model configuration and the parameter estimation setting in our experiments. <ref type="table" target="#tab_3">Table 2</ref> shows the results of the baseline EncDec and our proposed EncDec+WFE. Note that the DUC-2004 data was evaluated by recall-based ROUGE scores, while the Gigaword data was evaluated by F-score-based ROUGE, respectively. For a validity confirmation of our EncDec baseline, we also performed OpenNMT tool 6 . The results on Gigaword data with B = 5 were, 33.65, 16.12, and 31.37 for ROUGE-1(F), ROUGE-2(F) and ROUGE-L(F), respectively, which were almost similar results (but slightly lower) with our implementation. This supports that our baseline worked well as a strong baseline. Clearly, EncDec+WFE significantly outperformed the strong EncDec baseline by a wide margin on the ROUGE scores. Thus, we conclude that the WFE sub-model has a positive impact to gain the ABS performance since performance <ref type="bibr">DUC-2004 (w/ 75-byte limit)</ref>     gains were derived only by the effect of incorporating our WFE sub-model. <ref type="table" target="#tab_4">Table 3</ref> lists the current top system results. Our method EncDec+WFE successfully achieved the current best scores on most evaluations. This result also supports the effectiveness of incorporating our WFE sub-model. <ref type="bibr">MRT Ayana et al. (2016)</ref> previously provided the best results. Note that its model structure is nearly identical to our baseline. On the contrary, MRT trained a model with a sequence-wise minimum risk estimation, while we trained all the models in our experiments with standard (pointwise) log-likelihood maximization. MRT essentially complements our method. We expect to further improve its performance by applying MRT for its training since recent progress of NMT has suggested leveraging a sequence-wise optimization technique for improving performance Wiseman and Rush (2016); . We leave this as our future work. <ref type="figure" target="#fig_2">Figure 3</ref> shows actual generation examples. Based on our motivation, we specifically selected the redundant repeating output that occurred in the baseline EncDec. It is clear that EncDec+WFE successfully reduced them. This observation offers further evidence of the effectiveness of our method in quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main results: comparison with baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to current top systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generation examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance of the WFE sub-model</head><p>To evaluate the WFE sub-model alone, <ref type="table" target="#tab_5">Table 4</ref> shows the confusion matrix of the frequency estimation. We quantized? by ??[m]+0.5? for all m, where 0.5 was derived from the margin in ? wfe . Unfortunately, the result looks not so well. There seems to exist an enough room to improve the estimation. However, we emphasize that it already has an enough power to improve the overall quality as shown in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="figure" target="#fig_2">Figure 3</ref>. We can expect to further gain the overall performance by improving the performance of the WFE sub-model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper discussed the behavior of redundant repeating generation often observed in neural EncDec approaches. We proposed a method for reducing such redundancy by incorporating a submodel that directly estimates and manages the frequency of each target vocabulary in the output. Experiments on ABS benchmark data showed the effectiveness of our method, EncDec+WFE, for both improving automatic evaluation performance and reducing the actual redundancy. Our method is suitable for lossy compression tasks such as image caption generation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Algorithm for a K-best beam search decoding typically used in EncDec approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4. M = |V t | and, m always denotes the index of output vocabulary, namely, m ? {1, . . . , M }, and o[m] represents the score of the m-th word in V t , where o ? R M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples of generated summary. G: reference summary, A: baseline EncDec, and B: EncDec+WFE. (underlines indicate repeating phrases and words) ditionally, DUC-2004 evaluation data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="7">: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE-</cell></row><row><cell cols="4">x(F): F1-based ROUGE-x, where x ? {1, 2, L}, respectively.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DUC-2004 (w/ 75-byte limit)</cell><cell></cell><cell cols="2">Gigaword (w/o length limit)</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">ROUGE-1(R) ROUGE-2(R) ROUGE-L(R) ROUGE-1(F) ROUGE-2(F) ROUGE-L(F)</cell></row><row><cell>ABS Rush et al. (2015)</cell><cell>26.55</cell><cell>7.06</cell><cell>22.05</cell><cell>30.88</cell><cell>12.22</cell><cell>27.77</cell></row><row><cell>RAS Chopra et al. (2016)</cell><cell>28.97</cell><cell>8.26</cell><cell>24.06</cell><cell>33.78</cell><cell>15.97</cell><cell>31.15</cell></row><row><cell>BWL Nallapati et al. (2016a) 2</cell><cell>28.35</cell><cell>9.46</cell><cell>24.59</cell><cell>32.67</cell><cell>15.59</cell><cell>30.64</cell></row><row><cell>(words-lvt5k-1sent ?)</cell><cell>28.61</cell><cell>9.42</cell><cell>25.24</cell><cell>35.30</cell><cell>?16.64</cell><cell>32.62</cell></row><row><cell>MRT Ayana et al. (2016)</cell><cell>?30.41</cell><cell>?10.87</cell><cell>?26.79</cell><cell>?36.54</cell><cell>16.59</cell><cell>?33.44</cell></row><row><cell>EncDec+WFE [This Paper]</cell><cell>32.28</cell><cell>10.54</cell><cell>27.80</cell><cell>36.30</cell><cell>17.31</cell><cell>33.88</cell></row><row><cell>(perf. gain from  ?)</cell><cell>+1.87</cell><cell>-0.33</cell><cell>+1.01</cell><cell>-0.24</cell><cell>+0.72</cell><cell>+0.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results of current top systems: '*': previous best score for each evaluation. ?: using a larger vocab for both encoder and decoder, not strictly fair configuration with other results.</figDesc><table><row><cell>True a  *  \ Estimation?</cell><cell>0</cell><cell>1</cell><cell cols="2">2 3 4 ?</cell></row><row><cell>1</cell><cell cols="3">7,014 7,064 1,784 16</cell><cell>4</cell></row><row><cell>2</cell><cell>51</cell><cell>95</cell><cell>60 0</cell><cell>0</cell></row><row><cell>3 ?</cell><cell>2</cell><cell>4</cell><cell>1 0</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Confusion matrix of WFE on Gigaword data: only evaluated true frequency ? 1.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://duc.nist.gov/duc2004/tasks.html 6 http://opennmt.net</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS 2014)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling Coverage for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany, Au</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16" to="1008" />
		</imprint>
	</monogr>
	<note>gust 2016. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coverage embedding models for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Baskaran Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ittycheriah</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1096" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="955" to="960" />
		</imprint>
	</monogr>
	<note>November 2016. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DUC in context. Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1506" to="1520" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence rnns for text summarization. CoRR, abs/1602.06023</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1602.06023" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
	<note>California</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization using Sequenceto-sequence RNNs and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/K16-1028" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany, Au</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
	<note>gust 2016b. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoderdecoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural headline generation on abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1112" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1054" to="1059" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural headline generation with minimum risk training. CoRR, abs/1604.01904</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Ayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1604.01904" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointing the Unknown Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany, Au</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16" to="1014" />
		</imprint>
	</monogr>
	<note>gust 2016. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR 2015</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Sparse Rectifier Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</title>
		<editor>Geoffrey J. Gordon and David B. Dunson</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://dx.doi.org/10" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Aglar G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3555</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
		<ptr target="http://arxiv.org/abs/1609.08144" />
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><forename type="middle">Maxout</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Networks</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v28/goodfellow13.html" />
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequenceto-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1137" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16" to="1159" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
