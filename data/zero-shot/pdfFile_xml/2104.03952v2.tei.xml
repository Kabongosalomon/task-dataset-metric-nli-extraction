<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DATASET SUMMARIZATION BY K PRINCIPAL CONCEPTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DATASET SUMMARIZATION BY K PRINCIPAL CONCEPTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the new task of K principal concept identification for dataset summarizarion. The objective is to find a set of K concepts that best explain the variation within the dataset. Concepts are high-level human interpretable terms such as "tiger", "kayaking" or "happy". The K concepts are selected from a (potentially long) input list of candidates, which we denote the concept-bank. The concept-bank may be taken from a generic dictionary or constructed by task-specific prior knowledge. An image-language embedding method (e.g. CLIP) is used to map the images and the concept-bank into a shared feature space. To select the K concepts that best explain the data, we formulate our problem as a K-uncapacitated facility location problem. An efficient optimization technique is used to scale the local search algorithm to very large concept-banks. The output of our method is a set of K principal concepts that summarize the dataset. Our approach provides a more explicit summary in comparison to selecting K representative images, which are often ambiguous. As a further application of our method, the K principal concepts can be used to classify the dataset into K groups. Extensive experiments demonstrate the efficacy of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Summarizing a large image dataset into a small number of words is an important but challenging research task. This is particularly helpful for large photo storage service providers such as Google Photos or Instagram, each possessing more than a billion photo albums. Such summarization provides fast, but relatively accurate insights into dataset content <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Here we propose a new setting, focusing on an extreme form of summarization, finding a short list of K concepts that retain the maximal information on the variation in the dataset. As an example, let us assume a photo collection containing humans performing various activities. A possible summary of the dataset can be a set of the K most prominent featured activities ( <ref type="figure">Fig.1)</ref>. For example, for K = 3 the principal concepts may be {'running', 'applauding', 'gardening'}.</p><p>Our approach first requires a long list of potentially valid concepts, which can potentially describe groups in the dataset. This long list, that we name the concept-bank, can be generic (e.g. all the nouns in the WordNet database <ref type="bibr" target="#b2">[3]</ref>). Alternatively, the valid concepts may be more restricted, encoding prior knowledge about the desired results. Aiming to find concepts corresponding to activities, colors or musical instruments, we may use a concept-bank restricted to such attributes. Given the concept-bank, the task is to select the K principal concepts which provide the most informative description of the variation in the dataset. Although creating a concept-bank may initially appear to be a daunting task, we find that it can be performed by a very fast, semi-automatic process.</p><p>To select K principal concepts, we first use an image-language model (such as CLIP <ref type="bibr" target="#b4">[5]</ref>) to embed all the concepts in the concept-bank and all the images in the dataset into a common embedding space. The objective becomes the selection of K concepts such that the sum of distances between every image and its nearest selected concept is minimal. While this task is reminiscent of K-means, it is in fact different, as the search space for the centers is discrete while in K means the space is unconstrained. Instead, this task is an instance of the unconstrained K facility location problem, a well studied <ref type="bibr" target="#b5">[6]</ref>, NP-hard algorithmic problem. Although advanced methods exist for the solution with approximation guarantees, the most popular methods do not scale to our task. Instead, we suggest solving the optimization task using a more scalable approach that can be seen as a discretized version of K-means. arXiv:2104.03952v2 [cs.CV] 29 Sep 2022 <ref type="figure">Figure 1</ref>: Summarizing a dataset (e.g. Stanford Activity <ref type="bibr" target="#b3">[4]</ref>) with K representative images can be ambiguous. Identifying the key concepts in the data (e.g 'running', 'applauding', 'gardening') is often clearer.</p><p>As a downstream application, our method can be used for conceptual grouping within image datasets. Given the K principal concepts generated in the previous stage, a zero-shot learning approach may be used to classify each image into one (and only one) of the K concepts. One advantage of this approach is the ability to guide the grouping by selection of the concept-bank. The grouping of images by activity, for instance, can be easily accomplished by selecting a concept-bank only containing descriptions of activities, forcing the images to be grouped accordingly. Most related to this task are zero-shot classification <ref type="bibr" target="#b4">[5]</ref> and unsupervised clustering <ref type="bibr" target="#b6">[7]</ref>. Zero-shot classification is able to discover the concept to which each image is most related, but it does not by itself discover the K principal concepts for a dataset given a much longer list of allowed concepts. While some greedy heuristics may be attempted, they do not perform well for large concept-banks, and due to correlations may also choose overlapping concepts (e.g. 'dog' and 'poodle'). Alternatively, unsupervised clustering is limited only to visual similarity and requires ad-hoc inductive biases to obtain meaningful groups.</p><p>We evaluate our method on 3 standard object classification datasets, and 4 more complex datasets with non-standard groupings, demonstrating our K principal concept identification and grouping capabilities. We show that our method is able to identify principal concepts close to the ground truth class-names. We further show that concept-list guidance is necessary for achieving accurate grouping results on non-standard datasets and can also increase accuracy on standard datasets.</p><p>Our main contributions are: 1. Introducing the task of K principal concept identification for dataset summarization. 2. Reducing our objective to the well-studied facility location problem, and suggesting a scalable and effective solution for the optimization task. 3. Proposing pre-and post-processing methods for selecting concepts with the appropriate level of summarization. 4. Demonstrating that the identified principal concepts are effective for image grouping and allow better guidance for the grouping process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Summarizing visual datasets. Video or image dataset summarization is an important task that has typically been addressed by presenting several representative images. Video summarization typically identifies key frames that capture the main themes in the video <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Image summarization have been addressed using a range of approaches, including generative modelling <ref type="bibr" target="#b0">[1]</ref>, story graphs <ref type="bibr" target="#b9">[10]</ref> and multi-modal data <ref type="bibr" target="#b10">[11]</ref>.</p><p>Concept Based Learning. Concept Bottleneck Models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> use a network to find an intermediate set of human-specified concepts, allowing better interpretability. Another line of work uses language cues to localize visual objects within the image <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. These works aim to identify concepts within images, rather than identifying categories across the dataset.</p><p>Joint embedding for images and text. A key motivation for looking into joint embedding is reducing the requirement for image annotations <ref type="bibr" target="#b17">[18]</ref>  <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. <ref type="bibr" target="#b4">[5]</ref> presented a new method, CLIP, that maps images and sentences into a common space. Our method relies on the infrastructure provided by CLIP, but does not assume the set of image names is provided. CLIP was recently followed by a line of similar works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. As these models are rapidly developing, we use CLIP as our backbone, but results can be easily adapted to utilize any similar backbone model.</p><p>Uncapacitated facility location problem (UFLP). The UFLP <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> problem is a long-studied task in economics, computer science, operations research and discrete optimization. It aims to open a set of facilities, so that they serve all clients at a minimal cost. Different solutions methodologies have been applied to the task including: greedy methods <ref type="bibr" target="#b26">[27]</ref>, linear-programming with rounding <ref type="bibr" target="#b27">[28]</ref> and linear-programming primal-dual methods <ref type="bibr" target="#b28">[29]</ref>. Here, we are concerned with the Uncapacitated K-Facility Location Problem (UKFLP) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>, which limits the number of facilities to K. We formulate our optimization objective as the UKFLP and use a fast, relaxed variant of the Local Search method <ref type="bibr" target="#b26">[27]</ref>. The images and concepts are embedded into a joint space using CLIP. A K uncaptacitated facility location method is used to retrieve the K principal concept. Optionally, the K principal concepts can be used to classify the dataset into K groups using standard zero-shot classification methods.</p><p>Image Grouping. Deep features trained using self-supervised criteria are extensively used for image grouping e.g. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. More recent approaches directly optimize clustering objectives during feature learning e.g. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Choosing features that align with human semantics requires inductive bias. A promising line of approaches use carefully selected augmentations to remove the nuisance attributes and direct learning towards more semantic features <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. The work by Van Gansbeke et al. <ref type="bibr" target="#b6">[7]</ref> suggested a two stage approach, where features are first learned using a self-supervised task, and then used as a prior for learning the features for grouping. In practice, we often look for groups which would be balanced in size, at least approximately. Many works utilize an information theoretic criterion to impose such balancing <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Other work uses partial supervisory information to infer groups <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>Multi Modal Image Grouping. Approaches such as multi-modal clustering <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> address the tasks of grouping data, where the same kind of samples can be represented with different data modalities. We note that these interesting tasks significantly differ from ours: our additional modality (text) describes the set of possible groups, rather than additional samples given in a new modality. Other prior works, which are more similar to our approach include color quantization using colors that are named in the English language. In these works one divides all colors into a discrete number of color groups <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>. Color name-based identification was further applied to other tasks, such as image classification, visual tracking, action recognition <ref type="bibr" target="#b51">[52]</ref> and person identification <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. Our approach can be seen as extending these ideas from pixel color to whole images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">K Principal Concept Identification</head><p>We propose the novel task of K principal concept identification for dataset summarization. The goal is to select K concepts from a (potentially very long) concept-bank that best describe the variation in data ( <ref type="figure" target="#fig_0">Fig.2)</ref>. We describe the concept-bank creation process in Sec. 3.1. Given a concept-bank, the selection of K concepts corresponding to the group names is described in Sec. 3.2-3.4. Having selected the K principal concepts, images can be assigned to groups using standard zero-shot clustering, and accuracy can be further improved by an adapter network (Sec. 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept-bank Creation</head><p>Our method requires a list of allowed concepts. Although the list can be generated manually, this can be tedious. We suggest using a semi-automatic way of creating the list. First, the operator provides a single word (or phrase) describing the concept-bank they wish to group the images by. Examples of such attributes include: "activities", "objects", "dogs" or "musical instruments". A concept-bank is then retrieved from an online Word Bank such as <ref type="bibr">[55]</ref>. We note that even a very general list such as WordNet nouns (for "objects") containing over 82k can be very effective for a large variety of datasets as demonstrated in Sec.5. However, there is an advantage in more fine-grained guidance. If we wish to avoid identifying specific concept categories (e.g. "musical instruments"), such concepts should not be included in the concept-bank. By specifying just the desired concept class, we can limit the type of possible image groupings, and ambiguities can be avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">K Principal Concept Identification Formulation</head><p>Our goal is to discover the K concepts from the concept-bank that best describe the variation in the image dataset. We are given N I images, which are mapped into feature vectors</p><formula xml:id="formula_0">{v 1 . . . v N I }, v i ? R d .</formula><p>A concept-bank consisting of N W concepts describing possible image groupings is provided by the process explained in Sec.3.1. Every concept is mapped into a vector embedding {u 1 . . . u N W }, u i ? R d , the set of all concept embeddings is denoted as W. We aim to select K principal concepts that best describe the variation in the data. We denote the principal concepts by a corresponding set of vectors {p 1 ..p K }, p k ? W. Each image is assigned to its nearest principal concept, resulting in K image groups {S 1 ..S K }. Therefore, each group consists of conceptually homogeneous images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Removing Overly General Concepts</head><p>Although we assume all plausible concepts are contained in the concept-bank W, some concepts may have a meaning that is too general. Such concepts may not explain much of the dataset variation and may even be related to all the images in the dataset. Examples for such concept are: 'entity', 'abstraction', 'thing', 'object', 'whole'. Therefore, we wish to filter such uninformative concepts out of our list.</p><p>To remove such concepts we rely on the following intuition: concepts which are very general can describe many other concepts in the list. We therefore measure how well a concept describes other concepts using the inner product between the normalized embeddings. To filter such uninformative concepts, we look for concepts with high average correlation to other concepts in the list. We first calculate the average embedding of all concepts in the list:</p><formula xml:id="formula_1">u avg = 1 N W i=1..N W u i<label>(1)</label></formula><p>We than calculate the generality score s for each concept, as the inner product between its embedding u i and the average concept embedding u avg :</p><formula xml:id="formula_2">s(u i ) = u i ? u avg<label>(2)</label></formula><p>We find that this score is indeed higher for the less specific concepts described earlier. We remove from the list all concepts that have a "generality score" s higher than some quantile level 0 &lt; q ? 1, and define the new sublist W q ? W (|W q | ? q ? |W|, where |.| denotes the length of a set).</p><p>We propose an unsupervised distributional criterion for choosing the quantile q for each dataset. We first run our method on a set of values of q = [0, 0.05, 0.1...1]. For each value of q, we obtain concept assignments and calculate the distributional entropy. We select the q for which the principal concepts W q form the most balanced grouping i.e., with the highest entropy. See Sec.6 for an ablation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Grouping with the K Principal Concepts</head><p>We consider a group of images S k describable by a single concept p k if the embeddings of its associated images are near the embedding of the concept p k ? W q . We formulate this objective, using the within-group sum of squares (WCSS) loss:</p><formula xml:id="formula_3">min {p1..p K },{S1..S K } K k=1 v?S k v ? p k 2 s.t. p k ? W q<label>(3)</label></formula><p>The objective is to find assignments {S 1 ..S k } and principal concepts {p 1 ..p k } ? W q , such that the sum of square distances for images and the assigned concept is minimal. Note that this is different from K-means as the group centers are constrained to the discrete set of concepts W whereas in K-means they are unconstrained. An efficient optimization method is proposed in Sec. 4. <ref type="figure">Figure 3</ref>: Four images selected from the Stanford Activity dataset. The images can be grouped by coarse-object category ('adult humans', 'toddler') or by activity ('drinking', 'brushing teeth'). Our method uses a concept-bank to determine the concepts that are allowed for grouping (in this case activities: 'walking', 'drinking', 'brushing teeth' etc.). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Post-processing</head><p>Moving up the WordNet hierarchy When the initial concept-bank is as long as the entire WordNet dictionary, we aim to replace too specific principal concepts (e.g. 'bulbul') with ones that better describe the entire group of images (e.g. 'bird'). Our dataset was already grouped based on the semantics of the concept-bank W q by obtaining K principal concepts. For each group {S 1 ..S k }, we retrieve the M concepts that are nearest to the group center. All the M concepts are very close to the center of the group because M is chosen as a small number (M = 50) when compared to the original concept-bank length (|W| = 82k). We then select the most abstract concept out of the top M concepts. In order to measure the abstraction level, we count the number of concepts below the examined concept in the WordNet hierarchy.</p><p>Tip Adapter While direct application of zero-shot classification on the K principle concepts can already achieve high grouping accuracy, better results can be obtained by fine-tuning the discovered groupings using the Tip Adapter <ref type="bibr" target="#b54">[56]</ref>. Further details can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Uncapacitated Facility Location Problem</head><p>We formalize our optimization problem, by restating it as an uncapacitated K-facility location problem (UKFLP). The UKFLP is a long studied discrete optimization task (see Sec. 2). In the UKFLP task we are asked to "open" K "facilities" out of a larger set of sites W q , and assign each "client" to one of the K facilities, such that the sum of distances between the "clients" and their assigned "facilities" is minimal. In our case, the clients are the image embeddings v 1 , v 2 ..v N I , which are assigned to a set of K concept embeddings selected from the complete concept-bank W q . We look to optimize an assignment variable x ij ? {0, 1} indicating whether the "client" v i is assigned to the "facility" u j . We also use a variable y j ? {0, 1} to determine if a facility was opened in site j (if the concept u j is the center of a group). The optimal assignment should minimize the sum squared distance between each image and its assigned concept. The squared distance between image v i and concept u j is denoted d ij . We can now restate our loss as:</p><formula xml:id="formula_4">min xij ,yj i?1..N,j?1..N W d ij x ij s.t. ?i ? 1..N : j?1..N W x ij = 1 x ij ? y j j?1..N W y j ? K<label>(4)</label></formula><p>Where the bottom two constraints limit the number of concepts to at most K. Solving UKFLP is NP-hard, and the problem of approximation algorithms for UKFLP have been studied extensively both in terms of complexity and approximation ratio guarantees (see Sec.2). Yet, as the distance matrix d ij is very large, we could not run the existing solutions at the scale of many datasets (e.g. there may be as many as 82k concepts-"facilities" and a few hundred thousands images-"clients"). We therefore suggest a relaxed version of the popular Local Search algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Local Search algorithm</head><p>The Local Search algorithm <ref type="bibr" target="#b26">[27]</ref> is an effective, established method for solving facility location problems. Instead of looking for the optimal assignment at once, it looks for swaps between open and closed facilities that decrease the loss. It starts with "forward greedy" initialization: in the first K steps, we open the new facility (choose a new concept embedding as a group center) that minimizes the loss the most, among all unopened sites (unselected concepts). After initialization, we iteratively perform the following procedure: In each step, we look to swap r of our selected concepts by r unselected concepts, such that the loss is decreased. If such concepts are found, the swap is applied. We repeat this step until better swaps cannot be found or the maximal number of iterations is reached, making it slow to run even for a small dataset. With r = 1 this is also known as the Partitioning Around Medoids (PAM) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Local Search Location Relaxation Method</head><p>As our task is very high-dimensional, running Local Search (or similar UKFLP algorithms) becomes too slow to be practical. Therefore, we suggest an alternative inspired by the Expectation Minimization (EM) algorithm for K-means <ref type="bibr" target="#b55">[57]</ref>. We look for concepts {p 1 ..p K } nearest to the center of each group. This relaxation approach of searching concepts in continuous Euclidean space is much faster to compute (with complexity O(N I + N W )). Our method is initialized with a set of centers {c 1 ..c K } and iterates the following steps until convergence: (i) We assign each of our images v 1 ..v N to groups {S 1 ..S K } according to the nearest group center ("Voronoi tessellation")</p><formula xml:id="formula_5">S k = {v i | v i ? c k 2 ? v i ? c k 2 , ?k}<label>(5)</label></formula><p>(ii) After assignment, the center locations {c 1 ..c K } are set again to be the average feature in each group, which minimizes the WCSS (Eq.3) loss without the constraint. Precisely, we recompute each group center according to the image assignment S k : c j = 1 |Sj | v?Sj v. However, this is an infeasible solution as group centers will generally not be in W q . (iii) We therefore replace each group center c j with its nearest neighbor concept in p j ? W q . The result of this step is a new set of K principal concepts p 1 ..p K that form the group centers. Similarly to the swap in the Local Search algorithm, we only use the new centers if they obtain a smaller loss function. If no loss decreasing swap is found, we terminate.</p><p>Empty and excessively large group. In some cases, the discrete constraint results in a large proportion of images assigned to a single concept p k , or one of our groups S k being empty of samples. In the case of an empty group, we replace the center location with that of a concept which would attract most samples. Specifically, we choose the concept from W q that has the most samples as its nearest neighbors (among concepts not already in use). We also wish to address the problem of excessively large groups, that contain more than twice the average number of samples. In that case, we assign the samples in that group among all the concepts in W q (by distance). We find the concept embedding  that was chosen by the largest number of images from that excessively large group, and assign it as the new group center. Images are than reassigned between the new K concepts.</p><p>Initialization. We initialize the group assignments using Ward's clustering on the image embeddings v 1 ..v N I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate our method on several grouping tasks. We demonstrate that: (i) our method can identify concepts that are closely aligned with the ground truth names (ii) it can achieve high grouping accuracy.</p><p>Standard coarse-object-category datasets. To demonstrate the effectiveness of our method with generic conceptbanks, we evaluate our method on 3 standard datasets featuring different coarse-grained objects. We use Cifar10 <ref type="bibr" target="#b56">[58]</ref>,Cifar20 <ref type="bibr" target="#b56">[58]</ref>, and STL-10 <ref type="bibr" target="#b57">[59]</ref>.</p><p>Special attribute datasets. We hypothesize that the concept-bank guidance is critical when the grouping attribute is not the single coarse-grained category of the largest object in the image. To evaluate this, we performed experiments on the following datasets: Stanford Activity <ref type="bibr" target="#b3">[4]</ref>. A dataset presenting people performing 40 different activities ('fixing a bike', 'fixing a car', 'riding a horse', etc...). The images are of very high intra-class variability (see <ref type="figure">Fig.3</ref>). People Playing Musical Instrument (PPMI) <ref type="bibr" target="#b58">[60]</ref>. A dataset of people interacting with 12 different musical instruments (we use the PPMI+ version). Imagenet-Dog dataset <ref type="bibr" target="#b59">[61]</ref> A subset of 15 classes of the Imagenet dataset, featuring different fine-grained dogs species. All-Age-Faces dataset <ref type="bibr" target="#b60">[62]</ref>. A dataset containing over 82K images of human faces. The ground truth annotation of each person's age (between 2 and 80) is available. For evaluation, we split that dataset to five age groups in even intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept-banks.</head><p>For all the standard object datasets we use all nouns in the WordNet dataset. This demonstrates that a large generic concept-bank can be used successfully for multiple datasets. For the special attribute datasets we retrieve concept-banks from a "Vocabulary Word Lists" online resource <ref type="bibr">[55]</ref>. We use the "Verbs" list for the Stanford Activity data, the "Musical Instruments" list from PPMI and the "Dogs" list for Imagenet-Dog. Lastly, we demonstrate that such a list can also be composed with a set of possible numerical values; for the All-Age-Faces dataset, we provide a list of all human ages between 0 and 90 years. Further details can be found in the appendix.</p><p>Baselines. We evaluated our method against methods representing zero-shot learning and unsupervised clustering:</p><p>ZS-Naive. We apply CLIP zero-shot classification between the entire list of concepts and all dataset images. We reassign each image to its closest concept. We choose the K concepts to which most images were assigned as the principal concepts. We also perform zero-shot classification for each image using CLIP with the K principal concepts as a baseline for our image-grouping downstream task.</p><p>As further baselines to the image-grouping downstream task, we used the following methods: PT Only. Classical Ward's clustering with the CLIP image encoder for image feature extraction but without the concept priors. PT+SCAN. To allow adaptation of CLIP's pretrained features, we evaluate the image clustering method SCAN <ref type="bibr" target="#b6">[7]</ref> initialized with CLIP's pretrained visual features. The pretrained features are both used for selecting the neighbors in the first stage, and as the initialization of the second stage. When the original SCAN results were better, we used them instead.</p><p>Metrics. We evaluate the K principal concepts by computing the WordNet path similarity <ref type="bibr" target="#b61">[63]</ref> between each retrieved concept and each ground truth concept (class name). We find the optimal assignment between the retrieved and ground truth concepts (using bipartite matching) and report the total similarity between the sets. We only evaluate datasets that use the WordNet noun concept-bank, as other concepts are often not found in a the same WordNet hierarchy.</p><p>To evaluate the downstream grouping performance, we use standard clustering metrics: accuracy (ACC), normalized mutual information (NMI) and adjusted Rand index (ARI).</p><p>K principal concepts results. We can see in Tab.2 that our method finds principal concepts that are significantly closer to the groundtruth than those found by the ZS-Naive baseline. This is also visually shown for Cifar10 in Tab.1(tables for Stl10 and Cifar20 can be found in the supplementary). Our method generally finds appropriate concepts. ZS-Naive often chooses many concepts belonging to the same class. E.g. 'chukker', 'vaulting horse' and 'Seattle Slew' are all associated with the class horse, while no concept is assigned to the equally sized class frog. With our optimized loss function, additional concepts describe new degrees of variation, instead of repeating concepts associated with ones that were already selected (e.g. horse).</p><p>Image grouping results. The results presented in Tab.3-4, demonstrate the performance of our method in grouping unlabelled images. Our method is able to utilize the language guidance for better grouping performance. We achieve strong results both on the standard object datasets and on the special attribute datasets. While the methods which rely only on CLIP's visual features (PT Only and PT+SCAN) show some image grouping capabilities, we see that the concept-bank guidance provides significant improvement on top of the "visual only" baselines. The gap is especially significant in the attribute classification datasets, where visual only features are often misleading with respect to the ground truth groups (Tab.4). On the most standard smaller datasets (Tab.3), we find that the visual only baselines are strong as the groups are usually defined by a single coarse grained category. The long WordNet concept-bank used is especially problematic for the ZS-Naive baseline, which utilizes the same supervision as our method. This is because a naive use of the concept-bank can select related concepts (e.g. 'chukker' and 'horse') splitting single categories into multiple groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Filtering the concept-bank. Before running the algorithm, we filter out concepts whose "generality" score is above some quantile q, as mentioned in Sec.3.3. We show the effectiveness of our filtering method (without adapter) in <ref type="figure" target="#fig_1">Fig. 4</ref>.</p><p>Ground truth group name retrieval. Our retrieved principal concepts are similar to, but not exactly the same as the groundtruth name (Tab.1). Our post-processing phase (Sec.3.5) finds very similar concepts. Yet, different concepts ('milk float' &amp; and 'Jowett') might be mapped into the same high-level concept ('vehicle').</p><p>Facility location optimization methods. As explained in Sec.4.3, our optimization method can be viewed as a relaxed version of the Local Search algorithm. In the appendix we report metrics suggesting that both methods can effectively optimize the objective. Conversely, PAM is much slower than our method. For large dataset and larger concept-banks the time complexity of PAM is infeasible and significantly greater than that of our relaxed version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>Dependence on the multimodal embedding model. Our experiments indicated that grouping using the groundtruth class names achieves strong results. Yet, our grouping performance is often limited even when we retrieve concepts very close to the true group names. It happens as the joint image-language embedding space is still not perfectly accurate. This highlights a limitation of our approach, its sensitivity to the quality of the multimodal joint embedding space. We expect this to improve as multimodal embedding methods improve.</p><p>No approximation guarantees of our solution to the UKFLP. Our approach for UKFLP achieves strong results while being much faster than current methods despite its simplicity. Its main limitation is lack of theoretical guarantees. Their derivation is left for future work. Increased supervision over unsupervised clustering. Our approach requires humans to specify a single grouping attribute to derive the concept-bank. While the effort required is minor, we argue such supervision is necessary to resolve the natural ambiguity of grouping ( <ref type="figure">Fig.3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed a task of identifying K principal components in a dataset. We reduced the task to the well-studied, uncapacitated K-facility location problem. To solve it with acceptable runtime, we suggested an efficient optimization method. Our approach is able to recover concepts very similar to the ground truth class names and accurate image grouping. We also showed that our method can provide task-specific inductive priors by concept-bank selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A CLIP Adapter</head><p>At the end of the optimization process aimed to find the K principal concepts {c 1 ..c k } we also wish to obtain a final grouping of our image dataset. Our discovered concepts can be used to group the dataset images into the K concepts by assigning each image to its nearest concept by cosine similarity. We found that the grouping accuracy can be improved by further adaptation of the the features. Specifically, we train an adapter on top of CLIP <ref type="bibr" target="#b4">[5]</ref> using our discovered concepts and the image grouping achieved by our method. We follow the implementation of Tip-Adapter <ref type="bibr" target="#b54">[56]</ref>. In this method, each image is classified by a combination of CLIP's zero-shot classification and a simple few-shot learning technique. We use Tip-Adapter with the discovered K concepts, and for each of them 64 images randomly selected from the corresponding groups {S 1 ..S k } found at the end of the WCSS optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B No Adapter Results:</head><p>As an ablation, we report here the results of our method without the adapter stage using the grouping labels and K principal concepts found by our method (Tab. <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6)</ref>. We can see that our adapter stage consistently provide an additional boost to our performance. We can further see our performance sometime approaches the performance of the "ground-truth" principal concepts (class names), which is an approximate upper bound for our method without the adapter stage. We report both without adapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details:</head><p>Metrics: For the similarity measure between the retrieved K principal concepts and the ground truth class names, we use the WordNet path similarity <ref type="bibr" target="#b61">[63]</ref>. We begin by calculating the similarity between each retrieved concepts and each class name to get an N ? N matrix. We then solve the optimal assignment between the concepts and class names <ref type="bibr" target="#b62">[64]</ref>. We report the sum of similarities between each concept and its assigned class name as the total similarity.</p><p>For the NMI and accuracy score we used the code 1 provided by Shiran et. al. <ref type="bibr" target="#b37">[38]</ref>. For the ARI score, we use the adjusted_rand_score function from scikit-learn library <ref type="bibr" target="#b63">[65]</ref>.</p><p>Nearest neighbours retrieval: For nearest neighbours we used faiss library <ref type="bibr" target="#b64">[66]</ref>.</p><p>Grouping initialization: For Ward's agglomerative clustering we use scikit-learn library <ref type="bibr" target="#b63">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Validation:</head><p>We designed the algorithm using the Cifar10 dataset. Parameters were kept the same for the other datasets. For the number of concpets used for abstraction (M ) we tried two values: {25, 50}. We note that the q parameter (filtering level of the concept bank) is chosen for each run using an unsupervised criterion.</p><p>Our method results does not depend on a random seed. The adapter stage does depend on a random seed, but the variation in the final results is very small (? 0.1% accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Computational Resource:</head><p>Feature extractions should be once for each dataset and concept banks, and takes less than an hour using a GeForce RTX 2080 TI GPU. Wards clustring can take up to on 1-hour, but can be done on a subset of the data in 1 minute without degradation in the final results. Our K concepts retrieval method takes 1 minute to run on each dataset (CPU only). Using the Tip-Adapter as described takes 5 minutes using a GeForce RTX 2080 TI GPU. All code was run using Python3 on MATE Desktop Environment. We used numpy version 1.19.5 and scikit-learn version 0.24.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Detailed Concept-Banks:</head><p>We use the prompt "A photo showing" to embed each of the words in each of the lists used to built the concept-bank.</p><p>As explained in the main text, we used the WordNet nouns list for standard coarse-object-category datasets, and took all of our other lists for the special attribute datasets from a single resource. For completeness, we bring the entire lists below.  We note that our method performs well even when the list is not well curated. For example, the "Dogs" list contains words such as "bark", or "puppy", which is not specific to any single dog species.</p><p>"Activities" Concept-Bank: We used the following list: </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>System diagram. Our method takes as input an image dataset and a list of concepts (here a list of activities).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Grouping accuracy (?), Entropy (+) vs. filtering level (Cifar10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Glossary of CIFAR10 class names and assigned concepts for our method and ZS-Naive (ZSN). PP notes the post-processed version of the concepts. Full table can be found in the appendix.</figDesc><table><row><cell>Ground Truth</cell><cell>horse</cell><cell>frog</cell><cell>bird</cell><cell>dog</cell><cell>automobile</cell><cell>ship</cell><cell>truck</cell></row><row><cell>Ours</cell><cell cols="4">chukker southwestern toad policeman bird maltese dog</cell><cell>Jowett</cell><cell cols="2">pilot boat milk float</cell></row><row><cell>PP</cell><cell>equine</cell><cell>frog</cell><cell>bird</cell><cell>dog</cell><cell>vehicle</cell><cell>ship</cell><cell>vehicle</cell></row><row><cell>ZSN</cell><cell>chukker</cell><cell>vaulting horse</cell><cell>Seattle Slew</cell><cell cols="2">bumper car tachograph</cell><cell>Jowett</cell><cell>milk float</cell></row><row><cell>PP</cell><cell>horse</cell><cell>horse</cell><cell>ungulate</cell><cell>driver</cell><cell>container</cell><cell>fastness</cell><cell>truck</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Path similarity of retrieved word and the ground truth class names</figDesc><table><row><cell></cell><cell cols="3">CIFAR-10 CIFAR-20 STL-10</cell></row><row><cell>ZS-Naive</cell><cell>3.90</cell><cell>4.13</cell><cell>4.72</cell></row><row><cell>Ours</cell><cell>6.07</cell><cell>4.36</cell><cell>5.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Grouping of Standard Classification Datasets (%)</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell>CIFAR-20</cell><cell></cell><cell></cell><cell>STL-10</cell></row><row><cell></cell><cell cols="6">ACC NMI ARI ACC NMI ARI ACC NMI ARI</cell></row><row><cell>PT Only</cell><cell>73.4</cell><cell>66.9 56.5 41.0</cell><cell cols="3">45.2 24.0 92.1</cell><cell>89.4 85.5</cell></row><row><cell cols="2">PT+SCAN 87.6</cell><cell>78.7 75.8 46.7</cell><cell cols="3">45.8 40.7 98.3</cell><cell>95.8 96.4</cell></row><row><cell>ZS-Naive</cell><cell>49.9</cell><cell>48.5 26.0 20.8</cell><cell>25.0</cell><cell>4.3</cell><cell>56.6</cell><cell>56.9 40.2</cell></row><row><cell>Ours</cell><cell>93.4</cell><cell>85.9 86.1 48.4</cell><cell cols="3">51.5 34.3 97.9</cell><cell>95.2 95.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Grouping of Attribute Classification Datasets (%)</figDesc><table><row><cell></cell><cell cols="2">Stanford Activity</cell><cell cols="2">Imagenet-Dogs</cell><cell cols="2">All-Age-Faces</cell><cell>PPMI</cell></row><row><cell></cell><cell cols="6">ACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI</cell></row><row><cell>PT Only</cell><cell>61.4</cell><cell cols="2">66.4 49.0 36.5</cell><cell cols="2">38.0 20.3 47.5</cell><cell>28.6 19.7 34.2</cell><cell>26.5 15.8</cell></row><row><cell cols="2">PT+SCAN 54.0</cell><cell cols="2">66.0 45.3 49.3</cell><cell cols="2">54.4 36.9 48.8</cell><cell>33.4 20.2 27.5</cell><cell>24.1 12.3</cell></row><row><cell>ZS-Naive</cell><cell>49.8</cell><cell cols="2">58.8 35.6 60.9</cell><cell cols="2">65.1 47.8 50.6</cell><cell>38.7 24.2 37.9</cell><cell>38.7 19.2</cell></row><row><cell>Ours</cell><cell>64.9</cell><cell cols="2">70.6 56.4 69.1</cell><cell cols="2">73.4 59.2 55.4</cell><cell>40.6 27.4 49.0</cell><cell>45.1 30.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Grouping of Standard Classification Datasets (%)</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell>CIFAR-20</cell><cell>STL-10</cell></row><row><cell></cell><cell cols="4">ACC NMI ARI ACC NMI ARI ACC NMI ARI</cell></row><row><cell cols="2">No Adapter 85.2</cell><cell>73.3 70.1 40.8</cell><cell>44.5 24.3 96.2</cell><cell>91.8 91.9</cell></row><row><cell>Ours</cell><cell>93.4</cell><cell>85.9 86.1 48.4</cell><cell>51.5 34.3 97.9</cell><cell>95.2 95.4</cell></row><row><cell>ZS-GT</cell><cell>86.7</cell><cell>75.0 73.1 54.1</cell><cell>48.8 32.8 95.9</cell><cell>91.9 91.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Grouping of Attribute Classification Datasets (%)</figDesc><table><row><cell></cell><cell cols="2">Stanford Activity</cell><cell cols="2">Imagenet-Dogs</cell><cell cols="2">All-Age-Faces</cell><cell>PPMI</cell></row><row><cell></cell><cell cols="6">ACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI</cell></row><row><cell cols="2">No Adapter 63.0</cell><cell cols="2">67.9 52.8 64.5</cell><cell cols="2">65.9 50.8 56.1</cell><cell>38.3 26.5 49.0</cell><cell>41.1 29.6</cell></row><row><cell>Ours</cell><cell>64.9</cell><cell cols="2">70.6 56.4 69.1</cell><cell cols="2">73.4 59.2 55.4</cell><cell>40.6 27.4 49.0</cell><cell>45.1 30.2</cell></row><row><cell>ZS-GT</cell><cell>82.8</cell><cell cols="2">80.1 72.0 68.4</cell><cell cols="2">70.2 54.8 60.7</cell><cell>40.8 30.3 54.5</cell><cell>44.5 33.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>{accept, ache, acknowledge, act, add, admire, admit, admonish, adopt, advise, affirm, afford, agree, ail, alert, allege, allow, allude, amuse, analyze, announce, annoy, answer, apologize, appeal, appear, applaud, appreciate, approve, argue, arrange, arrest, arrive, articulate, ask, assert, assure, attach, attack, attempt, attend, attract, auction, avoid, avow, awake, babble, back, bake, balance, balk, ban, bandage, bang, bar, bare, bargain, bark, barrage, barter, baste, bat, bathe, battle, bawl, be, beam, bear, beat, become, befriend, beg, begin, behave, believe, bellow, belong, bend, berate, besiege, bestow, bet, bid, bite, bleach, bleed, bless, blind, blink, blot, blow, blurt, blush, boast, bob, boil, bolt, bomb, book, bore, borrow, bounce, bow, box, brag, brake, branch, brand, break, breathe, breed, bring, broadcast, broil, bruise, brush, bubble, build, bump, burn, burnish, bury, buy, buzz, cajole, calculate, call, camp, care, carry, carve, catch, cause, caution, challenge, change, chant, charge, chase, cheat, check, cheer, chew, chide, chip, choke, chomp, choose, chop, claim, clap, clean, clear, climb, clip, close, coach, coil, collect, color, comb, come, comfort, command, comment, communicate, compare, compete, complain, complete, concede, concentrate, concern, conclude, concur, confess, confide, confirm, connect, consent, consider, consist, contain, contend, continue, cook, copy, correct, cost, cough, count, counter, cover, covet, crack, crash, crave, crawl, criticize, croak, crochet, cross, cross-examine, crowd, crush, cry, cure, curl, curse, curve, cut, cycle, dam, damage, dance, dare, deal, debate, decay, deceive, decide, decipher, declare, decorate, delay, delight, deliver, demand, deny, depend, describe, desert, deserve, desire, deter, develop, dial, dictate, die, dig, digress, direct, disclose, dislike, dive, divide, divorce, divulge, do, dock, dole, dote, double, doubt, drag, drain, draw, dream, dress, drill, drink, drip, drive, drone, drop, drown, dry, dump, dupe, dust, dye, earn, eat, echo, edit, educate, elope, embarrass, emigrate, emit, emphasize, employ, empty, enchant, encode, encourage, end, enjoin, enjoy, enter, entertain, enunciate, envy, equivocate, escape, evacuate, evaporate, exaggerate, examine, excite, exclaim, excuse, exercise, exhort, exist, expand, expect, expel, explain, explode, explore, extend, extoll, face, fade, fail, fall, falter, fasten, favor, fax, fear, feed, feel, fence, fetch, fight, file, fill, film, find, fire, fish, fit, fix, flap, flash, flee, float, flood, floss, flow, flower, fly, fold, follow, fool, force, foretell, forget, forgive, form, found, frame, freeze, fret, frighten, fry, fume, garden, gasp, gather, gaze, gel, get, gild, give, glide, glue, gnaw, go, grab, grate, grease, greet, grill, grin, grip, groan, grow, growl, grumble, grunt, guarantee, guard, guess, guide, gurgle, gush, hail, hammer, hand, handle, hang, happen, harass, harm, harness, hate, haunt, have, head, heal, heap, hear, heat, help, hide, highlight, hijack, hinder, hint, hiss, hit, hold, hook, hoot, hop, hope, hover, howl, hug, hum, hunt, hurry, hurt, ice, identify, ignore, imagine, immigrate, implore, imply, impress, improve, include, increase, infect, inflate, influence, inform, infuse, inject, injure, inquire, insist, inspect, inspire, instruct, intend, interest, interfere, interject, interrupt, introduce, invent, invest, invite, iron, irritate, itch, jab, jabber, jail, jam, jeer, jest, jog, join, joke, jolt, judge, juggle, jump, keep, kick, kill, kiss, kneel, knit, knock, knot, know, label, lament, land, last, laugh, lay, lead, lean, learn, leave, lecture, lend, let, level, license, lick, lie, lift, light, lighten, like, list, listen, live, load, loan, lock, long, look, loosen, lose, love, lower, mail, maintain, make, man, manage, mar, march, mark, marry, marvel, mate, matter, mean, measure, meet, melt, memorize, mend, mention, merge, milk, mine, miss, mix, moan, molt, moor, mourn, move, mow, mug, multiply, mumble, murder, mutter, nag, nail, name, nap, need, nest, nod, note, notice, number, obey, object, observe, obtain, occur, offend, offer, ogle, oil, omit, open, operate, order, overflow, overrun, owe, own, pack, pad, paddle, paint, pant, park, part, pass, paste, pat, pause, pay, peck, pedal, peel, peep, peer, peg, pelt, perform, permit, pester, pet, phone,</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/guysrn/mmdc/blob/main/utils/metrics.py</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image corpus representative summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshay</forename><surname>Virmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Subramanyam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint summarization of large-scale collections of web images and videos for storyline reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4225" to="4232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">Lai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1331" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The uncapicitated facility location problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Cornu?jols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Wolsey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
		<respStmt>
			<orgName>Cornell University Operations Research and Industrial Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Category-specific video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danila</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="540" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating visual story graphs with application to photo album summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goksu</forename><surname>Bora Celikkale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erdogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">116033</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Aykut Erdem, and Erkut Erdem</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph-based multimodal ranking models for multimodal summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Asian and Low-Resource Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Concept bottleneck models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thao</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yew</forename><forename type="middle">Siang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5338" to="5348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Concept whitening for interpretable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="772" to="782" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Losch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10882</idno>
		<title level="m">Interpretability beyond classification output: Semantic bottleneck networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive image-language cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What are you talking about? text-toimage coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured matching for phrase localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Azab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriyuki</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="696" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image-to-word transformation based on dividing and vector quantizing images with words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hironobu</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Oka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First international workshop on multimedia intelligent storage and retrieval management</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning visual representations using images with captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="67" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Bulent Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01392</idno>
		<title level="m">Learning visual representations with caption annotations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lit: Zero-shot transfer with locked-image text tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="18123" to="18133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Masked unsupervised self-training for zero-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02967</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A heuristic program for locating warehouses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alfred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Kuehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamburger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="643" to="666" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Greedy strikes back: Improved facility location algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipto</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Khuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of algorithms</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="228" to="248" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local search heuristics for k-median and facility location problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Khandekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamesh</forename><surname>Munagala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayaka</forename><surname>Pandit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="544" to="562" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Approximation algorithms for facility location problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?va</forename><surname>Shmoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Tardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aardal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-ninth annual ACM symposium on Theory of computing</title>
		<meeting>the twenty-ninth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vazirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="274" to="296" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new greedy approach for facility location problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Mahdian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Saberi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thiry-fourth annual ACM symposium on Theory of computing</title>
		<meeting>the thiry-fourth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="731" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shiming Xiang, and Chunhong Pan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
	<note>Deep adaptive image clustering</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Associative deep clustering: Training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep comprehensive correlation mining for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8150" to="8159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gatcluster: Self-supervised gaussian-attention network for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="735" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multi-modal deep clustering: Unsupervised partitioning of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Shiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02678</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mi{ce}: Mixture of contrastive experts for unsupervised image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Tsung Wei Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1558" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Nicholas Darlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08821</idno>
		<title level="m">Dhog: Deep hierarchical object grouping</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to discover novel visual categories via deep transfer clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8401" to="8409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Supervised clustering with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Combining pretrained cnn feature extractors to enhance clustering of complex natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Gu?rin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Thiery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Gibaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Boots</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page" from="551" to="571" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structure-aware face clustering on a large-scale graph with 107 nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9085" to="9094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dm2c: Deep mixedmodal clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangbangyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-modal image clustering via canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning color names for real-world applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1512" to="1523" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Beyond eleven color names for image understanding. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongmei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Alejandro</forename><surname>Parraga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="361" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A computational model for color naming and describing color composition of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Mojsilovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="690" to="699" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An overview of color name applications in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Computational Color Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Predominant color name indexing structure for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Prates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Cristianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Robson</forename><surname>Dutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.03930</idno>
		<title level="m">Tip-adapter: Training-free clip-adapter for better vision-language modeling</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The expectation-maximization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Todd K Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploiting effective facial patches for robust gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science and Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="345" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Michelizzi</surname></persName>
		</author>
		<title level="m">Similarity-measuring the relatedness of concepts. In AAAI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="25" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A shortest augmenting path algorithm for dense and sparse linear assignment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Jonker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Volgenant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="340" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python. the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">rule, run, rush, sack, sail, satisfy, save, savor, saw, say, scare, scatter, scoff, scold, scoot, scorch, scrape, scratch, scream, screech, screw, scribble, seal, search, see, sell, send, sense, separate, serve, set, settle, sever, sew, shade, shampoo, share, shave, shelter, shift, shiver, shock, shoot, shop, shout, show, shriek, shrug, shut, sigh, sign, signal, sin, sing, singe, sip, sit, skate, skateboard, sketch, ski, skip, slap, sleep, slice, slide, slip, slow, smash, smell, smile, smoke, snap, snarl, snatch, sneak, sneer, sneeze, snicker, sniff, snoop, snooze, snore, snort, snow, soak, sob, soothe, sound, sow, span, spare, spark, sparkle, speak, speculate, spell, spend, spill, spin, spoil, spot, spray, sprout, sputter, squash, squeeze, stab, stain, stammer, stamp, stand, star, stare, start, stash, state, stay, steer, step, stipulate, stir, stitch, stop, store, storm, stow, strap, stray, strengthen, stress, stretch, strip, stroke, strum, strut, stuff, stun, stunt, stutter, submerge, succeed, suffer, suggest, suit, supply, support, suppose, surmise, surprise, surround, suspect, suspend, sway, swear, swim, swing, switch, swoop, sympathize, take, talk, tame, tap, taste, taunt, teach, tear, tease, telephone, tell, tempt, terrify, test, testify, thank, thaw, theorize, think, threaten, throw, thunder, tick, tickle, tie, time, tip, tire, toast, toss, touch, tour, tow, trace, track, trade, train, translate, transport, trap, travel, treat, tremble, trick, trickle, trim, trip, trot, trouble, trounce, trust, try, tug, tumble, turn, twist, type, understand, undress, unfasten, unite, unlock, unpack, untie, uphold, upset, upstage, urge, use, usurp, utter, vacuum, value, vanish, vanquish, venture, visit, voice, volunteer, vote, vouch, wail, wait, wake, walk, wallow, wander, want, warm, warn, wash, waste, watch, water, wave, waver, wear, weave, wed, weigh, welcome, whimper, whine, whip, whirl, whisper, whistle, win, wink, wipe, wish, wobble, wonder, work, worry, wrap, wreck, wrestle, wriggle, write, writhe, x-ray, yawn, yell, yelp, yield, yodel, zip, zoom } &quot;Dogs&quot; Concept-Bank: We used the following list: { Afghan hound, African wild dog, Airedale terrier, akita, Alaskan malamute, American cocker spaniel, Australian cattle dog, bark, basenji, basset hound, beagle, bergamasco, bichon frise, bird dog, bloodhound, border collie, borzoi, Boston terrier, boxer, breed, briard, Brittany, bull terrier, bulldog, bullmastiff, cairn terrier, Cape hunting dog, chihuahua, Chinese crested dog, chow chow, cocker spaniel, collie, companion dog, coon hound, corgi, cur, dachshund, Dalmatian, dhole, dingo, Doberman pinscher, dog, elkhound, feist, fighting dog, fox terrier, foxhound, German shepherd, golden retriever, great Dane, great Pyrenees, greyhound, growl, guard dog, gun dogs, harrier, herding dog, hound, hunting dog, husky, Irish setter, Jack Russell terrier, keeshond, kerry blue terrier, King Charles spaniel, Labrador retriever, lap dog, Lhasa apso, malamute, Maltese, mastiff, Mexican hairless, miniature schnauzer, mongrel, mutt</title>
	</analytic>
	<monogr>
		<title level="m">radiate, rain, raise, rant, rate, rave, reach, read, realize, rebuff, recall, receive, recite, recognize, recommend, record, reduce, reflect, refuse, regret, reign, reiterate, reject, rejoice, relate, relax, release, rely, remain, remember, remind, remove, repair, repeat, replace, reply, report, reprimand, reproduce, request, rescue, retire, retort, return, reveal, reverse, rhyme, ride, ring, rinse, rise, risk, roar, rob, rock, roll, rot, row, rub, ruin</title>
		<meeting><address><addrLine>Newfoundland, Norfolk terrier</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Portuguese water dog, pug, pup, puppy, purebred, rat terrier, rescue dog, retriever, Rhodesian ridgeback, Rottweiler, Saluki, samoyed, scent hound, schnauzer, Scottish terrier, search-and-rescue dog, service dog, setter, Siberian husky, sighthound, sled dog, spaniel, spitz, springer spaniel, St. Bernard, terrier, toy dog, utonagan, vizsla, water dog, weimaraner, Welsh corgi, West Highland white terrier, Westie, wheaten terrier, whippet, wild dog, working dog, Yorkshire terrier } &quot;Musical Instruments&quot; Concept-Bank: We used the following list: { accordion, acoustic guitar, Aeolian harp, Alphorn, alto saxophone, anvil, baby grand piano, bagpipe, balalaika, bandoneon, bandura, banjo, baritone horn, bass, bass clarinet, bass drum, bass guitar, bassoon, bell, bongo drum, bouzouki, bow, brass instruments, bugle, calliope, carillon, castanets, celesta, cello, Celtic harp, chimes, cimbalom, clarinet, classical guitar, clavichord, clavier, concertina, conch, conga drum, contrabass, cornet, cowbell, cymbals, didgeridoo, double bass, drum, drumsticks, dulcimer, electric guitar, electric organ, English horn, euphonium, fiddle, fife, flugelhorn, flute, French horn, glockenspiel, gong, grand piano, guitar, hammered dulcimer, harmonica, harmonium, harp, harpsichord, helicon, horn, hurdy-gurdy, instrument, jaw harp, Jew&apos;s harp, kazoo, kettledrum, keyboard, lute, lyre, mallets, mandolin, maracas, marimba, mellophone, melodeon, Moog synthesizer, musical instruments, musical saw, mute, oboe, ocarina, organ, pan pipes, penny whistle, percussion, piano, piccolo, pipa, pipe organ, player piano. pump organ, rainstick, rattle, recorder, reed, saw, saxophone, sitar, slide whistle, snare drum, sousaphone, spinet, spoons, steel drum, steel guitar, string bass, string instruments, strings, synthesizer, tabla, tambourine, theremin, thumb piano, timpani, tin whistle, tom-tom drum, triangle, trombone, trumpet, tuba, tubular bells, U-V, ukulele, upright piano, valve, vibraphone, viola, viola da gamba, violin, violoncello, vuvuzela, Wagner tuba, washboard, whistle, wind chime, wind instruments, woodwind instruments, xylophone, zither } &quot;Human Ages List&quot; Concept-Bank: We used the phrase &quot;* years old person&quot;, where * takes any integer value between 1 and 90</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
