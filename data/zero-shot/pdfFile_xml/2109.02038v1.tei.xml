<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
							<email>zhoufengwei@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Noah&amp;apos;</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ark</forename><surname>Lab</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanqing</forename><surname>Hong</surname></persName>
							<email>honglanqing@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Noah&amp;apos;</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ark</forename><surname>Lab</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><forename type="middle">Gary</forename><surname>Chan</surname></persName>
							<email>gchan@cse.ust.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<email>li.zhenguo@huawei.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances on Out-of-Distribution (OoD) generalization reveal the robustness of deep learning models against distribution shifts. However, existing works focus on OoD algorithms, such as invariant risk minimization, domain generalization, or stable learning, without considering the influence of deep model architectures on OoD generalization, which may lead to sub-optimal performance. Neural Architecture Search (NAS) methods search for architecture based on its performance on the training data, which may result in poor generalization for OoD tasks. In this work, we propose robust Neural Architecture Search for OoD generalization (NAS-OoD), which optimizes the architecture with respect to its performance on generated OoD data by gradient descent. Specifically, a data generator is learned to synthesize OoD data by maximizing losses computed by different neural architectures, while the goal for architecture search is to find the optimal architecture parameters that minimize the synthetic OoD data losses. The data generator and the neural architecture are jointly optimized in an end-to-end manner, and the minimax training process effectively discovers robust architectures that generalize well for different distribution shifts. Extensive experimental results show that NAS-OoD achieves superior performance on various OoD generalization benchmarks with deep models having a much fewer number of parameters. In addition, on a real industry dataset, the proposed NAS-OoD method reduces the error rate by more than 70% compared with the state-of-the-art method, demonstrating the proposed method's practicality for real applications. * Nanyang Ye is the corresponding author. <ref type="figure">Figure 1</ref>. NAS-OoD performs significantly better than existing OoD generalization baselines in terms of test accuracy and network parameter numbers. The upper left points are better than lower right ones because they have higher test accuracy and lower parameter numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Neural Architecture Search</head><p>EfficientNet [40] proposes a new scaling method that uniformly scales all dimensions of depth, width, and resolution via an effective compound coefficient. EfficientNet design a new baseline which achieves much better accuracy and efficiency than previous ConvNets. One-shot NAS <ref type="bibr" target="#b5">[6]</ref> </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning models have encountered significant performance drop in Out-of-Distribution (OoD) scenarios <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, where test data come from a distribution different from that of the training data. With their growing use in realworld applications in which mismatches of test and training data distributions are often observed <ref type="bibr" target="#b24">[25]</ref>, extensive efforts have been devoted to improving generalization ability <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b4">5]</ref>. Risk regularization methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b40">41]</ref> aim to learn invariant representations across different training environments by imposing different invariant risk regularization. Domain generalization methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b47">48]</ref> learn models from multiple domains such that they can generalize well to unseen domains. Stable learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b19">20]</ref> focuses on identifying stable and causal features for predictions. Existing works, however, seldom consider the effects of architectures on generalization ability. On the other hand, some pioneer works suggest that different architectures show varying OoD generalization abilities <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref>. How a network's architecture affects its ability to handle OoD distribution shifts is still an open problem.</p><p>Conventional Neural Architecture Search (NAS) methods search for architectures with maximal predictive performance on the validation data that are randomly divided from the training data <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>. The discovered architectures are supposed to perform well on unseen test data under the assumption that data are Independent and Identically Distributed (IID). While novel architectures discovered by recent NAS methods have demonstrated superior performance on different tasks with the IID assumption <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24]</ref>, they may suffer from over-fitting in OoD scenarios, where the test data come from another distribution. A proper validation set that can evaluate the performance of architectures on the test data with distribution shifts is crucial in OoD scenarios.</p><p>In this paper, we propose robust NAS for OoD generalization (NAS-OoD) that searches architectures with maximal predictive performance on OoD examples generated by a conditional generator. An overview of the proposed method is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. To do NAS and train an OoD model simultaneously, we follow the line of gradientbased methods for NAS <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b43">44]</ref>, however, we extend that on several fronts. The discrete selection of architectures is relaxed to be differentiable by building all candidate architectures into a supernet with parameter sharing and adopting a softmax choice over all possible network operations. The goal for architecture search is to find the optimal architecture parameters that minimize the validation loss under the condition that the corresponding network parameters minimize the training loss.</p><p>Instead of using part of the training set as the validation set, we train a conditional generator to map the original training data to synthetic OoD examples as the validation data. The parameters of the generator are updated to maximize the validation loss computed by the supernet. This update encourages the generator to synthesize data having a different distribution from the original training data since the supernet is optimized to minimize the error on the training data. To search for the architectures with optimal OoD generalization ability, the architecture parameters are optimized to minimize the loss on the validation set containing synthetic OoD data. This minimax training process effectively drives both the generator and architecture search to improve their performance and finally derive the robust architectures that perform well for OoD generalization.</p><p>Our main contributions can be summarized as follows:</p><p>1. To the best of our knowledge, NAS-OoD is the first attempt to introduce NAS for OoD generalization, where a conditional generator is jointly optimized to synthe-size OoD examples helping to correct the supervisory signal for architecture search.</p><p>2. NAS-OoD gets the optimal architecture and all optimized parameters in a single run. The minimax training process effectively discovers robust architectures that generalize well for different distribution shifts.</p><p>3. We take the first step to understanding the OoD generalization of neural network architectures systematically. We provide a statistical analysis of the searched architectures and our preliminary practice shows that architecture does influence OoD robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Extensive experimental results</head><p>show that NAS-OoD outperforms the previous SOTA methods and achieves the best overall OoD generalization performance on various types of OoD tasks with the discovered architectures having a much fewer number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Out-of-Distribution Generalization</head><p>Data distribution mismatches between training and testing set exist in many real-world scenes. Different methods have been developed to tackle OoD shifts. IRM <ref type="bibr" target="#b2">[3]</ref> targets to extract invariant representation from different environments via an invariant risk regularization. IRM-Games <ref type="bibr" target="#b0">[1]</ref> aims to achieve the Nash equilibrium among multiple environments to find invariants based on ensemble methods. REx <ref type="bibr" target="#b25">[26]</ref> proposes a min-max procedure to deal with the worst linear combination of risks across different environments. MASF <ref type="bibr" target="#b14">[15]</ref> adopts a framework to learn invariant features among domains. JiGen <ref type="bibr" target="#b8">[9]</ref> jointly classifies objects and solves unsupervised jigsaw tasks. CuMix <ref type="bibr" target="#b34">[35]</ref> aims to recognize unseen categories in unseen domains through a curriculum procedure to mix up data and labels from different domains. DecAug <ref type="bibr" target="#b4">[5]</ref> proposes a decomposed feature representation and semantic augmentation approach to address diversity and distribution shifts jointly. The work of <ref type="bibr" target="#b20">[21]</ref> finds that using pre-training can improve model robustness and uncertainty. However, existing OoD generalization approaches seldom consider the effects of architecture which leads to suboptimal performances. In this work, we propose NAS-OoD, a robust network architecture search method for OoD generalization. discusses the weight sharing scheme for one-shot architecture search and shows that it is possible to identify promising architectures without either hypernetworks or RL efficiently. DARTS <ref type="bibr" target="#b33">[34]</ref> presents a differentiable manner to deal with the scalability challenge of architecture search. ISTA-NAS <ref type="bibr" target="#b43">[44]</ref> formulates neural architecture search as a sparse coding problem. In this way, the network in search satisfies the sparsity constraint at each update and is efficient to train. SNAS <ref type="bibr" target="#b41">[42]</ref> reformulates NAS as an optimization problem on parameters of a joint distribution for the search space in a cell. DSNAS <ref type="bibr" target="#b22">[23]</ref> proposes an efficient NAS framework that simultaneously optimizes architecture and parameters with a low-biased Monte Carlo estimate. NASDA <ref type="bibr" target="#b32">[33]</ref> leverages a principle framework that uses differentiable neural architecture search to derive optimal network architecture for domain adaptation tasks. NADS <ref type="bibr" target="#b1">[2]</ref> learns a posterior distribution on the architecture search space to enable uncertainty quantification for better OoD detection and aims to spot anomalous samples. The work <ref type="bibr" target="#b9">[10]</ref> uses a robust loss to mitigate the performance degradation under symmetric label noise. However, NAS overfits easily, the work <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b18">19]</ref> points out that NAS evaluation is frustratingly hard. Thus, it is highly non-trivial to extend existing NAS algorithms to the OoD setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Robustness from Architecture Perspective</head><p>Recent studies show that different architectures present different generalization abilities. The work of <ref type="bibr" target="#b45">[46]</ref> uses a functional modular probing method to analyze deep model structures under the OoD setting. The work <ref type="bibr" target="#b21">[22]</ref> examines and shows that pre-trained transformers achieve not only high accuracy on in-distribution examples but also improvement of out-of-distribution robustness. The work <ref type="bibr" target="#b11">[12]</ref> presents CNN models with neural hidden layers that better simulate the primary visual cortex improve robustness against image perturbations. The work <ref type="bibr" target="#b13">[14]</ref> uses a pure transformer applied directly to sequences of image patches, which performs quite well on image classification tasks compared with relying on CNNs. The work of <ref type="bibr" target="#b12">[13]</ref> targets to improve the adversarial robustness of the network with NAS and achieves superior performance under various attacks. However, they do not consider OoD generalization from the architecture perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first introduce preliminaries on conventional NAS and their limitations in OoD scenarios (Section 3.1). Then, we describe the details of our robust Neural Architecture Search for OoD generalization (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries on Differentiable Neural Architecture Search</head><p>Conventional NAS methods mainly search for computation cells as the building units to construct a network <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b22">23]</ref>. The search space of a cell is defined as a directed acyclic graph with n ordered nodes {z 1 , z 2 , ..., z n } and edges ? = {e i,j |1 ? i &lt; j ? n}. Each edge includes m candidate network operations chosen from a predefined operation space O = {o 1 , o 2 , ..., o m }, such as maxpooling, identity and dilated convolution. The binary variable s (i,j) k ? {0, 1} denotes the corresponding active connection. Thus, the node can be formed as:</p><formula xml:id="formula_0">z j = j?1 i=1 m k=1 s (i,j) k o k (z i ) = s T j o j ,<label>(1)</label></formula><p>where s j is the vector consists of s (i,j) k and o j denotes the vector formed by o k (z i ). As the binary architecture variables s j is hard to optimize in a differentiable way, recent DARTS-based NAS methods make use of the continuous relaxation in the form of</p><formula xml:id="formula_1">s (i,j) k = exp(? (i,j) k )/ k exp(? (i,j) k ),<label>(2)</label></formula><p>and optimize ? (i,j) k as trainable architecture parameters <ref type="bibr" target="#b33">[34]</ref>, which can be formulated as the following bilevel optimization problem:</p><formula xml:id="formula_2">? * = arg min ? ? val (? * (?), ?), s.t. ? * (?) = arg min ? ? train (?, ?),<label>(3)</label></formula><p>where ? denotes the architecture variable vector formed by ?</p><formula xml:id="formula_3">(i,j) k</formula><p>, and ? denotes the supernet parameters. ? train and ? val denote the training and validation losses, respectively. In the search phase, ? and ? are optimized in an alternate manner.</p><p>The validation data used for the above architecture search method are usually divided from the training data. Previous research demonstrates that the derived architectures perform well on different tasks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b22">23]</ref> when the training and test data are IID. However, when dealing with OoD tasks, where the test data come from another distribution, using part of the training set as the validation set may cause NAS methods suffer from over-fitting and the searched architectures to be sub-optimal in this situation. Thus, a proper validation set is needed to effectively evaluate the performance of discovered architectures on the test set in OoD scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">NAS-OoD: Neural Architecture Search for OoD Generalization</head><p>In OoD learning tasks, we are provided with K source domains. The target is to discover the optimal network architecture that can generalize well to the unseen target domain. In the following descriptions, let ?, ? and ? G denote the parameters for architecture topology, the supernet and the conditional generator G(?, ?), respectively. The conditional generator G(?, ?) takes data x and domain labelsk as the input. Let ? train be the training loss function, and ? val be the validation loss function. Minimax optimization for NAS-OoD. To generate a proper validation set for OoD generalization in NAS, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, a conditional generator is learned to generate novel domain data by maximizing the losses on different neural architectures, while the optimal architecture variables are optimized by minimizing the losses on generated OoD images. This can be formulated as a constrained minimax optimization problem as follows: Generate novel domain data: x syn i ? G(xi,k); <ref type="bibr">5:</ref> ?G ? ?G ? ? ? ? ? G ?aux according to Eqn. (8); <ref type="bibr" target="#b5">6</ref>:</p><formula xml:id="formula_4">min ? max G ? val (? * (?), ?, G(x,k)), s.t. ? * (?) = arg min ? ? train (?, ?, x),<label>(4)</label></formula><formula xml:id="formula_5">? ? ? ? ? ? ???train(?, ?, xi) according to Eqn. (5); 7: ?G ? ?G + ? ? ? ? G ?val(?, ?, x syn i ) according to Eqn. (5); 8: ? ? ? ? ? ? ???val(?, ?, x syn i ) according to Eqn. (5); 9: until convergence;</formula><p>where G(x,k) is the generated data from the original input data x on the novel domaink. This is different from NAS methods' formulation as we introduce a generator to adversarially generate challenging data from original input for validation loss to search for network architectures. This can avoid over-fitting problem by using the same data for optimizing neural network parameters and architectures as shown in our experiment. Solving this problem directly will involve calculating second-order derivatives that will bring much computational overhead and the constraint is hard to realize. Thus, we introduce the following practical implementations of our algorithm. Practical implementation. Inspired by the previous work in meta-learning <ref type="bibr" target="#b15">[16]</ref>, we approximate the multi-step optimization with the one-step gradient when calculating the gradient for ?. Different source domains are mixed together in architecture search, while the domain labels are embedded in the generator auxiliary loss training process which will be explained later. For the architecture search training process, architecture parameters ?, network parameters ? and parameters for conditional generator ? G are updated in an iterative training process:</p><formula xml:id="formula_6">? ? ? ? ? ? ? ? ? train (?, ?, x), ? G ? ? G + ? ? ? ? G ? val (?, ?, G(x,k)), ? ? ? ? ? ? ? ? ? val (?, ?, G(x,k)).<label>(5)</label></formula><p>Generator's auxiliary losses. To train the generator and improve consistency, we apply an additional cycle consistency constraint to the generator:</p><formula xml:id="formula_7">? cycle = ||G(G(x k , k), k) ? x k || 1 ,<label>(6)</label></formula><p>where x k denotes data from K source domains with domain {s 1 , s 2 , ..., s K }, k denotes the domain index for the generated novel domain s K+1 , and || ? || 1 refers to L1 norm. This can regularize the generator to be able to produce data from and back to the source domains.</p><p>To preserve semantic information, we also require the generated data G(x k , k) to keep the same category as the original data x k .</p><formula xml:id="formula_8">? ce = CE(Y (G(x k , k)), Y * (x k )),<label>(7)</label></formula><p>where CE be the cross-entropy loss, Y is a classifier with a few convolutional layers pretrained on training data, Y * (?) is the ground-truth labels for the input data. The total auxiliary loss for generator is defined as follows:</p><formula xml:id="formula_9">? aux = ? cycle ? ? cycle + ? ce ? ? ce , ? ot ,<label>(8)</label></formula><p>where ? ce and ? cycle are hyper-parameters. Compared with the gradient-based perturbation <ref type="bibr" target="#b37">[38]</ref>, the conditional generator is able to model a more sophisticated distribution shift due to its intrinsic learnable nature. The NAS-OoD algorithm is outlined in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct numerical experiments to evaluate the effectiveness of our proposed NAS-OoD method. To provide a comprehensive comparison with baselines, We compare our proposed NAS-OoD with the SOTA algorithms from various OoD areas, including empirical risk minimization (ERM <ref type="bibr" target="#b2">[3]</ref>), invariant risk minimization (IRM <ref type="bibr" target="#b2">[3]</ref>), risk extrapolation (REx <ref type="bibr" target="#b25">[26]</ref>), domain generalization by solving jigsaw puzzles (JiGen <ref type="bibr" target="#b8">[9]</ref>), mixup (Mixup <ref type="bibr" target="#b46">[47]</ref>), curriculum mixup (Cumix <ref type="bibr" target="#b34">[35]</ref>), marginal transfer learning (MTL <ref type="bibr" target="#b6">[7]</ref>), domain adversarial training (DANN <ref type="bibr" target="#b16">[17]</ref>), correlation alignment (CORAL <ref type="bibr" target="#b38">[39]</ref>), maximum mean discrepancy (MMD <ref type="bibr" target="#b31">[32]</ref>), distributionally robust neural network (DRO <ref type="bibr" target="#b36">[37]</ref>), convnets with batch balancing (CNBB <ref type="bibr" target="#b19">[20]</ref>), cross-gradient training (Cross-Grad <ref type="bibr" target="#b37">[38]</ref>), and the recently proposed decomposed feature representation and semantic augmentation (DecAug <ref type="bibr" target="#b4">[5]</ref>), etc. More details about the baseline methods are shown in the Appendix.</p><p>For ablation studies, We also compare NAS-OoD with SOTA NAS methods, such as differentiable architecture search (DARTS <ref type="bibr" target="#b33">[34]</ref>), stochastic neural architecture search (SNAS <ref type="bibr" target="#b41">[42]</ref>), efficient and consistent neural architecture search by sparse coding (ISTA-NAS <ref type="bibr" target="#b43">[44]</ref>). This is to test whether naively combining NAS methods with OoD learning algorithms can improve the generalization performance.</p><p>Our framework was implemented with PyTorch 1.4.0 and CUDA v9.0. We conducted experiments on NVIDIA Tesla V100. Following the design of <ref type="bibr" target="#b10">[11]</ref>, our generator model has an encoder-decoder structure, which consists of two down-sampling convolution layers with stride 2, three residual blocks, and two transposed convolution layers with stride 2 for up-sampling. The domain indicator is encoded as a one-hot vector. The one-hot vector is first spatially expanded and then concatenated with the input image to train the generator. More implementation details can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Datasets</head><p>We evaluate our NAS-OoD on four challenging OoD datasets: NICO Animal, NICO Vehicle, PACS, and Out-of-Distribution Fingerprint (OoD-FP) dataset where methods have to be able to perform well on data distributions different from training data distributions. The evaluation metric is the classification accuracy of the test set. The number of neural network parameters is used to measure the computational complexity for comparison between different neural network architectures. NICO (Non-I.I.D. Image with Contexts) dataset: NICO consists of two datasets, i.e., NICO Animal with 10 classes and NICO Vehicle with 9 classes. The NICO dataset is a recently proposed OoD generalization dataset in the real scenarios <ref type="bibr" target="#b19">[20]</ref> (see <ref type="figure">Figure 3</ref>), which contains different contexts, such as different object poses, positions, and backgrounds across the training, validation, and test sets. PACS (Photo, Art painting, Cartoon, Sketch) dataset: This dataset is commonly used in OoD generalization (see <ref type="figure">Figure 4</ref>). It contains four domains with different image styles, namely photo, art painting, cartoon, and sketch with seven categories (dog, elephant, giraffe, guitar, horse, house, person). We follow the same leave-one-domain-out validation experimental protocol as in <ref type="bibr" target="#b29">[30]</ref>, i.e., we select three domains for training and the remaining domain for testing for each time.</p><p>OoD-FP (Out-of-Distribution Fingerprint) dataset: The OoD-FP dataset is a real industry dataset that contains three domains corresponding to different fingerprint collection devices on different brands of mobile phones (see <ref type="figure">Figure 5</ref>). In the fingerprint recognition task, the goal is to learn to distinguish whether input fingerprints are from the users' fingerprints stored in the dataset. Due to the hardware implementation differences, fingerprints exhibit different styles from different devices. In our setting, the goal is to learn a universal fingerprint recognition neural network to generalize on the fingerprints collected from unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Discussion</head><p>NAS-OoD achieves the SOTA performance simultaneously on various datasets from different OoD research areas, such as stable learning, domain generalization, and real industry dataset. The results for the challenging NICO dataset are shown in <ref type="table" target="#tab_1">Table 1</ref>. From <ref type="table" target="#tab_1">Table 1</ref>, the proposed NAS-OoD method achieves the SOTA performance simultaneously on the two subsets of the NICO dataset with a much fewer number of parameters. Specifically, NAS-OoD achieves 88.72% on NICO Animal and 81.59% on NICO Vehicle with only around 3.1 million parameters compared with DecAug achieving 82.67% accuracy but with 11.7 million parameters. The superior performance of NAS-OoD also confirms the possibility of improving the neural network's OoD generalization performance by searching for the architecture, which provides an orthogonal way to improve the OoD generalization.</p><p>We also compare our methods with different domain generalization methods on the PACS dataset. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. Similarly, we observe that NAS-OoD achieves SOTA performance on all the four domains and the best average generalization performance of 83.89% with only 3.36 million of network parameters. The generalization accuracy is much better than previous OoD algorithms DecAug (82.39%), JiGen (80.51%), IRM (75.92%) with ResNet-18 backbone, which are the best OoD approaches before NAS-OoD. The network parameters for ResNet-18 is 11.7 million which is much larger than the network searched by our NAS-OoD. Note that the relative performance for some algorithms may change drastically between NICO and PACS datasets whereas the proposed NAS-OoD algorithm can generalize well simultaneously on datasets from different OoD research areas.</p><p>To test the generalization performance of NAS-OoD on  <ref type="table" target="#tab_3">Table 3</ref>. NAS-OoD consistently achieves good generalization performance with the non-trivial improvement compared with other methods. NAS-OoD achieves a 1.23% error rate in the fingerprint classification task which almost reduces the error rate by around 70% compared with the second-best method-MMD. This demonstrates the superiority of NAS-OoD and especially its potential to be practically useful in real industrial applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we first test whether naively combining NAS methods with domain generalization methods can achieve good OoD generalization performances. We conduct experiments on the NICO dataset. The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. It can be seen that using NAS methods only, such as DARTS, can achieve only 79.61% average accuracy, significantly lower than most compared compositions. This is in stark contrast with the good performance for NAS methods on IID generalization tasks where training and test datasets have similar distributions. This is because NAS methods are doing variational optimization by finding not only the best parameters but also the best function for fitting whereas this can help NAS methods to achieve good performance in IID settings. In OoD settings, where test data distributions differ significantly from training data distributions, NAS methods can overfit the training data distribution and achieve sub-optimal performance. Besides, we can also observe that naively combining the NAS methods with OoD learning algorithms, such as IRM, brings no statistically significant performance gain. The reason is that many OoD learning algorithms are based on implicit or explicit regularization added to the ERM loss. NAS methods will explore the search space to fit the loss terms and the regularization term may be ignored as NAS methods may exploit only the ERM loss, thus bringing no performance gain. This also confirms that generating OoD data is needed during training to avoid over-fitting. As shown in <ref type="table">Table 5</ref>, the average results of randomly sampled architectures are 79.45% (Animal) and 75.70% (Vehicle), which is significantly lower than those of the architectures searched by NAS-OoD. We also conduct an ablation study on this auxiliary loss, the average accuracy on NICO without cycle loss is 83.86%, which is low than our proposed method 85.16%. This shows the effectiveness of the auxiliary cycle loss to facilitate the searching process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis of Searched Architectures</head><p>After setting up the NAS-OoD framework, we want to analyze whether any special patterns for searched cell-based network architectures and whether the NAS-OoD framework can stably find consistent architectures. Temporal stability of searched architecture To check whether the found special pattern is consistent during the training process, we plot the operation type's percentage during the training process in <ref type="figure">Figure 6</ref>. In <ref type="figure">Figure 6</ref>, we can find that as the training proceeds, the architecture found by NAS-OoD is converging to the pattern that the percentage of dilated convolution 3 ? 3 is higher and the separable convolution 3 ? 3 is lower. This might be because dilated convolution has a larger receptive field than separable convolution which only receives one channel for each convolution kernel and a large receptive field can better learn the shape features of objects rather than the spurious features, such as color and texture. Cross dataset architecture patterns To check whether the architecture patterns searched by NAS-OoD are similar across different datasets, we plot the operation type's percentage for different datasets in <ref type="figure">Figure 7</ref>. We found there are similarities of architectures found on different datasets. Specifically, the searched NAS-OoD architectures tend to contain more convolutional operations with a large kernel size compared with randomly sampled architectures. This may be because a larger kernel size convolution operation has larger receptive fields, which makes better use of contextual information compared with a small kernel size. NAS-OoD architectures also present more skip connection operations compared with random selection and locate on both skip edges and direct edges, which can better leverage both low-level texture features and high-level semantic information for recognition. There is some previous study show that densely connected pattern benefits model robustness. NAS-OoD searched for more dilated convolutions than normal convolutions, which may be due to that the dilated convolutions enlarge the receptive fields. Visualization of the searched architecture cells. As illustrated in <ref type="figure">Figure 8</ref>, we present the detailed structures of the best cells discovered on different datasets using NAS-OoD. <ref type="figure">Figure 8</ref>   Visualization of generated OoD data. We also visualize the generated OoD data in <ref type="figure">Figure 9</ref>. We find that the generated images show different properties and are clearly different from the source images. The conditional generator tends to generate images with different background patterns, textures, and colors. The semantic different make them helpful for improving out-of-distribution generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose a robust neural architecture search framework that is based on differentiable NAS to understand the importance of network architecture against Out-of-Distribution robustness. We jointly optimize NAS and a conditional generator in an end-to-end manner. The generator is learned to synthesize OoD instances by maximizing their losses computed by different neural architectures, while the goal for architecture search is to find the optimal architecture parameters that minimize the synthesized OoD data losses. Our study presents several valuable observations on designing robust network architectures for OoD generalization. Extensive experiments show the effectiveness of NAS-OoD, achieving state-of-the-art performance on different OoD datasets with discovered architectures having a much fewer number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cross Check of Discovered Architectures</head><p>To check whether the architecture discovered by NAS-OoD can perform well across datasets, we present the OoD performance on different datasets in <ref type="table" target="#tab_5">Table 6</ref>. The architectures searched on PACS and NICO are finetuned and evaluated on OoD-FP. As shown in <ref type="table" target="#tab_5">Table 6</ref>, we find that the architectures discovered by NAS-OoD on PACS and NICO also show good performance on OoD-FP. Architectures discovered on PACS (Photo) can achieve 98.40% average accuracy, significantly higher than other OoD algorithms on OoD-FP. This may be because the architectures found by NAS-OoD contain some common patterns such as larger kernel size and more percentage of dilated convolutions that can better make use of contextual information and shows better OoD generalization ability. It can be seen that the architecture discovered on NICO Animal achieves 94.41% on OoD-FP with 3.25 million parameters, while the architecture discovered on NICO Vehicle achieves 90.73% on OoD-FP with 3.00 million parameters. This might be because that the architecture discovered with a higher number of parameters may achieve better OoD generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Ablation Study Results</head><p>We present more ablation results to test whether directly combining NAS with domain generalization methods can achieve good OoD generalization performance or not. The results on the PACS dataset are shown in <ref type="table" target="#tab_6">Table 7</ref>. In <ref type="table" target="#tab_6">Table 7</ref>, we test the performance of NAS methods with different OoD generalization algorithms on the PACS dataset <ref type="table" target="#tab_6">(Table 7</ref>) and NICO dataset ( <ref type="table" target="#tab_7">Table 8</ref>). We  We present more experimental results on PACS datasets. The methods are trained from scratch, and the backbone for the baselines is ResNet-18. As shown in <ref type="table" target="#tab_8">Table 9</ref>, we observe that the architecture searched by NAS-OoD achieves SOTA performance on PACS dataset. The generalization accuracy is much better than previous OoD algorithms. This further demonstrates the superiority of NAS-OoD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results on PACS datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Architectures Discovered on ImageNet</head><p>More recent architectures discovered on ImageNet like EfficientNet-B0 (5.3M) achieves 82.66% on NICO Animal and 78.57% on NICO Vehicle, which is lower than architectures searched by NAS-OoD. EfficientNet is searched by traditional NAS methods focusing on IID settings which can easily overfit the training data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Recent Data Augmentation Methods</head><p>We tried other data augmentation methods RandAugment (NeurIPS 2020) and obtained 77.84% on NICO Vehicle, which is lower than NAS-OoD. Instead of just adding noise to images, our proposed method generates worst-case OoD samples with semantic meanings automatically, which helps train a robust classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. NAS-OoD in the Standard Setting</head><p>Compared with Top1 accuracy 93.02% obtained by ResNet-18 (11.7M) from He et al.</p><p>(CVPR 2016), our method achieves Top1 accuracy 95.28% on standard datasets CIFAR-10 without extra training data and under low model capacity (3.32M). This shows that our method also works well in standard settings without domain shift. We do t-SNE visualization to visualize the embedding of the topology of ? in the two-dimensional space to see whether the top performance architecture has different patterns compared with average architectures. We randomly selected nine top performance architectures found by NAS-OoD and compared them with other randomly selected architectures. The results are shown in <ref type="figure" target="#fig_5">Figure 10</ref>. The top performance architecture is significantly different from random architectures. This demonstrates that there are indeed special architecture patterns that can help the deep neural network to generalize well on OoD settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. T-SNE of Architecture Topology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Implementation Details</head><p>We conduct a fair comparison of NAS-OoD with various OoD generalization methods and SOTA NAS methods on challenging OoD datasets. For NAS-OoD, the optimizer for network parameters is SGD with a learning rate of 0.025. The optimizer for architecture parameters and data generator is Adam. The total number of layers is 20 and the number of initial channels is 36. We conduct hyper-parameter optimization (HPO) for all baseline methods and compare our NAS-OoD with their performance under the best hyperparameters. The learning rate and batch size are fixed during HPO. For NICO, the experiment setting for all methods follows the protocol of NICO. The network for all baseline methods is ResNet-18. The number of training epochs for all methods is 300. The batch size is 128. We use Adam optimizer with a learning rate of 0.001. Hyper-parameters are selected by HPO with 5 trials. The listed results are the testing accuracy at the time of the highest validation accuracy. The results are averaged over 3 runs with the best set of hyper-parameters.</p><p>For PACS, the network for all baseline methods is ResNet-18 initialized with pre-trained weights on Ima-geNet. As for model selection, we use the leave-onedomain-out method to calculate the testing accuracy for each target domain and the average accuracy of these four environments is reported. The number of training epochs is 100. The batch size is 64. We use Adam optimizer with a learning rate of 0.001. Hyper-parameters are selected by HPO with 5 trials. The listed results are the testing accuracy at the time of the highest validation accuracy, averaged over 10 runs with the best set of hyper-parameters.</p><p>For the fingerprint dataset, we use ResNet-18 as the backbone network for baseline methods. The number of training epochs is 20. The batch size is 96. Hyperparameters are selected by HPO with 5 trials. The listed results are the testing accuracy at the time of the highest validation accuracy, averaged over 3 runs with the best set of hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. More Visualization Results of Discovered Architecture Cells</head><p>We also provide more detailed structures of the best cells discovered on different datasets such as PACS ( <ref type="figure">Figure 11</ref>) and OoD-FP ( <ref type="figure" target="#fig_0">Figure 12</ref>) using NAS-OoD. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>An overview of the proposed NAS-OoD. A conditional generator is learned to map the original training data to synthetic OoD examples by maximizing their losses computed by different neural architectures. Meanwhile, the architecture search process is optimized by minimizing the synthetic OoD data losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .Figure 4 . 3 Figure 5 .</head><label>3435</label><figDesc>(a) In water (b) On snow (c) On grass (d) Others Examples of the out-of-distribution data from the NICO dataset with contexts (a) In water, (b) On snow, (c) On grass, and(d) Others. Typical examples of out-of-distribution data with diversity shift from the PACS dataset. (a) Photo. (b) Art Painting. (c) Cartoon. (d) Sketch. Typical examples of out-of-distribution data in the OoD-FP dataset with three different domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Temporal stability of search architecture.(Better viewed in the zoom-in mode) Statistical analysis of searched architectures on different datasets.(Better viewed in the zoom-in mode)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) show the normal cells and (b) demonstrate the reduce cells. The searched cell contains two input nodes, four intermediate nodes, and one output node. Each intermediate node has two edges to the previous nodes which consist of both direct edge and skip edge, and the operation is presented on each edge. Besides, all the intermediate nodes are connected and aggregated to the output node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Typical examples of searched robust architectures on NICO dataset.(Better viewed in the zoom-in mode) Some examples of synthetic images. The first row shows the original images, and the second row is its corresponding synthetic images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>t-SNE visualization of architecture topology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>(a) Normal cell (art painting) (b) Reduce cell (art painting) (c) Normal cell (cartoon) (d) Reduce cell (cartoon) (e) Normal cell (sketch) (f) Reduce cell (sketch) (g) Normal cell (photo) (h) Reduce cell (photo) Typical examples of searched architectures on PACS dataset. (a) Normal cell (domain 1) (b) Reduce cell (domain 1) (c) Normal cell (domain 2) (d) Reduce cell (domain 2) (e) Normal cell (domain 3) (f) Reduce cell (domain 3) Typical examples of searched architectures on OoD-FP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results of NAS-OoD compared with different methods with ResNet-18 (11.7M) on the NICO dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">Animal Vehicle Average</cell></row><row><cell>ERM [3]</cell><cell>75.87</cell><cell>74.52</cell><cell>75.19</cell></row><row><cell>IRM [3]</cell><cell>59.17</cell><cell>62.00</cell><cell>60.58</cell></row><row><cell>REx [26]</cell><cell>74.31</cell><cell>66.20</cell><cell>70.25</cell></row><row><cell>JiGen [9]</cell><cell>84.95</cell><cell>79.45</cell><cell>82.20</cell></row><row><cell>Mixup [47] *</cell><cell>80.27</cell><cell>77.00</cell><cell>78.63</cell></row><row><cell>Cumix [35]</cell><cell>76.78</cell><cell>74.74</cell><cell>75.76</cell></row><row><cell>MTL [7] *</cell><cell>78.89</cell><cell>75.11</cell><cell>77.00</cell></row><row><cell>DANN [17]</cell><cell>75.59</cell><cell>72.23</cell><cell>73.91</cell></row><row><cell cols="2">CORAL [39] * 80.27</cell><cell>71.64</cell><cell>75.95</cell></row><row><cell>MMD [32] *</cell><cell>70.91</cell><cell>68.04</cell><cell>69.47</cell></row><row><cell>DRO [37] *</cell><cell>77.61</cell><cell>74.59</cell><cell>76.10</cell></row><row><cell>CNBB [20]</cell><cell>78.16</cell><cell>77.39</cell><cell>77.77</cell></row><row><cell>DecAug [5]</cell><cell>85.23</cell><cell>80.12</cell><cell>82.67</cell></row><row><cell>NAS-OoD</cell><cell>88.72</cell><cell>81.59</cell><cell>85.16</cell></row><row><cell>Params (M)</cell><cell>3.25</cell><cell>3.00</cell><cell>3.13</cell></row></table><note>* Implemented by ourselves.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy on the PACS dataset compared with different methods with ResNet-18 (11.7M).</figDesc><table><row><cell>Model</cell><cell>A</cell><cell>C</cell><cell>S</cell><cell>P</cell><cell>Average</cell></row><row><cell>ERM [3]</cell><cell cols="4">77.85 74.86 67.74 95.73</cell><cell>79.05</cell></row><row><cell>IRM [3]</cell><cell cols="4">70.31 73.12 75.51 84.73</cell><cell>75.92</cell></row><row><cell>REx [26]</cell><cell cols="4">76.22 73.76 66.00 95.21</cell><cell>77.80</cell></row><row><cell>JiGen [9]</cell><cell cols="4">79.42 75.25 71.35 96.03</cell><cell>80.51</cell></row><row><cell>Mixup [47] *</cell><cell cols="4">82.01 72.58 72.48 93.29</cell><cell>80.09</cell></row><row><cell>CuMix [35]</cell><cell cols="4">82.30 76.50 72.60 95.10</cell><cell>81.60</cell></row><row><cell>MTL [7] *</cell><cell cols="4">76.76 71.87 76.73 92.65</cell><cell>79.50</cell></row><row><cell>MLDG [29]</cell><cell cols="4">79.50 77.30 71.50 94.30</cell><cell>80.70</cell></row><row><cell>MASF [15]</cell><cell cols="4">80.29 77.17 71.69 94.99</cell><cell>81.03</cell></row><row><cell>DANN [17]</cell><cell cols="4">81.30 73.80 74.30 94.00</cell><cell>80.80</cell></row><row><cell>CORAL [39] *</cell><cell cols="4">80.49 74.32 75.06 94.09</cell><cell>80.99</cell></row><row><cell>MMD [32] *</cell><cell cols="4">79.34 73.76 72.61 94.19</cell><cell>79.97</cell></row><row><cell>DRO [37] *</cell><cell cols="4">78.09 74.18 77.00 93.45</cell><cell>80.68</cell></row><row><cell cols="5">CrossGrad [38] 78.70 73.30 65.10 94.00</cell><cell>80.70</cell></row><row><cell>L2A-OT [48]</cell><cell cols="4">83.30 78.20 73.60 96.20</cell><cell>82.80</cell></row><row><cell>DecAug [5]</cell><cell cols="4">79.00 79.61 75.64 95.33</cell><cell>82.39</cell></row><row><cell>NAS-OoD</cell><cell cols="4">83.74 79.69 77.27 96.23</cell><cell>84.23</cell></row><row><cell>Params (M)</cell><cell>3.51</cell><cell>3.44</cell><cell>3.35</cell><cell>3.15</cell><cell>3.36</cell></row></table><note>* Implemented by ourselves.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Classification accuracy compared to different methods with ResNet-18 backbone (11.7M) on the OoD-FP dataset. All methods are implemented by ourselves.</figDesc><table><row><cell>Model</cell><cell cols="4">Domain 1 Domain 2 Domain 3 Average</cell></row><row><cell>ERM [3]</cell><cell>93.75</cell><cell>92.70</cell><cell>92.70</cell><cell>93.05</cell></row><row><cell>IRM [3]</cell><cell>95.83</cell><cell>87.50</cell><cell>84.37</cell><cell>89.23</cell></row><row><cell>REx [26]</cell><cell>97.91</cell><cell>91.66</cell><cell>92.70</cell><cell>94.09</cell></row><row><cell>Mixup [47]</cell><cell>96.87</cell><cell>97.91</cell><cell>90.62</cell><cell>95.13</cell></row><row><cell>MTL [7]</cell><cell>95.83</cell><cell>97.91</cell><cell>90.62</cell><cell>94.78</cell></row><row><cell>DANN [17]</cell><cell>95.83</cell><cell>97.91</cell><cell>86.45</cell><cell>93.39</cell></row><row><cell>CORAL [39]</cell><cell>94.79</cell><cell>97.91</cell><cell>91.66</cell><cell>94.78</cell></row><row><cell>MMD [32]</cell><cell>96.87</cell><cell>95.83</cell><cell>94.79</cell><cell>95.83</cell></row><row><cell>DRO [37]</cell><cell>96.87</cell><cell>95.83</cell><cell>89.58</cell><cell>94.09</cell></row><row><cell>NAS-OoD</cell><cell>99.27</cell><cell>99.49</cell><cell>97.54</cell><cell>98.77</cell></row><row><cell>Params (M)</cell><cell>2.28</cell><cell>2.28</cell><cell>2.43</cell><cell>2.33</cell></row></table><note>real industry datasets, we compare NAS-OoD with other methods on OoD-FP dataset. The results are shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Results of NAS-OoD compared with other NAS methods. The baselines are implemented by ourselves.</figDesc><table><row><cell>Model</cell><cell cols="3">Animal Vehicle Average</cell></row><row><cell>DARTS [34]</cell><cell>83.67</cell><cell>75.55</cell><cell>79.61</cell></row><row><cell>DARTS + IRM [34]</cell><cell>82.29</cell><cell>72.24</cell><cell>77.26</cell></row><row><cell>SNAS [42]</cell><cell>85.96</cell><cell>80.04</cell><cell>83.00</cell></row><row><cell>SNAS + IRM [42]</cell><cell>82.94</cell><cell>76.51</cell><cell>79.73</cell></row><row><cell>ISTA-NAS [44]</cell><cell>86.70</cell><cell>80.56</cell><cell>83.63</cell></row><row><cell>ISTA-NAS + IRM [44]</cell><cell>82.57</cell><cell>77.61</cell><cell>80.09</cell></row><row><cell>NAS-OoD</cell><cell>88.72</cell><cell>81.59</cell><cell>85.16</cell></row><row><cell cols="3">Table 5. NAS-OoD variants.</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Animal Vehicle Average</cell></row><row><cell>Random Sample</cell><cell>80.92</cell><cell>76.43</cell><cell>78.68</cell></row><row><cell>NAS-OoD w/o cycle loss</cell><cell>86.88</cell><cell>80.85</cell><cell>83.86</cell></row><row><cell>NAS-OoD</cell><cell>88.72</cell><cell>81.59</cell><cell>85.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Classification accuracy of the discovered architectures using NAS-OoD cross-check on different datasets.</figDesc><table><row><cell>Model</cell><cell cols="4">Domain 1 Domain 2 Domain 3 Average</cell></row><row><cell>PACS (Art Painting)</cell><cell>99.10</cell><cell>98.87</cell><cell>96.55</cell><cell>98.17</cell></row><row><cell>PACS (Cartoon)</cell><cell>99.35</cell><cell>98.86</cell><cell>94.82</cell><cell>97.68</cell></row><row><cell>PACS (Sketch)</cell><cell>99.07</cell><cell>98.23</cell><cell>96.98</cell><cell>98.09</cell></row><row><cell>PACS (Photo)</cell><cell>99.45</cell><cell>99.06</cell><cell>96.68</cell><cell>98.40</cell></row><row><cell>NICO Animal</cell><cell>98.53</cell><cell>97.55</cell><cell>87.16</cell><cell>94.41</cell></row><row><cell>NICO Vehicle</cell><cell>96.33</cell><cell>90.16</cell><cell>85.70</cell><cell>90.73</cell></row><row><cell>NAS-OoD</cell><cell>99.27</cell><cell>99.49</cell><cell>97.54</cell><cell>98.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Results of NAS-OoD variants on PACS dataset.</figDesc><table><row><cell>Model</cell><cell>A</cell><cell>C</cell><cell>S</cell><cell>P</cell><cell>Average</cell></row><row><cell cols="5">NAS + IRM 79.59 75.34 65.11 95.69</cell><cell>78.93</cell></row><row><cell cols="5">NAS + REx 80.86 75.68 73.94 96.05</cell><cell>81.63</cell></row><row><cell>NAS-OoD</cell><cell cols="4">82.42 79.69 77.27 96.17</cell><cell>83.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>More results of NAS-OoD variants on NICO dataset. The baselines are implemented by ourselves.</figDesc><table><row><cell>Model</cell><cell cols="3">Animal Vehicle Average</cell></row><row><cell>DARTS + IRM</cell><cell>82.29</cell><cell>72.24</cell><cell>77.26</cell></row><row><cell>DARTS + REx</cell><cell>79.17</cell><cell>77.39</cell><cell>78.28</cell></row><row><cell>SNAS + IRM</cell><cell>82.94</cell><cell>76.51</cell><cell>79.73</cell></row><row><cell>SNAS + REx</cell><cell>73.30</cell><cell>69.15</cell><cell>71.23</cell></row><row><cell>ISTA-NAS + IRM</cell><cell>82.57</cell><cell>77.61</cell><cell>80.09</cell></row><row><cell>ISTA-NAS + REx</cell><cell>73.71</cell><cell>69.80</cell><cell>71.76</cell></row><row><cell>NAS-OoD</cell><cell>88.72</cell><cell>81.59</cell><cell>85.16</cell></row><row><cell cols="4">can observe that directly combining the NAS methods</cell></row><row><cell cols="4">with different OoD algorithms, such as IRM (78.93%) and</cell></row><row><cell cols="4">REx (81.63%), achieves lower performance than NAS-OoD</cell></row><row><cell cols="4">(83.89%). This is expected since the regularization terms</cell></row><row><cell cols="4">combined with NAS methods are computed on the original</cell></row><row><cell cols="4">training data that cannot provide the correct supervisory sig-</cell></row><row><cell cols="4">nal for architecture search. The results confirm that jointly</cell></row><row><cell cols="4">optimizing the data generator and the neural network ar-</cell></row><row><cell cols="4">chitecture is needed to discover robust architectures against</cell></row><row><cell>distribution shifts.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Classification accuracy on the PACS dataset (train from scratch). The backbone for the baselines is ResNet-18 (11.7M).</figDesc><table><row><cell>Model</cell><cell>A</cell><cell>C</cell><cell>S</cell><cell>P</cell><cell>Average</cell></row><row><cell>ERM [3] *</cell><cell cols="4">33.11 42.92 40.39 53.83</cell><cell>42.56</cell></row><row><cell>IRM [3] *</cell><cell cols="4">30.08 41.85 35.56 39.10</cell><cell>36.65</cell></row><row><cell>REx [26] *</cell><cell cols="4">31.93 45.95 35.84 44.19</cell><cell>39.48</cell></row><row><cell>Mixup [47] *</cell><cell cols="4">35.16 47.87 42.12 53.59</cell><cell>44.69</cell></row><row><cell>MTL [7] *</cell><cell cols="4">38.48 49.06 46.55 51.98</cell><cell>46.52</cell></row><row><cell cols="5">DANN [17] * 32.71 48.76 40.88 54.79</cell><cell>44.29</cell></row><row><cell cols="5">CORAL [39] * 42.63 49.96 46.78 56.29</cell><cell>48.92</cell></row><row><cell>MMD [32] *</cell><cell cols="4">39.89 51.11 43.09 53.41</cell><cell>46.88</cell></row><row><cell>DRO [37] *</cell><cell cols="4">38.92 46.72 46.73 54.67</cell><cell>46.76</cell></row><row><cell cols="5">DecAug [5] * 45.51 54.22 45.05 59.04</cell><cell>50.96</cell></row><row><cell>NAS-OoD</cell><cell cols="4">48.24 54.27 46.93 65.75</cell><cell>53.80</cell></row><row><cell>Param (M)</cell><cell>3.51</cell><cell>3.44</cell><cell>3.35</cell><cell>3.15</cell><cell>3.36</cell></row></table><note>* Implemented by ourselves.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported, in part, by Hong Kong General Research Fund (under grant number 16200120)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04692</idno>
		<title level="m">Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nads: Neural architecture distribution search for uncertainty awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Ardywibowo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahin</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning de-biased representations with biased representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojin</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Decaug: Out-of-distribution generalization via decomposed feature representation and semantic augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanqing</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-H Gary</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09382</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain generalization by marginal transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Anand Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urun</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07910</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On robustness of neural architecture search under label noise. Frontiers in Big Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simulating a primary visual cortex at the front of cnns improves robustness to image perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Dapello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00902,2020.3</idno>
		<title level="m">Adversarially robust neural architectures</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards non-iid image classification: A dataset and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pretrained transformers improve out-of-distribution robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Dziedzic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06100</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dsnas: Direct neural architecture search without parameter retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00217</idno>
		<title level="m">Nascount: Counting-by-density with neural architecture search</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07421</idno>
		<title level="m">A benchmark of in-the-wild distribution shifts</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00688</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stable prediction across unknown environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxuan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stable prediction with model misspecification and agnostic distribution shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxuan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03463</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Network architecture search for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05706</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards recognizing unseen categories in unseen domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worstcase generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10745</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Risk variance penalization: From distributional robustness to causality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanlong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07544</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">Snas: stochastic neural architecture search</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperan?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12522</idno>
		<title level="m">Nas evaluation is frustratingly hard</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Ista-nas: Efficient and consistent neural architecture search by sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06176</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09929</idno>
		<title level="m">SM-NAS: Structural-to-modular neural architecture search for object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Can subnetwork structure be the key to out-of-distribution generalization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02890</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning to generate novel domains for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03304</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
