<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02">2022. February 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acm Reference Format: Sindhu Tipirneni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chandan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddy</surname></persName>
						</author>
						<title level="a" type="main">Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Trans. Knowl. Discov. Data</title>
						<imprint>
							<biblScope unit="volume">1</biblScope>
							<biblScope unit="issue">1</biblScope>
							<date type="published" when="2022-02">2022. February 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Computing methodologies ? Neural networks; Transfer learning Additional Key Words and Phrases: time-series</term>
					<term>neural networks</term>
					<term>deep learning</term>
					<term>healthcare</term>
					<term>transformer</term>
					<term>self-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multivariate time-series data are frequently observed in critical care settings and are typically characterized by sparsity (missing information) and irregular time intervals. Existing approaches for learning representations in this domain handle these challenges by either aggregation or imputation of values, which in-turn suppresses the fine-grained information and adds undesirable noise/overhead into the machine learning model. To tackle this problem, we propose a Self-supervised Transformer for Time-Series (STraTS) model which overcomes these pitfalls by treating time-series as a set of observation triplets instead of using the standard dense matrix representation. It employs a novel Continuous Value Embedding technique to encode continuous time and variable values without the need for discretization. It is composed of a Transformer component with multi-head attention layers which enable it to learn contextual triplet embeddings while avoiding the problems of recurrence and vanishing gradients that occur in recurrent architectures. In addition, to tackle the problem of limited availability of labeled data (which is typically observed in many healthcare applications), STraTS utilizes self-supervision by leveraging unlabeled data to learn better representations by using timeseries forecasting as an auxiliary proxy task. Experiments on real-world multivariate clinical time-series benchmark datasets demonstrate that STraTS has better prediction performance than state-of-the-art methods for mortality prediction, especially when labeled data is limited. Finally, we also present an interpretable version of STraTS which can identify important measurements in the time-series data. Our data preprocessing and model implementation codes are available at https://github.com/sindhura97/STraTS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Time-series data is routinely collected in various healthcare settings where different measurements are recorded for patients throughout their course of stay (See <ref type="figure" target="#fig_0">Figure 1</ref> for an illustrative example). Predicting clinical outcomes like mortality, decompensation, length of stay, and disease risk from such complex multivariate time-series data can facilitate both effective management of critical care units and automatic personalized treatment recommendation for patients. The success of deep learning in image and text domains realized by convolutional and recurrent networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">28]</ref>, and Transformer models <ref type="bibr" target="#b32">[30]</ref> have inspired the application of these architectures to develop better prediction models for time-series data as well. However, time-series in the clinical domain portray a unique set of challenges that are described below.</p><p>? Missingness and Sparsity: A patient's condition may demand observing only a subset of variables of interest. Thus, not all the variables are observed for every patient. Also, the observed time-series matrices are very sparse as some variables may be measured more frequently than others for a given patient. ? Irregular time intervals and Sporadicity: Not all clinical variables are measured at regular time intervals. Thus, the measurements may occur sporadically in time depending on the underlying condition of the patient. ? Limited labeled data: Patient-level clinical data is often expensive to obtain and labeled data subsets pertaining to a specific prediction task may be even more limited (for e.g., building a severity classifier for Covid-19 patients.)</p><p>A straight-forward approach to deal with irregular time intervals and missingness is to aggregate measurements into discrete time intervals and add missingness indicators, respectively. However, this suppresses important fine-grained information because the granularity of observed time-series may differ from patient to patient based on the underlying medical condition. Existing sequence models for clinical time-series <ref type="bibr" target="#b3">[4]</ref> and other interpolation-based models <ref type="bibr" target="#b28">[26]</ref> address this issue by including a learnable imputation or interpolation component. Such techniques add undesirable noise and extra overhead to the model which usually worsens as the time-series become increasingly sparse. These models rely on an effective imputation/interpolation scheme in order to achieve strong performance on the target task. But it is unreasonable to impute clinical variables without careful consideration of the domain knowledge about each variable which might be non-trivial to obtain.</p><p>Considering these shortcomings, we design a framework that does not need to perform any such operations and directly builds a model based only on the observations that are available in the data. Thus, unlike conventional approaches which view each time-series as a matrix of certain dimensions (#features ? #time-steps), our model regards each time-series as a set of observation triplets (a triple containing time, variable, and value) without the necessity for aggregation or imputation. The proposed STraTS (acronym for Self-supervised Transformer for Time-Series) model embeds these triplets by using a novel Continuous Value Embedding (CVE) scheme to avoid the need for binning continuous values before embedding them. The use of CVE for representing the time dimension preserves the fine grained information which is lost when the time-axis is discretized. STraTS encodes contextual information of observation triplets using a Transformer-based architecture with multi-head attention. We choose this over recurrent neural network (RNN) architectures because the sequential nature of RNN models hinders parallel processing while the Transformer bypasses this by using self-attention to attend from every token to every other token in a single step.</p><p>To build robust representations using limited labeled data, we employ self-supervision and develop a time-series forecasting task to pretrain STraTS. This enables learning generalized representations in the presence of limited labeled data and alleviates sensitivity to noise. Furthermore, interpretable models are usually preferred in healthcare but existing deep models for clinical timeseries lack this component. Thus, we also propose an interpretable version of our model (I-STraTS) which slightly compromises on performance metrics but can identify important measurements in the input. Though we evaluate the proposed model only on binary classification tasks, our framework can also be utilized in other supervised and unsupervised settings, where learning robust and generalized representations of sparse and sporadic time-series is desired. The main contributions of our work can be summarized as follows.</p><p>? Propose a Transformer-based architecture called STraTS for clinical time-series which addresses the unique challenges of missingness and sporadicity of such data by avoiding aggregation and imputation. ? We Develop a novel Continuous Value Embedding (CVE) mechanism using a one-to-many feed-forward network to embed continuous times and measured values in order to preserve fine grained information. ? Utilize forecasting as a self-supervision (proxy) task to leverage unlabeled data to learn more generalized and robust representations. ? Propose an interpretable version of STraTS that can be used when interpretability is more desired compared to quantitative performance gains. ? Demonstrate through an extensive set of experiments that the design choices of STraTS lead to a better performance compared to competitive baseline models for mortality prediction on two real-world clinical datasets.</p><p>The rest of this paper is organized as follows. In Section 2, we review relevant literature about tackling sparse and sporadic time-series data, and self-supervised learning. Section 3 formally defines the prediction problem and gives a detailed description of the architecture of STraTS along with the self-supervision approach. Section 4 presents experimental results comparing STraTS with various baselines and demonstrates the interpretability of I-STraTS with a case study. Finally, Section 5 concludes the paper and provides future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Clinical Time-Series</head><p>A straightforward approach to address missing values and irregular time intervals is to impute and aggregate the time-series, respectively, before feeding them to a classifier <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">20]</ref>. However, such classifiers ignore the missingness in the data which can be quite informative. Lipton et al. <ref type="bibr" target="#b22">[21]</ref> show that phenotyping performance can be improved by passing missingness indicators as additional features to an RNN classifier. But they still lose fine-grained information by aggregating each time-series into hourly intervals.</p><p>Several early works rely on Gaussian Processes (GP) <ref type="bibr" target="#b26">[24]</ref> to model irregular time-series. For example, Lu et al. <ref type="bibr" target="#b24">[23]</ref> represent each time-series as a smooth curve in a reproducing kernel Hilbert space (RKHS) using GP by optimizing GP parameters using Expectation Maximization (EM), and then derive a distance measure on the RKHS which is used to define the SVM classifier's kernel. To account for uncertainty in GP, Li and Marlin <ref type="bibr" target="#b19">[18]</ref> formulate the kernel by applying an uncertainty-aware base kernel (called the expected Gaussian kernel) to a series of sliding windows. These works take a two-step approach by first optimizing GP parameters and then training the classification model. To enable end-to-end training, Li and Marlin <ref type="bibr" target="#b20">[19]</ref> again represent time-series using GP posterior at predefined time points but use the reparametrization trick to back-propagate the gradients through a black-box classifier (learnable by gradient-descent) into the GP model. The end-to-end model is uncertainty-aware as the output is formulated as a random variable. Futoma et al. <ref type="bibr" target="#b12">[11]</ref> extend this idea to multivariate time-series with the help of multitask GP <ref type="bibr" target="#b2">[3]</ref> to consider inter-variable similarities. Though Gaussian Processes provide a systematic way to deal with uncertainty, they are expensive to learn and their flexibility is limited by the choice of covariance and mean functions.</p><p>Shukla and Marlin <ref type="bibr" target="#b28">[26]</ref> also propose an end-to-end method that constitutes interpolation and classification networks stacked in a sequence. They develop learnable interpolation layers to approximate the time-series at regular predefined time points in a deterministic fashion (unlike GPbased methods) and allow information sharing across both time and variable dimensions. However, the input to the classifier is a densely interpolated multivariate time-series which causes loss of information if the number of interpolation points is small and slows down computations while adding noise otherwise.</p><p>Instead of using a separate interpolation module followed by a traditional classifier, other approaches modify traditional recurrent architectures for clinical time-series to deal with missing values and/or irregular time intervals. For example, Baytas et al. <ref type="bibr" target="#b1">[2]</ref> developed a time-aware longshort term memory (T-LSTM) which is a modification of the LSTM cell to adjust the hidden state according to the irregular time gaps. ODE-RNN <ref type="bibr" target="#b27">[25]</ref> uses ODEs to model the continuous-time dynamics of the hidden state while also updating the hidden state at each observed time point using a standard GRU cell. The GRU-D model <ref type="bibr" target="#b3">[4]</ref> is a modification of the GRU cell which decays inputs (to global means) and hidden states through unobserved time intervals. DATA-GRU <ref type="bibr" target="#b31">[29]</ref>, in addition to decaying the GRU hidden state according to elapsed time, also employs a dual attention mechanism based on missingness and imputation reliability to process inputs before feeding them to a GRU cell. All these methods use an RNN with sequence length being the number of unique timestamps in the input, which can be quite large for irregular time-series, and as a result, can slow down computations.</p><p>The imputation/interpolation schemes in the models discussed above can lead to excessive computations and unnecessary noise particularly when missing rates are quite high. Our model is designed to circumvent this issue by representing sparse and irregular time-series as a set of observations. Horn et al. <ref type="bibr" target="#b14">[13]</ref> develop SeFT with a similar idea and use a parametrized set function for classification. The attention-based aggregation used in SeFT contains the same queries for all observations to facilitate low memory and time complexity while compromising on accuracy. The initial embedding in SeFT contains fixed time encodings while our approach uses learnable embeddings for all the three components (time, variable, value) of the observation triplet.</p><p>The challenge of training in scenarios with limited labeled data still remains. In order to address this issue, we turn towards self-supervision for a better utilization of the available data to learn effective representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-supervised Learning</head><p>Supervised deep learning models often rely on large amounts of labeled data to learn generalized and robust representations. Limited labeled data can make the model easily overfit to training data and make the model more sensitive to noise. Since labeled data is expensive to obtain, selfsupervised learning was introduced as a technique to solve this challenge. This technique trains the model on carefully constructed proxy tasks that improve the model's performance on target prediction tasks. The labeled datasets for proxy tasks are obtained from the unlabeled data in an inexpensive semi-automatic process. Yann Le Cunn 1 describes self-supervised learning as to "predict any part of the input from any other part". Self-supervised learning enables the model to learn correlations in input data which enhance the model's learning of supervised target prediction tasks. Liu et al. <ref type="bibr" target="#b23">[22]</ref> review the state-of-the-art self-supervised learning methods in computer vision, natural language processing, and graph representation learning. Though this technique has shown great performance boosts with image <ref type="bibr" target="#b16">[15]</ref> and text <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">31]</ref> data, its application to time-series data has been limited. One such effort is made by Jawed et al. <ref type="bibr" target="#b15">[14]</ref> which uses a 1D CNN for dense univariate time-series classification and shows increased accuracy by using forecasting as an additional task in a muti-task learning framework. Zerveas et al. <ref type="bibr" target="#b35">[33]</ref> pretrained a Transformer model using a denoisining objective and showed improved performance on regression and classification tasks with dense multivariate time-series. In our work, we demonstrate time-series forecasting as a viable and effective self-supervision task for a Transformer model. Our work is the first to explore self-supervised learning in the context of sparse and irregular multivariate time-series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head><p>In this section, we describe our STraTS model by first introducing the problem with relevant notation and definitions and then explaining the different components of the model which are illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>As stated in the previous sections, STraTS represents each time-series as a set of observation triplets. Formally, an observation triplet is defined as a triple ( , , ) where ? R ?0 is the time, ? F is the feature/variable, and ? R is the value of the observation. A multivariate time-series T of length is a defined as a set of observation triplets i.e., T = {( , , )} =1 .</p><p>Consider a dataset D = {(d , T , )} =1 with labeled samples, where the ? sample contains a demographic vector d ? R , a multivariate time-series T , and a corresponding binary label ? {0, 1}. In this work, each sample corresponds to a single ICU stay where several clinical variables of the patient are measured at irregular time intervals and the binary label indicates in-hospital mortality. The underlying set of time-series variables denoted by F may include vitals (such as temperature), lab measurements (such as hemoglobin), and input/output events (such as fluid intake and urine output). Thus, the target task aims to predict given (d , T ).</p><p>Our model also incorporates forecasting as a self-supervision task. For this task, we consider a bigger dataset with ? ? samples given by</p><formula xml:id="formula_0">D ? = {(d , T , m , z )} ? =1 . Here, m ? {0, 1} | F |</formula><p>is the forecast mask which indicates whether each variable was observed in the forecast window and z ? R | F | contains the corresponding variable values when observed. The forecast mask is necessary because the unobserved forecasts cannot be used in training and are hence masked out in the loss function. The time-series in this dataset are obtained from both the labeled and unlabeled time-series by considering different observation windows. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the construction of inputs and outputs for the target task and forecasting task. The target task uses a fixed length observation window to predict in-hospital mortality. The forecasting task has an observation window that is followed by a fixed length prediction window in which only a subset of variables may be observed. Note that several observation windows are considered for each time-series for the forecasting task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture of STraTS</head><p>The architecture of STraTS is illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. Unlike most of the existing approaches which take a time-series matrix as input, STraTS defines its input as a set of observation triplets. Each observation triplet in the input is embedded using the Initial Triplet Embedding module. The initial triplet embeddings are then passed through a Contextual Triplet Embedding module which utilizes the Transfomer architecture to encode the context for each triplet. The Fusion Self-attention module then combines these contextual embeddings via self-attention mechanism to generate an embedding for the input time-series which is concatenated with demographics embedding and passed through a feed-forward network to make the final prediction. The notations used in the paper are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Feature embeddings e f (?) are obtained from a simple lookup table similar to word embeddings. Since feature values and times are continuous unlike feature names which are categorical objects, we cannot use a lookup table to embed these continuous values unless they are categorized. Some researchers <ref type="bibr" target="#b32">[30,</ref><ref type="bibr" target="#b34">32]</ref> have used sinusoidal encodings to embed continuous  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Contextual Triplet</head><p>Embedding. The initial triplet embeddings {e 1 , ..., e } are then passed through a Transformer architecture <ref type="bibr" target="#b32">[30]</ref> with blocks, each containing a Multi-Head Attention (MHA) layer with ? attention heads and a FFN with one hidden layer. Each block takes input embeddings E ? R ? and outputs the corresponding output embeddings C ? R ? that capture the contextual information. MHA layers use multiple attention heads to attend to information contained in different embedding projections in parallel. The computations of the MHA layer are given by</p><formula xml:id="formula_1">H = (EW ) (EW ) ?? /? (EW ) = 1, ..., ?<label>(1)</label></formula><formula xml:id="formula_2">(E) = (H 1 ? ... ? H ? ) W<label>(2)</label></formula><p>Each head projects the input embeddings into query, key, and value subspaces using matrices {W , W , W } ? R ? ? . The queries and keys are then used to compute the attention weights which are used to compute weighted averages of value (different from value in observation triplet) vectors. Finally, the outputs of all heads are concatenated and projected to original dimension with W ? R ? ? ? . The FFN layer takes the form</p><formula xml:id="formula_3">F(X) = (XW 1 + b 1 ) W 2 + b 2 (3) with weights W f 1 ? R ?2 , b f 1 ? R 2 , W f 2 ? R 2 ? , b f 2 ? R .</formula><p>Dropout, residual connections, and layer normalization are added for every MHA and FFN layer. Also, attention dropout randomly masks out some positions in the attention matrix before the softmax computation during training. The output of each block is fed as input to the succeeding one, and the output of the last block gives the contextual triplet embeddings {c 1 , ..., c }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Fusion Self-attention.</head><p>After computing contextual embeddings using a Transformer, we fuse them using a self-attention layer to compute time-series embedding e ? R . This layer first computes attention weights { 1 , ..., } by passing each contextual embedding through a FFN and computing a softmax over all the FFN outputs.</p><formula xml:id="formula_4">= u ?(W c + b ) (4) = ( ) =1 ( ) ? = 1, ...,<label>(5)</label></formula><p>W ? R ? , b ? R , u a ? R are the weights of this attention network which has neurons in the hidden layer. The time-series embedding is then computed as</p><formula xml:id="formula_5">e = ?? =1 c i<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Demographics Embedding.</head><p>We realize that demographics can be encoded as triplets with a default value for time. However, we found that the prediction models performed better in our experiments when demographics are processed separately by passing d through a FFN as shown below. The demographics embedding is thus obtained as</p><formula xml:id="formula_6">e = ?(W 2 ?(W 1 d + b 1 ) + b 2 ) ? R<label>(7)</label></formula><p>where the hidden layer has a dimension of 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Prediction</head><p>Head. The final prediction for target task is obtained by passing the concatenation of demographics and time-series embeddings through a dense layer with weights w ? R , ? R and sigmoid activation.?=</p><formula xml:id="formula_7">(w [e ? e ] + )<label>(8)</label></formula><p>The model is trained on the target task using cross-entropy loss.</p><p>3.2.6 Self-supervision. We experimented with both masking and forecasting as pretext tasks for providing self-supervision and found that forecasting improved the results on target tasks. The forecasting task uses the same architecture as the target task except for the prediction layer i.e.,</p><formula xml:id="formula_8">z = W [e ? e ] + b ? R | F |<label>(9)</label></formula><p>A masked MSE loss is used for training on the forecasting task to account for missing values in the forecast outputs. Thus, the loss for self-supervision is given by</p><formula xml:id="formula_9">L = 1 | ? | ? ?? =1 | F | ?? =1 (z ? z ) 2<label>(10)</label></formula><p>where m = 1 (or m = 0) if the ground truth forecast z is available (or unavailable) for ? variable in ? sample. The model is first pretrained on the self-supervision task and is then fine-tuned on the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interpretability</head><p>We also propose an interpretable version of our model which we refer to as I-STraTS. Inspired by Choi et al. <ref type="bibr" target="#b5">[6]</ref> and Zhang et al. <ref type="bibr" target="#b36">[34]</ref>, we alter the architecture of STraTS in such a way that the output can be expressed using a linear combination of components that are derived from individual features. Specifically, the output of I-STraTS is formulated as</p><formula xml:id="formula_10">= w d ? ?? =1 e +<label>(11)</label></formula><p>Contrary to STraTS, (i) we combine the initial triplet embeddings using the attention weights in Fusion Self-attention module, and (ii) directly use the raw demographics vector as the demographics embedding. The above equation can also be written as  </p><formula xml:id="formula_11">= ?? =1 w [ ] d[ ] + ?? =1 ?? =1 w [ + ] e [ ] +<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluated our proposed STraTS model against state-of-the-art baselines on two real-world EHR databases for the mortality prediction task. This section starts with a description of the datasets and baselines, followed by a discussion of results focusing on generalization and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We experiment with time-series extracted from two real-world EHR datasets which are described below. The dataset statistics are summarized in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIMIC-III [16]</head><p>: This is a publicly available database containing medical records of about 46 critical care patients in Beth Israel Deaconess Medical Center between 2001 and 2012. We filtered ICU stays to include only adult patients and extracted 129 features from the following tables: input events, output events, lab events, chart events, and prescriptions for each ICU stay. For mortality prediction task, we only include ICU stays that lasted for atleast one day with the patient alive at the end of first day, and predict in-hospital mortality using the first 24 hours of data. For forecasting, the set of observation windows is defined (in hours) as {[ (0, ? 24), ) | 20 ? ? 124, %4 = 0} and the prediction window is the 2-hour period following the observation window. Note that we only consider those samples which have atleast one time-series measurement in both observation and prediction windows. The data is split at patient level into training, validation, and test sets in the ratio 64 : 16 : 20.</p><p>PhysioNet Challenge 2012 <ref type="bibr" target="#b13">[12]</ref>: This processed dataset from Physionet Challenge 2012 2 contains records of 11, 988 ICU stays of adult patients. The target task aims to predict in-hospital mortality given the first 48 hours of data for each ICU stay. Since demographic variables 'gender' and 'height' are not available for all ICU stays, we perform mean imputation and add missingness indicators for them as additional demographic variables. To generate inputs and outputs for forecasting, the set of observation windows is defined (in hours) as {[0, ) | 12 ? ? 44, %4 = 0} and the prediction window is the 2-hour period following the observation window. The data from set-b and set-c together is split into training and validation (80:20) while set-a is used for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>To demonstrate the effectiveness of STraTS over the state-of-the-art methods, we compare it with the following baseline models.</p><p>? Gated Recurrent Unit (GRU) <ref type="bibr" target="#b6">[7]</ref>: The input is a time-series matrix with hourly aggregation where missing variables are mean-imputed. Binary missingness indicators and time since the last observation of each variable are also included as additional features at each time step. The final hidden state is transformed by a dense layer to generate output. ? Temporal Convolutional Network (TCN) <ref type="bibr" target="#b0">[1]</ref>: This model takes the same input as GRU which is passed through a stack of temporal convolution layers with residual connections. The representation from the last time step of the last layer is transformed by a dense layer to generate output. ? Simply Attend and Diagnose (SaND) <ref type="bibr" target="#b29">[27]</ref>: This model also has the same input representation as GRU and the input is passed through a Transformer with causal attention and a dense interpolation layer. ? GRU with trainable Decays (GRU-D) <ref type="bibr" target="#b3">[4]</ref>: The GRU-D cell takes a vector of variable values at each time one or more measurements are seen. The GRU-D cell, which is a modification to the GRU cell, decays unobserved values in this vector to global mean values and also adjusts the hidden state according to elapsed times since the last observation of each variable. ? Interpolation-prediction Network (InterpNet) <ref type="bibr" target="#b28">[26]</ref>: This model consists of a semi-parametric interpolation network that interpolates all variables at regular predefined time points, followed by a prediction network which is a GRU. It also uses a reconstruction loss to enhance the interpolation network. The input representation is similar to that of GRU-D and therefore, no aggregation is performed. ? Set Functions for Time Series (SeFT) <ref type="bibr" target="#b14">[13]</ref>: This model also inputs a set of observation triplets, similar to STraTS. It uses sinusoidal encodings to embed times and the deep network used to combine the observation embeddings is formulated as a set function using a simpler but faster variation of multi-head attention.</p><p>For all the baselines, we use two dense layers to get the demographics encoding and concatenate it to the time-series representation before the last dense layer. All the baselines use sigmoid activation at the last dense layer for mortality prediction. The time-series measurements (by variable) and demographics vectors are normalized to have zero mean and unit variance. All models are trained using the Adam optimizer <ref type="bibr" target="#b18">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>The following metrics are used to quantitatively compare the baselines and proposed models for the binary classification task of mortality prediction.  <ref type="table" target="#tab_3">Table 3</ref> lists the hyperparameters used in the experiments for all models for MIMIC-III and PhysioNet-2012 datasets. All models are trained using a batch size of 32 with Adam optimizer and training is stopped when sum of ROC-AUC and PR-AUC does not improve for 10 epochs. For pretraining phase using the self-supervision task, the patience is set to 5 epochs and epoch size is set to 256, 000 samples. For MIMIC-III dataset, we set the maximum number of time-steps for GRU-D and InterpNet, and the maximum no. of observations for STraTS using the 99 ? percentile for the same. This is done to avoid memory overflow with batch gradient descent. The deep models are  implemented using keras with tensorflow backend. For InterpNet, we adapted the official code from https://github.com/mlds-lab/interp-net. For GRU-D and SeFT, we borrowed implementations from https://github.com/BorgwardtLab/Set_Functions_for_Time_Series. The experiments are conducted on a single NVIDIA GRID P40-12Q GPU. Our implementation and data-processing codes for STraTS are available at https://github.com/sindhura97/STraTS.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Prediction Performance</head><p>We train each model using 10 different random samplings of 50% labeled data from the train and validation sets. Note that STraTS uses the entire labeled data and additional unlabeled data (if available) for self-supervision. <ref type="table" target="#tab_4">Table 4</ref> shows the results for mortality prediction on MIMIC-III and PhysioNet-2012 datasets which are averaged over the 10 runs. STraTS achieves the best performance on all metrics, improving PR-AUC by 3.2% and 3.5% on MIMIC-III and PhysioNet-2012 datasets over the best baseline, respectively. This shows that our design choices of triplet embedding, attentionbased architecture, and self-supervision enable STraTS to learn better representations. We expected the interpolation-based models GRU-D and InterpNet to outperform the simpler models GRU, TCN, and SaND. This was true for all cases except that GRU showed a better performance than GRU-D and InterpNet on the MIMIC-III dataset, for reasons that are unclear.</p><p>To test the generalization ability of different models, we evaluate STraTS and the baseline models by training them on varying percentages of labeled data. Lower proportions of labeled data can be </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>We compared the predictive performance of STraTs and I-STraTS, with and without self-supervision and the results are reported in <ref type="table" target="#tab_5">Table 5</ref>. 'ss+' and 'ss-' are used to refer to models trained with and without self-supervision, respectively. We observe that (i) Adding interpretability to STraTS slightly reduces the prediction scores as a result of constraining model representations. (ii) Adding self-supervision improves performance of both STraTS and I-STraTS. (iii) I-STraTS(ss+) outperforms STraTS(ss-) on all metrics on MIMIC-III dataset, and on the PR-AUC metric for PhysioNet-2012 dataset. This demonstrates that the performance drop from introducing interpretability can be compensated by the performance improvements obtained through self-supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Interpretability</head><p>To illustrate how I-STraTS explains its predictions, we present a case study for an 85 year old female patient from the MIMIC-III dataset who expired on the 6 ? day after ICU admission. The I-STraTS model predicts the probability of her in-hospital mortality as 0.94 using only the data collected on the first day. The patient had 380 measurements corresponding to 58 time-series variables. The top 5 variables ordered by their average 'contribution score' along with the range (for multiple observations) or value (for only one observation) are shown in <ref type="table" target="#tab_6">Table 6</ref>. In addition to old age, we can also observe that I-STraTS considers the abnormal values of Lactate, LDH, Platelet count, and RDW as the most important factors in predicting that the patient is at high risk of mortality. The discharge summary for this patient indicates PEA arrest as the cause of death. Elevated Lactate and LDH levels as seen in this case are known to be associated with cardiac arrest <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Such predictions can not only guide the care givers in identifying high-risk patients for better resource allocation but also guide the clinicians into understanding the contributing factors and make better diagnoses and treatment choices, especially at the early stages of treatment before the condition becomes more severe and uncontrollable.</p><p>To obtain a more fine-grained intuition, the observed time-series for some variables in this ICU stay are plotted in <ref type="figure" target="#fig_8">Figure 6</ref> along with the corresponding contribution scores. It is interesting to see that the contribution scores appear to be positively or negatively correlated with the underlying  values or time for several variables. For example, the model gives more weight to higher values of Lactate and LDH that are linked to cardiac arrest which is the patient's cause of death. Similarly, the model pays more attention to increased blood glucose of 210 mg/dL. As GCS-verbal remains at a constant low of 1, the model gives it more and more weight as time progresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed a Transformer-based model, STraTS, for prediction tasks on multivariate clinical time-series to address the challenges faced by existing methods in this domain. Our approach of using observation triplets as time-series components avoids the problems faced by aggregation and imputation methods for sparse and sporadic multivariate time-series. We used a novel CVE technique which uses parameterized embeddings for continuous values and a multi-head attention to learn contextual representations. The self-supervision task of forecasting using unlabeled data enables STraTS to learn more generalized representations, thus outperforming state-of-the-art baselines. In addition, we also showed that STraTS generalizes well even when labeled data is scarce and is also more robust to noise compared to existing methods. We also proposed an interpretable version of STraTS, called I-STraTS, for which self-supervision compensates the drop in prediction performance from introducing interpretability. This work can motivate other researchers to explore more self-supervision tasks for clinical time-series data. Along with exploring more self-supervision tasks, future work should look at adapting STraTS or optimizing its computational efficiency for longer time series where attention matrices can become large and infeasible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An illustrative example of a multivariate clinical time-series with irregular time points and missing values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An illustration of input and output construction for target and self-supervision (forecasting) tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3. 2 . 1</head><label>21</label><figDesc>Initial Triplet Embedding. Given an input time-series T = {( , , )} =1 , the initial embedding for the ? triplet e i ? R is computed by summing the following component embeddings: (i) Feature embedding e f i ? R , (ii) Value embedding e v i ? R , and (iii) Time embedding e t i ? R . In other words, e i = e f i + e v i + e t i ? R .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Feed-Forward Network (FFN) with learnable parameters i.e., e v i = ( ), and e t i = ( ). Both FFNs have one input neuron and output neurons and a single hidden layer with ? ? ? neurons and ?(?) activation. They are of the form ( ) = ?( + ) where the dimensions of weights { , , } can be inferred from the size of hidden and output layers of the FFN. Unlike sinusoidal encodings with fixed frequencies, this technique offers more flexibility by allowing end-to-end learning of continuous value and time embeddings without the need to categorize them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>The overall architecture of the proposed STraTS model. The Input Triplet Embedding module embeds each observation triplet, the Contextual Triplet Embedding module encodes contextual information for the triplets, the Fusion Self-Attention module computes time-series embedding which is concatenated with demographics embedding and passed through a dense layer to generate predictions for target and selfsupervision (forecasting) tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(i) ROC-AUC: Area under ROC curve. (ii) PR-AUC: Area under precision-recall curve. (iii) min(Re, Pr): This metric is computed as the maximum of 'minimum of recall and precision' across all thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Mortality prediction performance on MIMIC-III dataset for different percentages of labeled data averaged over 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Mortality prediction performance on PhysioNet-2012 dataset for different percentages of labeled data averaged over 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Case study: An illustration of a few time-series with contribution scores for a patient from MIMIC-III dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Notations used in this paper.</figDesc><table><row><cell>Notation</cell><cell>Definition</cell></row><row><cell></cell><cell># Time-series for target task</cell></row><row><cell>?</cell><cell># Time-series for forecasting task</cell></row><row><cell>d ? R</cell><cell>Demographics vector</cell></row><row><cell>F</cell><cell>Set of clinical variables</cell></row><row><cell>? R ?0</cell><cell>Time of ? observation</cell></row><row><cell>? F</cell><cell>Variable of ? observation</cell></row><row><cell>? R</cell><cell>Value of ? observation</cell></row><row><cell>( , , )</cell><cell>Observation triplet</cell></row><row><cell cols="2">T = {( , , )} =1 Multivariate time-series</cell></row><row><cell>,?? {0, 1}</cell><cell>True and predicted outputs for target task</cell></row><row><cell>z,z ? R | F |</cell><cell>True and predicted outputs for forecasting task</cell></row><row><cell>m ? {0, 1} | F |</cell><cell>Forecast mask</cell></row><row><cell>e , e ? R</cell><cell>CVE for time and value</cell></row><row><cell>e ? R</cell><cell>Variable embedding</cell></row><row><cell>e ? R</cell><cell>Initial triplet embedding</cell></row><row><cell>e ? R</cell><cell>Time-series embedding</cell></row><row><cell>e ? R</cell><cell>Demographics embedding</cell></row></table><note>values. We propose a novel continuous value embedding (CVE) technique using a one-to-many</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Basic statistics of the two datasets used in our experiemnts. Note that the Avg. variable missing rate and Avg. # observations/stay are calculated using only the supervised samples.</figDesc><table><row><cell>MIMIC-III</cell><cell>PhysioNet-2012</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Hyperparameters used for our experiments in this paper.</figDesc><table><row><cell>Model</cell><cell></cell><cell>MIMIC-III</cell><cell>PhysioNet-2012</cell></row><row><cell>GRU</cell><cell></cell><cell>units=50, rec d/o=0.2, output</cell><cell>units=43, rec d/o=0.2, output</cell></row><row><cell></cell><cell></cell><cell>d/o=0.2, lr=0.0001</cell><cell>d/o=0.2, lr=0.0001</cell></row><row><cell>TCN</cell><cell></cell><cell>layers=4, filters=128, kernel size=4,</cell><cell>layers=6, filters=64, kernel size=4,</cell></row><row><cell></cell><cell></cell><cell>d/o=0.1, lr=0.0001</cell><cell>d/o=0.1, lr=0.0005</cell></row><row><cell>SAnD</cell><cell></cell><cell>N=4, r=24, M=12, d/o=0.3, d=64, h=2,</cell><cell>N=4, r=24, M=12, d/o=0.3, d=64, h=2,</cell></row><row><cell></cell><cell></cell><cell>he=8, lr=0.0005</cell><cell>he=8, lr=0.0005</cell></row><row><cell>GRU-D</cell><cell></cell><cell>units=60, rec d/o=0.2, output</cell><cell>units=49 rec d/o=0.2, output</cell></row><row><cell></cell><cell></cell><cell>d/o=0.2, lr=0.0001</cell><cell>d/o=0.2, lr=0.0001</cell></row><row><cell>SeFT</cell><cell></cell><cell>lr=0.001, n_phi_layers=4,</cell><cell>lr=0.00081, n_phi_layers=4,</cell></row><row><cell></cell><cell></cell><cell>phi_width=128, phi_dropout=0.2,</cell><cell>phi_width=128, phi_dropout=0.2,</cell></row><row><cell></cell><cell></cell><cell>n_psi_layers=2, psi_width=64,</cell><cell>n_psi_layers=2, psi_width=64,</cell></row><row><cell></cell><cell></cell><cell>psi_latent_width=128,</cell><cell>psi_latent_width=128,</cell></row><row><cell></cell><cell></cell><cell>dot_prod_dim=128, n_heads=4,</cell><cell>dot_prod_dim=128, n_heads=4,</cell></row><row><cell></cell><cell></cell><cell>attn_dropout=0.5, la-</cell><cell>attn_dropout=0.5, la-</cell></row><row><cell></cell><cell></cell><cell>tent_width=32, n_rho_layers=2,</cell><cell>tent_width=32, n_rho_layers=2,</cell></row><row><cell></cell><cell></cell><cell>rho_width=512, rho_dropout=0.0,</cell><cell>rho_width=512, rho_dropout=0.0,</cell></row><row><cell></cell><cell></cell><cell>max_timescale=100.0,</cell><cell>max_timescale=100.0,</cell></row><row><cell></cell><cell></cell><cell>n_positional_dims=4</cell><cell>n_positional_dims=4</cell></row><row><cell>InterpNet</cell><cell></cell><cell>ref_points=96, units=100, input</cell><cell>ref_points=192, units=100, input</cell></row><row><cell></cell><cell></cell><cell>d/o=0.2, rec d/o=0.2, lr=0.001</cell><cell>d/o=0.2, rec d/o=0.2, lr=0.001</cell></row><row><cell cols="2">STraTS(ss-) &amp; I-</cell><cell>d=32, M=2, h=4, d/o=0.2, lr=0.0005</cell><cell>d=32, M=2, h=4, d/o=0.2, lr=0.001</cell></row><row><cell>STraTS(ss-)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>STraTS</cell><cell>&amp;</cell><cell>d=50, M=2, h=4, d/o=0.2, lr=0.0005</cell><cell>d=50, M=2, h=4, d/o=0.2, lr=0.0005</cell></row><row><cell>I-STraTS</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Mortality prediction performance on MIMIC-III and PhysioNet-2012 datasets. The results show mean and standard deviation of the metrics after repeating the experiment 10 times by sampling 50% labeled data each time.</figDesc><table><row><cell></cell><cell></cell><cell>ROC-AUC</cell><cell>PR-AUC</cell><cell>min(Re,Pr)</cell></row><row><cell></cell><cell>GRU</cell><cell cols="2">0.886 ? 0.002 0.559 ? 0.005 0.533 ? 0.007</cell></row><row><cell></cell><cell>TCN</cell><cell cols="2">0.879 ? 0.001 0.540 ? 0.004 0.525 ? 0.005</cell></row><row><cell></cell><cell>SAnD</cell><cell cols="2">0.876 ? 0.002 0.533 ? 0.011 0.515 ? 0.008</cell></row><row><cell>MIMIC-III</cell><cell>GRU-D</cell><cell cols="2">0.883 ? 0.003 0.544 ? 0.007 0.527 ? 0.005</cell></row><row><cell></cell><cell cols="3">InterpNet 0.881 ? 0.002 0.540 ? 0.007 0.516 ? 0.005</cell></row><row><cell></cell><cell>SeFT</cell><cell cols="2">0.881 ? 0.003 0.547 ? 0.011 0.524 ? 0.01</cell></row><row><cell></cell><cell>STraTS</cell><cell cols="2">0.891 ? 0.002 0.577 ? 0.006 0.541 ? 0.008</cell></row><row><cell></cell><cell>GRU</cell><cell cols="2">0.831 ? 0.003 0.468 ? 0.008 0.465 ? 0.009</cell></row><row><cell></cell><cell>TCN</cell><cell cols="2">0.813 ? 0.005 0.430 ? 0.01 0.433 ? 0.009</cell></row><row><cell></cell><cell>SAnD</cell><cell cols="2">0.800 ? 0.013 0.406 ? 0.021 0.418 ? 0.018</cell></row><row><cell>PhysioNet-2012</cell><cell>GRU-D</cell><cell cols="2">0.833 ? 0.005 0.481 ? 0.008 0.468 ? 0.012</cell></row><row><cell></cell><cell cols="3">InterpNet 0.822 ? 0.007 0.460 ? 0.017 0.455 ? 0.017</cell></row><row><cell></cell><cell>SeFT</cell><cell cols="2">0.832 ? 0.005 0.454 ? 0.017 0.465 ? 0.009</cell></row><row><cell></cell><cell>STraTS</cell><cell cols="2">0.839 ? 0.008 0.498 ? 0.012 0.483 ? 0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation Study: comparing mortality prediction performance of STraTS and I-STraTS with and without self-supervision. ('ss+' and 'ss-' are used to indicate models trained with and without self-supervision, respectively.) III and PhysioNet-2012 datasets, respectively. The performance of all models degrades with reduced amount of labeled data. But STraTS is seen to have a crucial advantage compared to other models in scarce labeled data settings which can be attributed to self-supervision.</figDesc><table><row><cell></cell><cell></cell><cell>ROC-AUC</cell><cell>PR-AUC</cell><cell>min(Re,Pr)</cell></row><row><cell></cell><cell cols="3">I-STraTS (ss-) 0.878 ? 0.002 0.542 ? 0.006 0.516 ? 0.008</cell></row><row><cell>MIMIC-III</cell><cell cols="3">I-STraTS (ss+) 0.887 ? 0.003 0.556 ? 0.008 0.531 ? 0.005 STraTS (ss-) 0.881 ? 0.002 0.546 ? 0.007 0.525 ? 0.012</cell></row><row><cell></cell><cell>STraTS (ss+)</cell><cell cols="2">0.891 ? 0.002 0.577 ? 0.006 0.541 ? 0.008</cell></row><row><cell></cell><cell cols="3">I-STraTS (ss-) 0.826 ? 0.008 0.456 ? 0.018 0.467 ? 0.025</cell></row><row><cell>PhysioNet-2012</cell><cell cols="3">I-STraTS (ss+) 0.833 ? 0.007 0.478 ? 0.015 0.466 ? 0.010 STraTS (ss-) 0.835 ? 0.009 0.467 ? 0.023 0.471 ? 0.017</cell></row><row><cell></cell><cell>STraTS (ss+)</cell><cell cols="2">0.839 ? 0.008 0.498 ? 0.012 0.483 ? 0.010</cell></row><row><cell cols="4">observed in real-world when there are several right-censored samples. Figures 4 and 5 show the</cell></row><row><cell>results for MIMIC-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Case study: Top 5 variables ordered by 'average contribution score' obtained from I-STraTS model for an ICU stay from MIMIC-III dataset.</figDesc><table><row><cell>Variable</cell><cell>Range/Value</cell><cell>'avg. contribution score'</cell></row><row><cell>Age</cell><cell>85</cell><cell>0.458</cell></row><row><cell>Lactate</cell><cell>[1.7, 6.4] mmol/L</cell><cell>0.175</cell></row><row><cell>LDH</cell><cell>[275, 306] IU/L</cell><cell>0.115</cell></row><row><cell cols="2">Platelet Count [127, 132] K/uL</cell><cell>0.100</cell></row><row><cell>RDW</cell><cell>[22.0-22.1]%</cell><cell>0.083</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://drive.google.com/file/d/1r-mDL4IX_hzZLDBKp8_e8VZqD7fOzBkF/view</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://physionet.org/content/challenge-2012/1.0.0/ ACM Trans. Knowl. Discov. Data., Vol. 1, No. 1, Article . Publication date: February 2022.Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Knowl. Discov. Data., Vol. 1, No. 1, Article . Publication date: February 2022.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Lakshmi Tipirneni for her help with the clinical domain knowledge related to the extraction of our time-series dataset from MIMIC-III database and for providing clinical insights on the case study presented in Section 4.7. This work was supported in part by the US National Science Foundation grants IIS-1838730 and Amazon AWS credits.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Patient Subtyping via Time-Aware LSTM Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inci</forename><forename type="middle">M</forename><surname>Baytas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3097983.3097997</idno>
		<ptr target="https://doi.org/10.1145/3097983.3097997" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-08-13" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task Gaussian Process Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian Ming Adam</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2007-12-03" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-018-24271-9</idno>
		<ptr target="https://doi.org/10.1038/s41598-018-24271-9" />
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic Illness Severity Prediction via Multi-task RNNs for Intensive Care Unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2018.00111</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2018.00111" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining, ICDM 2018</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-11-17" />
			<biblScope unit="page" from="917" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taha</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="3504" to="3512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prognostic implications of blood lactate concentrations after cardiac arrest: a retrospective study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Maria</forename><surname>Dell&amp;apos;anna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Sandroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Lamanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Belloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katia</forename><surname>Donadello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Creteur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Louis</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Silvio</forename><surname>Taccone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of intensive care</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the NAACL-HLT</title>
		<meeting>the 2019 Conference of the NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Biochemistry, Lactate Dehydrogenase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sl Lappin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-05-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Statpearls</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Internet</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Treasure Island (FL</title>
		<imprint>
			<date type="published" when="2021" />
			<publisher>StatPearls Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Futoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Heller</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1174" to="1182" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Plamen</forename><forename type="middle">Ch</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">B</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Kang</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Eugene</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="215" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Set Functions for Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4353" to="4363" />
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised Learning for Semi-supervised Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayan</forename><surname>Jawed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josif</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-47426-3_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-47426-3_39" />
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining -24th Pacific-Asia Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-05-11" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="499" to="511" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I (Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.2992393</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2020.2992393" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="4037" to="4058" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Lehman</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification of Sparse and Irregularly Sampled Time Series with Mixtures of Expected Gaussian Kernels and Random Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI 2015</title>
		<meeting>the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI 2015<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2015-07-12" />
			<biblScope unit="page" from="484" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="1804" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall C</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07641</idno>
		<title level="m">Phenotyping of clinical time series with LSTM recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Directly Modeling Missing Data in Sequences with RNNs: Improved Classification of Clinical Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><forename type="middle">C</forename><surname>Wetzel</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Machine Learning in Health Care</title>
		<meeting>the 1st Machine Learning in Health Care<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-19" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="253" to="270" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised Learning: Generative or Contrastive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2021.3090866</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2021.3090866" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A reproducing kernel Hilbert space framework for pairwise time series distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Erdogmus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-06-05" />
			<biblScope unit="volume">307</biblScope>
		</imprint>
	</monogr>
	<note>International Conference Proceeding Series</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/1390156.1390235</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390235" />
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="624" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gaussian Processes in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-28650-9_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-28650-9_4" />
	</analytic>
	<monogr>
		<title level="m">Advanced Lectures on Machine Learning, ML Summer Schools</title>
		<meeting><address><addrLine>Canberra, Australia; T?bingen, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003-02-02" />
			<biblScope unit="volume">3176</biblScope>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
	<note>Revised Lectures (Lecture Notes in</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Latent Ordinary Differential Equations for Irregularly-Sampled Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="5321" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpolation-Prediction Networks for Irregularly Sampled Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Satya Narayan Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marlin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1efr3C9Ym" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attend and Diagnose: Clinical Time Series Analysis Using Attention Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepta</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayaraman</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Spanias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="4091" to="4098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DATA-GRU: Dual-Attention Time-Aware Gated Recurrent Unit for Irregular Multivariate Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">Jinhua</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Cheuk-Fung Yip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Lai-Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong Chi</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="930" to="937" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Identifying Sepsis Subphenotypes via Time-Aware Multi-Modal Auto-Encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changchang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403129</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403129" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Virtual Event, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="862" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Transformer-based Framework for Multivariate Time Series Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srideepika</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhaval</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuradha</forename><surname>Bhamidipaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467401</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467401" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-08-14" />
			<biblScope unit="page" from="2114" to="2124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">INPREM: An Interpretable and Trustworthy Predictive Model for Healthcare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyue</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Davidson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403087</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403087" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="450" to="460" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
