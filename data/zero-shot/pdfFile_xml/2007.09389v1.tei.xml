<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SRNet: Improving Generalization in 3D Human Pose Estimation with a Split-and-Recombine Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SRNet: Improving Generalization in 3D Human Pose Estimation with a Split-and-Recombine Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human pose estimation</term>
					<term>2D to 3D</term>
					<term>long-tailed distribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human poses that are rare or unseen in a training set are challenging for a network to predict. Similar to the long-tailed distribution problem in visual recognition, the small number of examples for such poses limits the ability of networks to model them. Interestingly, local pose distributions suffer less from the long-tail problem, i.e., local joint configurations within a rare pose may appear within other poses in the training set, making them less rare. We propose to take advantage of this fact for better generalization to rare and unseen poses. To be specific, our method splits the body into local regions and processes them in separate network branches, utilizing the property that a joint's position depends mainly on the joints within its local body region. Global coherence is maintained by recombining the global context from the rest of the body into each branch as a low-dimensional vector. With the reduced dimensionality of less relevant body areas, the training set distribution within network branches more closely reflects the statistics of local poses instead of global body poses, without sacrificing information important for joint inference. The proposed split-and-recombine approach, called SRNet, can be easily adapted to both single-image and temporal models, and it leads to appreciable improvements in the prediction of rare and unseen poses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human pose estimation is a longstanding computer vision problem with numerous applications, including human-computer interaction, augmented reality, and computer animation. For predicting 3D pose, a common approach is to first estimate the positions of keypoints in the 2D image plane, and then lift these keypoints into 3D. The first step typically leverages the high performance of existing 2D human pose estimation algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>. For the second stage, a variety of techniques have been proposed, based on structural or kinematic body models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>, learned dependencies and relationships among body parts <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref>, and direct regression <ref type="bibr" target="#b20">[21]</ref>. Besides algorithm design, an important factor in the performance of a machine learning system is its training data. A well-known issue in pose estimation is the difficulty of predicting poses that are rare or unseen in the training set.</p><p>Since few examples of such poses are available for training, it is hard for the network to learn a model that can accurately infer them. Better generalization to such poses has been explored by augmenting the training set with synthetically generated images <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b14">15]</ref>. However, the domain gap that exists between real and synthesized images may reduce their efficacy. Different viewpoints of existing training samples have also been simulated to improve generalization to other camera positions <ref type="bibr" target="#b8">[9]</ref>, but this provides only a narrow range of pose augmentations.</p><p>In this work, we propose to address the rare/unseen pose problem through a novel utilization of the data in the original training set. Our approach is based on the observation that rare poses at the global level are composed of local joint configurations that are generally less rare in the training set. For example, a bicycling pose may be uncommon in the dataset, but the left leg configuration of this pose may resemble the left legs of other poses in the dataset, such as stair climbing and marching. Many instances of a local pose configuration may thus exist in the training data among different global poses, and they could be leveraged for learning local pose. Moreover, it is possible to reconstruct unseen poses as a combination of local joint configurations that are presented in the dataset. For example, an unseen test pose may be predicted from the upper body of one training pose and the lower body of another, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Based on this observation, we design a network structure that splits the human body into local groups of joints that have strong inter-relationships within each group and relatively weak dependencies on joints outside the group. Each group is processed in a separate network branch. To account for the weak dependencies that exist with the rest of the body, low-dimensional global context is computed from the other branches and recombined into the branch. The dimensionality reduction of less-relevant body areas within the global context decreases their impact on the feature learning for local joints within each branch. At the same time, accounting for some degree of global context can avoid local pose estimates that are incoherent with the rest of the body. With this split-andrecombine approach, called SRNet, generalization performance is enhanced to effectively predict global poses that are rare or absent from the training set.</p><p>In extensive comparisons to state-of-the-art techniques, SRNet exhibits competitive performance on single-frame input and surpasses them on video input. More importantly, we show that SRNet can elevate performance considerably on rare/unseen poses. Moreover, we conduct various ablation studies to validate our approach and to examine the impact of different design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Extensive research has been conducted on reconstructing 3D human pose from 2D joint predictions. In the following, we briefly review methods that are closely related to our approach.</p><p>Leveraging Local Joint Relations Many recent works take advantage of the physical connections that exist between body joints to improve feature learning and joint prediction. As these connections create strong inter-dependencies and spatial correlations between joints, they naturally serve as paths for information sharing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref> and for encoding kinematic <ref type="bibr" target="#b8">[9]</ref> and anatomical <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36]</ref> constraints.</p><p>The structure of these connections defines a locality relationship among joints. More closely connected joints have greater inter-dependency, while distantly connected joints have less and indirect dependence via the joints that lie on the paths between them. These joint relationships have been modeled hierarchically, with feature learning that starts within local joint groups and then expands to account for all the joint groups together at a global level <ref type="bibr" target="#b24">[25]</ref>. Alternatively, these relationships have been represented in a graph structure, with feature learning and pose estimation conducted in a graph convolutional network (GCN) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref>. Within a GCN layer, dependencies are explicitly modeled between connected joints and expand in scope to more distant joints through the stacking of layers.</p><p>For both hierarchical and GCN based techniques, feature learning within a local group of joints can be heavily influenced by joints outside the group. In contrast, the proposed approach SRNet restricts the impact of non-local joints on local feature learning through dimensionality reduction of the global context, which facilitates the learning of local joint configurations without ignoring global pose coherence. By doing so, SRNet can achieve better generalization to rare and unseen poses in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization in Pose Estimation</head><p>A common approach to improve generalization in pose estimation is to generate more training images through data augmentation. Approaches to augmentation have included computer graphics rendering <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref>, image-based synthesis <ref type="bibr" target="#b31">[32]</ref>, fitting 3D models to people and deforming them to generate new images <ref type="bibr" target="#b28">[29]</ref>, and changing the background, clothing and occluders in existing images <ref type="bibr" target="#b21">[22]</ref>. These methods can produce a large amount of data for training, but the gap in realism between artificially constructed images and actual photographs may limit their effectiveness. Since our technique obtains improved training data distributions by focusing on local pose regions instead of global poses, it does not involve image manipulation or synthesis, thereby maintaining the realism of the original training set.</p><p>Other works have explored generalization to different viewpoints <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref> and to in-the-wild scenes <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b9">10]</ref>, which are orthogonal to our goal of robustly estimating rare and unseen poses.</p><p>Robustness to Long-tailed Distributions In visual recognition, there exists a related problem of long-tailed training set distributions, where classes in the distribution tail have few examples. As these few examples have little impact on feature learning, the recognition performance for tail classes is often poor. Recent approaches to this problem include metric learning to enforce inter-class margins <ref type="bibr" target="#b10">[11]</ref> and meta-learning that learns to regress many-shot model parameters from few-shot model parameters <ref type="bibr" target="#b37">[38]</ref>. Such techniques improve the discriminability of tail classes from other classes, but they are not compatible with the problem of keypoint localization in pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>To address the issue of rare and unseen poses, our approach is to decompose global pose estimation into a set of local pose estimation problems. For this  The four types of layers can be stacked to form different network structures as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. strategy to be effective, the local problems must be defined in a manner that allows the feature learning of a local region to primarily reflect the statistical distribution of its local poses, yet account for other local regions such that the final overall pose estimate is globally coherent.</p><p>In the following, we start with the most common baseline for lifting 2D joints to 3D, and then describe how to modify this baseline to follow our strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully-Connected Network Baseline</head><p>For lifting 2D keypoints to 3D, a popular yet simple baseline is to use a fully-connected network (FCN) consisting of several layers <ref type="bibr" target="#b20">[21]</ref>. Given the 2D keypoint detections K = {K i |i, ..., N } ? R 2N in the image coordinate system, the FCN estimates the 3D joint locations J = {J i |i, ..., N } ? R 3N in the camera coordinate system with the origin at the root joint J 0 . Formally, a fully-connected network layer can be expressed as</p><formula xml:id="formula_0">f l+1 = ? l f l<label>(1)</label></formula><p>where ? l ? R D l+1 ?D l is the fully-connected weight matrix to be learned, and D l is the feature dimension for the l th layer, namely f l ? R D l . Batch normalization and ReLU activation are omitted for brevity. For the input and output layers, their feature dimensions are D 1 = 2N and D L+1 = 3N , respectively. It can be noted that in this FCN baseline, each output joint and each intermediate feature is connected to all of the input joints indiscriminately, allowing the prediction of an output joint to be overfitted to the positions of distant joints with little relevance. In addition, all the output joints share the same set of features entering the final layer and have only this single linear layer (? L i ) to determine a solution particular to each joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Body Partitioning into Local Pose Regions</head><p>In turning global pose estimation into several local pose estimation problems, a suitable partitioning of the human body into local pose regions is needed. A local pose region should contain joints whose positions are heavily dependent on one another, but less so on joints outside the local region.</p><p>Here, we adopt the partitioning used in <ref type="bibr" target="#b24">[25]</ref>, where the body is divided into left/right arms, left/right legs, and torso. These parts have distinctive and coordinated behaviors such that the joint positions within each group are highly correlated. In contrast, joint positions between groups are significantly less related.</p><p>To accommodate this partitioning, the FCN layers are divided into groups. Formally, G l g represents the feature/joint indexes of the g th group at layer l. Specifically, for the first input layer,</p><formula xml:id="formula_1">G 1 0 = [joint indices of the right arm],<label>(2)</label></formula><p>and for the intermediate feature layers, we have G l 0 = [feature indices of the right leg at the l th layer].</p><p>Then, a group connected layer for the g th group is expressed as</p><formula xml:id="formula_3">f l+1 [G l+1 g ] = ? l g f l [G l g ].<label>(4)</label></formula><p>In a group connected layer, the connections between joint groups are removed. This "local connectivity" structure is commonly used in convolutional neural networks to capture spatially local patterns <ref type="bibr" target="#b15">[16]</ref>. The features learned in a group depend on the statistical distribution of the training data for only its local pose region. In other words, this local feature is learned independently of the pose configurations of other parts, and thus it generalizes well to any global pose that includes similar local joint configurations.</p><p>However, a drawback of group connected layers is that the status of the other body parts is completely unknown when inferring the local pose. As a result, the set of local inferences may not be globally coherent, leading to low performance. There is thus a need to account for global information while largely preserving local feature independence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SRNet: Incorporating Low-Dimensional Global Context</head><p>To address this problem, we propose to incorporate Low-Dimensional Global Context (LDGC) in a group connected layer. It coarsely represents information from the less relevant joints, and is brought back to the local group in a manner that limits disruption to the local pose modeling while allowing the local group to account for nonlocal dependencies. This split-and-recombine approach for global context can be expressed as the following modification of Eq. 4:</p><formula xml:id="formula_4">f l+1 [G l+1 g ] = ? l g (f l [G l g ] ? Mf l [G l \ G l g ])<label>(5)</label></formula><p>where f l [G l \G l g ] is the global context for the g th group. M is a mapping function that defines how the global context is represented. Special cases of the mapping function are M = Identity, equivalent to a fully-connected layer, and M = Zero, which is the case for a group connected layer. The mapped global context is recombined with local features by an operator ?, typically concatenation for an FCN.</p><p>The mapping function M acts as a gate controlling the information passed from the non-local joints. If the gate is wide open (FCN), the local feature learning will account for the exact joint positions of other body parts, weakening the ability to model the local joint configurations of rare global poses. However, if the gate is closed (Group), the local feature learning receives no knowledge about the state of other body parts and may lose global coherence. We argue that the key to achieving high performance and strong generalization to rare poses is to learn a low-dimensional representation of the global context, namely</p><formula xml:id="formula_5">M = ? l g ? R H?(D l ?D l g )<label>(6)</label></formula><p>where D l g is the feature dimension for the gth group at layer l. ? l g is a weight matrix that maps the global context f l [G l \ G l g ] of dimensions D l ? D l g to a small number of dimensions H.</p><p>In our experiments, we empirically evaluate different design choices for the mapping function M and the combination operator ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Input Network Structure With our split-and-recombine approach for processing local pose regions and global context, the SRNet model can be illustrated as shown at the right side of <ref type="figure" target="#fig_3">Fig. 3</ref>. This network follows the structure of a group connected network, with the body joints split into separate groups according to their local pose region. Added to this network are connections to each group in a layer from the other groups in the preceding layer. As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref> (c,d), these connections representing the global context reduce the dimensionality of the context features and recombine them with the local features in each group. These inputs are mapped into outputs of the original feature dimensions. SRNet balances feature learning dependency between the most related local region and the less related global context. For demonstrating the efficacy of this split-and-recombine strategy, we describe several straightforward modifications to the FC network, illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>, that will serve as baselines in our experiments:</p><formula xml:id="formula_6">Input Input Input GP FC LF ES SFS Input SR-Ours</formula><p>-Late Fusion (LF), which postpones feature sharing among groups by cutting their connections in the earlier layers. This structure is similar to that used in <ref type="bibr" target="#b24">[25]</ref>, which first learns features within each group and fuses them later with stacked fully connected layers. Formally, the LF baseline is defined as</p><formula xml:id="formula_7">f l+1 = Cat{f l [G l g ]|g = 1, ..., G}, if l &lt; L f use ? l f l ,</formula><p>otherwise.</p><p>-Early Split (ES), which aims to provide more expressive features for each group, by cutting the connections between groups in the latter layers. Formally,</p><formula xml:id="formula_9">f l+1 = ? l f l , if l &lt; L split Cat{f l [G l g ]|g = 1, ..., G}, otherwise.<label>(8)</label></formula><p>-Group Connected (GP), which is the standard group connected network. Late Fusion degenerates to this form when L f use = L, and Early Split does when L split = 1.</p><p>-Split-Fuse-Split (SFS), where the middle L link layers of the network are fullyconnected and the connections between joint groups are cut in the first and last layers.</p><p>The differences among FCN, these baselines, and SRNet are summarized in <ref type="table" target="#tab_1">Table 1</ref>. The table also highlights differences of SRNet from GCN <ref type="bibr" target="#b40">[41]</ref> and its modification LCN <ref type="bibr" target="#b6">[7]</ref>, specifically in controlling the feature dependencies between local regions and global context.  SR Convolution for Temporal Models Temporal information in 2D to 3D mapping is conventionally modeled with Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Recently, Pavllo et al. <ref type="bibr" target="#b26">[27]</ref> introduce an alternative temporal convolutional model that stacks all the spatial information of a frame into the channel dimensions and replaces fully-connected operations in the spatial domain by convolutions in the temporal domain. The temporal convolution enables parallel processing of multiple frames and brings greater accuracy, efficiency, and simplicity. As illustrated in <ref type="figure" target="#fig_2">Fig. 2 (d)</ref>, our split-and-recombine modification to the fully connected layer can be easily adapted to the temporal convolution model by applying the same split-and-recombine strategy to the channels during convolution. Specifically, the group connected operations for local joint groups are replaced with corresponding temporal convolutions for local feature learning, where the features from other joint groups undergo an extra convolution for dimension reduction in channels and are concatenated back with the local joint group as global context. We call this split-and-recombine strategy in the channel dimensions during convolution the SR convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets and Rare-Pose Evaluation Protocols</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>Our approach is validated on two popular benchmark datasets: -Human3.6M <ref type="bibr" target="#b13">[14]</ref> is a large benchmark widely-used for 3D human pose estimation. It consists of 3.6 million video frames from four camera viewpoints with 15 activities. Accurate 3D human joint locations are obtained from motion capture devices. Following convention, we use the mean per joint position error (MPJPE) for evaluation, as well as the Procrustes Analysis MPJPE (PA-MPJPE).</p><p>-MPI-INF-3DHP <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> is a more challenging 3D pose dataset, containing not only constrained indoor scenes but also complex outdoor scenes. Compared to Human3.6M, it covers a greater diversity of poses and actions. For evaluation, we follow common practice <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref> by using the Percentage of Correct Keypoints (PCK) with a threshold of 150mm and the Area Under Curve (AUC) for a range of PCK thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Protocols</head><p>The most widely-used evaluation protocol <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b6">7]</ref> on Human3.6M uses five subjects (S1, S5, S6, S7, S8) as training data and the other two subjects (S9, S11) as testing data. We denote it as the Subject Protocol. However, the rare pose problem is not well examined in this protocol since each subject is asked to perform a fixed series of actions in a similar way.</p><p>To better demonstrate the generalization of our method to rare/unseen poses, we use two other protocols, introduced in <ref type="bibr" target="#b6">[7]</ref>. The first is the Cross Action Protocol that trains on only one of the 15 actions in the Human3.6M dataset and tests on all actions. The second is the Cross Dataset Protocol that applies the model trained on Human3.6M to the test set of MPI-INF-3DHP. In addition, we propose a new protocol called Rare Pose Protocol.</p><p>Rare Pose Protocol. In this protocol, we use all the poses of subjects S1, S5, S6, S7, S8 for training but only use a subset of poses from subjects S9, S11 for testing. The subset is selected as the rarest poses in the testing set. To identify rare poses, we first define pose similarity (P S) as</p><formula xml:id="formula_10">P S(J , I) = 1 N N i exp(? ||J i ? I i || 2 2? 2 i )<label>(9)</label></formula><p>where ||J i ? I i || is the Euclidean distance between corresponding joints of two poses. To compute P S, we pass ?J i through an unnormalized Gaussian with a standard deviation ? i controlling falloff. This yields a pose similarity that ranges between 0 and 1. Perfectly matched poses will have P S = 1, and if all joints are distant by more than the standard deviation ? i , P S will be close to 0. The occurrence of a pose J in a pose set O is defined as the average similarity between itself and all the poses in O:</p><formula xml:id="formula_11">OCC O (J ) = 1 M O I P S(J , I)<label>(10)</label></formula><p>where M is the total number of poses in the pose set O. A rare pose will have a low occurrence value with respect to other poses in the set. We select the R% of poses with the lowest occurrence values in the testing set for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>All our ablation studies are evaluated on the Human3.6M dataset. Both the Subject Protocol and our new Rare Pose Protocol are used for evaluation. MPJPE serves as the evaluation metric. Please refer to the supplementary material for details on data pre-processing and training settings.</p><p>Effect of the SRNet In <ref type="table" target="#tab_3">Table 2</ref>, we compare our SR network with the baseline networks FC, GP, LF, ES, SFS described in Section 3. By default, the mapped dimensions H is set to 1 and the combination operator ? is set to multiplication. It is found that both the LF and ES baselines are superior to the FC and GP baselines. A more detailed comparison is shown in <ref type="figure">Figure 4</ref>, with different splitting configurations for each. In <ref type="figure">Figure 4</ref>, both LF and ES start on the left with the FC network (L f use = 0 and L split = 6). When the fully connected layers are gradually replaced by group connected layers, from the beginning of the network for LF (L f use ?) or the end for ES (L split ?), testing error decreases at first. This indicates that local feature learning and local inference are each helpful. However, replacing all the fully connected layers with group layers leads to a sharp performance drop. This indicates that a local joint region still needs information from the other regions to be effective.</p><p>Combining the LF and ES baselines to create SFS yields further improvements from sharing the merits of both. These two simple modifications already improve upon FC by a large margin. Specifically, it improves upon FC by 7.4mm (relative 15.8%), <ref type="bibr" target="#b15">16</ref>.8mm (relative 22.1%) and 20.9mm (relative 23.5%) on the Subject, Rare Pose 20% and Rare Pose 10% protocols, respectively.</p><p>SRNet with its use of low-dimensional global context performs significantly better than the SFS baseline. Specifically, it improves upon SFS by 2.8mm (relative 7.1%), 10.6mm (relative 17.9%) and 14.5mm (relative 21.2%) on the Subject, Rare Pose 20% and Rare Pose 10% protocols, respectively. Note that the improvement is especially large on rare poses, indicating the stronger generalization ability of SRN et to such poses. <ref type="figure">Figure 5</ref> breaks down the results of the FC, SFS and SR networks for different degrees of pose rareness. It can be seen that the performance improvements of SRNet increase for rarer poses.    <ref type="table" target="#tab_5">Table 3</ref> presents the performance of SRNet with different dimensions H for the global context. To readily accommodate different dimensions, the combination operator ? is set to concatenation. When H = 0, SRNet degenerates into the GP baseline. It can be seen that the GP baseline without any global context obtains the worst performance. This shows the importance of bringing global context back to local groups. Also, it is found that a lower dimension of global context leads to better results. The best result is achieved by keeping only a single dimension for global context. This indicates that a low-dimensional representation of the global context can better account for the information of less relevant joints while preserving effective local feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-Dimensional Representation for Global Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Grouping Strategy</head><p>We compare the results of using different numbers of local groups in <ref type="table" target="#tab_6">Table 4</ref>. The corresponding sets of joints for each group are shown in <ref type="figure">Figure 6</ref>. More joint groups leads to greater local correlation of joints within a group. It is shown that the performance first improves with more groups, especially in the Rare Pose case. However, with more than five groups,    <ref type="table">Table 5</ref>: Mean testing error with respect to the number of shuffled groups (5 in total). the performance drops due to weakening of local features when there are fewer than three joints in a group. Moreover, to show that separation into local groups is important, we randomly shuffle the joints among a subset of groups, with five groups in total, as illustrated in <ref type="figure">Figure 6</ref> (f). In <ref type="table">Table 5</ref>, it is shown that the performance decreases when randomly shuffling joints among more groups. This indicates that a strong physical relationship among joints in a group is a prerequisite for learning effective local features.</p><formula xml:id="formula_12">Dimension H 0(GP) 1 3 10 25% D l g 50% D l g 100% D l g 200% D l g</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with State-of-The-Art Methods</head><p>We compare with the state of the art in three different experiment settings: crossdataset on MPI-INF-3DHP, single-frame model on Human3.6M, and temporal model on Human3.6M. MPJPE is used as the evaluation metric on Human3.6M. Please refer to the supplementary material for more results that use PA-MPJPE as the evaluation metric. Shuffle two groups into: <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref> &amp; <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> Fig. 6: Different numbers of groups and their corresponding sets of joints. Joints in the same group are shown in the same color. A random shuffle setting is also shown and evaluated in <ref type="table">Table 5</ref>.</p><p>Cross-Dataset Results on MPI-INF-3DHP. In this setting, we follow common practice <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>     Single-Frame Model Results on Human3.6M. In this setting, we first compare with the state-of-the-art method <ref type="bibr" target="#b6">[7]</ref> using the Cross Action Protocol. <ref type="table" target="#tab_9">Table  7</ref> shows that our method yields an overall improvement of 11.4mm (relative 12.2% improvement) over <ref type="bibr" target="#b6">[7]</ref> and performs better on 93% of the actions, indicating strong generalization ability on unseen actions. We also compare our model to previous works using the standard Subject Protocol in <ref type="table" target="#tab_10">Table 8</ref> and <ref type="table" target="#tab_12">Table 9</ref>, which use 2D keypoint detection and 2D ground truth as inputs, respectively. Under the standard protocol, our model surpasses the state-of-the-art   <ref type="table" target="#tab_1">Table 10</ref>: Comparison on Temporal Pose input in terms of mean per-joint position error (MPJPE). Below the double line, indicates use of 2D ground truth pose as input, which is examined to explore the upper bound of these methods. Best results in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed SRNet, a split-and-recombine approach that improves generalization performance in 3D human pose estimation. The key idea is to design a network structure that splits the human body into local groups of joints and recombines a low-dimensional global context for more effective learning. Experimental results show that SRNet outperforms state-of-the-art techniques, especially for rare and unseen poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Supplementary Materials-SRNet: Improving Generalization in 3D Human Pose</head><p>Estimation with a Split-and-Recombine Approach</p><p>In this supplementary material, we present more implementation details of our model, additional experimental results and more qualitative results which are not shown in the main paper due to the space limitation. First, Section 1 gives more details on experiment settings. Second, Section 2 discuses different design choices for the combination operator ? in Equation 5 of the main paper. Third, Section 3 shows results of using both MPJPE and PA-MPJPE as the evaluation metrics in a comparison with state-of-the-art methods on the Hu-man3.6M dataset. Next, Section 4 shows more results under the cross action protocol. Finally, Section 5 demonstrates additional qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Implementation Details</head><p>Training Data. Following many previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7]</ref>, we show results of using two different kinds of 2D keypoints as input for our model in the experiments. They are 2D ground truth and 2D detections from an off-the-shelf 2D keypoint detector. Following those works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>, we use the smoothed CPN model <ref type="bibr" target="#b5">[6]</ref> which finetuned on the Human3.6M dataset by an eight-layer residual fully-connected temporal model as our 2D keypoint detector, which is pretrained on the COCO dataset. No extra 2D data has been used for mixed training. In the ablation study (Section 5.1) of the main paper, we use 2D ground truth as input. When comparing with previous works in Section 5.2, both inputs are used and compared respectively. For data normalization, we use two methods. One is provided by <ref type="bibr" target="#b26">[27]</ref> called the Basic normalization and another is provided by <ref type="bibr" target="#b6">[7]</ref> called the Pixel normalization. Please refer to their code base <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> for detailed implementation. By default, the Basic normalization is used in our ablation study. When comparing with the state-of-the-arts ( <ref type="bibr" target="#b6">[7]</ref> in single pose and <ref type="bibr" target="#b26">[27]</ref> in temporal pose), we use the same data normalization method as each for a fair comparison. For data augmentation, we follow <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7]</ref> by using horizontal flip data augmentation at both training and test stages.</p><p>Training Setting. Amsgrad <ref type="bibr" target="#b30">[31]</ref> is used as the optimizer. The initial learning rate is 0.001 and it decays by 5% after each epoch of training. 80 epochs are used in total. The total channel dimension of each connected/convolution layer is 1024. Batch Normalization <ref type="bibr" target="#b12">[13]</ref> and Leaky ReLU <ref type="bibr" target="#b38">[39]</ref> activation are applied to each connected/convolution layer. The final network consists of 8 stacked layers, and every two layers (except for the first and last ones) are wrapped with a residual connection as in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. Batch size is 1024. L1 loss is used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Design Choices for the Combination Operator</head><p>In Equation 5 of the main paper, we show how the low-dimensional global contexts can be brought back to the local group using a combination operator ?. By default, the combination operator ? is implemented using multiplication in the main paper. Here, we empirically evaluate the design choices of using addition, multiplication and concatenation in <ref type="table" target="#tab_1">Table 1</ref>. It is shown that both addition and multiplication obtain favorable results. They surpass the F C and SF S baselines, indicating their effectiveness in recombining the low-dimensional global contexts.  3 More Results on Human3.6M</p><p>In <ref type="table" target="#tab_10">Tables 8, 9</ref>, and 10 of the main paper, we compare our model with previous works under different input settings (using 2D ground truth or detection, with or without temporal information). We summarise them in <ref type="table" target="#tab_3">Table 2</ref> and 3 with more detailed results on different actions. Our approach achieves the new state-of-theart with either 2D keypoint detection or 2D ground truth as input. Specifically, we improve upon <ref type="bibr" target="#b6">[7]</ref> from 36.3mm to 33.9mm (relative 6.6% improvement) with 2D ground truth input for single pose inputs. We improve upon <ref type="bibr" target="#b17">[18]</ref> from 46.6mm to 44.8mm (relative 3.9% improvement) with 2D temporal keypoint detection input.</p><p>In <ref type="table" target="#tab_6">Table 4</ref> and 5, we compare with the previous works using the PA-MPJPE metric where available. Our approach achieves the new state-of-the-art with either 2D keypoint detection or 2D ground truth (denoted by ) as input. Specifically, we improve upon <ref type="bibr" target="#b6">[7]</ref> from 27.9mm to 24.3mm (relative 14.8% improvement) with 2D ground truth input. We improve upon <ref type="bibr" target="#b26">[27]</ref> from 36.5mm to 34.9mm (relative 4.4% improvement) with 2D keypoint detection input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cross Action Results Using 2D Ground Truth Input</head><p>In <ref type="table" target="#tab_9">Table 7</ref> of the main paper, we compare our cross-action results with <ref type="bibr" target="#b6">[7]</ref> under the same data settings. Here, we provide more results of using 2d ground truth as input under the cross-action protocol. The FCN baseline <ref type="bibr" target="#b20">[21]</ref>      <ref type="table">Table 5</ref>: Comparison temporal pose results regarding PA-MPJPE after rigid transformation from the ground truth. 243f means inputs contain 243 frame poses. indicates the use of 2D ground truth poses as input. Best results in bold.</p><p>In <ref type="table" target="#tab_8">Table 6</ref>, our method gains improvements in terms of MPJPE from 80.6mm to 64.3mm, by <ref type="bibr" target="#b15">16</ref>.3mm (relatively 20.2%). For PA-MPJPE, the improvement is from 60.5mm to 49.4mm, by 11.1mm (relatively 18.3%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Direct  <ref type="table" target="#tab_8">Table 6</ref>: Cross Action comparison to the FCN baseline with 2D ground truth input on Human3.6M in terms of mean per-joint position error (MPJPE) and PA-MPJPE (denoted by ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Additional Qualitative Results</head><p>Besides the aforementioned quantitative results, we also present some qualitative results. First, we visualize some hard poses, which are also rare in the subject protocol evaluation, in <ref type="figure" target="#fig_0">Figure 1</ref>. Under this protocol, our method can predict well even on challenging poses such as kowtow, side-lying and legs lifting. Next, <ref type="figure" target="#fig_2">Figure 2</ref> demonstrates some unseen poses in the cross-action protocol to verify our method's generalization ability. Finally, <ref type="figure" target="#fig_3">Figure 3</ref> shows some qualitative results with training only on the Human3.6M dataset and testing on unseen poses and unseen camera angles. Nevertheless, our method is still able to reconstruct many plausible 3D poses well. , our method, and the 3D ground truth poses on two kinds of test actions. When training action is "greet", poses like in (a), test on the action "sit" to get those predictions in (c). Similarly, when training action is "take photos" in (b), test on the action "sit down" to show the differences between the FCN and our method in (d). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The work is done when Ailing Zeng is an intern at Microsoft Research Asia.arXiv:2007.09389v1 [cs.CV] 18 Jul 2020 An unseen test pose (b) may be decomposed into local joint configurations (c) that appear in poses that exist in the training set (a). Our method takes advantage of this property to improve estimation of rare and unseen poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of (a) a fully connected layer, (b) group connected layer, (c) our split-and-recombine layer, and (d) our convolution layer for temporal models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of fully connected (FC), group connected (GP), late fusion (LF), early split (ES), split-fuse-split (SPS) and split-and-recombine (SR) models. The components of each layer are shown in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Training loss and test error (MPJPE in mm) of the baseline networks (a) LF, (b) ES, and (c) SFS with respect to different splitting configurations. Rare Pose Protocol 20% is used for evaluation. Testing errors of FC, SFS and our SR networks on poses with different degrees of rareness. The horizontal axis represents the top R% of rarest poses. (a) Cumulative mean error. (b) Mean error at 10% intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 1 :Fig. 2 :</head><label>12</label><figDesc>Visualization results trained with the subject protocol settings on the Human3.6M dataset. (a), (e) are the original test images. (b), (f) show the 3D pose predictions of temporal 3D pose baseline [27]. (c), (g) are the 3D pose predictions of our method. (d), (h) are the 3D ground truth poses.(a) Training Action : Greet (b) Training Action : Take Photos (c) Test Action: Sit (d) Test Action: Sit Down Visualization results for the cross-action protocol. (a), (b) are two kinds of original training actions. (c), (d) show the 3D predicted results by FCN [21]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization results for the MPI-INF-3DHP dataset. (a) are the original images. (b), (e) show the 3D predicted results by [21] from the front viewpoint and the top viewpoint. (c), (f) show the prediction poses of our method, and (d), (g) are the 3D ground truth poses from the front viewpoint and the top viewpoint, separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Different network structures used for 2D to 3D pose estimation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>?7.4 36.6 ? 2.8(7.1%) Rare Pose (20%) 89.1 76.0 60.1 62.5 59.2 ?16.8 48.6 ? 10.6(17.9%) Rare Pose (10%) 98.9 89.1 69.9 73.5 68.2 ?20.9 53.7 ? 14.5(21.3%)</figDesc><table><row><cell>Protocol</cell><cell>GP FC LF ES</cell><cell>SFS</cell><cell>Ours (SR)</cell></row><row><cell cols="3">Subject (100%) 62.7 46.8 39.8 42.0 39.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparing the SR network to different baseline networks under the Subject protocol and the Rare Pose protocol (with 10% and 20% of the rarest poses). MPJPE is used as the evaluation metric. The improvements of SFS from FC, and of SR from SFS, are shown as subscripts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Effect of using different dimension H for the proposed global context. 100% corresponds to the same dimensionality as the local feature.</figDesc><table><row><cell cols="2">Group Num 1(FC) 2</cell><cell>3</cell><cell>5</cell><cell>6</cell><cell>8</cell><cell>17</cell></row><row><cell cols="7">Subj.(100%) 46.8 41.4 37.7 36.6 41.1 46.7 91.5</cell></row><row><cell>Rare(20%)</cell><cell cols="6">76.0 55.9 51.0 48.6 55.6 60.6 115.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Mean testing error with different group numbers for Subject and Rare Pose 20% protocols.</figDesc><table><row><cell>Shuffled Groups 5</cell><cell>4</cell><cell>3</cell><cell>2 0(Ours)</cell></row><row><cell cols="4">Subject (100%) 53.8 49.7 45.9 43.4 36.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>by training the model on the Human3.6M training set and evaluating on the MPI-INF-3DHP test set to examine cross-dataset generalization ability. PCK and AUC are used as evaluation metrics, for which higher is better. InTable 6, our approach achieves the best cross-data generalization performance. It improves upon the state-of-the-art<ref type="bibr" target="#b6">[7]</ref> by a large margin (4.9% on PCK and 19.3% on AUC), indicating superior generalization ability.</figDesc><table><row><cell cols="9">Method Martinez [21] Mehta [22] Luo [19] Biswas [3] Yang [40] Zhou [42] Wang [37] Ci [7]</cell><cell>Ours</cell></row><row><cell>Outdoor</cell><cell>31.2</cell><cell>58.8</cell><cell>65.7</cell><cell>67.4</cell><cell>-</cell><cell>72.7</cell><cell>-</cell><cell cols="2">77.3 80.3 ? 3.9%</cell></row><row><cell>PCK</cell><cell>42.5</cell><cell>64.7</cell><cell>65.6</cell><cell>65.8</cell><cell>69.0</cell><cell>69.2</cell><cell>71.2</cell><cell cols="2">74.0 77.6 ? 4.9%</cell></row><row><cell>AUC</cell><cell>17.0</cell><cell>31.7</cell><cell>33.2</cell><cell>31.2</cell><cell>32.0</cell><cell>32.5</cell><cell>33.8</cell><cell cols="2">36.7 43.8 ? 19.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Martinez [21] 127.3 104.6 95.0 116.1 95.5 117.4 111.4 125.0 116.9 93.6 111.0 126.0 131.0 106.3 140.5 114.5 Ci [7] 102.8 88.6 79.1 99.3 80.0 91.5 93.2 89.6 90.4 76.6 89.6 102.1 108.8 90.8 118.9 93.4 Ours 92.3 71.4 71.8 86.4 66.8 79.1 82.5 86.6 88.9 93.4 66.1 83.0 74.4 90.0 97.8 82.0</figDesc><table /><note>Cross-dataset results on MPI-INF-3DHP. All models are trained on Human3.6M and tested on the MPI-INF-3DHP test set.Method Direct Discuss Eat Greet Phone Photo Pose Purcha. Sit SitD Smoke Wait WalkD Walk WalkT Avg.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Cross Action results compared with Fully Connected Network and Locally Connected Network on Human3.6M. Smaller values are better.</figDesc><table><row><cell>Method</cell><cell cols="8">Luvizon[20] Martinez[21] Park[25] Wang [37] Zhao[41] Ci[7] Pavllo [27] Cai[4] Ours</cell></row><row><cell>Subject (100%)</cell><cell>64.1</cell><cell>62.9</cell><cell>58.6</cell><cell>58.0</cell><cell>57.6</cell><cell>52.7</cell><cell>51.8</cell><cell>50.6 49.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparison on single-frame 2D pose detection input in terms of mean per-joint position error (MPJPE). Best result in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparison on single-frame 2D ground truth pose input in terms of MPJPE. With ground truth 2D pose as input, the upper bound of these methods is explored. 44.8 42.6 44.2 48.5 57.1 42.6 41.4 56.5 64.5 47.4 43.0 48.1 33.0 35.1 46.6 Ours 46.6 47.1 43.9 41.6 45.8 49.6 46.5 40.0 53.4 61.1 46.1 42.6 43.1 31.5 32.6 44.8</figDesc><table><row><cell cols="17">in each case, namely [4] for 2D keypoint detection and [7] for 2D ground truth</cell></row><row><cell>as input.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="17">Temporal Model Results on Human3.6M In Table 10, our approach achieves</cell></row><row><cell cols="17">the new state-of-the-art with either 2D keypoint detection or 2D ground truth</cell></row><row><cell cols="17">(with ) as input. Specifically, SRNet improves upon [27] from 37.2mm to</cell></row><row><cell cols="17">32.0mm (relative 14.0% improvement) with 2D ground truth input and from</cell></row><row><cell cols="17">46.8mm to 44.8mm (relative 4.3% improvement) with 2D keypoint detection in-</cell></row><row><cell cols="17">put. Besides, SRNet has around one fifth parameters 3.61M of [27](16.95M) with</cell></row><row><cell cols="4">243 frame poses as input.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hossain et al. [30]</cell><cell cols="16">48.4 50.77 57.2 55.2 63.1 72.6 53.0 51.7 66.1 80.9 59.0 57.3 62.4 46.6 49.6 58.3</cell></row><row><cell>Lee et al. [17]</cell><cell cols="16">40.2 49.2 47.8 52.6 50.1 75.0 50.2 43.0 55.8 73.9 54.1 55.6 58.2 43.3 43.3 52.8</cell></row><row><cell>Cai et al. [4]</cell><cell cols="16">44.6 47.4 45.6 48.8 50.8 59.0 47.2 43.9 57.9 61.9 49.7 46.6 51.3 37.1 39.4 48.8</cell></row><row><cell>Pavllo et al. [27]</cell><cell cols="16">45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8</cell></row><row><cell cols="17">Lin et al. [18] 42.5 Hossain et al. [30] 35.2 40.8 37.2 37.4 43.2 44.0 38.9 35.6 42.3 44.6 39.7 39.7 40.2 32.8 35.5 39.2</cell></row><row><cell>Lee et al. [17]</cell><cell cols="16">32.1 36.6 34.3 37.8 44.5 49.9 40.9 36.2 44.1 45.6 35.3 35.9 37.6 30.3 35.5 38.4</cell></row><row><cell>Pavllo et al. [27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>37.2</cell></row><row><cell>Ours</cell><cell cols="16">34.8 32.1 28.5 30.7 31.4 36.9 35.6 30.5 38.9 40.5 32.5 31.0 29.9 22.5 24.5 32.0</cell></row></table><note>Method Direct Discuss Eat Greet Phone Photo Pose Purcha. Sit SitD Smoke Wait WalkD Walk WalkT Avg.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 1 :</head><label>1</label><figDesc>Comparison on different design choices for the combination operator under the Subject protocol. MPJPE is used as the evaluation metric. 2D ground truth is used as input. The third row shows the number of learnable parameters of different models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>(with our implementation) and our SRNet are compared. Both MPJPE and PA-MPJPE (with ?) are used as the evaluation metrics. Both Basic and Pixel [7] normalization results of our method are reported. 38.8 29.7 37.8 34.6 42.5 39.8 32.5 36.2 39.5 34.4 38.4 38.2 31.3 34.2 36.3 Ours-Pixel 32.9 34.5 27.6 31.7 33.5 42.5 35.1 29.5 38.9 45.9 33.3 34.9 34.4 26.5 27.1 33.9</figDesc><table><row><cell>Method</cell><cell>Direct Discuss Eat Greet Phone Photo Pose Purcha. Sit SitD Smoke Wait WalkD Walk WalkT Avg.</cell></row><row><cell>Luvizon et al. [20]</cell><cell>63.8 64.0 56.9 64.8 62.1 70.4 59.8 60.1 71.6 91.7 60.9 65.1 51.3 63.2 55.4 64.1</cell></row><row><cell cols="2">Martinez et al. [21] 51.8 56.2 58.1 59.0 69.5 78.4 55.2 58.1 74.0 94.6 62.3 59.1 65.1 49.5 52.4 62.9</cell></row><row><cell>Park et al.[25]</cell><cell>49.4 54.3 51.6 55.0 61.0 73.3 53.7 50.0 68.5 88.7 58.6 56.8 57.8 46.2 48.6 58.6</cell></row><row><cell>Wang et al. [37]</cell><cell>47.4 56.4 49.4 55.7 58.0 67.3 46.0 46.0 67.7 102.4 57.0 57.3 41.1 61.4 40.7 58.0</cell></row><row><cell>Zhao et al. [41]</cell><cell>47.3 60.7 51.4 60.5 61.1 49.9 47.3 68.1 86.2 55.0 67.8 61.0 42.1 60.6 45.3 57.6</cell></row><row><cell>Ci et al. [7]</cell><cell>46.8 52.3 44.7 50.4 52.9 68.9 49.6 46.4 60.2 78.9 51.2 50.0 54.8 40.4 43.3 52.7</cell></row><row><cell>Pavllo et al. [27]</cell><cell>47.1 50.6 49.0 51.8 53.6 61.4 49.4 47.4 59.3 67.4 52.4 49.5 55.3 39.5 42.7 51.8</cell></row><row><cell>Cai et al. [4]</cell><cell>46.5 48.8 47.6 50.9 52.9 61.3 48.3 45.8 59.2 64.4 51.2 48.4 53.5 39.2 41.2 50.6</cell></row><row><cell>Ours</cell><cell>44.5 48.2 47.1 47.8 51.2 56.8 50.1 45.6 59.9 66.4 52.1 45.3 54.2 39.1 40.3 49.9</cell></row><row><cell cols="2">Martinez et al. [21] 37.7 44.4 40.3 42.1 48.2 54.9 44.4 42.1 54.6 58.0 45.1 46.4 47.6 36.4 40.4 45.5</cell></row><row><cell>Pham et al. [28]</cell><cell>36.6 43.2 38.1 40.8 44.4 51.8 43.7 38.4 50.8 52.0 42.1 42.2 44.0 32.3 35.9 42.4</cell></row><row><cell>Zhao et al. [41]</cell><cell>37.8 49.4 37.6 40.9 45.1 41.4 40.1 48.3 50.1 42.2 53.5 44.3 40.5 47.3 39.0 43.8</cell></row><row><cell>Wang et al. [37]</cell><cell>35.6 41.3 39.4 40.0 44.2 51.7 39.8 40.2 50.9 55.4 43.1 42.9 45.1 33.1 37.8 42.0</cell></row><row><cell>Ours-Basic</cell><cell>35.9 36.7 29.3 34.5 36.0 42.8 37.7 31.7 40.1 44.3 35.8 37.2 36.2 33.7 34.0 36.4</cell></row><row><cell>Ci et al.-Pixel [7]</cell><cell>36.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 2 :</head><label>2</label><figDesc>Detailed single pose comparison in terms of the mean per-joint position error (MPJPE) on Human3.6M. Below the double line are results from 2d ground truth inputs (indicated by ) to explore the upper bound of these methods. Best results in bold. 44.8 42.6 44.2 48.5 57.1 42.6 41.4 56.5 64.5 47.4 43.0 48.1 33.0 35.1 46.6 Ours 43.1 47.1 43.9 41.6 45.8 49.6 46.5 40.0 53.4 61.1 46.1 42.6 46.6 31.5 32.6 44.8 Hossain et al. [30] 35.2 40.8 37.2 37.4 43.2 44.0 38.9 35.6 42.3 44.6 39.7 39.7 40.2 32.8 35.5 39.2 Lee et al. [17] 32.1 36.6 34.3 37.8 44.5 49.9 40.9 36.2 44.1 45.6 35.3 35.9 37.6 30.3 35.5 38.4 Ours-243f 34.8 32.1 28.5 30.7 31.4 36.9 35.6 30.5 38.9 40.5 32.5 31.0 29.9 22.5 24.5 32.0</figDesc><table><row><cell>Method</cell><cell cols="15">Direct Discuss Eat Greet Phone Photo Pose Purcha. Sit SitD Smoke Wait WalkD Walk WalkT Avg.</cell></row><row><cell>Hossain et al. [30]</cell><cell cols="15">48.4 50.77 57.2 55.2 63.1 72.6 53.0 51.7 66.1 80.9 59.0 57.3 62.4 46.6 49.6 58.3</cell></row><row><cell>Lee et al. [17]</cell><cell cols="15">40.2 49.2 47.8 52.6 50.1 75.0 50.2 43.0 55.8 73.9 54.1 55.6 58.2 43.3 43.3 52.8</cell></row><row><cell>Cai et al. [4]</cell><cell cols="15">44.6 47.4 45.6 48.8 50.8 59.0 47.2 43.9 57.9 61.9 49.7 46.6 51.3 37.1 39.4 48.8</cell></row><row><cell>Pavllo et al. [27]</cell><cell cols="15">45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8</cell></row><row><cell cols="2">Lin et al. [18] 42.5 Pavllo et al.-243f [27] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-37.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 3 :</head><label>3</label><figDesc>Detailed temporal pose comparison in terms of the mean per-joint position error (MPJPE) on Human3.6M. Below the double line are results from 2d ground truth inputs (indicated by ) to explore the upper bound of these methods. Best results in bold. Direct Discuss Eat Greet Phone Photo Pose Purcha. Sit SitD Smoke Wait WalkD Walk WalkT Avg. Martinez et al. [21] 39.5 43.2 46.4 47.0 51.0 56.0 41.4 40.6 56.5 69.4 49.2 45.0 49.5 38.0 43.1 47.7 Pavlakos et al. [26] 34.7 39.8 41.8 38.6 42.5 47.5 38.0 36.6 50.7 56.8 42.6 39.6 43.9 32.1 36.5 41.8 Pavllo et al.[27] 36.0 38.7 38.0 41.7 40.1 45.9 37.1 35.4 46.8 53.4 41.4 36.9 43.1 30.3 34.8 40.0 Ours 35.8 39.2 36.6 36.9 39.8 45.1 38.4 36.9 47.7 54.4 38.6 36.3 39.4 30.3 35.4 39.4</figDesc><table><row><cell>Method</cell><cell></cell></row><row><cell>Fang et al.[9]</cell><cell>38.2 41.7 43.7 44.9 48.5 55.3 40.2 38.2 54.5 64.4 47.2 44.3 47.3 36.7 41.7 45.7</cell></row><row><cell>Park et al. [25]</cell><cell>38.3 42.5 41.5 43.3 47.5 53.0 39.3 37.1 54.1 64.3 46.0 42.0 44.8 34.7 38.7 45.0</cell></row><row><cell>Ci et al. [7]</cell><cell>36.9 41.6 38.0 41.0 41.9 51.1 38.2 37.6 49.1 62.1 43.1 39.9 43.5 32.2 37.0 42.2</cell></row><row><cell>Ours-Basic</cell><cell>26.0 28.9 23.7 26.9 27.4 33.1 27.9 25.0 32.4 40.9 28.8 29.2 29.3 23.3 24.5 28.5</cell></row><row><cell>Ours-Pixel</cell><cell>24.1 28.6 24.2 26.6 26.3 35.1 27.7 24.5 32.8 39.1 27.8 28.0 29.6 22.3 23.0 28.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 4 :</head><label>4</label><figDesc>Comparison single pose results regarding PA-MPJPE after rigid transformation from the ground truth. indicates the use of 2D ground truth poses as input. Best results in bold. Direct Discuss Eat Greet Phone Photo Pose Purcha. Sit SitD Smoke Wait WalkD Walk WalkT Avg. Lee et al.[17] 38.0 39.3 46.3 44.4 49.0 55.1 40.2 41.1 53.2 68.9 51.0 39.1 33.9 56.4 38.5 46.2 Hossain et al.[30] 35.7 39.3 44.6 43.0 47.2 54.0 38.3 37.5 51.6 61.3 46.5 41.4 47.3 34.2 39.4 44.1 Cai et al.[4] 35.7 37.8 36.9 40.7 39.6 45.2 37.4 34.5 46.9 50.1 40.5 36.1 41.0 29.6 33.2 39.0 Lin et al. [18] 32.5 35.3 34.3 36.2 37.8 43.0 33.0 32.2 45.7 51.8 38.4 32.8 37.5 25.8 28.9 36.8 Pavllo et al.-243f [27] 34.1 36.1 34.4 37.2 36.4 42.2 34.4 33.6 45.0 52.5 37.4 33.8 37.8 25.6 27.3 36.5 Ours-243f 31.9 33.7 34.7 35.0 35.5 42.8 36.4 30.5 43.6 51.3 36.7 32.5 36.5 27.5 25.7 34.9 Ours-243f 23.7 25.2 22.9 23.1 24.0 28.7 25.0 22.1 31.8 32.8 24.8 23.5 23.4 17.0 18.3 24.3</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Discuss Eat Greet Phone Photo Pose Purcha. Sit SitD Smoke Wait WalkD Walk WalkT Avg. FCN-Pixel [21] 117.0 67.4 62.6 93.0 59.5 72.8 66.7 80.0 71.2 71.6 58.6 75.2 73.3 114.9 125.0 80.6 Ours-Basic 91.1 54.8 59.0 71.2 50.9 61.5 65.0 71.4 76.6 74.0 50.3 64.8 58.1 78.0 85.8 67.5 Ours-Pixel 86.2 53.0 55.0 70.5 47.9 57.9 63.1 68.4 71.2 72.9 47.5 59.4 56.3 70.8 83.8 64.3 FCN-Pixel[21] ? 91.9 55.3 51.8 75.2 49.3 60.6 57.3 64.7 62.2 60.6 49.5 62.7 61.3 95.4 99.8 60.5 Ours-Basic ? 65.9 42.4 46.3 54.5 39.8 46.6 50.6 55.8 58.4 57.4 39.3 49.6 45.0 56.7 61.8 51.3 Ours-Pixel? 61.7 42.0 44.2 53.1 38.5 45.2 49.5 53.6 55.5 55.5 37.9 46.4 43.8 54.7 59.7 49.4</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Code for 3d human pose estimation in video with temporal convolutions and semisupervised training</title>
		<ptr target="https://github.com/facebookresearch/VideoPose3D" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Code for optimizing network structure for 3d human pose estimation</title>
		<ptr target="https://chunyuwang.netlify.app/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lifting 2d human pose to 3d: A weakly supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhowmick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10905" to="10914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deepfuse: An imu-aware network for real-time 3d human pose estimation from multi-view image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04071</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating multiple diverse hypotheses for human 3d pose consistent with 2d joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08289</idno>
		<title level="m">Trajectory space factorization for deep video-based 3d human pose estimation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04989</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08961</idno>
		<title level="m">3d human pose estimation with relational networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A unified deep framework for joint 3d pose estimation and action recognition from a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salmane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khoudour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crouzil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06968</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thorm Ahlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rayat Imtiaz Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<title level="m">On the convergence of adam and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3108" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d human pose estimation with siamese equivariant embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>V?ges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?rincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">339</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05512</idno>
		<title level="m">Generalizing monocular 3d human pose estimation in the wild</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
