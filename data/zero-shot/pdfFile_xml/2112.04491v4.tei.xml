<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Image Restoration by Revisiting Global Information Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
							<email>chenliangyu@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengpeng</forename><surname>Chen</surname></persName>
							<email>chencp@live.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Image Restoration by Revisiting Global Information Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Global operations, such as global average pooling, are widely used in top-performance image restorers. They aggregate global information from input features along entire spatial dimensions but behave differently during training and inference in image restoration tasks: they are based on different regions, namely the cropped patches (from images) and the full-resolution images. This paper revisits global information aggregation and finds that the image-based features during inference have a different distribution than the patch-based features during training. This train-test inconsistency negatively impacts the performance of models, which is severely overlooked by previous works. To reduce the inconsistency and improve test-time performance, we propose a simple method called Test-time Local Converter (TLC). Our TLC converts global operations to local ones only during inference so that they aggregate features within local spatial regions rather than the entire large images. The proposed method can be applied to various global modules (e.g., normalization, channel and spatial attention) with negligible costs. Without the need for any fine-tuning, TLC improves state-of-the-art results on several image restoration tasks, including single-image motion deblurring, video deblurring, defocus deblurring, and image denoising. In particular, with TLC, our Restormer-Local improves the state-of-the-art result in single image deblurring from 32.92 dB to 33.57 dB on GoPro dataset. The code is available at https://github.com/megvii-research/tlc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image restoration is the task of estimating the clean image from a corrupt (e.g., motion blur, noise, etc.) image. Recently, deep learning based models <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b6">7]</ref> have achieved state-of-the-art (SOTA) performance in this field. The global information, which is aggregated along entire spatial dimensions, are increasingly indispensable for the top performance restorers: HINet <ref type="bibr" target="#b6">[7]</ref> adopts Instance Normalization (IN <ref type="bibr" target="#b42">[43]</ref>) module which performs global normalization along the entire spatial dimension. MPRNet <ref type="bibr" target="#b54">[55]</ref>, SPDNet <ref type="bibr" target="#b50">[51]</ref>, FFA-Net <ref type="bibr" target="#b33">[34]</ref>, etc. adopt Squeeze and Excitation (SE <ref type="bibr" target="#b12">[13]</ref>) module which learns to use global average-pooled features to selectively emphasise informative features. Restormer <ref type="bibr" target="#b52">[53]</ref> adopt transposed self-attention for encoding the global information implicitly.</p><p>However, restoration models are usually trained on patches cropped from images and inference directly on full-resolution images <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b52">53]</ref>. In contrast to arXiv:2112.04491v4 [cs.CV] 2 Aug 2022 resizing the input images during both training and inference in the high-level vision task, resizing the images in the low-level vision task is avoided to preserve the image details. As a result, the regional range of the inputs for training and inference varies widely. For example, during training in MPRNet <ref type="bibr" target="#b54">[55]</ref>, the range of region for each patch is only 7% of full-resolution images (256 ? 256 vs. 720 ? 1280) in GoPro dataset. In this case, the model can only learn to encode a local part of the image due to the limited region of patches ( <ref type="figure" target="#fig_0">Figure 1a</ref>). It may be difficult to encode the global clues of full-resolution images, thereby providing sub-optimal performance at test time. This potential issue is severely overlooked by previous works. This paper revisits the global information aggregation in image restoration tasks. We analyze the global avg-pooled features and find that the entire-imagebased features during inference may distribute very differently from the patchbased features during training <ref type="figure" target="#fig_4">(Figure 3a</ref> Left). This shifts in the global information distribution in training and inference can negatively impact the performance of model. To solve this issue, we proposed a novel test-time approach called Test-time Local Converter (TLC) for bridging the gap of information aggregation between training and inference. The global operation (e.g., global average pooling in SE module <ref type="bibr" target="#b12">[13]</ref>) is converted to a local one only during inference, so that they aggregate features within local spatial regions as in the training phase ( <ref type="figure" target="#fig_0">Figure 1b</ref>). As a result, the entire-image-based "local" information during inference has similar distribution as patches-based "global" information during training <ref type="figure" target="#fig_4">(Figure 3a</ref> Right). The proposed technique is generic in the sense that it can be applied on top of any global operation without any fine-tuning, and boost the performance of various modules (e.g., SE, IN) with negligible costs.</p><p>Our TLC can be conveniently applied to already trained models. We conduct extensive experiments to demonstrate the effectiveness of TLC over a variety of models and image restoration tasks. For example, for single-image motion deblurring on GoPro dataset <ref type="bibr" target="#b27">[28]</ref>, our TLC improves the PSNR of HINet <ref type="bibr" target="#b6">[7]</ref>, MPRNet <ref type="bibr" target="#b54">[55]</ref>, and Restormer <ref type="bibr" target="#b52">[53]</ref> by 0.37 dB, 0.65 dB, and 0.65 dB, respectively.</p><p>Remarkably, TLC improves the state-of-the-art results on single-image motion deblurring, video motion deblurring, defocus deblurring (single-image and dualpixel data), and image denoising (gaussian grayscale/color denoising).</p><p>Our contributions can be summarized as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image Restoration. Image Restoration tasks, e.g. denoising, deblurring, deraining, dehazing, etc. aim to restore the degraded image to the clean one. Deep learning based restoration models have achieved state-of-the-art results <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b52">53]</ref> recently. The training data are cropped into patches and fed into the model in the training phase. Most methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55]</ref> inference by the full-resolution image, which leads to a train-test inconsistency problem. Some methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref> divide the input image into patches with fixed size and process each patch independently, but this strategy may introduce boundary artifacts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Global Information in Image Restoration Models. Attention modules are designed to model long-range dependency using a single layer directly. SENet <ref type="bibr" target="#b12">[13]</ref> and GENet <ref type="bibr" target="#b11">[12]</ref> reweight channel dependency with global information aggregated by global average pooling. CBAM <ref type="bibr" target="#b46">[47]</ref> uses both avg-pooled and maxpooled features to rebalance the importance of different spatial positions and channels. These channel and spatial attention modules have been successfully adopted to image restoration models for various tasks, e.g., deblurring <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b5">6]</ref> deraining <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b50">51]</ref>, super-resolution <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b8">9]</ref>, denoising <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b52">53]</ref> and dehazing <ref type="bibr" target="#b33">[34]</ref>. Besides, HINet <ref type="bibr" target="#b6">[7]</ref> introduces Instance Normalization (IN <ref type="bibr" target="#b42">[43]</ref>) to image restoration tasks, which normalizes each channel of the features by its mean and variance. Once again, the performance improvement brought by IN proves the effectiveness of global information.</p><p>This paper mainly discusses these modules, which aggregate information from all spatial positions in input features (i.e., globally), as representatives. We find that the performance of these modules may be sub-optimal due to the train-test inconsistency mentioned above.</p><p>Local Spatial Information Modules. In local spatial schemes, the information is computed within a local spatial area for each pixel. Local Response Normalization (LRN) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> computes the statistics in a small neighborhood for each pixel. To reduce the computational loads, SwinIR <ref type="bibr" target="#b24">[25]</ref> and Uformer <ref type="bibr" target="#b45">[46]</ref> apply self-attention within small spatial windows of size 8 ? 8 around each pixel. In semantic image synthesis tasks, SPatially-Adaptive (DE)normalization (SPADE) <ref type="bibr" target="#b31">[32]</ref> utilize the input semantic layout for modulating the activations through a spatially-adaptive, learned transformation. Spatial region-wise normalization (RN) <ref type="bibr" target="#b51">[52]</ref>, is proposed for better inpainting network training.</p><p>However, directly applying those modules to existing restoration models is not practical, as retraining or finetuning are required. Besides, these modules are designed to model local context in both the training and inference phase that they are constrained to have limited sizes of receptive field. Conversely, our proposed approach does not need to retrain or finetune the model. The region's size used for information aggregation during inference will be equal to or larger than the size of the input during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis and Approach</head><p>In this section, we first introduce the image restoration pipeline and analyze the train-test inconsistency of global information aggregation induced by it. Next, to solve the inconsistency, we illustrate our novel approach, Test-time Local Converter (TLC), and the details of extending TLC to existing modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisit Global Operations in Image Restoration Tasks</head><p>Image Restoration Pipeline. We briefly describe the image restoration pipeline used in the state-of-the-art methods. For practical application, datasets for image restoration tasks (e.g., deblurring) are usually composed of high-resolution images. Due to the need for data augmentation and the limitation of GPU memory, it is common practice to train models with small patches cropped from high-resolution images ( <ref type="figure" target="#fig_0">Fig. 1a</ref>). For example, MPRNet and HINet are trained on 256 ? 256 patches cropped from 720 ? 1280 images in GoPro datasets. During inference, the trained model directly restores high-resolution images ( <ref type="figure" target="#fig_0">Fig. 1b</ref> Left). Therefore, there are train-test inconsistencies of the inputs to the model: a local region of the image during training and the entire image during inference.</p><p>Train-Test Inconsistency of Global Information Aggregation. Unlike local operations (e.g., convolution) that operate within a local spatial area for each pixel, global operations (e.g., global average pool and global attention) operate along entire spatial dimensions. As a result, global operations have global receptive fields on arbitrary input resolutions.</p><p>However, the range of receptive fields for global operation is limited by the size of input features. This property introduces significantly different behaviors for global operations during training and inference in image restoration tasks: their input features are based on different range of regions, namely the cropped patches (from images) and the full-resolution images. This inconsistency will affect the generalization of models. In training, parameters are optimized by the patches-based features. While in the test phase, the layer inference the results by the entire-image-based features. In the following, we analyze the behavior of global information aggregation, both qualitatively and statistically.</p><p>Statistical Analysis. To analyze the effects of train-test inconsistency of global information, we compare the mean statistics based on patches in the training set and full-resolution images in the test set. The mean statistics (i.e., global averagepooled features) are aggregated by the first SE layer of the second encoder in MPRNet <ref type="bibr" target="#b54">[55]</ref>. As shown in <ref type="figure" target="#fig_4">Fig. 3a</ref>, the mean statistics distribution shifts from training (green) to inference (blue). It is hard for restorers to adapt to the severe changes in information distribution, resulting in performance degradation.</p><p>Qualitative Analysis. Intuitively, consistent with the training phase, cropping the images into patches and predicting the results independently during inference can alleviate the patch/full-image inconsistency issue described above. We conduct a visual comparison of the MPRNet deblurring results on GoPro datasets. <ref type="figure" target="#fig_1">Fig. 2</ref> shows a challenging visual example. The image-based result ( <ref type="figure" target="#fig_1">Fig. 2b</ref>) fails to remove blurs completely. On the contrary, the patch-based result ( <ref type="figure" target="#fig_1">Fig. 2c</ref>) is cleaner with less motion blur but introduces the artifacts at the patch boundaries. This confirms that direct inference on full-resolution results in sub-optimal performance. Though cropping images for inference improves the quality of image recovery, such a strategy will inevitably cause a new problem, i.e., patch boundary artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Test-time Local Converter</head><p>In order to reducing train-test inconsistency and improve test-time performance of model, we propose a test-time solution named Test-time Local Converter (TLC). Instead of changing the training strategy or cropping the images, our TLC directly change the range of region for information aggregation at feature   level during inference phase. As shown in <ref type="figure" target="#fig_0">Figure 1b</ref>, TLC converts the spatial information aggregation operation from global to local, i.e., each pixel of the feature aggregates its feature locally. In detail, the input feature X of global operation is sliced into overlapping window with size of K h ?K w (which are treated as hyper-parameters). Then, information aggregation operation is applied independently to each overlapping window. As a result, the statistics distribution shifts are reduced by TLC as shown in <ref type="figure" target="#fig_4">Fig. 3a</ref>: the statistics distribution obtained by our MPRNet-Local (red) is close to the original MPRNet in the training phase (green). Besides, as shown in <ref type="figure" target="#fig_1">Fig. 2d</ref>, our TLC generates a sharp image without boundary artifacts.</p><p>An advantage of our intentionally simple design is that efficient implementations of local processing make extra computation cost negligible, allowing image restorers to use TLC feasibly. Next, we will discuss the implementations of average operation, which is an example of information aggregation and is widely used in models for image restoration.</p><p>Efficient Implementation of Information Aggregation. The (global) information aggregation of a feature layer X ? R H?W (without loss of generality, we ignore the channel dimension), can be formulated as:</p><formula xml:id="formula_0">?(X, f ) = 1 HW H p=1 W q=1 f (X p,q ).<label>(1)</label></formula><p>where f : R ? R defines how information are calculated, and ?(X, f ) ? R denotes the aggregated information. It's computational complexity is O(HW ). For local information aggregation, each pixel e.g.(i, j) aggregates the information in a local window (size K h ? K w ) of feature X ? R H?W could be formulated as:</p><formula xml:id="formula_1">?(X, f ) i,j = 1 K h K w p q f (X p,q ),<label>(2)</label></formula><p>where (p, q) in the local window of (i, j), ?(X, f ) ? R H?W indicates the aggregated local information, and K h , K w are hyperparameters. The edge case, eg. (i, j) is the boundary of X, is not considered above for simplicity. In practice, we implement ?(X, f ) by two steps. First, sliding windows (size of K h ?K w ) with stride equals 1 to aggregate the local information for each pixel in non-edge case. Second, padding the result by replication of its boundary for edge case. The first step's computational complexity is O(HW K h K w ). But mean/sum aggregation within each local window could be treated as submatrix sum problem and solved by prefix sum trick <ref type="bibr" target="#b10">[11]</ref> with O(1) complexity <ref type="bibr" target="#b2">[3]</ref>. As a result, the overall complexity could be reduced to O(HW ) which is consistent with global information aggregation operation, i.e. Eq.(1). Therefore, our TLC do not induce a computational bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extending TLC to Existing Modules</head><p>In this subsection, we borrow the notations defined above (e.g., ?/? denotes global/local information aggregation operation, respectively). To extend TLC to existing modules, we convert the information aggregation operation from global (i.e., ?) to local (i.e., ?). In the following, we take Squeeze-and-Excitation(SE) and Instance Normalization(IN) as representatives, and it can be easily applied to other normalization modules such as Group Normalization (GN <ref type="bibr" target="#b47">[48]</ref>) or variants of SE (e.g. CBAM <ref type="bibr" target="#b46">[47]</ref>, GE <ref type="bibr" target="#b11">[12]</ref>).</p><p>Extending TLC to SE Block. We briefly revisit the squeeze-and-excitation (SE <ref type="bibr" target="#b12">[13]</ref>) block first. For a feature map X ? R H?W ?C with a spatial size of (H, W ) and C channels, SE block first squeezes the global spatial information into channels, it could be denoted as ?(X (c) , id), ?c ? [C], where id(t) = t, ?t ? R. And then, a multilayer perceptron (MLP) follows to evaluate the channel attention, which re-weights the feature map. The squeeze on the global spatial dimension could be sub-optimal as global information distribution shifts. To solve this, we extend TLC to SE by replacing ?(X (c) , id) to ?(X (c) , id), ?c ? [C]. As in SE, an MLP along the channel dimension follows. Differently, the feature map is re-weighted by the element-wise attention in this case.</p><p>Extending TLC to IN. For a feature map X ? R H?W (we omit the channel dimension for simplicity), the normalized feature Y by IN is computed as: Y = (X ? ?)/?, where statistics ? and ? are the mean and variance computed over the global spatial of X:</p><formula xml:id="formula_2">? = ?(X, id), ? 2 = ?(X, sq) ? ? 2 ,<label>(3)</label></formula><p>where id(t) = t, sq(t) = t 2 , ?t ? R. Besides, learnable parameters ?, ? are used to scale and shift the normalized feature Y, we omit them for simplicity. During inference, we can extend TLC to IN by replacing ?(X, id) and ?(X, sq) in Eq. <ref type="formula" target="#formula_2">(3)</ref> to ?(X, id) and ?(X, sq) respectively. As a result, each pixel is normalized by statistics in neighborhood.</p><p>Extending to transposed self-attention. As introduced in Sec. 3.2, our TLC can convert the transposed self-attention in Restormer <ref type="bibr" target="#b52">[53]</ref> from global to local regions. However, due to inefficiency and limitation of GPU memory, i.e., different attention map for each pixel, we use a large stride rather than one to the TLC in transposed self-attention. Specifically, transposed self-attention is applied independently to each overlapping windows of K h ? K w sliced from input features. The overlapping outputs are then fused by concatenating along spatial dimensions and averaging over the overlapping regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Apart from our method, the range of input at image level also has a direct impact on train-test inconsistency. On the one hand, larger size of patches used for training pushes the patch-based information closer to image-based information. On the other, dividing the image into patches for inference can avoid the patch/entire-image inconsistency. We will discuss these two possible solutions and their drawbacks next.</p><p>Dividing the image into patches for independently inference may alleviate the inconsistency issue. However, such a strategy inevitably gives rise to two drawbacks <ref type="bibr" target="#b21">[22]</ref>. First, border pixels cannot utilize neighbouring pixels that are out of the patch for image restoration. Second, the restored image may introduce "boundary artifacts" <ref type="bibr" target="#b18">[19]</ref> around each patch. As shown in <ref type="figure" target="#fig_1">Fig. 2c</ref>, an obvious vertical split line is introduced by patch partition which severely damages the image quality. In contrast, our "partition" is at feature level instead of image level so that different local windows can still interact with each other through other modules (e.g., convolutions) in the network. As shown in <ref type="figure" target="#fig_1">Fig. 2d</ref>, the proposed method generates much clearer images without artifacts.</p><p>Besides, inference with overlapping patches will introduce considerable additional computational costs, as the overlapping regions are restored twice or more by the entire model. While models with our TLC directly restore whole images and TLC has low extra computing costs <ref type="table" target="#tab_8">(Table 7)</ref>. Furthermore, boundary artifacts are also found in the predictions based on overlapping patches. More details of discussion and comparison are in the supplemental material.</p><p>Training on full-images instead of patches is another straightforward idea to bridge the gap in global information distribution between training and inference, but it is impracticable due to limited device constrains. The scaling up of resolution leads to prohibitively high GPU memory consumption with existing image restorers. For example, using V100-32G, the size of patches for training Restormer <ref type="bibr" target="#b52">[53]</ref> can only up to 384 ? 384, which is still significantly smaller than the original image size (e.g., 720 ? 1280 in GoPro dataset). Furthermore, though Restormer is trained with larger patches than common practice, our TLC can significantly improve its performance <ref type="table" target="#tab_1">(Table 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we do quality and quantity experiments to show the effects of train-test inconsistency, and our proposed approach Test-time Local Converter (TLC) can reduce this inconsistency. Next, the extensibility of TLC and the choice of hyperparameters are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>To verify the effectiveness of the proposed TLC, we apply it to various existing top-performing models for six image restoration tasks: (1) single-image motion deblurring, (2) video deblurring, (3) defocus deblurring, (4) image denoising, (5) image deraining and (6) image dehazing. We report the standard metrics in image restoration, including Peak Signal to Noise Ratio (PSNR) and Structural SIMilarity index (SSIM). Implementation Details. We use the publicly available trained models with global operations (e.g., global attention, global normalization) and directly apply proposed TLC to them without any extra training. Specifically, for Restormer <ref type="bibr" target="#b52">[53]</ref> with TLC, the forward pass of transposed attention is applied independently to each overlapping window sliced from original input features. While for SE <ref type="bibr" target="#b12">[13]</ref> (used in MPRNet <ref type="bibr" target="#b54">[55]</ref>, RNN-MBP <ref type="bibr" target="#b67">[68]</ref>, SPDNet <ref type="bibr" target="#b50">[51]</ref> and FFANet <ref type="bibr" target="#b33">[34]</ref>) and Instance Normalization <ref type="bibr" target="#b42">[43]</ref> (used in HINet <ref type="bibr" target="#b6">[7]</ref>), TLC is extended to them as illustrated in Sec. 3.3. Models with our TLC is marked with "-Local" suffix and the local window size is set to 384 ? 384 if not specified. We will discuss the impact of this hyper-parameter in the Sec. 4.2.</p><p>Single-image Motion Deblurring. We integrate our TLC with existing top-performing models (e.g. HINet <ref type="bibr" target="#b6">[7]</ref>, MPRNet <ref type="bibr" target="#b54">[55]</ref>, and Restormer <ref type="bibr" target="#b52">[53]</ref>) and evaluate them on test set of GoPro <ref type="bibr" target="#b27">[28]</ref> and HIDE <ref type="bibr" target="#b35">[36]</ref> dataset. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the performance of both three models are significantly improved by our approach and our models achieve new state-of-the-art results on all datasets.</p><p>In detail, the PSNR on GoPro of HINet, MPRNet and Restormer are improved by 0.37 dB, 0.65 dB and 0.65dB, respectively. And our Restormer-local exceeds the previous best result (i.e., Restormer <ref type="bibr" target="#b52">[53]</ref>   and 0.27 dB, respectively. And our Restormer-local exceeds the previous best result (i.e., Restormer <ref type="bibr" target="#b52">[53]</ref>) by 0.27 dB. Visual results of our methods are shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. As one can see, based on its significant quantitative improvements, TLC can help the original model generate more sharp images with clearer numeric symbols. Video Motion Deblurring. We apply our TLC to state-of-the-art video deblurring method (i.e., RNN-MBP <ref type="bibr" target="#b67">[68]</ref>) and evaluate different video deblurring algorithms on GoPro datasets. As shown in <ref type="table" target="#tab_3">Table 2</ref>, our TLC improve previous state-of-the-art method by 0.48 dB on PSNR, and set a new state-of-the-art result at 33.80 dB.</p><p>Defocus Deblurring. <ref type="table" target="#tab_4">Table 3</ref> shows image fidelity scores of different defocus deblurring methods on the DPDD dataset <ref type="bibr" target="#b0">[1]</ref>. Following <ref type="bibr" target="#b0">[1]</ref>, results are reported on traditional signal processing metrics (i.e., PSNR, SSIM, and MAE) and learned perceptual image patch similarity (LPIPS) proposed by <ref type="bibr" target="#b62">[63]</ref>. TLC are applied to the state-of-the-art method Restormer and get Restormer-Local. Our Restormer-Local significantly outperforms the state-of-the-art schemes for the single-image and dual-pixel defocus deblurring tasks on all scene categories. Take PSNR as evaluation metrics, our TLC improves Restormer by 0.21?0.3 dB and 0.35?0.40 dB on single-image and dual-pixel defocus deblurring, respec-  tively. <ref type="figure" target="#fig_6">Fig. 5</ref> shows that our model recovered images of better quality on texture and edge detail. Image Denoise. We perform denoising experiments on synthetic benchmark dataset Urban100 <ref type="bibr" target="#b13">[14]</ref> generated with additive white Gaussian noise. <ref type="table" target="#tab_5">Table 4b</ref> and <ref type="table" target="#tab_5">Table 4b</ref> show PSNR scores of different approaches on several benchmark datasets for grayscale and color image denoising, respectively. Consistent with existing methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b52">53]</ref>, we include noise levels 15, 25 and 50 in testing. The evaluated methods are divided into two experimental categories: (1) learning a single model to handle various noise levels, and (2) learning a separate model for each noise level. We apply TLC to state-of-the-art method Restormer. Our TLC brings 0.06?0.12 dB improvement on grayscale image denoising and brings 0.08?0.15 dB improvement on color image denoising. <ref type="figure" target="#fig_7">Fig. 6</ref> shows that our method clearly removes noise while maintaining fine details.</p><p>Image Deraining. We compare the deraining results of SPDNet <ref type="bibr" target="#b50">[51]</ref> and our SPDNet-local on SPA-Data <ref type="bibr" target="#b43">[44]</ref> benchmark. As shown in <ref type="table" target="#tab_6">Table 5</ref>, our ap-  proach brings 0.18 dB improvement to SPDNet. <ref type="figure">Fig. 7</ref> shows that our approach recovered images of better quality on both details and color fidelity. Image Dehazing. We compare the dehazing results of FFANet <ref type="bibr" target="#b33">[34]</ref> and our FFANet-local on Synthetic Objective Testing Set (SOTS) from RESIDE <ref type="bibr" target="#b19">[20]</ref> dataset. The local window size is set to 416 ? 416. As shown in <ref type="table" target="#tab_7">Table 6</ref>, our approach brings 0.42 dB improvement to FFANet in outdoor scenarios. We also test the results on realistic hazy images in RESIDE <ref type="bibr" target="#b19">[20]</ref> dataset for subjective assessment. As shown in <ref type="figure">Fig. 8</ref>, our FFANet-Local effectively removes hazy and generate visually pleasing result with high color fidelity. More high-resolution visualization results are in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Size of Local Window</head><p>Size of local window is a hyper-parameter for TLC, which controls the scope of local information aggregation operation. To determinate the hyperparameter,   i.e., local window size of each layer (which aggregates the spatial context) as we mentioned in Sec.3.2, we propose a simple strategy: A calibration image is fed into the model, and the spatial sizes of these feature layers are recorded as their local window size. Therefore, the hyperparameter could be determined by the spatial size of the calibration image, and we denoted the image size as "local window size" for simplicity in the following. Besides, the calibration could be accomplished offline, thus does not increase the test latency. We apply TLC on MPRNet <ref type="bibr" target="#b54">[55]</ref> to investigate the impact of different size of local window on the model performance. As shown in <ref type="figure" target="#fig_4">Fig. 3b</ref>, TLC can significantly improve the performance of MPRNet over a wide range of window size (from 256 ? 256 to 640 ? 640). Interestingly, the optimal window size (i.e., 384 ? 384) for the test phase is not exactly equal but may be larger than the training patch size (i.e., 256 ? 256). We conjecture this is caused by the trade-off between the benefits of more information provided by the larger window and the side-effects of statistic inconsistency between training and inference. In addition, since our approach does not require retraining, it is easy and flexible to tune the size of local window. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Extensibility and Complexity</head><p>We apply TLC to various modules and compare its improvement of performance and complexity of computation. We use a simple UNet model (i.e., HINet Simple without HIN <ref type="bibr" target="#b6">[7]</ref>) as baseline (denoted as UNet). Attention modules (e.g., SE <ref type="bibr" target="#b12">[13]</ref>, GE-? ? <ref type="bibr" target="#b11">[12]</ref> and CBAM <ref type="bibr" target="#b46">[47]</ref>) are added to UNet encoder following SENet <ref type="bibr" target="#b12">[13]</ref>, while Normalization modules (e.g., IN <ref type="bibr" target="#b42">[43]</ref> and GN <ref type="bibr" target="#b47">[48]</ref>) are added to the UNet following HINet <ref type="bibr" target="#b6">[7]</ref>. Implementation details. Models are trained on GoPro <ref type="bibr" target="#b27">[28]</ref> dataset following the most training detail of HINet Simple <ref type="bibr" target="#b6">[7]</ref>. Specially, the default size of patches for training is 256 ? 256, and the default batch size is 64. We also use warm-up strategy in the first 5000 iterations. According to Sec. 4.2, the local window size is set to 384 ? 384 during inference. We use MACs (i.e. multiplieraccumulator operations) to evaluate the computational cost of models, which is estimated when the input is 512 ? 512.</p><p>Results. As shown in <ref type="table" target="#tab_8">Table 7</ref>, our approach achieves performance gains with marginal costs. In detail, TLC improves the performance (i.e., PSNR) of IN, GN and GE-? ? by 0.16 dB, 0.12 dB, and 0.17 dB, respectively. For SE and CBAM, TLC boosts the performance(i.e. PSNR) by 0.39 dB and 0.52 dB respectively with less than 0.2% extra MACs. It demonstrates the extensibility, effectiveness and efficiency of TLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we reveal the global information distribution shifts between training and inference due to train-test inconsistency of global operation, which negatively impacts the performance of restoration model. We propose simple yet test-time solutions, dubbed Test-time Local Converter, which replaces the information aggregation region from the entire spatial dimension to the local window to mitigate the inconsistency between training and inference. Our approach does not require any retraining or finetuning, and boosts the performance of models on various tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this document, we provide details of comparison between inference with (overlapping) patches and our TLC (Section A) and additional visualized results (Section B) of our approach and existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Inference with (overlapping) patches vs our TLC</head><p>In this section, we compare our TLC with patch inference. First, <ref type="table" target="#tab_1">Table A1</ref> experiments on GoPro dataset found that MPRNet model with our test-time method (TLC) achieves higher PSNR (33.31 dB vs. 33.15 dB) with less inference time (1.69s vs. 6.50s) than inference on overlapping patches cropped from image. Second, boundary artifacts are also found in the predictions based on overlapping patches ( <ref type="figure" target="#fig_0">Figure A1</ref>) while our TLC generates natural result without visible artifacts <ref type="figure" target="#fig_1">(Figure 2d</ref>). We will describe the details next.</p><p>A.1 Discussion of inference on overlapping patches.</p><p>Inference with overlapping patches reduces the train-test statistic inconsistency so that it also improves the performance of models. However, it has three main drawbacks:</p><p>First, it introduces additional computational costs, as the overlapping regions are restored twice or more by the entire model. While models with our TLC directly restore whole images and TLC has low extra computing costs <ref type="table" target="#tab_8">(Table 7)</ref>.</p><p>Second, it can not alleviate boundary artifacts on deblurring tasks. We speculate that this is because the global statistics of the  two overlapping patches may differ significantly. Restoration of images affected by severe blur requires large receptive field information and the inference of many models (e.g., MPRNet) highly depends on global information. As a result, predictions of different patches have different estimate of motion blur so that their fusion also show unnatural dividing lines ( <ref type="figure" target="#fig_0">Figure A1</ref>).</p><p>Third, limited size of patch limits the receptive field of information, which harms the model performance. Pixels in an overlapping region have more range of other pixels for interactions, so that the results based on more overlapping regions are better than fewer ones <ref type="table" target="#tab_1">(Table A1</ref>). While our TLC utilises full image information and achieves the best results. <ref type="table" target="#tab_1">Table A1</ref> shows the inference time of MPRNet with different test methods on RTX2080Ti GPU for a 720 ? 1280 image. With naive implementation using the cumulative sum function provided by Pytorch, TLC introduces 27% extra times (2.03s vs. 1.6s). Note that MPRNet with our TLC still faster and better than inference with overlapping patches. This shows the efficiency of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Inference Speed</head><p>The cumulative sum operator is unfriendly for GPU because it is a sequential algorithm. As a result, the extra test time caused by TLC will be greater than the theoretical value. A simple way to speed up is to reduce the number of cumulative sum calculations by sampling. Specifically, we can reduce the size of the matrix to r 2 times its original size by grid sampling with stride r and use the mean of the sampled matrix to approximate the mean of the original matrix. As a result, the number of calculations needed to do the (onedimensional) cumulative sum is reduce by r times so that TLC will has 4.78? faster speed. With this careful design, faster TLC only introduces 5.6% extra times (1.69s vs. 1.6s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Boundary artifacts.</head><p>Inference with patches Cropping the image into patches and predict the result independently induces unsmoothness of the boundary (i.e. "boundary artifacts" as demonstrated in <ref type="bibr" target="#b18">[19]</ref>. We give some example images of block boundary artifacts in <ref type="figure" target="#fig_1">Figure A2</ref>, which are generated by MPRNet <ref type="bibr" target="#b55">[56]</ref> on GoPro <ref type="bibr" target="#b27">[28]</ref> dataset. There are obvious boundary artifacts in <ref type="figure" target="#fig_1">Figure A2</ref> which seriously degrades the quality of the image. Inference with overlapping patches. We follow the implementation details of SwinIR to do inference with overlapping 256 ? 256  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Visualized Results</head><p>In this section, we provide additional visual results of statistics distribution (Section B.1) and qualitative comparisons between our approach and existing methods (Section B.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Global Information Distribution</head><p>We provide more results of global information (i.e., mean statistics) distribution between training and inference on GoPro <ref type="bibr" target="#b27">[28]</ref> dataset. Statistics aggregated by HINet <ref type="bibr" target="#b6">[7]</ref> are shown in <ref type="figure" target="#fig_4">Figure A3i</ref>. Statistics aggregated by MPRNet <ref type="bibr" target="#b54">[55]</ref> are shown in <ref type="figure" target="#fig_4">Figure A3ii</ref>. For both HINet <ref type="bibr" target="#b6">[7]</ref> and MPRNet <ref type="bibr" target="#b54">[55]</ref>, the statistics distribution shifts from training (green) to inference (blue). The statistics distribution shifts is reduced by TLC as shown in <ref type="figure" target="#fig_4">Figure A3i</ref>-b and <ref type="figure" target="#fig_4">Figure A3ii</ref>-b compares to <ref type="figure" target="#fig_4">Figure A3i</ref>-a and <ref type="figure" target="#fig_4">Figure A3ii</ref>-a: the statistics distribution obtained by our HINet-local/MPRNet-local (red) is close to the original HINet/MPRNet in the training phase (green).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Qualitative Comparisons</head><p>In this section, provide additional qualitative results on various image restoration tasks (e.g. deburring, deraining and dehazing) for qualitative comparisons. Deburring. We give the comparison of the visual effect in <ref type="figure" target="#fig_5">Figure A4</ref> for qualitative comparisons. Compared to the original MPRNet <ref type="bibr" target="#b55">[56]</ref> which test with patches ( <ref type="figure" target="#fig_5">Figure A4b</ref>), our approach ( <ref type="figure" target="#fig_5">Figure A4d</ref>) restores high quality images without boundary artifacts. Compared to the original MPRNet <ref type="bibr" target="#b55">[56]</ref> which test with images ( <ref type="figure" target="#fig_5">Figure A4c</ref>), our approach restores clearer and sharper images. Deraining. We give the comparison of the visual effect in <ref type="figure" target="#fig_6">Figure A5</ref> for qualitative comparisons. Compared to the original SPDNet <ref type="bibr" target="#b55">[56]</ref>, our approach restores clearer images. Dehazing. We give the comparison of the visual effect in <ref type="figure" target="#fig_7">Figure A6</ref> for qualitative comparisons. Compared to the original FFANet <ref type="bibr" target="#b55">[56]</ref>, our approach restores clearer images.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of global operation and our TLC: (a) During training, limited by the cropped patches, global operation learns representation to local region in the original image; (b) During inference, global operation extract global representations based on full-resolution image. Our TLC convert the global operation to a local one so that it extract representations based on local spatial region of features as in training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>(a) Blurry Image (PSNR) (b) Origin + Image (31.89dB) (d) Ours + Image (33.11dB) (c) Origin + Patches (32.91dB) Visual comparison with different test-time methods to MPRNet [56] for image deblurring. (a) Blurry image; (b) Inference with image; (c) Inference with cropped patches; (d) Ours: TLC is adopted and inference based on images. Our TLC generates sharp image without boundary artifacts in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) TLC can reduce distribution shifts (in red) caused by the inconsistency between training and testing (green vs. blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>TLC can significantly improve the performance of MPRNet over a wide range of hyperparameters (i.e., size of local window).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>The effectiveness of TLC on MPRNet<ref type="bibr" target="#b54">[55]</ref> (denoted as MPRNet-Local).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative evaluation of our TLC on single image motion deblurring methods. Models with our TLC (denoted with -Local suffix) generates sharper result than original ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative evaluation of our TLC for Dual-pixel defocus deblurring on the DPDD dataset<ref type="bibr" target="#b0">[1]</ref>. Restormer with our TLC (i.e., Restormer-Local) more effectively removes blur while preserving the fine image details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative evaluation of our TLC for Gaussian image denoising. Our Restormer-Local removes noise while preserving the fine image details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Qualitative evaluation of our TLC for image deraining. SPDNet with our TLC (i.e., SPDNet-Local) superior in the realistic performance of image details and color fidelity. Qualitative evaluation of our TLC for image dehazing on realistic hazy image. FFANet with our TLC (i.e., FFANet-Local) generate cleaner result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. A1 :</head><label>A1</label><figDesc>MPRNet's deblurring result when inference with overlapping 256 ? 256 patches, which also presents vertical and horizontal artifact split lines. Overlapping of different patches is 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. A2 :</head><label>A2</label><figDesc>Deblurring results of MPRNet<ref type="bibr" target="#b55">[56]</ref> on GoPro<ref type="bibr" target="#b27">[28]</ref> when inference with patches. Them introduce visible artifacts at patch boundaries, which look like there are vertical stripes cutting through the picture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Mean distribution of Instance Normalizations<ref type="bibr" target="#b42">[43]</ref> in HINet<ref type="bibr" target="#b6">[7]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Mean distribution of SE<ref type="bibr" target="#b12">[13]</ref> layers in MPRNet<ref type="bibr" target="#b54">[55]</ref> Fig. A3: Visualization of the statistics (mean) distribution of features. Green: the distribution when training with patches; Blue: the distribution when inference with images; Red: the distribution when inference with images and TLC is adopted (denoted with -local suffix). For each sub-figure: (a) The original test scheme results in distribution shifts. (b) Distribution shifts could be reduced by our proposed TLC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(a) Blurry Image (PSNR) (b) origin + patches (29.02dB) (c) origin + full-image (29.87dB) (d) ours + full-image (30.70dB) Fig. A4: Deblurring results of MPRNet [56] on GoPro [28] generated by different inference schemes. Left: full-images. Right: crops from left image. (b) Test with patches; (c) Test with images; (d) Test with images and TLC is adopted (ours). It illustrates that (d) provides sharper results than (c) while avoids the boundary artifacts in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Fig. A5: Deraining results of SPDNet [51] on SPA-Data [44] dataset generated by different inference methods. (a) Rainy images as inputs. (b) Results based on full-image produced by the original SPDNet. Some of the rainwater in images is not removed cleanly. (c) Results based on full-image produced by SPDNet with the proposed TLC, which are clearer. (d) Ground truth for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Fig. A6: Dehazing results of FFANet<ref type="bibr" target="#b33">[34]</ref> on Synthetic Objective Testing Set (SOTS) from RESIDE<ref type="bibr" target="#b19">[20]</ref> dataset generated by different inference methods.(a) Hazy images as inputs. (b) Result produced by original FFANet, which is gray with obvious noise. (c) Result produced by FFANet with our TLC, which is brighter with fewer noises. (d) Ground truth for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1. To the best of our knowledge, we are the first to point out the inconsistency of the global information distribution in training (with cropped patches from images) and inference (with the full-resolution image) in image restoration tasks, which may harm model performance. 2. To reduce the distribution shifts between training and inference, we propose Test-time Local Converter (TLC) that converts the region of feature aggregation from global to local only at test time. Without retraining or fine-tuning, TLC significantly improves the performance of various modules with negligible costs by reducing the train-test inconsistency. 3. Extensive experiments show that our TLC improves state-of-the-art results on various image restoration tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Image motion deblurring comparisons on GoPro<ref type="bibr" target="#b27">[28]</ref> and HIDE<ref type="bibr" target="#b35">[36]</ref> Method Gao et al . DBGAN MT-RNN DMPHN Suin et al . SPAIR MIMO-UNet+ IPT</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>[10]</cell><cell>[62]</cell><cell>[31]</cell><cell>[57]</cell><cell>[40]</cell><cell>[33]</cell><cell>[8]</cell><cell>[5]</cell></row><row><cell>GoPro</cell><cell>PSNR? SSIM?</cell><cell>30.90 0.935</cell><cell>31.10 0.942</cell><cell>31.15 0.945</cell><cell>31.20 0.940</cell><cell>31.85 0.948</cell><cell>32.06 0.953</cell><cell>32.45 0.957</cell><cell>32.52 -</cell></row><row><cell>HIDE</cell><cell>PSNR? SSIM?</cell><cell>29.11 0.913</cell><cell>28.94 0.915</cell><cell>29.15 0.918</cell><cell>29.09 0.924</cell><cell>29.98 0.930</cell><cell>30.29 0.931</cell><cell>29.99 0.930</cell><cell>--</cell></row><row><cell></cell><cell cols="9">Method HINet HINet-Local MPRNet MPRNet-Local Restormer Restormer-Local</cell></row><row><cell>Dataset</cell><cell></cell><cell>[7]</cell><cell>(Ours)</cell><cell>[55]</cell><cell cols="2">(Ours)</cell><cell>[53]</cell><cell>(Ours)</cell><cell></cell></row><row><cell>GoPro</cell><cell cols="3">PSNR? 32.71 SSIM? 0.959 0.962 +0.003 33.08 +0.37</cell><cell>32.66 0.959</cell><cell cols="2">33.31 +0.65 0.964 +0.005</cell><cell>32.92 0.961</cell><cell cols="2">33.57 +0.65 0.966 +0.005</cell></row><row><cell>HIDE</cell><cell cols="3">PSNR? 30.33 SSIM? 0.932 0.936 +0.004 30.66 +0.33</cell><cell>30.96 0.939</cell><cell cols="2">31.19 +0.23 0.942 +0.003</cell><cell>31.22 0.942</cell><cell cols="2">31.49 +0.27 0.945 +0.003</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>) by 0.65 dB. The PSNR on HIDE of HINet, MPRNet and Restormer are improved by 0.33 dB, 0.23 dB</figDesc><table><row><cell></cell><cell></cell><cell>23.71 dB</cell><cell>25.88 dB</cell></row><row><cell></cell><cell></cell><cell>HINet [7]</cell><cell>HINet-Local</cell></row><row><cell>21.38 dB</cell><cell></cell><cell>24.02 dB</cell><cell>28.53 dB</cell></row><row><cell cols="2">Blurry Image</cell><cell>MPRNet [55]</cell><cell>MPRNet-Local</cell></row><row><cell>PSNR</cell><cell>21.38 dB</cell><cell>25.36 dB</cell><cell>29.00 dB</cell></row><row><cell>Reference</cell><cell>Blurry</cell><cell>Restormer [53]</cell><cell>Restormer-Local</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>-Local</cell></row></table><note>Video deblurring comparisons on the GoPro [28] dataset Method SFE IFI-RNN ESTRNN EDVR TSP PVDNet GSTA RNN-MBP RNN-MBP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Defocus deblurring comparisons on the DPDD testset<ref type="bibr" target="#b0">[1]</ref> (containing 37 indoor and 39 outdoor scenes). S: single-image defocus deblurring. D: dualpixel defocus deblurring. Our Restormer-Local sets new state-of-the-art for both single-image and dual pixel defocus deblurring</figDesc><table><row><cell></cell><cell></cell><cell>Indoor Scenes</cell><cell cols="2">Outdoor Scenes</cell><cell>Combined</cell></row><row><cell>Method</cell><cell cols="5">PSNR? SSIM? MAE? LPIPS? PSNR? SSIM? MAE? LPIPS? PSNR? SSIM? MAE? LPIPS?</cell></row><row><cell>EBDBS [5]</cell><cell cols="5">25.77 0.772 0.040 0.297 21.25 0.599 0.058 0.373 23.45 0.683 0.049 0.336</cell></row><row><cell>DMENetS [17]</cell><cell cols="5">25.50 0.788 0.038 0.298 21.43 0.644 0.063 0.397 23.41 0.714 0.051 0.349</cell></row><row><cell>JNBS [37]</cell><cell cols="5">26.73 0.828 0.031 0.273 21.10 0.608 0.064 0.355 23.84 0.715 0.048 0.315</cell></row><row><cell>DPDNetS [1]</cell><cell cols="5">26.54 0.816 0.031 0.239 22.25 0.682 0.056 0.313 24.34 0.747 0.044 0.277</cell></row><row><cell>KPACS [38]</cell><cell cols="5">27.97 0.852 0.026 0.182 22.62 0.701 0.053 0.269 25.22 0.774 0.040 0.227</cell></row><row><cell>IFANS [18]</cell><cell cols="5">28.11 0.861 0.026 0.179 22.76 0.720 0.052 0.254 25.37 0.789 0.039 0.217</cell></row><row><cell>RestormerS [53]</cell><cell cols="5">28.87 0.882 0.025 0.145 23.24 0.743 0.050 0.209 25.98 0.811 0.038 0.178</cell></row><row><cell cols="6">Restormer-LocalS 29.08 0.888 0.024 0.139 23.54 0.765 0.049 0.195 26.24 0.825 0.037 0.168</cell></row><row><cell>DPDNetD [1]</cell><cell cols="5">27.48 0.849 0.029 0.189 22.90 0.726 0.052 0.255 25.13 0.786 0.041 0.223</cell></row><row><cell>RDPDD [2]</cell><cell cols="5">28.10 0.843 0.027 0.210 22.82 0.704 0.053 0.298 25.39 0.772 0.040 0.255</cell></row><row><cell>UformerD [46]</cell><cell cols="5">28.23 0.860 0.026 0.199 23.10 0.728 0.051 0.285 25.65 0.795 0.039 0.243</cell></row><row><cell>IFAND [18]</cell><cell cols="5">28.66 0.868 0.025 0.172 23.46 0.743 0.049 0.240 25.99 0.804 0.037 0.207</cell></row><row><cell>RestormerD [53]</cell><cell cols="5">29.48 0.895 0.023 0.134 23.97 0.773 0.047 0.175 26.66 0.833 0.035 0.155</cell></row><row><cell cols="6">Restormer-LocalD 29.83 0.903 0.022 0.120 24.37 0.794 0.045 0.159 27.02 0.847 0.034 0.140</cell></row><row><cell>20.72 dB</cell><cell></cell><cell>PSNR</cell><cell>20.72 dB</cell><cell>25.88 dB</cell><cell>27.16 dB</cell></row><row><cell cols="2">Blurry Image</cell><cell>Reference</cell><cell>Blurry</cell><cell cols="2">Restormer[53] Restormer-Local</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Gaussian image denoising comparisons for two kinds of images and two categories of methods on Urban100<ref type="bibr" target="#b13">[14]</ref> dataset. Top super row: learning a single model to handle various noise levels. Bottom super row: training a separate model for each noise level (a) Gaussian grayscale image denoising</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) Gaussian color image denoising</cell></row><row><cell>Method</cell><cell cols="3">?=15 ?=25 ?=50</cell><cell>Method</cell><cell></cell><cell cols="3">?=15 ?=25 ?=50</cell></row><row><cell>DnCNN [59]</cell><cell cols="3">32.28 29.80 26.35</cell><cell cols="2">IRCNN [60]</cell><cell cols="3">33.78 31.2 27.7</cell></row><row><cell>FFDNet [61]</cell><cell cols="3">32.40 29.90 26.50</cell><cell cols="2">FFDNet [61]</cell><cell cols="3">33.83 31.4 28.05</cell></row><row><cell>IRCNN [60]</cell><cell cols="3">32.46 29.80 26.22</cell><cell cols="2">DnCNN [59]</cell><cell cols="3">32.98 30.81 27.59</cell></row><row><cell>DRUNet [58]</cell><cell cols="3">33.44 31.11 27.96</cell><cell cols="2">DRUNet [58]</cell><cell cols="3">34.81 32.60 29.61</cell></row><row><cell>Restormer [53]</cell><cell cols="3">33.67 31.39 28.33</cell><cell cols="2">Restormer [53]</cell><cell cols="3">35.06 32.91 30.02</cell></row><row><cell cols="4">Restormer-Local 33.73 31.48 28.45</cell><cell cols="5">Restormer-Local 35.14 33.01 30.16</cell></row><row><cell>MWCNN [24]</cell><cell cols="3">33.17 30.66 27.42</cell><cell cols="2">RPCNN [49]</cell><cell>-</cell><cell cols="2">31.81 28.62</cell></row><row><cell>NLRN [23]</cell><cell cols="3">33.45 30.94 27.49</cell><cell cols="2">BRDNet [42]</cell><cell cols="3">34.42 31.99 28.56</cell></row><row><cell>RNAN [65]</cell><cell>-</cell><cell>-</cell><cell>27.65</cell><cell cols="2">RNAN [65]</cell><cell>-</cell><cell>-</cell><cell>29.08</cell></row><row><cell>DeamNet [35]</cell><cell cols="3">33.37 30.85 27.53</cell><cell cols="2">RDN [66]</cell><cell>-</cell><cell>-</cell><cell>29.38</cell></row><row><cell>DAGL [27]</cell><cell cols="3">33.79 31.39 27.97</cell><cell>IPT [5]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>29.71</cell></row><row><cell>SwinIR [22]</cell><cell cols="3">33.70 31.30 27.98</cell><cell cols="2">SwinIR [22]</cell><cell cols="3">35.13 32.90 29.82</cell></row><row><cell>Restormer [53]</cell><cell cols="3">33.79 31.46 28.29</cell><cell cols="2">Restormer [53]</cell><cell cols="3">35.13 32.96 30.02</cell></row><row><cell cols="4">Restormer-Local 33.85 31.55 28.41</cell><cell cols="5">Restormer-Local 35.21 33.06 30.17</cell></row><row><cell>14.90 dB</cell><cell></cell><cell cols="2">14.90 dB</cell><cell>PSNR</cell><cell cols="2">31.36 dB</cell><cell></cell><cell>31.60 dB</cell></row><row><cell>Noisy Image</cell><cell></cell><cell></cell><cell>Noisy</cell><cell>Reference</cell><cell cols="4">Restormer[53] Restormer-Local</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Deraining results on SPA-Data [44] dataset</figDesc><table><row><cell>Method</cell><cell>PSNR? SSIM?</cell></row><row><cell>SPDNet [51]</cell><cell>43.55 0.988</cell></row><row><cell cols="2">SPDNet-Local (Ours) 43.73 0.989</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Dehazing results on outdoor scene images in SOTS<ref type="bibr" target="#b19">[20]</ref> dataset</figDesc><table><row><cell>Method</cell><cell>PSNR? SSIM?</cell></row><row><cell>FFANet [34]</cell><cell>33.57 0.984</cell></row><row><cell cols="2">FFANet-Local (Ours) 33.99 0.985</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>The results of applying TLC to different modules on GoPro dataset. TLC improves the performance of all models with negligible costs PSNR? MACs? PSNR? MACs? PSNR? MACs? PSNR? MACs? PSNR? MACs? Origin 30.95 62.13G 30.91 62.13G 30.74 62.14G 30.82 62.14G 30.53 62.19G +TLC 31.11 62.13G 31.03 62.13G 30.91 62.14G 31.21 62.19G 31.05 62.27G ? +0.16 +0.00G +0.12 +0.00G +0.17 +0.00G +0.39 +0.05G +0.52 +0.08G</figDesc><table><row><cell>Module</cell><cell>IN [43]</cell><cell>GN [48]</cell><cell>GE-? ? [12]</cell><cell>SE [13]</cell><cell>CBAM [47]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A1 :</head><label>A1</label><figDesc>PSNR of MPRNet on GoPro when test with different methods. Inference time per image is measured on RTX 2080Ti.</figDesc><table><row><cell>Input</cell><cell cols="4">#Overlap TLC PSNR (dB) Time (s)</cell></row><row><cell></cell><cell>-</cell><cell>?</cell><cell>32.66</cell><cell>1.60</cell></row><row><cell>Whole Image</cell><cell>-</cell><cell>?</cell><cell>33.31</cell><cell>2.03</cell></row><row><cell></cell><cell>-</cell><cell>?  ?</cell><cell>33.31</cell><cell>1.69</cell></row><row><cell>Overlapping Patches</cell><cell>16 128</cell><cell>--</cell><cell>33.09 33.15</cell><cell>2.60 6.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? denotes optimised implementation of TLC</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Defocus deblurring using dual-pixel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abuolaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to reduce defocus blur by realistically modeling dual-pixel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abuolaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2289" to="2298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The submatrices character count problem: an efficient solution using separable values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Computation</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="116" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real image denoising with feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3155" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12299" to="12310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04676</idno>
		<title level="m">Simple baselines for image restoration</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hinet: Half instance normalization network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking coarse-to-fine approach in single image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4641" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nafssr: Stereo image super-resolution using nafnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3848" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parallel prefix sum (scan) with cuda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GPU gems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="851" to="876" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep defocus map estimation using domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12222" to="12230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Iterative filter adaptive network for single image defocus deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2034" to="2042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Block-iterative richardson-lucy methods for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Benchmarking single-image dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="492" to="505" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent squeeze-and-excitation context aggregation net for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="254" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-level wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear image representation using divisive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic attentive graph learning for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4328" to="4337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3883" to="3891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with intra-frame iterations for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8102" to="8111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cascaded deep video deblurring using temporal sharpness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3043" to="3051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="327" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatially-adaptive image restoration using distortion-guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2309" to="2319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ffa-net: Feature fusion attention network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11908" to="11915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive consistency prior based deep network for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8596" to="8606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human-aware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5572" to="5581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Just noticeable defocus blur detection and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="657" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single image defocus deblurring using kernelsharing parallel atrous convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2642" to="2650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recurrent video deblurring with blurinvariant motion estimation and pixel volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatially-attentive patch-hierarchical network for adaptive motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3606" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gated spatio-temporal attention-guided video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7802" to="7811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image denoising using deep cnn with batch renormalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="461" to="473" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatial attentive single-image deraining with a high quality real rain dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12270" to="12279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="17683" to="17693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Identifying recurring patterns with deep neural networks for natural image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2426" to="2434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep video deblurring using sharpness features from exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8976" to="8987" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Structure-preserving deraining with residue channel prior guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Region normalization for image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12733" to="12740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Restormer: Efficient transformer for high-resolution image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="5728" to="5739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Cycleisp: Real image restoration via improved data synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Multi-stage progressive image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-stage progressive image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14821" to="14831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5978" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Plug-and-play image restoration with deep denoiser prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for cnnbased image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4608" to="4622" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deblurring by realistic blurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2737" to="2746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10082</idno>
		<title level="m">Residual non-local attention networks for image restoration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Residual dense network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2480" to="2495" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Efficient spatio-temporal recurrent neural network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep recurrent neural network with multi-scale bi-directional propagation for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="3598" to="3607" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
