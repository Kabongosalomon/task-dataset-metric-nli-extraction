<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RENOIR -A Dataset for Real Low-Light Image Noise Reduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josue</forename><surname>Anaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Florida State University</orgName>
								<address>
									<addrLine>117 N Woodward Ave</addrLine>
									<postCode>32306</postCode>
									<settlement>Tallahassee</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Florida State University</orgName>
								<address>
									<addrLine>117 N Woodward Ave</addrLine>
									<postCode>32306</postCode>
									<settlement>Tallahassee</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RENOIR -A Dataset for Real Low-Light Image Noise Reduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image denoising</term>
					<term>denoising dataset</term>
					<term>low light noise</term>
					<term>Poisson-Gaussian noise model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image denoising algorithms are evaluated using images corrupted by artificial noise, which may lead to incorrect conclusions about their performances on real noise. In this paper we introduce a dataset of color images corrupted by natural noise due to lowlight conditions, together with spatially and intensity-aligned low noise images of the same scenes. We also introduce a method for estimating the true noise level in our images, since even the low noise images contain small amounts of noise. We evaluate the accuracy of our noise estimation method on real and artificial noise, and investigate the Poisson-Gaussian noise model. Finally, we use our dataset to evaluate six denoising algorithms: Active Random Field, BM3D, Bilevel-MRF, Multi-Layer Perceptron, and two versions of NL-means. We show that while the Multi-Layer Perceptron, Bilevel-MRF, and NL-means with soft threshold outperform BM3D on gray images with synthetic noise, they lag behind on our dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and Motivation</head><p>In the field of computer vision and computational photography, noise reduction is the application in which granular discrepancies found in images are removed. The task of performing noise reduction is synonymous with improvement in image quality.</p><p>Many consumer cameras and mobile phones deal with the issues of low-light noise due to small sensor size and insufficient exposure time. The issue of noise for a particular digital camera is so important that it is used as a valuable metric of the camera sensor and for comparing camera performance <ref type="bibr" target="#b0">[1]</ref>.</p><p>Besides the noise in digital camera images, another example of images that deal with noise due to limited acquisition time are Magnetic Resonance Images (MRI).</p><p>Other important types of image modalities such as X-ray and CT (Computed Tomography) also suffer from noise artifacts due to insufficient exposure because of low radiation dose limits. While the image acquisition process is different in all of these examples, the reason for the noise is in most part the same. This is why the problem of low-light image noise reduction is studied and has led to a variety of different noise reduction methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>In general most of the performance evaluations for these various noise reduction methods are done on small images contaminated with noise of a known type (Gaussian, Poisson, salt and pepper, etc), which is artificially added to a clean image to obtain a noisy version. Measuring the performance of a noise reduction algorithm on small images corrupted by artificial noise might not give an accurate enough picture of the denoising performance of the algorithm on real digital camera images in low-light conditions. The nature of the noise in low-light camera images is more complex than just i.i.d., for example its variance depends on the image intensity <ref type="bibr" target="#b10">[11]</ref> and has smallrange correlations <ref type="bibr" target="#b11">[12]</ref>, so it would be desirable to obtain images naturally corrupted by low-light noise and their noise-free counterparts. For this purpose, we bring the following contributions:</p><p>? A dataset of color images naturally corrupted by low-light noise, taken with two digital cameras (Canon PowerShot S90, Canon EOS Rebel T3i) and a mobile phone camera (Xiaomi Mi3).</p><p>? A process for the collection of noisy and low-noise pixel-aligned images of the same scene.</p><p>? A method for aligning the intensity values of all the images of the same scene.</p><p>? A technique for computing the noise level and PSNR (Peak Signal-to-Noise Ratio) <ref type="bibr" target="#b12">[13]</ref> of the images in our dataset and an evaluation of its accuracy.</p><p>? An evaluation of the Poisson-Gaussian mixture model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> and its parameter estimation method.</p><p>? An evaluation of the denoising performance of six algorithms on our dataset, with some surprising results.</p><p>Different cameras produce different kinds of noise due to their sensor size, sensor type, and other factors on the imaging pipeline. A learning-based denoising method (e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref> or <ref type="bibr" target="#b9">[10]</ref>) could be trained for a specific type of camera on noisy-clean image pairs for that specific camera, but it is not clear how well it would generalize to images from another camera. Trying to construct a dataset of various image pairs from different cameras may help determine which denoising method generalizes well over many different cameras, however it does not evaluate the full potential of a method on any one specific camera at various noise levels. We therefore selected to obtain an equal number of images from three cameras with different sensor sizes: one with a small sensor (Xiaomi Mi3), one with a slightly larger sensor (Canon S90) and one with a mid-size sensor (Canon T3i) and obtain many images with different noise levels for each camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Works</head><p>The Tampere Image Database (TID 2013) <ref type="bibr" target="#b14">[15]</ref> is a dataset intended for evaluating image quality assessment metrics such as the SSIM <ref type="bibr" target="#b15">[16]</ref>. It contains 25 reference images and 3000 images obtained by corrupting the reference images by 24 types of noise and distortions such additive or multiplicative Gaussian noise, high frequency noise, image encoding artifacts, image denoising artifacts, etc. However, these corrupted images are all obtained by artificially transforming the reference images in different ways.</p><p>In contrast, our dataset provides images where the noise is obtained naturally by short time exposure to low-light scenes and clean images obtained by long time exposure to the same scene. Besides evaluating quality assessment metrics, our dataset could be used for other applications such as studying noise formation and noise statistics in digital camera images, and for training or evaluating image denoising algorithms.</p><p>The only database for benchmarking image denoising that we are aware of is <ref type="bibr" target="#b16">[17]</ref> (discussed in <ref type="bibr" target="#b5">[6]</ref>). It evaluates various denoising methods on color images corrupted by artificial Gaussian noise. The problem with artificial Gaussian noise is that it is a very simple noise model that is not present in real world images where many times the noise level changes with the image intensity. In this respect it has been shown that the distribution of low-light camera noise is not Gaussian, but follows a more complex Poisson-Gaussian mixture distribution with intensity dependent variance <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>. Our dataset contains images corrupted by real low-light noise, which besides offering a more realistic setting for image denoising, it allows to see how well the Poisson-Gaussian model fits real noisy images obtained in low-light conditions. We will show such a study in Section 4.3.</p><p>A small number of real low-light noise images with corresponding clean image pairs have been used in <ref type="bibr" target="#b11">[12]</ref> to study the noise and intensity relationship. However, we are not aware of any public database or collection of images that have been corrupted by real low-light noise like the ones presented in our paper, and which took all the necessary steps for carefully acquiring the images, intensity aligning them and diagnosing the quality of the obtained pairs.</p><p>A dataset with real low-light noisy images and their clean counterparts such as the one introduced in this paper would bring many benefits to just using images artificially corrupted by noise from a Poisson-Gaussian mixture model:</p><p>? It would contribute to the further study of the noise structure in digital cameras, such as how much it differs from the Poisson-Gaussian mixture model, spatial correlation structure, how noise parameters relate to camera acquisition parameters, etc.</p><p>? It would present a more realistic range of levels of noise, similar to what happens "in the wild". In contrast, the Poisson-Gaussian noise model is usually studied with fixed (and known) noise parameters, such as in <ref type="bibr" target="#b13">[14]</ref>.</p><p>? It would allow for an end-to-end evaluation of denoising algorithms. Evaluating using artificial noise models, even if they are accurate, might not entirely reflect the reality of noise in digital cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Acquisition of Natural Image Pairs</head><p>The dataset 1 acquired in this paper consists low-light uncompressed natural images of 120 scenes. About four images per scene were acquired, where two images contain noise and the other two images contain very little noise. The presence of noise in the images is mainly due to the number of photons that are received by the camera's sensor and the amplification process, as discussed in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Acquisition procedure</head><p>All the images in our dataset are of static scenes and are acquired under low-light conditions using the following "sandwich" procedure:</p><p>? A low-noise image is obtained with low light sensitivity (ISO 100) and long exposure time. This will be the reference image.</p><p>? One or two noisy images are then obtained with increased light sensitivity and reduced exposure time.</p><p>? Finally, another low noise image is taken with the same parameters as the reference image. This will be the clean image.</p><p>The two low-noise (reference and clean) images are acquired at the beginning and at the end of the sequence, while the one or two noisy images are shot in between. This is done to evaluate the quality of the whole acquisition process for that particular scene. This process is somehow similar to the process discussed in <ref type="bibr" target="#b11">[12]</ref> which used pair images that were taken with flash. The problem with taking the images with flash is that the flash can change the scene illumination. Moreover, in <ref type="bibr" target="#b11">[12]</ref> no brightness alignment has been performed on their images. The two low-noise images are used as a first level of quality control. Any motion or lighting change during acquisition could make the two low-noise images be sufficiently different, as measured by the PSNR. In fact, we discarded the scenes with PSNR of the clean images less than 34.</p><p>The actual acquisition parameters for each camera are presented in <ref type="table" target="#tab_0">Table 1</ref>. while trying to preserve the static scene in the images by not moving or refocusing the camera. The sandwich approach that we used to obtain our images also helped insure that the only visual difference between the images of a scene was simply due to noise.</p><p>All of the images were collected and saved in RAW format (CR2 for Canon). The</p><p>Mi2Raw Camera app was used to capture the RAW images for the Xiaomi Mi3 (in DNG format).</p><p>An example of one of the images in the dataset can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>. In the end we collected 40 scenes for the S90, 40 for the T3i, and another 40 for the Mi3.</p><p>The image denoising database in <ref type="bibr" target="#b16">[17]</ref> contains 300 noisy images at 5 noise levels (? = 5, 10, 15, 25, 35) for a total of 1500 images. The dimensionality of these images is 481 by 321 . The dimensionality of the images for just the S90 images is 3684 by 2760 while the images from the other cameras are even larger, as shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Although our image database contains fewer noisy images, our images contain about 60 times more pixels and therefore more patch variability for studying noise models from just one of the three cameras.</p><p>Many various denoising methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref> train models from noisy-clean image pairs that are supposed to generalize well to future noisy images. For this reason and for evaluation in general it is very important to maintain a careful construction of these noisy-clean image pairs and to have many examples for a representative dataset. The difficulty in constructing such pairs is why artificial noise is used in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Mobile camera difficulties</head><p>In trying to collect images for our dataset we decided on also collecting mobile phone camera images. In doing so we ran into many of difficulties, some of which are described below and are illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The first difficulty that arose was collecting the RAW images on phone cameras.</p><p>Only a few mobile phones collect true RAW images (data dump directly from the sensor). For example, the Iphone can have an application installed that will allow it to take RAW images, however these RAW images are not in fact truly RAW because the sensor data has gone through some unknown post processing. Therefore, only some of the most recent phones that have been allowed by the device manufacturer can truly collect RAW images.</p><p>The second difficulty that we found when trying to collect mobile phone images we tried to use a Google Nexus 5 with the FV-5 camera application to capture RAW images, the limits of control over settings like the exposure time and ISO, and its tiny sensor size led to many scenes failing our 'sandwich' procedure selection benchmark ( did not have sufficient amount of light needed for the PSNR to be around 35 for the reference and clean image.) We also noted for a phone like the Google Nexus 5 a nonlinearity relationship issue in the brightness alignment procedure (this could also be due to an insufficient amount of light.) The final difficulty we experienced came in the form of tools to help maintain a static scene. With the other cameras we used a tripod and were able to program the automatic acquisition of the scene. With the mobile phone camera we had to use a small phone tripod and a bluetooth mouse to preserve the static scene when taking the images manually.</p><p>Settling on the Xiaomi Mi3 phone we collected 6 images per scene. The first two images were both low-noise images and these images were averaged and set as the reference image in the alignment process. Similarly, the last two images were also both low-noise images and the last two images as well were averaged and used in the overall PSNR computation of the 'sandwich' procedure. If any movement or saturation was detected the images were cropped appropriately post alignment. In the end many of the scenes for the Mi3 were cropped, but all are static with PSNR around 35 or more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Main Assumptions and Notations</head><p>In this section we present the main assumptions that form the basis of the acquisition procedure, intensity alignment, and noise level estimation.</p><p>The following notations will be used in this paper:</p><p>? R, I r -the reference image </p><formula xml:id="formula_0">I n (x) = I GT (x) + n (x) I r (x) = I GT (x) + r (x) I c (x) = I GT (x) + c (x)<label>(1)</label></formula><p>where n (x), r (x), and c (x) are zero-mean and independent of each other. We also assume that the reference and clean images have the same noise distribution since the two images are of the same static scene with the same ISO level and exposure time. Note that the reference and clean images have low amounts of noise because many photons have been accumulated on the sensor during the long exposure time.</p><p>In summary our assumptions are:</p><p>1. The images are formed as described in eq. (1) with</p><formula xml:id="formula_1">E[ n (x)] = E[ r (x)] = E[ c (x)] = 0.</formula><p>2. For any x, the random variables n (x), r (x), c (x) are independent.</p><p>3. For any x, r (x) and c (x) are identically distributed.</p><p>It is shown in <ref type="bibr" target="#b11">[12]</ref> that the noise in the digital camera images has short range correlations. We don't need to make any assumptions about the spatial correlations inside one image, just between the three images at the same location.</p><p>We will see in experiments that our estimation method based on these assumptions works very well in estimating the noise level in images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Intensity Alignment</head><p>The dataset construction went beyond just the acquisition of the images. For the purpose of properly aligning the pixel intensities of the image pairs, we developed a new form of brightness adjustment that mapped our RAW images to an 8-bit uncompressed format.</p><p>The reference image was first mapped from 16-bit to 8-bit as follows. We computed the cumulative distribution of the 16-bit pixel intensities of the RAW reference image and constructed a linear scaling of the RAW reference image that sets the 99th percentile value to the intensity value 230 in the 8-bit image. Thus 1% of the pixels are mapped to intensities above 230, and even fewer will be saturated to value 255. We chose the value 230 so that most of the noisy images will not have much saturation after alignment with the reference image.</p><p>Each of the other images of the same scene is at the same time reduced to 8-bit and aligned to the 8-bit reference image by finding a linear mapping specified by parameter ? such that if I is the 16-bit image, the 8-bit aligned image is obtained from ?I after its values larger than 255 or less than 0 are truncated. For better accuracy, instead of working with the two images I and R, we use blurred versions? andR obtained by convolution with a Gaussian kernel with ? = 5 to estimate the intensity alignment parameter ?. This way the level of noise is reduced. To avoid biases obtained near intensity discontinuities, the alignment parameter is computed based on the low gradient</p><formula xml:id="formula_2">pixels M = {i, |?R(i)| &lt; 1}.</formula><p>The parameter ? is found to minimize</p><formula xml:id="formula_3">E(?) = i?M (R(i) ? max[min(??(i), 255), 0]) 2</formula><p>This is done by coordinate optimization using the Golden section search in one dimension <ref type="bibr" target="#b19">[20]</ref> optimizing on ? until convergence. The parameter ? obtained for the mapping is robust to outliers.</p><p>In <ref type="figure" target="#fig_3">Figure 3</ref>, left is shown an example of the correspondence between the pixels of  <ref type="figure" target="#fig_4">Figure 4</ref> shows an example of the green channel for a particular image in the dataset before and after alignment is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Noise Estimation</head><p>As stated previously the amount of noise present in the dataset is due to the sensor and the amplification process. The fact that not all of the images were taken in the same environment under the same camera settings means that we have a wide variety of noise in our images. The fact that we are not dealing with artificial noise also means that we do not know beforehand what will be the noise variance ? 2 . Thankfully our "sandwich" procedure for image acquisition, as influenced by <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b11">12]</ref>, allows us to estimate the noise level for any of our images. The noise level can be estimated locally for an image patch or globally for the entire image.</p><p>We will use the fact that if two random variables A, B are independent, then var(A? from the independence of r (x) and c (x) and the fact that r (x) and c (x) are identically distributed (so we can represent them as (x)). We obtain the estimation of the noise level in the clean and reference images:</p><formula xml:id="formula_4">? 2 (I r (x) ? I GT (x)) = ? 2 (I c (x) ? I GT (x)) = ? 2 ( (x)) = 1 2 ? 2 (I r (x) ? I c (x)) (2)</formula><p>For the noisy images we use</p><formula xml:id="formula_5">? 2 (I n (x) ? I r (x)) = var( n (x) ? r (x)) = ? 2 ( n (x)) + ? 2 ( r (x))</formula><p>to obtain the estimation of the noise level as</p><formula xml:id="formula_6">? 2 ( n (x)) = ? 2 (I n (x) ? I GT (x)) = ? 2 (I n (x) ? I r (x)) ? 1 2 ? 2 (I r (x) ? I c (x)) (3)</formula><p>If we want to use the best estimate of the GT, which is I a (x) = (I r (x) + I c (x))/2, then we have an alternative formula for the noise level in the noisy images ? 2 ( n (x)) = var(I n (x)?I GT (x)) = var(I n (x)?I a (x))? 1 4 var(I r (x)?I c (x)) (4)</p><p>We can use equations <ref type="formula">(2)</ref> and <ref type="formula">(3)</ref> to estimate the true noise level for any image in our dataset. Again, these noise levels can be computed globally for the whole image or locally on a patch basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset Information</head><p>Aside from estimating the noise level in every image, we also quantified the image fidelity across the image batches using various metrics such as PSNR <ref type="bibr" target="#b12">[13]</ref>, SSIM <ref type="bibr" target="#b15">[16]</ref>, and VSNR <ref type="bibr" target="#b21">[22]</ref>. In particular we modified the PSNR measurement by incorporating our estimate of the noise from (3) as opposed to using the standard noise estimate from the difference image between a clean and noisy image pair. Although there exist specialized metrics for low-light conditions such as <ref type="bibr" target="#b22">[23]</ref> we decided to use measures that are the most prevalent and common in practice. <ref type="table" target="#tab_1">Table 2</ref> lists some specific characteristics about the various cameras and their images in the dataset. Note that the ? in <ref type="table" target="#tab_1">Table 2</ref> comes from the estimates from equations <ref type="formula">(2)</ref> and <ref type="formula">(3)</ref>. <ref type="figure" target="#fig_6">Figure 5</ref> shows the distribution of noise levels for the noisy and clean images for each camera. <ref type="figure" target="#fig_7">Figure 6</ref> shows box-plots of the variation in PSNR and noise levels for each camera.</p><p>One interesting observation that we draw from our dataset is that the low noise images still have a noise level ? of about 3 and as high as 5, which is invisible to the eye. It tells us something about the local nature of the manifold of natural image patches, that the manifold is "thick" in the sense that perturbing a patch with (probably even Gaussian) noise of ? ? 5 obtains another natural image patch. Such information could be useful in the study of natural image statistics or for learning generative models from natural images (e.g. autoencoders).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we will evaluate the accuracy of our noise estimation procedure and compare it to the standard noise estimate (standard deviation of the difference image)</p><p>for both synthetic and real noise data. We will also compare our noise estimation framework to the Poisson-Gaussian noise model and estimation procedure presented in <ref type="bibr" target="#b10">[11]</ref>. Afterwards we will examine the denoising performances of four algorithms on our dataset using three image fidelity metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation of Alignment and Noise Estimation Using Artificial Noise</head><p>To evaluate our intensity alignment and noise estimation method we constructed scenes with added artificial noise, as illustrated in <ref type="figure" target="#fig_8">Figure 7</ref>. For this we chose ten 16-bit RAW reference images from each of the three digital cameras and used them as ground truth images I GT for constructing artificial sequences from them. We then used our alignment method as described in Section 2.4 to construct an 8-bit version of I GT . We then generated I r ,I n , and I c by adding artificial Gaussian noise to the 16-bit I GT . For 16-bit I r and I c we added ? = 3 ? amount of noise where ? is the multiplication factor to map the 16-bit I GT to an 8-bit I GT . A 16-bit I n was generated using ? = 10 ? . This way the standard deviation of the difference from the 8-bit I GT to the 8-bit I r (or I c ) will be 3 and to the 8-bit I n it will be 10. We then performed our standard alignment on I r ,I n , and I c to map them over to 8-bit images obtaining parameters ? , ? i , ? 2 , as illustrated in <ref type="figure" target="#fig_8">Figure 7</ref>. Observe that multiplying the alignment parameters ? , ? i , ? 2 by the same factor produces another alignment of the same quality, thus the alignment is only identifiable up to a multiplicative constant. For this reason, the alignment is evaluated indirectly, through the quality of the noise level estimation.  For all but four of the 90 images evaluated, the relative estimation error for our alignment and noise estimation method was below 1%. This gives us confidence that our intensity alignment method together with the proposed noise level estimation method provide an accurate estimation of the true noise level in images, at least on data with artificial noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of Noise Estimation Using Real Noise</head><p>To further investigate how well the assumptions we made in Section 2.3 about the noise hold, we acquired a special scene with the S90 camera. The scene was of a constant intensity surface in low-light settings. Using our intensity alignment method-ology, instead of mapping our clean image from the 99th quantile to intensity 230, we mapped the median to intensity 128. Using this mapping we then aligned the other two noisy and the clean image using the Golden section method, as described in Section 2.4. <ref type="figure" target="#fig_11">Figure 9</ref> shows the alignments of the calibration dataset as well as a histogram of pixel difference between the reference image and the other images in the calibration dataset. Because we know that the I GT was constant since the scene contained a constant intensity surface, we can immediately obtain a true value for ? 2 for each image by directly computing the intensity variance in the image. However, to account for smoothly changing illumination, we constructed a GT version for each image by Gaussian blurring it with a large spatial kernel (? = 20) and then calculated the noise level as the variance of the difference image between the original image and its smoothed version.</p><p>We then looked to see if the standard estimate of using the difference image between the reference image and the other calibration images provided similar results to those we obtained using our methodology from equations <ref type="formula">(2)</ref> and <ref type="formula">(3)</ref>. Analysis of the estimated noise levels for the three image channels and the overall estimate are summarized with boxplots in <ref type="figure" target="#fig_0">Figure 10</ref>.</p><p>As <ref type="figure" target="#fig_0">Figure 10</ref> shows, our estimated ? values are less biased and have smaller variance than the standard estimation of ? from the difference images. The average relative error for our method of estimation is 1.58% and for the standard method of estimation is 36.22%. The results that we obtained for this evaluation are in line with the results <ref type="figure" target="#fig_0">Figure 10</ref>: Comparison between our method of estimating ? and the standard method based on the difference image. Both methods were tasked with estimating the ? for the red, green, blue channels, and the overall image for the calibration scene.</p><p>we obtained for noise estimation for images with artificial noise. Thus our investigation gives us enough confidence in our estimation going forward. Consequently, the noise estimation described in Section 2.5 will be used as our noise estimation method for all of the images in our dataset and for estimating the PSNR of the denoised images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of the Poisson-Gaussian Noise model</head><p>As stated previously, the Poisson-Gaussian noise model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> depends on the image intensity. With our notation introduced in Section 2.3, the observed image intensity I n (x) is represented under the Poisson-Gaussian noise model as</p><formula xml:id="formula_7">I n (x) = ?p(x) + n(x)<label>(5)</label></formula><p>where p(x) is an independent Poisson random variable with expected value y(x) = I GT (x)/? and n(x) is an i.i.d. Gaussian n(x) ? N (0, ? 2 ). This way we obtain the noise model</p><formula xml:id="formula_8">n (x) = I n (x) ? I GT (x) = ?p(x) + n(x) ? I GT (x)<label>(6)</label></formula><p>which is independent and has zero mean. Therefore the Poisson-Gaussian noise model obeys the assumptions made in Section 2.3.</p><p>Under the Poisson-Gaussian noise model the noise level (standard deviation) has an exact relationship with the noise-free image through ?( n (x)) = ?I GT (x) + ? 2 .</p><p>In <ref type="bibr" target="#b10">[11]</ref> is presented a maximum likelihood approach for estimating of the Poisson-Gaussian model parameters (?, b), (where b = ? 2 ) from a single image. The authors also observe that b could also be negative due to the pedestal level, a constant offset from zero of the digital imaging sensor.</p><p>Using local noise estimation through equations (2), (3), and (4), we can calculate the the intensity level sets I GT i of each image patch and the variance v i of the noise in each level set, then we can find the Poisson-Gaussian model parameters (?, b) so that v i = ?I GT i + b by the maximum likelihood method from <ref type="bibr" target="#b10">[11]</ref>. We can then see how well the Poisson-Gaussian noise model fits our data and we can we can compare our model parameters with the single-image parameter estimates from <ref type="bibr" target="#b10">[11]</ref>.</p><p>A special scene was acquired for this purpose using a uniform background with a smoothly changing intensity and our "sandwich" procedure. We converted the images to gray-scale to be able to compute the model from <ref type="bibr" target="#b10">[11]</ref> and divided the image into 400?400 blocks and the blocks into intensity level sets of a smoothed image, following the method described in <ref type="bibr" target="#b10">[11]</ref>. Based on the different ways to estimate the noise variance ? in each level set we considered the following variants:</p><p>? In the Foi model, the noise level ? is estimated as the standard deviation of the wavelet detail coefficients z wdet , as described in <ref type="bibr" target="#b10">[11]</ref>.</p><p>? In the three image model, the noise level ? is estimated using three images (reference, clean and noisy) and equations (2) and (4).</p><p>? In the blurred reference model, the I GT is obtained by blurring the reference image with a large Gaussian kernel and the noise level ? is estimated as the standard deviation of the difference between the noisy image and the blurred reference image.</p><p>? The blurred noisy model takes as I GT the blurred noisy image, thus it is obtained entirely from the noisy image.</p><p>The smooth image for obtaining the level sets was obtained as follows: for the Foi method we used the blurred z wapp wavelet approximation image, for the three image model we used the average (I r + I c )/2 between the reference and clean images, for the blurred reference model we used the blurred reference image, and for the blurred noisy model we used the blurred noisy image. On the obtained (? i ,? i ) intensity-noise level pairs for each variant we fitted Poisson-Gauss model parameters (?, b) by maximum likelihood as described in <ref type="bibr" target="#b10">[11]</ref>.</p><p>To validate our implementation of the Poisson-Gaussian model estimation procedure, we also plotted the Foi original model, where the parameters were obtained from the noisy image using code available online 2 .</p><p>The obtained curves are shown in <ref type="figure" target="#fig_0">Figure 11</ref>. In <ref type="figure" target="#fig_0">Figure 11</ref>, left are also shown the data points (? i ,? i ) from which the three image model and the Foi model were obtained.</p><p>In <ref type="figure" target="#fig_0">Figure 11</ref>, right are shown the the three estimation methods based on our data: the three image mode, the blurred reference model and the blurred noisy model. Observe that these curves are very close to each other and that the blurred reference model exactly coincides with the three image model. This shows that the blurred reference is a very good approximation of I GT in this case and the blurred noisy image is also a good approximation of I GT . Observe that out of these three methods only the three image model directly generalizes to images with edges, while the other methods need to carefully avoid pixels close to the edges where the blurred image does not approximate the I GT correctly.</p><p>It can be immediately noted that the Foi estimation method is consistently below our three curves. At high intensities (around 0.9 on the normalized scale), Foi's estimate and the corresponding data points are underestimating the noise level by about 40%. We suspect that this is due to the local correlations in the image noise, which interfere with the noise estimation based on wavelets and a simple convolution. To investigate this further, we modified the Foi model by estimating the noise level ? as the standard deviation of the image difference z dif f = I n ? 2?z smo , where I n ? 2 means the noisy image downsampled by a factor of 2. This curve can also be seen in <ref type="figure" target="#fig_0">Figure   11</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation of Denoising Algorithms</head><p>In this section we use our dataset to evaluate six popular image denoising algorithms: the Active Random Field (ARF) <ref type="bibr" target="#b4">[5]</ref>, Block Matching and 3D Filtering (BM3D) <ref type="bibr" target="#b3">[4]</ref>, Bilevel optimization (opt-MRF) <ref type="bibr" target="#b9">[10]</ref>, Multi Layer Perceptron (MLP) <ref type="bibr" target="#b7">[8]</ref>, Nonlocal means using a James-Stein estimator(NLM-JS) <ref type="bibr" target="#b23">[24]</ref>, and Non-local means with a soft threshold (NLM-ST) <ref type="bibr" target="#b24">[25]</ref>. These algorithms were selected because they are efficient enough to handle our large images and have code available online. Each of these methods depends on a noise level parameter ?. We tested the ARF filters <ref type="bibr" target="#b2">3</ref>  In <ref type="table" target="#tab_3">Table 3</ref> are shown the denoising results of the various methods on the three cameras. We computed the PSNR, SSIM, and VSNR values between the denoised and the best GT estimate which is the average of the two clean images.  that denoising was performed, while SSIM is not.</p><p>The best results obtained for the ARF, opt-MRF, and MLP methods occurred with a ? = 25 filter while the BM3D provided its best results with a ? = 50 filter. The results from <ref type="table" target="#tab_3">Table 3</ref> show that the BM3D outperformed the other methods on all the cameras using all similarity measures. In particular when comparing the performance of BM3D with MLP, opt-MRF, and NLM-ST for real noisy images these results do not lead to the same conclusions as in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref> where these methods performed as good or better than BM3D on small gray synthetic noisy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we introduced a dataset of images containing real noise due to lowlight settings and acquired from two digital cameras and a mobile phone. Additionally, we developed a method for obtaining pixel-aligned RAW images of low and high noise, and intensity-aligned BMP images so that proper studying of the images and their noise need not be done in RAW format. We also presented a technique to calculate the PSNR of an image without a ground truth and we conducted extensive evaluations of our noise estimation and our alignment procedure to make sure that the difference between the noisy and clean images is just noise.</p><p>We used our data to evaluate the Poisson-Gaussian noise model <ref type="bibr" target="#b10">[11]</ref> and its parameter estimation procedure. We observed that the noise has an overall trend that fits the Poisson-Gaussian model but the wavelet-based estimation method has difficulty estimating the correct model parameters due to short-range interactions in the noise structure and to the use of a box filter instead of a Gaussian filter for smoothing the image.</p><p>We tested our dataset on six denoising algorithms: ARF, BM3D, opt-MRF, MLP, and two version of Non-local Means. For all methods we computed the noise levels in the denoised images using a variety of methods such as PSNR, VSNR, and SSIM.</p><p>Note that these denoising algorithms were trained or tuned on images corrupted by artificial Gaussian noise. Some of these methodologies (ARF opt-MRF, and MLP) and many other recent state-of-the-art denoising methods such as: CSF <ref type="bibr" target="#b25">[26]</ref>, LSSC <ref type="bibr" target="#b6">[7]</ref>, and RTF <ref type="bibr" target="#b26">[27]</ref> learn the noise structure in a supervised way from the noisy-clean image pairs. These methods could in fact perform even better for denoising low light images if trained on our dataset. With so many different denoising methods having been developed or currently in development, our dataset allows for proper analysis of these tools, and for the quantitative evaluation of noise models for digital and mobile phone cameras.</p><p>Our dataset poses one more training and testing challenge compared to using images corrupted by artificial noise. The images in our dataset have a large range of noise levels in them, while usually denoising methods are trained and evaluated for one known noise level only. Data with different noise levels poses many challenges in training and testing, but at the same time it helps denoising algorithms advance to the level where they could be used in practice for automatically denoising digital camera images, without any user interaction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of a clean and noisy image pair as well as their corresponding blue channel. The noise present is the result of the low-light environment. The images were taken using a Canon PowerShot S90.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Intensity alignment issues observed on scatter plots of the intensity difference between the reference image and aligned noisy image vs reference image intensity. Left: image movement during the 'sandwich' procedure. Middle: light saturation. Right: non-linearity at high ISO levels. came in the control over certain image acquisition parameters such as the exposure time and ISO values. These mobile phone cameras already have a very small sensor, so when</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>C, I c -the clean image ? N, I n -the noisy image ? GT, I GT -the unknown ground truth image ? , r , c -random variables for noise in the low-noise images ? n -random variable for noise the noisy images ? ? 2 (X) = var(X) the variance of a random variable X We assume that the two low-noise images I r (reference) and I c (clean) as well as the noisy image(s) I n (acquired with the "sandwich" procedure from Section 2) are all noisy versions of a common (unknown) ground truth image I GT , corrupted by zeromean noise. Thus:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Scatter plots of the pixel intensity correspondence between a reference image and its noisy counterpart. Left: the correspondence between the red channel of the 8-bit reference image and the red channel for the 16-bit noisy image. The line shows the estimated linear mapping to align the noisy image to the reference image. Middle: the difference between corresponding pixel intensities of the reference 8-bit and aligned noisy 8-bit image vs reference image intensities for all three color channels. Right: the difference between corresponding pixel intensities between the reference 8-bit and the aligned clean 8-bit image vs reference image intensities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>An example of the pixel intensity histogram for the Clean and Noisy Green Channels before and after our brightness alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>the 8 -</head><label>8</label><figDesc>bit reference image R and the RAW noisy image I of the same scene, with the alignment line parameterized by ? superimposed. The alignment of the obtained 8-bit image I with the 8-bit reference can be diagnosed by plotting the intensity differenc? I ?R (blurred versions) vs the reference intensityR. This is shown in Figure 3, middle for the noisy image and Figure 3, right for the clean image. We obtained plots like Figure 3 for all the images in the dataset as a way of diagnosing any misalignment or nonlinear correspondence between the reference image and the corresponding noisy or clean images. The dark dashed horizontal line in the middle and right plots are 95% noise bounds for clean images with at least PSNR = 35.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>BFigure 5 :</head><label>5</label><figDesc>) = var(A) + var(B), or in other words ? 2 (A ? B) = ? 2 (A) + ? 2 (B) where var(A), ?(A) are the variance and standard deviation of A respectively. Then from equation (1) we get ? 2 (I r (x) ? I c (x)) = var( r (x) ? c (x)) = var( r (x)) + var( c (x)) = 2? 2 ( (x)) Frequency distributions of various noise levels for the noisy and clean images obtained from the Mi3, S90, and T3i cameras respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Variation of PSNR and ? values for noisy and clean images for each camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>The process for constructing the proper reference, clean, noisy, and ground truth images necessary for the noise estimation evaluation. The values of ?', ? 1 , and ? 2 represent the usual alignment of those respective images from 16-bit to 8-bit as described in Section 2.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>The relative error in estimating noisy and clean images.The true values of the noise levels for the I r ,I n , and I c images can be computed as the standard deviation of the difference between each of them and I GT (all in 8-bit versions). Our noise level estimation method for I c and I n described in Section 2.5 is compared with the true noise level to obtain the relative error (defined as estimation error divided by the true noise level value). The same type of relative error is also computed for the standard estimate of the noise, which is ?(I n ? I r ) and ?(I c ? I r ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8</head><label>8</label><figDesc>shows the relative error (defined as error divided by the true value) of estimating the noise level for both I n and I c . When it came to estimating the noise level I n , our estimation method kept the relative error to below 0.5%, while the standard method of estimating the noise level had a relative error around 5%. For the low noise images I r and I c our method had an error below 1% while the standard method had an error of around 40%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Analysis of the calibration images. Left: the intensity histograms of the green channels of the calibration images. Right: the distribution of the intensity difference between the reference image and the various other images in the calibration dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Left: The noise curve of our pair image model and the Foi Poisson-Gaussian Mixture model. Right: The noise curves for various noise estimation models using image pairs and the Foi Poisson-Gaussian Mixture model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>, right. Compared to Foi's original model, this modified Foi model brings the curve quite close to our estimates. It is still not exactly the same because the z smo is not a good approximation of the I GT since it was obtained by smoothing with a box kernel which does not remove the noise as well as a Gaussian kernel. Indeed the blurred noisy model, which does the smoothing of the noisy image with a large Gaussian kernel to obtain the I GT and level sets, comes very close to our estimates.Foi's Poisson-Gaussian noise model is using just the noisy image to infer the noise level in the image. With our data acquisition methodology we have both clean and noisy images and are able to infer more accurately the noise level in the image. Also, note that this evaluation was done on a special scene of a uniform background of continuously changing intensity and no edges. Foi's estimation method would have more difficulty in estimating the noise curve in images with a lot of edges or a lot of textures. At the same time, the three image approach only used the aligned images and no blurring, so it should work as well when edges are present.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>that were trained using Gaussian noise (in particular the trained filters for ? = 10,<ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25</ref> and 50) and using four iterations. A special version of the BM3D algorithm meant for color image denoising 4 was used on the noisy images. For BM3D we evaluated the algorithm's performance at ? = 5, 10, 15, 20, 25, and 50. For opt-MRF we used the Gaussian trained filters (? = 15 and 25) and a maximum limit of 30 iterations for the optimization<ref type="bibr" target="#b4">5</ref> . We also used MLPs trained on Gaussian filters 6 to denoise our images. In particular we used filters for ? =10, 25, 35, 50, and 75. These methods were evaluated for the values of the noise level ? given above and then for each method the parameter was fixed to the value that gave the best results.For the NLM-JS 7 algorithm a patch size of 3 ? 3, with a search window of 15 ? 15, and block size of 15 ? 15 was used for denoising. Finally, for the NLM-ST 8 algorithm a patch size of 5 ? 5, with a search window of 13 ? 13, and block size of 21 ? 21 was used for denoising. The noise parameter value for the NLM-JS and NLM-ST algorithms were estimated for each image using our previously discussed "sandwich" procedure. For the ARF, opt-MRF, MLP, NLM-JS, and NLM-ST algorithms the image channels were denoised in the YUV color space for better performance. For BM3D there was no transformation necessary because it could directly handle color images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">ISO (and Exposure time) per camera</cell></row><row><cell></cell><cell cols="2">Reference/Clean Images</cell><cell cols="2">Noisy Images</cell></row><row><cell cols="2">Camera ISO</cell><cell>Time(s)</cell><cell>ISO</cell><cell>Time(s)</cell></row><row><cell>Mi3</cell><cell>100</cell><cell>auto</cell><cell cols="2">1600 or 3200 auto</cell></row><row><cell>S90</cell><cell>100</cell><cell>3.2</cell><cell cols="2">640 or 1000 auto</cell></row><row><cell>T3i</cell><cell>100</cell><cell>auto</cell><cell cols="2">3200 or 6400 auto</cell></row><row><cell cols="5">Using the Canon Developer Tool Kit for the Canon S90 and the EOS Utility for the</cell></row><row><cell cols="5">Canon Rebel T3i we were able to program the automatic collection of the four images</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Description of the dataset and size</figDesc><table><row><cell></cell><cell></cell><cell>Noisy Images</cell><cell>Clean Images</cell></row><row><cell>RAW</cell><cell>Sensor</cell><cell># of</cell></row></table><note>? PSNR PSNR PSNR ? PSNR PSNR PSNR Camera Image Size Size(mm) Scenes Avg. Min. Avg. Max. Avg. Min. Avg. Max.S90 3684?2760 7.4?5.6 40 18.25 17.43 26.19 33.39 3.07 35.03 38.70 43.43 T3i 5202?3465 22.3?14.9 40 11.71 18.94 27.44 35.26 2.57 34.98 40.43 48.13 Mi3 4208?3120 4.69?3.52 40 19.23 12.75 23.49 36.68 3.71 33.50 37.09 45.25</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Note the high values given by the SSIM prior to denoising and the lower values for two cameras after denoising. It is not clear how to interpret the SSIM results since the other two measures (PSNR and VSNR) are consistent with each other and with the fact 3 from http://ani.stat.fsu.edu/?abarbu/ARF/demo.zip 4 from http://www.cs.tut.fi/?foi/GCF-BM3D/BM3D.zip 5 from http://gpu4vision.icg.tugraz.at/index.php?content=downloads.php 6 from http://people.tuebingen.mpg.de/burger/neural_denoising/ 7 fromhttps://www.mathworks.com/matlabcentral/fileexchange/</figDesc><table><row><cell>40162-james-stein-type-center-pixel-weights-for-non-local-means?s_</cell></row><row><cell>tid=srchtitle</cell></row><row><cell>8 fromhttp://ieeexplore.ieee.org/document/6957527/media</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of various denoising algorithms on our dataset. 24.820 22.521 24.132 24.201 23.398 S90 23.789 26.769 28.635 27.357 27.255 27.282 25.559 T3i 22.318 28.567 30.481 29.803 29.429 28.834 28.391 Average 21.284 25.908 27.979 26.560 26.605 26.828 25.864</figDesc><table><row><cell cols="4">Camera Before Denoising ARF BM3D opt-MRF MLP NLM-JS NLM-ST</cell></row><row><cell>PSNR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mi3</cell><cell>23.492</cell><cell cols="2">30.918 32.347 31.641 31.230 31.348 30.866</cell></row><row><cell>S90</cell><cell>26.187</cell><cell cols="2">33.797 36.752 34.983 34.073 34.135 33.076</cell></row><row><cell>T3i</cell><cell>27.442</cell><cell cols="2">36.550 39.966 38.646 37.584 37.400 36.822</cell></row><row><cell>Average</cell><cell>25.707</cell><cell cols="2">33.755 36.355 35.090 34.296 34.294 33.589</cell></row><row><cell>SSIM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mi3</cell><cell>0.989</cell><cell>0.972 0.982 0.964 0.929 0.965</cell><cell>0.970</cell></row><row><cell>S90</cell><cell>0.988</cell><cell>0.959 0.979 0.958 0.920 0.973</cell><cell>0.970</cell></row><row><cell>T3i</cell><cell>0.991</cell><cell>0.993 0.994 0.993 0.933 0.993</cell><cell>0.992</cell></row><row><cell>Average</cell><cell>0.989</cell><cell>0.981 0.985 0.972 0.927 0.977</cell><cell>0.977</cell></row><row><cell>VSNR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mi3</cell><cell>17.746</cell><cell>22.387</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available at: http://adrianbarburesearch.blogspot.com/p/renoir-dataset. html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code obtained from http://www.cs.tut.fi/?foi/sensornoise.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>. This work was supported in part by DARPA MSEE grant FA 8650-11-1-7149.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Labs</surname></persName>
		</author>
		<ptr target="http://www.dxomark.com/About/Sensor-scores/Use-Case-Scores/" />
		<title level="m">Dxomark sensor scores</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image denoising using scale mixtures of gaussians in the wavelet domain, Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Strela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering, Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training an active random field for real-time image denoising, Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2451" to="2462" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
		<title level="m">Stochastic image denoising</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Image denoising: Can plain neural networks compete with bm3d?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hamerling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A generative perspective on mrfs in low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Revisiting loss-specific training of filterbased mrfs for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Practical poissonian-gaussian noise modeling and fitting for single-image raw-data, Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trimeche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1737" to="1754" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Automatic estimation and removal of noise from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="299" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual image distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Electronic Imaging: Science and Technology</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="127" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Noise parameter mismatch in variance stabilization, with an application to poisson-gaussian noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?kitalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5348" to="5359" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image database tid2013: Peculiarities, results and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vozel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="57" to="77" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Image quality assessment:from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
		<ptr target="http://www.cs.utoronto.ca/?strider/Denoise/Benchmark/" />
	</analytic>
	<monogr>
		<title level="j">Image denoising benchmark</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Online; accessed 15</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image denoising in mixed poisson-gaussian noise, Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thierry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="696" to="708" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Denoising via nonlinear image decomposition for a digital color camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">309</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Numerical recipes 3rd edition: The art of scientific computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Radiometric ccd camera calibration and noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondepudy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Vsnr: A wavelet-based visual signal-to-noise ratio for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2284" to="2298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An intensity similarity measure in low-light conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="267" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">James-stein type center pixel weights for non-local means image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tracey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Noonan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="411" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local means image denoising with a soft threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="833" to="837" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Loss-specific training of nonparametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="112" to="125" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
