<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">USING PANORAMIC VIDEOS FOR MULTI-PERSON LOCALIZATION AND TRACKING IN A 3D PANORAMIC COORDINATE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">RIKEN</orgName>
								<orgName type="department" key="dep2">Center for Advanced Intelligence Project</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiran</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">RIKEN</orgName>
								<orgName type="department" key="dep2">Center for Advanced Intelligence Project</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">RIKEN</orgName>
								<orgName type="department" key="dep2">Center for Advanced Intelligence Project</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Osaka University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">USING PANORAMIC VIDEOS FOR MULTI-PERSON LOCALIZATION AND TRACKING IN A 3D PANORAMIC COORDINATE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D localization</term>
					<term>multi-target tracking</term>
					<term>panoramic videos</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D panoramic multi-person localization and tracking are prominent in many applications, however, conventional methods using LiDAR equipment could be economically expensive and also computationally inefficient due to the processing of point cloud data. In this work, we propose an effective and efficient approach at a low cost. First, we utilize RGB panoramic videos instead of LiDAR data. Then, we transform human locations from a 2D panoramic image coordinate to a 3D panoramic camera coordinate using camera geometry and human bio-metric property (i.e., height). Finally, we generate 3D tracklets by associating human appearance and 3D trajectory. We verify the effectiveness of our method on three datasets including a new one built by us, in terms of 3D single-view multi-person localization, 3D single-view multi-person tracking, and 3D panoramic multiperson localization and tracking. Our code is available at https://github.com/fandulu/MPLT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In daily life, we understand surrounding visual scenes in 3D. For instance, we usually decide how to interact with surrounding people by first localizing and tracking them in a 3D egocentric coordinate: when we are walking down the street, we plan our path to avoid collisions by analyzing the trajectories of the surrounding people; when we see friends walking towards us, we might also walk to them and have a greeting. For applications (e.g., social robotics) that also require visual scene understanding, performing multi-person localization and tracking in a 3D coordinate is strongly desired.</p><p>Typical single-view 3D coordinate localization methods fall into two categories: using depth sensors, or, using object size and camera geometry. Previous studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> relied on depth sensors (e.g., LiDAR) and instance segmentation to obtain the target location in a 3D camera coordinate. In practice, however, the instance person segmentation algorithm is imperfect in crowd scenes, resulting in the assigning of incorrect locations to a target person. To some extent, these methods are more suitable for multiple vehicle tracking <ref type="bibr" target="#b2">[3]</ref>, since they are rigid objects with known shapes and the distance between them is generally larger. In contrast, other methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> infer 3D camera-coordinate locations by object bounding box size and camera geometry. However, there is a scale variance between standing persons and sitting persons in terms of bounding box height. Moreover, when a person is near the camera, only the upper body can be observed. Consequently, simply taking the bounding box height as a reference is inappropriate. Recently, a study <ref type="bibr" target="#b5">[6]</ref> demonstrated that using the skeleton length can obtain more accurate locations than using bounding boxes. We embrace this idea into our framework to obtain target locations in a single-view 3D camera coordinate.</p><p>Conventional multi-person tracking takes two stages. The first detects each person by an object detector, and the second associates the cross-frame identities by considering their appearance similarity and trajectory trend <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Most existing studies work on 2D/3D single-view <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>, or, 2D panoramic multi-target tracking <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. However, previous works have limitations: the 2D tracking results could not be directly used in some applications (i.e., robotics) since the real-world coordinate is 3D; it is easy to lose the tracking target in a 3D narrow-angle-view coordinate since it only covers a part of the surrounding environment. Therefore, we propose 3D panoramic multi-target tracking to address the aforementioned issues.</p><p>We propose a novel framework for multi-person localization and tracking in a 3D panoramic coordinate with panoramic RGB videos 1 . In our framework, 2D human poses are estimated for each person to obtain the 2D location and body height. Utilizing single-view intrinsic camera parameters, a person's 3D location can be approximated by assuming the body height is a constant. We further transform locations from a 3D single-view camera coordinate to a 3D panoramic coordinate using extrinsic camera parameters. Unlike in a 2D image coordinate, the real-scale location and motion are preserved in a 3D coordinate. As a benefit, it is easier to harness the power of Kalman filter to model human trajectories. To further address issues like occlusion and miss detection, we associate the appearance similarity and the trajectory trend together to approach multi-person tracking. We annotated a Multi-person Panoramic Localization and Tracking (MPLT) dataset to evaluate our framework. We also compared our framework with others on the KITTI dataset <ref type="bibr" target="#b10">[11]</ref> and the 3D MOT dataset <ref type="bibr" target="#b11">[12]</ref>, where only singleview 3D localization and tracking results are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head><p>Our framework includes four modules: Pose Detection Module, Geometry Transformation Module, Appearance Reidentification Module, and Tracking Module (see <ref type="figure">Fig. 1</ref>). They work seamlessly together to achieve the target goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Obtain 2D Person Poses</head><p>Similar to previous work <ref type="bibr" target="#b5">[6]</ref>, we use off-the-shelf PifPaf <ref type="bibr" target="#b12">[13]</ref> as our Pose Detection Module to estimate 2D human poses. Depends on the need, 2D person poses can be obtained either by a top-down approach or a bottom-up approach. In the former, an object detector (e.g., YOLO <ref type="bibr" target="#b13">[14]</ref>) is used to acquire the 2D bounding box for each person, and then PifPaf estimates 2D poses within each single bounding box. Alternatively, PifPaf can simultaneously estimate 2D poses for all persons and assign them to each person, which is a bottomup approach. Compared with the top-down approach, the bottom-up approach is faster but less accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Coordinate Transformation</head><p>We build a Geometry Transformation Module to map person locations from 2D image coordinates to a 3D panoramic coordinate. In our setting, four single-view cameras are used to capture panoramic videos. By removing the overlapping areas, we obtain four single-view images with a 90 ? Horizontal Field of View at each frame. Following a clockwise path, we can assign each single-view image with view angle ?, where </p><formula xml:id="formula_0">? ? {0 ? Y , 90 ? Y , 180 ? Y , 270 ? Y }. Let [u ? , v ? ] T be</formula><formula xml:id="formula_1">? ? u ? v ? 1 ? ? = K ? ? X ? Y ? Z ? ? ? ,<label>(1)</label></formula><p>where K is the intrinsic matrix.</p><p>To transform locations from 3D camera coordinates to a 3D panoramic coordinate, we construct an extrinsic matrix:</p><formula xml:id="formula_2">R t 0 T 1 ? [R|t] ? R 4?4 |R ? SO(3), t ? R 3 ,<label>(2)</label></formula><p>where SO(n) denotes a Special Orthogonal Group with dimension n; 0 indicates a zero vector; R and t are the 3D rotation matrix and the translation matrix. In our settings, all single-view coordinate centers are close to each other, so that t can be approximated by a zero vector and R only contain Y-axis rotation. Accordingly, for location [X, Y, Z] T in a 3D panoramic coordinate, the complete projection matrix can be defined:</p><formula xml:id="formula_3">P ? = K[R(?)|t],<label>(3)</label></formula><formula xml:id="formula_4">and we have ? ? u ? v ? 1 ? ? = P ? ? ? ? ? X Y Z 1 ? ? ? ? .<label>(4)</label></formula><p>At first glance, [X, Y, Z] T cannot be determined by [u ? , v ? ] T in the above equation. However, we assume that real-world body height H body is a constant value. Since the corresponding body height in a 2D image coordinate (i.e., h body ) can be obtained by a pose estimator, for each person, the corresponding X and Z can be calculated by solving</p><formula xml:id="formula_5">? ? u ? h body 1 ? ? = P ? ? ? ? ? X H body Z 1 ? ? ? ? , ?H body ? constant.<label>(5)</label></formula><p>Hence, we can transform a target from a 2D single-view image coordinate to a 3D panoramic coordinate. Since most real applications focus on the ground plane scenario, we treat Y = 0 for all the persons in the 3D panoramic coordinate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Matching Cost</head><p>The appearance of people can be utilized as an important tracking cue to alleviate the occlusion issue in tracking. Although existing works exploit the entire body appearance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, we suppose that only using the upper body appearance can alleviate occlusion problems in crowd scenes. We further demonstrate this point in our experimental results of <ref type="table" target="#tab_6">Table 4</ref>. Since 2D body poses are estimated in this work, the upper body image patches can be cropped accordingly. We use an off-the-shelf model <ref type="bibr" target="#b14">[15]</ref> as our Appearance Re-identification Module. Given an upper-body image patch, it extracts the correspondent appearance embedding vector.</p><p>In the tracking processes, appearance similarly is used to re-identify each person in the spatio-temporal domain. More specifically, the appearance cost between two consecutive frames is formulated as</p><formula xml:id="formula_6">C app i,j = 1 ? a i a j a i a j , i ? {1, . . . , N i }, j ? {1, . . . , N j }<label>(6)</label></formula><p>where C app i,j is the appearance cost of instance i of the previous frame to instance j of the current frame; N i and N j are the corresponding number of instances; a i and a j are the appearance embedding vectors with dimension 2048.</p><p>Apart from the appearance cue, the trajectory trend is also a critical cue to track targets. With regard to previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, Kalman filter [? ] is commonly used to model the trajectory trend. In contrast with modeling the trajectory trend in a 2D image coordinate, modeling it in a 3D coordinate can alleviate the position and motion distortions, which simplifies the procedure of applying Kalman filter to model trajectories. To be consistent with C app at value range 0 ? 1, we apply an exponential kernel to calculate the distance between detected locations and Kalman filter estimated locations that are normalized by H body . The trajectory cost between two consecutive frames is defined by</p><formula xml:id="formula_7">C traj i,j = 1 ? exp ? (X i ? X j ) 2 + (? i ? Z j ) 2 H 2 body<label>(7)</label></formula><p>where C traj i,j is the trajectory cost of instance i of the previous frame to instance j of current frame. Additionally, [X i ,? i ] denotes the estimated location of instance i at current frame by Kalman filter, while L j,: = [X j , Z j ] presents the detected location of instance j at current frame, where L denotes the location values of all the detected instances.</p><p>We can simply associate C app i,j and C traj i,j by letting</p><formula xml:id="formula_8">C i,j = C traj i,j + C app i,j ,<label>(8)</label></formula><p>where C i,j is the associate cost of matching instance i of the previous frame to instance j of the current frame. Then optimal assignment M * is obtained by minimizing the total cost</p><formula xml:id="formula_9">M * = arg min M i j C i,j M i,j ,<label>(9)</label></formula><p>where M is a Boolean matrix. When row i is assigned to column j, we have M i,j = 1. Note that, each row can be assigned to at most one column and each column to at most one row. The optimization can be done by the Hungarian method.  if m j = 0 (i.e., instance j is unmatched) then <ref type="bibr" target="#b17">18</ref> Add one more active instance to T: </p><formula xml:id="formula_10">19 T |T|+1 [location] ? T |T|+1 [location] ? {L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Multi-person Tracking</head><p>In the Tracking Module, we create a tracking set T to store and update tracked instances. At the k-th tracked frame, we obtain a set of 3D panoramic locations L by Eq. (5) and the cost matrix C by Eq. <ref type="bibr" target="#b7">(8)</ref>. In the first frame, all the observed locations are assigned to a tracking set. After that, the acrossframe connections are determined by M i,j and C i,j . When M i,j = 1 and C i,j is smaller than a threshold ?, the instance i of frame k ? 1 is likely to be the instance j of frame k. However, across-frame instances may not always be perfectly matched. For unmatched instance j, we assign it to T as a new instance. For unmatched instance i, which is already recorded in T, we reduce its lifespan by 1. While new instances come into the tracking area, old instances may also leave. Therefore, we delete unseen instances in the tracking set after 10 frames. We summarize this process in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>Experimental Datasets. We annotate a Multi-person Panoramic Localization and Tracking (MPLT) Dataset to enable model evaluation on 3D panoramic multi-person localization and tracking. It represents real-world scenarios and contains a crowd of people in each frame. And, over 1.8K frames and densely annotated 3D trajectories are included. For comparison with related works, we also evaluate our framework on the KITTI <ref type="bibr" target="#b10">[11]</ref> and 3D MOT <ref type="bibr" target="#b11">[12]</ref> datasets. The properties of three experimental datasets are listed as follows:  <ref type="table" target="#tab_5">Table 3</ref>, we compare our framework with others on the 3D MOT Benchmark 2 , which targets at 3D single-view localization and tracking. We achieve the state-of-the-art performance (i.e., 1 st place of the public leaderboard) on the dominant criterion (i.e., MOTA <ref type="bibr" target="#b15">[16]</ref>), which outperforms the second place method by 1.5. For our proposal dataset MPLT, we list the performance of our framework and make it as a baseline (see <ref type="table" target="#tab_6">Table 4</ref>). Furthermore, we also show, due to the occlusion, selecting the whole body appearance may impair the model performance. The qualitative evaluation results are available on our project page 3 .   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 : 3 for j ? 1 to N j do 4 T 5 T</head><label>13145</label><figDesc>Tracking algorithm Input : k (current tracked frame number), C (association cost matrix), L (instance location matrix), T (active instance set), ? (matching cost threshold) 1 if k = 1 then 2 Initialize active instance set T ? ?. j [location] ? T j [location] ? {L j,: }. j [lif espan] = 10. 6 else 7 Obtain M * by optimizing Eq. (9) with the Hungarian method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>8 14 T 15 m j = 1. 16 for j ? 1 to N j do 17</head><label>14151617</label><figDesc>Initialize the N j ? 1 dimensional matching indicator vector m = 0 for current frame k.9 for i ? 1 to N i do 10 T i [lif espan] = T i [lif espan] ? 1. 11 for j ? 1 to N j do 12 if M * i,j = 1 and C i,j &lt; ? then 13 T i [location] ? T i [location] ? {L j,: }. i [lif espan] = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>a point in the 2D image coordinate and let [X ? , Y ? , Z ? ] T be the corresponding point in the 3D camera coordinates of each single view. Then, we have</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>j,: }.</figDesc><table /><note>20 T |T|+1 [lif espan] = 10.21 if T i [lif espan] = 0 then22 Remove T i from T.23 for l ? 1 to |T| do24 Update Kalman filter with T l [location].25 Tl [location estimated] = [X l ,? l ], estimated using the updated Kalman filter.Output: T</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Properties of experimental datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">3D single-view 3D single-view 3D panoramic</cell></row><row><cell></cell><cell>localization</cell><cell>localization&amp;</cell><cell>localization&amp;</cell></row><row><cell></cell><cell></cell><cell>tracking</cell><cell>tracking</cell></row><row><cell>KITTI [11]</cell><cell></cell><cell></cell></row><row><cell>3D MOT [12]</cell><cell></cell><cell></cell></row><row><cell>MPLT</cell><cell></cell><cell></cell></row><row><cell cols="4">Experimental Setup. Since off-the-shelf pose detector and</cell></row><row><cell cols="4">appearance extractor are applied, we do not train any models</cell></row><row><cell cols="4">in this work. For the KITTI and MPLT datasets, we apply the</cell></row><row><cell cols="4">bottom-up pose estimation approach. For 3D MOT dataset,</cell></row><row><cell cols="4">we apply the top-down pose estimation with the given public</cell></row><row><cell cols="4">bounding boxes. Based on the properties of each dataset, we</cell></row><row><cell cols="4">evaluate the model performance from different perspectives.</cell></row><row><cell cols="4">Experimental Results. In Table 2, we report the localiza-</cell></row><row><cell cols="4">tion precision under three thresholds for the KITTI Dataset. It</cell></row><row><cell cols="4">shows good generalization property of our method. Although</cell></row><row><cell cols="4">without any training on KITTI Dataset, its performance can</cell></row><row><cell cols="4">reach the second place in terms of 3D single-view localization</cell></row><row><cell cols="2">without any extra training. In</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Monocular-camera-based localization precision on KITTI Dataset. If distance from predicted locations to ground-truth location is within a threshold, it is correctly predicted.</figDesc><table><row><cell>Methods</cell><cell cols="3">Localization precision by threshold &lt; 0.5m &lt; 1.0m &lt; 2.0m</cell></row><row><cell>Mono3D [17](training on KITTI)</cell><cell>13.2%</cell><cell>23.3%</cell><cell>39.0%</cell></row><row><cell>SAMono[18](training on KITTI)</cell><cell>19.8%</cell><cell>33.9%</cell><cell>48.5%</cell></row><row><cell>MonoDepth [19](training on KITTI)</cell><cell>20.6%</cell><cell>35.4%</cell><cell>50.7%</cell></row><row><cell>MonoLoco [6] (training on KITTI)</cell><cell>29.0%</cell><cell>49.6%</cell><cell>71.2%</cell></row><row><cell>Ours (w/o KITTI)</cell><cell>22.0%</cell><cell>39.4%</cell><cell>63.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>3D MOT Benchmark. ?(?) indicates that the larger(smaller) the value is, the better the performance. Multiple Object Tracking Accuracy (MOTA) is the dominant criterion. The details of the evaluation metrics were previously explained in<ref type="bibr" target="#b15">[16]</ref>.</figDesc><table><row><cell>Methods</cell><cell>MOTA?</cell><cell>MT?</cell><cell>ML?</cell><cell>FP?</cell><cell>FN?</cell></row><row><cell>AMIR3D [20]</cell><cell>25.0</cell><cell>3.0%</cell><cell cols="3">27.6% 2,038 9,084</cell></row><row><cell>MCFPHD [21]</cell><cell>39.9</cell><cell cols="4">25.7% 16.8% 3,029 6,700</cell></row><row><cell>GPDBN [22]</cell><cell>49.8</cell><cell cols="4">25.7% 17.2% 1,813 6,300</cell></row><row><cell>MOANA [23]</cell><cell>52.7</cell><cell cols="4">28.4% 22.0% 2,226 5,551</cell></row><row><cell>Ours</cell><cell>54.2</cell><cell cols="4">30.6% 20.9% 2,385 4,930</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Performance of our framework on MPLT dataset. We evaluate localization and tracking performance within 10 m of the coordinate center.We proposed a simple yet effective solution for 3D panoramic multi-person localization and tracking with panoramic videos. On two existing datasets, the effectiveness of our method is demonstrated by the promising performance. Meanwhile, a strong baseline is offered for our new benchmark dataset. Since our method can faithfully keep the realistic locations and motions for tracking targets in a 3D panoramic coordinate, it can help human-related video understanding applications. As future work, we plan to integrate our framework with a previous work [? ] for automatically detecting human activities in a 3D panoramic coordinate.</figDesc><table><row><cell cols="3">Appearance Selection Threshold MOTA?</cell></row><row><cell>Whole body</cell><cell>&lt; 0.5m</cell><cell>62.4</cell></row><row><cell>Whole body</cell><cell>&lt; 1.0m</cell><cell>70.2</cell></row><row><cell>Upper body</cell><cell>&lt; 0.5m</cell><cell>65.2</cell></row><row><cell>Upper body</cell><cell>&lt; 1.0m</cell><cell>74.9</cell></row><row><cell cols="2">4. CONCLUSION</cell><cell></cell></row></table><note>ACKNOWLEDGEMENTS. Part of this work was sup- ported by JSPS KAKENHI Grant Numbers JP17H06101 and JP17K00237, and a MSRA Collaborative Research 2019 Grant by Microsoft Research Asia.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Yang Wu is the corresponding author of this work. arXiv:1911.10535v5 [cs.CV] 8 Mar 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://motchallenge.net/results/3D_MOT_2015/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/fandulu/MPLT</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combined image-and world-space tracking in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljo?a</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep 3d perception of people and their mobility aids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Kollmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Vasquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A multi-sensor fusion system for moving object detection and tracking in urban driving environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunggi</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Woo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragunathan Raj</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mono-camera 3d multiobject tracking using deep learning detections and pmbm filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Benjaminsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrit</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Granstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaid</forename><surname>Ahmed Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K Madhava</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Monoloco: Monocular 3d pedestrian localization and uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Poi: Multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning for object tracking in 360 degree videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Delforouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Holighaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Grzegorzek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Recognition Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A polar model for fast object tracking in 360-degree camera images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Delforouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><surname>Amir Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimiaki</forename><surname>Tabatabaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Shirahama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grzegorzek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Global data association for the probability hypothesis density filter using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic multi-person localisation and tracking in image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Moana: An online learned adaptive appearance model for robust multiple object tracking in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
