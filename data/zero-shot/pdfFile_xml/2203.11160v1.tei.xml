<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Drive&amp;Segment: Unsupervised Semantic Segmentation of Urban Scenes via Cross-modal Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonin</forename><surname>Vobecky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Czech Institute of Informatics, Robotics and Cybernetics</orgName>
								<orgName type="department" key="dep2">CTU in Prague</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hurych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Czech Institute of Informatics, Robotics and Cybernetics</orgName>
								<orgName type="department" key="dep2">CTU in Prague</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriane</forename><surname>Sim?oni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Czech Institute of Informatics, Robotics and Cybernetics</orgName>
								<orgName type="department" key="dep2">CTU in Prague</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Czech Institute of Informatics, Robotics and Cybernetics</orgName>
								<orgName type="department" key="dep2">CTU in Prague</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Czech Institute of Informatics, Robotics and Cybernetics</orgName>
								<orgName type="department" key="dep2">CTU in Prague</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Czech Institute of Informatics, Robotics and Cybernetics</orgName>
								<orgName type="department" key="dep2">CTU in Prague</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Czech Institute of Informatics, Robotics and Cybernetics</orgName>
								<orgName type="department" key="dep2">CTU in Prague</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Drive&amp;Segment: Unsupervised Semantic Segmentation of Urban Scenes via Cross-modal Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>autonomous driving ? unsupervised semantic segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work investigates learning pixel-wise semantic image segmentation in urban scenes without any manual annotation, just from the raw noncurated data collected by cars which, equipped with cameras and LiDAR sensors, drive around a city. Our contributions are threefold. First, we propose a novel method for cross-modal unsupervised learning of semantic image segmentation by leveraging synchronized LiDAR and image data. The key ingredient of our method is the use of an object proposal module that analyzes the LiDAR point cloud to obtain proposals for spatially consistent objects. Second, we show that these 3D object proposals can be aligned with the input images and reliably clustered into semantically meaningful pseudo-classes. Finally, we develop a crossmodal distillation approach that leverages image data partially annotated with the resulting pseudo-classes to train a transformer-based model for image semantic segmentation. We show the generalization capabilities of our method by testing on four different testing datasets (Cityscapes, Dark Zurich, Nighttime Driving and ACDC) without any finetuning, and demonstrate significant improvements compared to the current state of the art on this problem. 3</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this work, we investigate whether it is possible to learn pixel-wise semantic image segmentation of urban scenes without the need for any manual annotation, just from the raw non-curated data that are collected by cars equipped with cameras and LiDAR sensors while driving in town. This topic is important as current methods require large amounts of pixel-wise annotations over various driving conditions and situations. Such a manual segmentation of images on a large scale is very expensive, time consuming, and prone to biases.</p><p>Currently, the best methods for unsupervised learning of semantic segmentation assume that images contain centered objects [48] rather than full scenes, or use spatial self-supervision available in the image domain <ref type="bibr" target="#b14">[15]</ref>. They do not leverage additional modalities, such as the LiDAR data, available for urban scenes in the autonomous driving set-ups. In this work, we develop an approach for unsupervised semantic segmentation that learns to segment complex scenes containing many objects, including thin <ref type="figure">Fig. 1</ref>. Proposed fully-unsupervised approach. From uncurated images and LiDAR data, our Drive&amp;Segment approach learns a semantic image segmentation model with no manual annotations. The resulting model performs unsupervised semantic segmentation of new unseen datasets without any human labeling. It can segment complex scenes with many objects, including thin structures such as people, bicycles, poles or traffic lights. Black denotes the ignored label. structures such as pedestrians or traffic lights, without the need for any manual annotation, but instead leveraging cross-modal information available in (aligned) LiDAR point clouds and images, see <ref type="figure">Fig. 1</ref>. Exploiting point clouds as a form of supervision is, however, not straightforward: data from LiDAR and camera are rarely perfectly synchronized; moreover, point clouds are unstructured and of much lower resolution compared to RGB images; finally, extracting semantic information from LiDAR is still a very hard problem. In this work, we overcome these issues and demonstrate that it is nevertheless possible to extract useful pixel-wise semantic supervision from LiDAR data.</p><p>The contributions of our work are threefold. First, we propose a novel method for cross-modal unsupervised learning of semantic image segmentation by leveraging synchronized LiDAR and image data. The key ingredient is a module analyzing the LiDAR point cloud to obtain proposals for spatially consistent objects that can be clearly separated from each other and from the ground plane in the 3D scene. Second, we show that these 3D object proposals can be aligned with input images and reliably clustered into semantically meaningful pseudo-classes by using image features from a network trained without supervision. We demonstrate that this approach is robust to noise in point clouds and delivers, without the need for any manual annotation, pseudo-classes with pixel-wise segmentation for a variety of objects present in driving scenes. These classes include objects such as pedestrians or traffic lights that are notoriously hard to segment automatically in the image domain. Third, we develop a novel cross-modal distillation approach that first trains a teacher network with the available partial pseudo labels, and then exploits its predictions for training the student with pixel-wise pseudo annotations that cover the whole image. In addition, our approach exploits geometric constraints extracted from the LiDAR point cloud during the teacher-student learning process to refine the teacher predictions that are distilled into the student network. Implemented with transformer-based networks, this cross-modal distillation approach results in a trained student model that performs well in a variety of challenging conditions such as day, night, fog, or rain, outside the domain of the original training dataset, as shown in <ref type="figure">Fig. 1</ref> Overview of Drive&amp;Segment. We first perform cross-modal segment extraction on training dataset by exploiting raw LiDAR point clouds P and raw images I. This yields segments S I projected onto the image space ( ? 3.1). By clustering their self-supervised features, we obtain an unsupervised labeling of these segments ( ? 3.2) and, as a consequence, of their pixels. This provides pixel-wise pseudo ground truth for the next learning step. Finally, given the pseudo-labels and the segments, we perform distillation with cross-modal constraints ( ? 3.3) that conjugates information of the LiDAR and the images to learn a final segmentation model using a teacherstudent architecture. The learnt segmentation model S -highlighted in the figure-is used for inference on unseen datasets, yielding compelling results ( ? 4).</p><p>We train our proposed unsupervised semantic segmentation method on Waymo Open [45] and nuScenes <ref type="bibr" target="#b7">[8]</ref> datasets (nuScenes results are in the appendix), and test it on four different datasets in the autonomous driving domain, Cityscapes <ref type="bibr" target="#b15">[16]</ref>, Dark-Zurich [42], Nighttime driving <ref type="bibr" target="#b16">[17]</ref> and ACDC <ref type="bibr">[43]</ref>. We demonstrate significant improvements compared to the current state of the art, improving the current best published unsupervised semantic segmentation results on Cityscapes from 15.8 to 21.8 and from 4.6 to 14.2 on Dark Zurich, measured by mean intersection over union.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this work, we investigate the task of semantic image segmentation with no human supervision. We discuss the corresponding prior art below. Image semantic segmentation. Semantic segmentation is a challenging key visual perception task, especially for autonomous driving <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr">43,</ref><ref type="bibr">49,</ref><ref type="bibr">54]</ref>. Current top-performing models are based on fully convolutional networks <ref type="bibr" target="#b33">[34]</ref> with encoder-decoder structures and a large diversity of designs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr">41,</ref><ref type="bibr">50,</ref><ref type="bibr">59]</ref>. Recent progress in vision transformers (ViT) <ref type="bibr" target="#b18">[19]</ref> opened the door for a new wave of decoders [44,52,55,60] with appealing performance. All methods, in particular transformer-based <ref type="bibr" target="#b18">[19]</ref>, attain impressive performance by exploiting large amounts of pixel-wise labeled data. Yet, urban scenes are expensive to annotate manually (1.5h-3h per image <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">43]</ref>). This motivates recent works to rely less on pixel-wise supervision. Reducing supervision for semantic segmentation. A popular strategy when dealing with limited labeled data is to pre-train some of the blocks of the architecture, e.g., the encoder, on related auxiliary tasks with plentiful labels <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">56]</ref>. Pretraining encoder for image classification, e.g., on ImageNet <ref type="bibr" target="#b17">[18]</ref>, has been shown to be a successful recipe for both convnets <ref type="bibr" target="#b11">[12]</ref> and ViT-based models <ref type="bibr">[44]</ref>. Pretraining can be conducted even without any human annotations on artificially-designed self-supervised pretext tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> with impressive results on a variety of downstream tasks. Fully unsupervised semantic segmentation has been recently addressed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr">38,</ref><ref type="bibr">48,</ref><ref type="bibr">57]</ref> via generative models for generating object masks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">38]</ref> or self-supervised clustering <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>. Prior methods are limited to segmenting foreground objects of a single class <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref> or to stuff pixels that outnumber by far things pixels <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">38]</ref>. Others assume that images contain centered objects [48], rely on weak spatial cues from the image domain <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref> or require instance masks during pre-training and annotated data at test time <ref type="bibr" target="#b27">[28]</ref>. In contrast, our approach exploits cross-modal supervision from aligned LiDAR point clouds and images. We show that leveraging this information can considerably improve segmentation performance in complex autonomous driving scenes with multiple classes and strong class imbalance, outperforming PiCIE <ref type="bibr" target="#b14">[15]</ref>, the current state of the art in unsupervised segmentation.</p><p>Cross-modal self-supervised learning. Leveraging language, vision, and/or audio, self-supervised representation learning has seen tremendous progress in recent years <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">39,</ref><ref type="bibr">40,</ref><ref type="bibr">58]</ref>. Besides learning useful representations, these approaches show that signals from one modality can help train object detectors in the other, e.g., detecting instruments that sound in a scene <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">39,</ref><ref type="bibr">58]</ref>, and even other object types <ref type="bibr" target="#b0">[1]</ref>. In autonomous driving, a vehicle is equipped with diverse sensors (e.g., camera, LiDAR, radar) and cross-modal self-supervision is often used to generate labels from a sensor for augmenting the perception of another <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr">46,</ref><ref type="bibr">51]</ref>. LiDAR clues <ref type="bibr">[46]</ref> have been recently shown to improve unsupervised object detection. In contrast, we consider the problem of pixel-wise unsupervised semantic segmentation, a particularly challenging task given the sparsity and low resolution of LiDAR point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed unsupervised semantic segmentation</head><p>Our goal is to train an image segmentation model with no human annotation, by exploiting easily-available aligned LiDAR and image data. To that end, we propose a novel method, Drive&amp;Segment, that consists of three major steps and is illustrated in <ref type="figure">Figure 2</ref>. First, as discussed in Section 3.1, we extract segment proposals for the objects of interest from 3D LiDAR point clouds and project them to the aligned RGB images. In the second step, presented in Section 3.2, we build pseudo-labels by clustering self-supervised image features corresponding to these segments. Finally, in Section 3.3, we propose a new teacher-student training scheme that incorporates spatial constraints from the Li-DAR data and learns an unsupervised segmentation model from the noisy and partial pseudo-annotations generated in the previous two steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cross-modal segment extraction</head><p>Throughout the next sections, we consider a dataset composed of a set P of 3D point clouds and a set I of images aligned with the point clouds. In this section, we detail the process for extracting segments of interest in an image I ? I using the corresponding aligned LiDAR point cloud P ? P. The process, illustrated in <ref type="figure">Fig. 3</ref>, consists of three major steps. We start by segmenting the LiDAR point cloud P using its geometrical <ref type="figure">Fig. 3</ref>. Cross-modal segment extraction. Input raw point cloud (a) is first segmented with <ref type="bibr" target="#b6">[7]</ref> into object segment candidates (b), which are then projected into the image (c); Projected segments are densified to get pixel-level pseudo-labels, with missed pixels being labeled as "ignore", as shown in black (d).</p><p>properties. Then, we project the resulting 3D segments into the image I, and densify the output to obtain pixel-level segments. Geometric point cloud segmentation. We first extract J non-overlapping object segmentation proposals (segments), from the LiDAR point cloud P. Let S P ={s P j } J j=1 be this set, where each segment s P j is a subset of the 3D point cloud P and, ?j ? = j ? , s P j ? s P j ? = ?. Additionally, we refer to the set of segments over the entire dataset as S P , with S P ? S P . The J segments detected in one point cloud should ideally correspond to J individual objects in the scene. To get them, we use the unsupervised 3D point cloud segmentation proposed in <ref type="bibr" target="#b6">[7]</ref>, which exploits the geometrical properties of point clouds and range images. <ref type="bibr" target="#b3">4</ref> It is a two-stage process that segments the ground plane and objects using greedy labeling by breadth-first search in the range image domain. Urban scenes are particularly suited to this purely geometry-based method as most objects are spatially well separated and the ground plane is relatively easy to segment out. Point-cloud-to-image transfer. The next step of the segment extraction is to transfer the set S P of point cloud segments to the image I, producing the set S I . Despite LiDAR data and camera images being captured at the same time, one-to-one matching is not straightforward. Indeed, among other difficulties, LiDAR data only covers a fraction of the image plane because of its different field of view, its lower density and its lack of usable measurements on far away objects or on the sky for instance. To overcome the mismatch between the two modalities, we proceed as follows. First, we project the points from the point cloud to the image using the known sensors' calibration. This gives us the locations of 3D points from the point cloud in the image. We also identify locations with invalid measurements in the LiDAR's range image, e.g., reflective surfaces or the sky, and assign an "ignore" label to the respective locations. Densify &amp; pad. Next, we perform nearest-neighbor interpolation to propagate the J +1 segment labels to all pixels, where J is the number of segments (ideally corresponding to objects) and +1 denotes the additional "ignore" label. Last, we pad the image with "ignore" label to the input image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Segment-wise unsupervised labeling</head><p>Next, the objective is to produce pseudo-labels for all extracted segments of interest in the image space, and to do so without any supervision. In particular, we leverage the <ref type="figure">Fig. 4</ref>. Segment-wise unsupervised pseudo-labeling. First, given object segments S I obtained in the segment extraction stage (left), we take crops around all N objects and feed them to a feature extractor to get a set of N feature vectors. Then, we use the k-means algorithm to cluster the feature vectors into k clusters. Finally, we assign pixel-wise pseudo-labels to all pixels belonging to each segment based on the corresponding cluster id. Pixels not covered by a segment are assigned the label "ignore" (black). very recent ViT <ref type="bibr" target="#b18">[19]</ref> model pre-trained in a fully unsupervised fashion <ref type="bibr" target="#b9">[10]</ref> which has shown impressive results on a range of downstream tasks. We use this representation for unsupervised learning of pseudo-labels as described next and illustrated in <ref type="figure">Figure 4</ref>.</p><p>Considering here the image I, we crop a tight rectangular region in the image around each segment s I j ? S I obtained using the proposal mechanism described in the previous section. We resize it and feed it to the ViT model to extract the feature f j corresponding to the output features of the CLS token. To limit the influence of pixels outside the object segment, which may correspond to other objects or the background, we mask out these pixels before computing the features. We repeat this operation for all segments in each image I in the training dataset and cluster the CLS token features using k-means algorithm, thus discovering k clusters of visually similar segments. Therefore, each feature f j and its corresponding segment s I j , is assigned a cluster id l j in 1, k . To get a dense segmentation map M corresponding to the image I, we assign discovered cluster ids to each pixel belonging to a segment in the image. We additionally assign a predefined ignore label to pixels not covered by segments, which correspond to missing annotations. This allows us to construct a set M of dense maps of pseudoannotations, that we later use as a pseudo-ground-truth. Examples of resulting segmentation maps are shown in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distillation with cross-modal spatial constraints</head><p>After previous steps, we now have a set of pseudo-annotated segmentation maps M ? M, one for every image I in the training dataset. However, as explained above, the pseudo-annotations are only partial. This is because the segments that were used to construct them do not cover all pixels of an image. Furthermore, due to imperfections in the segment extraction process or the segment clustering step, these annotations are noisy. Therefore, directly training an image segmentation model with those pseudolabels might be sub-optimal. Instead, we propose a new teacher-student training ap- <ref type="figure">Fig. 5</ref>. Teacher prediction refinement using spatial constraints. First, the teacher T is trained using loss LT on images in I together with segmentation maps in M obtained from segment-wise unsupervised pseudo-labeling. The teacher predictions YT are refined, using LiDAR segments S P , into maps?T that are then used to train the student. Note that teacher's predictions span the whole image, producing outputs even in areas where LiDAR segments S P are not available.</p><p>proach with cross-modal distillation, which is able to learn more accurate unsupervised segmentation models under such partial and noisy pseudo-annotations.</p><p>Training the teacher. The first step of our teacher-student approach is to train the teacher T to make pixel-wise predictions only on the pixels for which pseudoannotations are available, i.e., only for the pixels that belong to a segment. We denote Y T = T (I) ? R H?W the segmentation predictions made by the teacher model on image I with a resolution of H ? W pixels. We train the teacher T using loss L T (I) on image I:</p><formula xml:id="formula_0">L T (I) = 1 h,w B (h,w) h,w CE Y T,(h,w) , M (h,w) B (h,w) ,<label>(1)</label></formula><p>where CE is the cross-entropy loss measuring the discrepancy between the predicted labels Y T and target pseudo-labels M for each pixel (h, w), and B is a H ? W binary mask for filtering out pixels without pseudo-annotations. The loss is normalized w.r.t. the number of pseudo-labeled pixels in the image. The trained teacher T is then able to predict pixel-wise segmentation for all pixels in an image, even if they do not belong to a segment. Moreover, since the teacher T is trained on a large set of pseudo-annotated segments, it learns to smooth out some of the noise in the raw pseudo-annotations.</p><p>Integrating spatial constraints. Considering this smoothing property, we can exploit the trained teacher T for generating new, complete (instead of partial) and smooth pseudo-segmentation maps for the training images. In addition, we propose to refine these teacher-generated pseudo-segmentation maps by using the projected LiDAR segments; indeed, these segments encode useful 3D spatial constraints as they often correspond to complete 3D objects, thus respecting the depth discontinuities and occlusion boundaries. In particular, for each image segment s I j in image I, we apply majority voting to pixel-wise teacher predictions Y T inside the segment. Then, we annotate each pixel belonging to the segment with its most frequently predicted label, giving us a new refined segmentation map? T ? R H?W . This procedure is illustrated in <ref type="figure">Figure 5</ref>.</p><p>Training the student. Having computed these complete, teacher-generated and spatially-refined pseudo-segmentation maps? T , we train a student network S using the following loss</p><formula xml:id="formula_1">L distill (I) = 1 HW h,w CE ? T,(h,w) , Y S,(h,w) ,<label>(2)</label></formula><p>where the cross-entropy is computed between? T and the segmentation map Y S ? R H?W predicted by the student at the same resolution as the teacher. The outputs of the trained student are our final unsupervised image segmentation predictions. Further details about our training can be found in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we give the implementation details, compare our results with the stateof-the-art unsupervised semantic segmentation methods on four different datasets, and ablate the key components of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setting</head><p>Methods and architectures. We investigate the benefits of our approach using two different semantic segmentation models to demonstrate the generality of our method. We implement Drive&amp;Segment with both a classical convolutional model and a transformer-based architecture. For the convolutional architecture, we follow <ref type="bibr" target="#b14">[15]</ref> and use a ResNet18 <ref type="bibr" target="#b25">[26]</ref> backbone followed by an FPN <ref type="bibr" target="#b32">[33]</ref> decoder. For the transformerbased architecture, we use the state-of-the-art Segmenter [44] model. We use the ViT-S/16 <ref type="bibr" target="#b18">[19]</ref> model as the Segmenter's encoder and use a single layer of the mask transformer [44] as a decoder. We compare our method to three recent unsupervised segmentation models: IIC <ref type="bibr" target="#b29">[30]</ref>, modified version of DeepCluster <ref type="bibr" target="#b8">[9]</ref> (DC), and PiCIE <ref type="bibr" target="#b14">[15]</ref>. Please refer to <ref type="bibr" target="#b14">[15]</ref> for implementation details of IIC and DC.</p><p>Training. In the following, we first discuss how we obtain segment labels by k-means clustering, then we talk about details of pre-training the model backbones, which is followed by the discussion of the datasets for actual training of the models. Finally, we give details of the training procedure. K-means. We use k = 30 in the k-means algorithm (the ablation of the value of k is in the appendix Section A.2). To extract segment-wise features used for k-means clustering, we use CLS token features of the DINO-trained <ref type="bibr" target="#b9">[10]</ref> ViT-S <ref type="bibr" target="#b18">[19]</ref> model. The ablation of the feature extractor is in Section 4.3. Obtained segment-wise labels serve as pseudo-annotations for training the ResNet18-FPN and Segmenter models, as discussed in Section 3.2.</p><p>Pre-training data and networks. To be directly comparable to <ref type="bibr" target="#b14">[15]</ref>, in our experiments with the ResNet18+FPN model, we initialize its backbone with a ResNet18 trained with supervision on the ImageNet-1k <ref type="bibr" target="#b17">[18]</ref>  its backbone with a ViT-S also trained on ImageNet-1k <ref type="bibr" target="#b17">[18]</ref> but with the self-supervised DINO approach <ref type="bibr" target="#b9">[10]</ref>. Please note that for both backbones we use the same pre-training data (ImageNet-1k) exactly as the prior methods PiCIE, DC, and IIC with which we compare. These pre-trained backbones are then used as an initialization for training the actual segmentation models on specific autonomous driving datasets as described next.</p><p>Training datasets. We train our models on about 7k images from the Waymo Open [45] dataset, which has both image and LiDAR data available. For the baseline methods (IIC <ref type="bibr" target="#b29">[30]</ref>, modified DC <ref type="bibr" target="#b8">[9]</ref> and PiCIE <ref type="bibr" target="#b14">[15]</ref>) we follow the setup from <ref type="bibr" target="#b14">[15]</ref>, i.e., we train the models on all available images of Cityscapes <ref type="bibr" target="#b15">[16]</ref>, meaning the 24.5k images from the train, test, and train extra splits. Note that those models then do not face the problem of domain gap when evaluated on the Cityscapes <ref type="bibr" target="#b15">[16]</ref> dataset. To be directly comparable with our approach, we also train a variant of modified DC <ref type="bibr" target="#b8">[9]</ref> and PiCIE <ref type="bibr" target="#b14">[15]</ref> on the same subset of the Waymo Open [45] dataset as used in our approach. Furthermore, to test the generalizability of our method to other training datasets, we provide results of modified DC, PiCIE, and our Drive&amp;Segment approach when trained on the nuScenes <ref type="bibr" target="#b7">[8]</ref> dataset in Sec. A.1 in the appendix.</p><p>Optimization. To train IIC <ref type="bibr" target="#b29">[30]</ref>, modified DC <ref type="bibr" target="#b8">[9]</ref>, and PiCIE <ref type="bibr" target="#b14">[15]</ref>, we use the setup provided in <ref type="bibr" target="#b14">[15]</ref>. For our Drive&amp;Segment, we train the teacher and student models with batches of size 32 and with a learning rate of 2e?4 with a polynomial schedule on a single V100 GPU. During training, we perform data augmentation consisting of random image resizing in the (0.5, 2.0) range, random cropping with the crop size of 512 ? 512 pixels, random horizontal flipping, and photometric distortions. Evaluation protocol. Mapping. To evaluate our models in the unsupervised setup, we run trained models on every image, thus getting segmentation predictions with values from 1 to k. Then, we compute the confusion matrix between the C ground-truth classes of the target dataset and the k ? C pseudo-classes. We map the C ground-truth classes to C out of the k pseudo-classes using Hungarian matching <ref type="bibr" target="#b31">[32]</ref> that minimizes the overall confusion. Finally, we compute the mean IoU and pixel accuracy based on this mapping. The pixel predictions for the k ? C unmapped pseudo-classes are considered as false negatives.</p><p>Test datasets. We evaluate the trained models on Cityscapes <ref type="bibr" target="#b15">[16]</ref>, Dark Zurich [42], Nighttime driving <ref type="bibr" target="#b16">[17]</ref> and ACDC [43] datasets in this fully unsupervised setup 5 without any finetuning. Cityscapes <ref type="bibr" target="#b15">[16]</ref> is a well-established dataset with 500 validation images that we use for evaluation. Dark Zurich [42] and Nighttime driving <ref type="bibr" target="#b16">[17]</ref> are two nighttime datasets, each with 50 validation images annotated for semantic segmentation that we use for evaluation. ACDC [43] is a recent dataset providing four different adverse weather conditions with 400 training and 100 validation samples per weather condition. We test our approach on the validation images annotated for semantic segmentation. The Cityscapes dataset defines 30 different semantic classes for the pixelwise semantic segmentation task. We follow prior work, and, unless stated otherwise, we evaluate our approach on the pre-defined subset of 19 classes <ref type="bibr" target="#b15">[16]</ref>. The same set of 19 classes is used for all other datasets.</p><p>Metrics. We evaluate results with two standard metrics for the semantic segmentation task, the mean Intersection over Union, mIoU, and the pixel accuracy, PA, as done in prior work <ref type="bibr" target="#b14">[15]</ref>. The mIoU is the mean intersection over union averaged over all classes, while PA defines the percentage of pixels in the image that are segmented correctly, averaged over all images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to state of the art</head><p>Here we evaluate our trained models in the unsupervised setup using the evaluation protocol described in Section 4.1. We compare our method using both the ResNet18+FPN and Segmenter [44] models to three recent unsupervised segmentation models: IIC <ref type="bibr" target="#b29">[30]</ref>, modified version of DeepCluster <ref type="bibr" target="#b8">[9]</ref> (DC), and PiCIE <ref type="bibr" target="#b14">[15]</ref>. In the appendix, we assess   We provide results on the Cityscapes, Dark Zurich, and Nighttime Driving datasets in <ref type="table" target="#tab_1">Table 1</ref>, and show qualitative results in <ref type="figure">Figure 6</ref>. As shown in the first two columns of <ref type="table" target="#tab_1">Table 1</ref>, our approach outperforms <ref type="bibr" target="#b14">[15]</ref> on the Cityscapes dataset by a large margin in both the 19-class and 27-class set-ups. Improvements are visible for both architectures, but in most cases, the best results are obtained with the distilled Segmenter architecture using the ViT-S/16 backbone. The last two columns in <ref type="table" target="#tab_1">Table 1</ref> show results for the two nighttime segmentation datasets: Dark Zurich [42] and Nighttime Driving <ref type="bibr" target="#b16">[17]</ref>. Our models again outperform <ref type="bibr" target="#b14">[15]</ref> in all setups. In addition, we observe a better performance of our models compared to <ref type="bibr" target="#b14">[15]</ref>    <ref type="bibr" target="#b19">[20]</ref>. (b) Benefits of our distillation approach showing an improvement of the student (S) over the the teacher (T) and benefits of our LiDAR cross-modal spatial constraints (LiD). (c) Ablation of different feature extractors for the k-means clustering.  Finally, <ref type="table" target="#tab_2">Table 2</ref> shows results on the ACDC dataset in four different adverse weather conditions. Results follow a similar trend as in <ref type="table" target="#tab_1">Table 1</ref> and show the superiority of our approach measured by mIoU compared to the current state-of-the-art unsupervised semantic segmentation method of <ref type="bibr" target="#b14">[15]</ref> on images out of the training distribution, such as images at night or in snow. Please see the appendix for the complete set of results, including results using nuScenes (Sec. A.1), pixel accuracy (Sec. A.4), per-class results (Sec. A.5) and analysis of the confusion matrices (Sec. B.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations</head><p>In this section, we ablate the main components of our approach. In particular, we study the benefits of our cross-modal segment extraction <ref type="table" target="#tab_4">(Table 3a)</ref>, of our distillation with cross-modal spatial constraints <ref type="table" target="#tab_4">(Table 3b)</ref>, effect of varying feature extractors for the k-means clustering <ref type="table" target="#tab_4">(Table 3c)</ref>, variance of the results over multiple runs, the influence of LiDAR resolution and the number of clusters used in k-means. Segment extraction approach. To evaluate the benefits of our cross-modal segment extraction module, we investigate using segment proposals generated with a purely image-based segmentation approach by Felzenszwalb and Huttenlocher (FH) <ref type="bibr" target="#b19">[20]</ref>. It groups pixels into segments based on similar color and texture properties. We use the same set of hyperparameters as <ref type="bibr" target="#b26">[27]</ref>. The results are shown in <ref type="table" target="#tab_4">Table 3a</ref> and demonstrate clear benefits of our LiDAR-based cross-modal segment extraction method despite the difficulties of using LiDAR data discussed in Section 3.1. We attribute the better results of our approach to the fact that LiDAR data segmentation operates with range information, which is much stronger at separating objects from the background and from each other compared to the purely image-based approach of FH <ref type="bibr" target="#b19">[20]</ref>. Indeed, FH relies only on color/texture and is therefore much more likely to join multiple objects into one segment or separate a single object into multiple segments. The benefits of our cross-modal segment extraction are observed for both studied architectures. Distillation with cross-modal spatial constraints. To evaluate the benefits of our teacher-student distillation method with cross-modal spatial constraints (Section 3.3), we compare the predictions of the teacher T (before distillation) and the student S (after distillation). <ref type="table" target="#tab_4">Table 3b</ref> presents results on the Cityscapes dataset using both convolutional-and transformer-based architectures. The results show consistent improvements using our distillation technique, particularly regarding the pixel accuracy metric. We believe that this could be attributed to improvements in predictions for classes such as vegetation and buildings. They often occupy large areas of the image and benefit most from the distillation as they are usually not well covered by the Li-DAR scans. Furthermore, the results show clear benefits of using this distillation step both with and without cross-modal spatial constraints (LiD) by Student S outperforming Teacher T in both scenarios. Please also note that our distillation technique works well even in combination with another training approach (PiCIE <ref type="bibr" target="#b14">[15]</ref>).</p><p>Sensitivity to the initialization. To study the influence of initialization, we take the features extracted by DINO <ref type="bibr" target="#b9">[10]</ref> and run the k-means clustering (Section 3.2) four times. For each of the k-means clustering outcomes, we run the segmentation model training four times with different initializations. The variance over all k-means and training runs is only 0.5 for mIoU and 1.5 for pixel accuracy (i.e., 20.4 ? 0.5/65.4 ? 1.5). These results clearly show that our method is not very sensitive to k-means initialization or to the network initialization.</p><p>Feature extractors. An ablation of seven different feature extractors (convolutionaland Transformer-based) for the task of segment-wise unsupervised labelling is shown in <ref type="table" target="#tab_4">Table 3c</ref>. The results on the Cityscapes <ref type="bibr" target="#b15">[16]</ref> dataset using our Segmenter model demonstrate that our approach works well with several different feature extractors.</p><p>LiDAR resolution and number of clusters. An ablation of the influence of LiDAR resolution is shown in Sec. A.3 in the appendix and demonstrates that our method is robust to LiDAR's sparsity. Furthermore, we study the choice of the number of clusters for the k-means clustering in Sec. A.2 in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Limitations and failure modes</head><p>Our approach has the following three main limitations. First, LiDAR point clouds do not provide information about very distant or even infinitely distant objects, e.g., the sky, which our approach cannot learn to segment. Second, LiDAR point clouds paired with geometric segmentation can not correctly distinguish road from sidewalk or grass, when all surfaces are similarly flat. Both the above limitations might be possibly tackled by pairing our LiDAR-based segment proposals with an unsupervised image-based method such as <ref type="bibr" target="#b19">[20]</ref>. Also, the LiDAR points must not be too sparse (e.g., only 4 beams), since otherwise the LiDAR-based segments would be of poor quality. However, this is not an overly restricting requirement as it is common to use LiDAR sensors with sufficient beam resolution, e.g., as in the recent Waymo Open <ref type="bibr">[45]</ref> or ONCE <ref type="bibr" target="#b34">[35]</ref> datasets. Finally, we encounter semantically similar objects appearing in multiple pseudo-classes, a natural side effect of clustering. This issue may be mitigated by using different feature clustering methods, e.g., from the family of graph clustering methods, that allow the measurement of similarities on manifolds in the feature space instead of the currently used Euclidean metric in the k-means clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have developed Drive&amp;Segment-a fully unsupervised approach for semantic image segmentation in urban scenes. The approach relies on novel modules for (i) crossmodal segment extraction and (ii) distillation with cross-modal constraints that leverage LiDAR point clouds aligned with images. We evaluate our approach on four different autonomous driving datasets in challenging weather and illumination conditions and demonstrate major gains over prior work. This work opens up the possibility of largescale autonomous learning of embodied perception models without explicit human supervision. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer: Simple and efficient design for semantic segmentation with transformers. In: NeurIPS (2021) 3 53. Xie, Z., Lin, Y., Zhang, Z., Cao, Y., Lin, S., Hu, H.: Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In: CVPR (2021) 12 54. Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell, T.: Bdd100k:</p><p>A diverse driving dataset for heterogeneous multitask learning. In: CVPR (2020) 3 55. Yuan, Y., Chen, X., Wang, J.: Object-contextual representations for semantic segmentation.</p><p>In: ECCV (2020) 3 56. Zamir, A.R., Sax, A., Shen, W., Guibas, L.J., Malik, J., Savarese, S.: Taskonomy: Disentangling task transfer learning. In: CVPR (2018) 3 57. Zhang, X., Maire, M.: Self-supervised visual representation learning from hierarchical grouping. In: NeurIPS (2020) 4 58. Zhao, H., Gan, C., Rouditchenko, A., Vondrick, C., McDermott, J., Torralba, A.: The sound of pixels. In: ECCV (2018) 4 59. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In: CVPR (2017) 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional quantitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Results with another training dataset</head><p>We report in Tables 4 and 5 the performance of Drive&amp;Segment when trained using a subset of ?8k images from the nuScenes dataset <ref type="bibr" target="#b7">[8]</ref>. As shown in <ref type="table" target="#tab_7">Table 4</ref>, the mIoU on Cityscapes is 19.8. Although there is a small drop from the 21.8 achieved with Drive&amp;Segment trained on Waymo Open, the results are still significantly better than those of the competing methods. This drop might be caused by differences of statistics between the two datasets, e.g., nuScenes has fewer examples of smaller-object classes, such as pedestrians.   <ref type="bibr" target="#b8">[9]</ref> yes nuSC 6.7 (+2.4) 11.7 (+2.8) 10.4 (+0.9) 9.6 (+2.1) 9.6 (+2.1) Drive&amp;Segment (Ours, S) yes nuSC 7.9 (+3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Ablation of the number of clusters in unsupervised labeling</head><p>Here we investigate the sensitivity of our method to the number k of clusters used for unsupervised labeling. <ref type="figure">Figure 7</ref> shows the mIoU results on Cityscapes for k ? {20, 25, 30, 35, 40}. In all cases we use a ViT-S/16 feature extractor trained with DINO. The results show that for k ? {20, 25, 30, 35} the mIoU performance is fairly stable. As expected, when the number of clusters becomes much higher than the number of Cityscapes classes (e.g., k = 40), the performance drops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Influence of the LiDAR's density</head><p>We investigate here the performance of Drive&amp;Segment when provided with sparser LiDAR data. We performed experiments on the Waymo Open dataset and downsampled the LiDAR data from 64 to 32 beam channels by dropping every other channel. We retrained the teacher model three times and report the average performance (following the setup of the main paper). We obtained 20.3 mIoU, which is only slightly lower than the 20.4 obtained with the full LiDAR resolution, demonstrating the robustness of our method to this considerable decrease of LiDAR resolution. However, as already discussed in the main paper, our method will likely not work well with extremely sparse LiDAR data (e.g., low-cost LiDARs with 4-beam channels). Such a sparsity would lead to poor LiDAR-based segments and geometric priors that would rather confuse the model, instead of teaching it to recognize objects.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Pixel accuracy results</head><p>In <ref type="table" target="#tab_9">Tables 6 and 7</ref>, we report results measured with the pixel accuracy (PA) metric corresponding to all experiments of our main paper. We observe that results follow a similar trend to those measured with mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Category-wise results</head><p>In the main paper, we have presented results averaged over all classes. We report in <ref type="table" target="#tab_12">Table 8</ref>   To evaluate the quality of the learned representations, we compare the representations produced by a ResNet18 backbone trained (a) on Imagenet in a fully-supervised fashion for the classification task, (b) using PiCIE <ref type="bibr" target="#b14">[15]</ref> trained on Waymo Open, and (c) using our Drive&amp;Segment trained on Waymo Open. For this comparison we perform k-NN based pixel-wise classification on the Cityscapes validation set using a low-shot scenario where only 100 Cityscapes training images are available (we consider three random splits of 100 images from <ref type="bibr" target="#b20">[21]</ref> and report the average results). Our goal is to analyze the ability of the representations to learn with few training examples. In <ref type="table" target="#tab_13">Table 9</ref>, we report results in terms of pixel accuracy for k ? {1, 5, 20} and observe that Drive&amp;Segment outperforms both the supervised baseline and PiCIE <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Representation analysis via PCA</head><p>In <ref type="figure">Figure 8</ref>, we visualize the three main PCA components of the decoder features as RGB. We observe that our features learned with Segmenter separate better object classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Confusion matrices for class mapping</head><p>Here we analyze the confusion matrices, presented in <ref type="figure">Figure 9</ref>, which provide the mapping between ground truth and pseudo classes. For each confusion matrix, we reorder the columns based on the matching obtained from the Hungarian algorithm, and L 1normalize the values per row, i.e., per ground-truth class (for simplicity, we do not illustrate the un-matched pseudo-classes in the figures). Thus, a value of 1 would signify that all pixels in a ground-truth class belong to a single pseudo-class. Moreover, due to the reordering, the largest values should ideally be on the diagonal of the confusion matrix.</p><p>For each row, the highest and the diagonal entry are reported. We note that, for Drive&amp;Segment ( <ref type="figure">Fig. 9(a)</ref>), 90% of the road pixels are covered by the first pseudoclass. However, this pseudo-class also covers large portions of sidewalk and vegetation as all these labels belong to ground pixels and hence are segmented together by our LiDAR-based segment proposal mechanism. Similarly, pseudo-class 12 overlaps person, rider, motorcycle and bicycle, i.e., with human-related ground-truth classes. Regarding PiCIE ( <ref type="figure">Fig. 9(b)</ref>), only a few pseudo-classes have a significant overlap with ground-truth classes. In particular, the pseudo-class 3 overlaps with the majority of the ground-truth classes. Objects that belong to the same semantic category (e.g., cars) might end-up clustered into different pseudo-classes due to differences in appearance (e.g., a separate pseudo-class that corresponds to the rear of the cars). (c) The road?sky misplacement/confusion is caused by the absence of sky-occupied labeled pixels at training as they are not covered by the LiDAR data. Therefore, the model assigns the most common label to the sky, which is the pseudo-label that corresponds to the road. This leads to either predicting the road as sky (third column), or predicting sky as road (fourth column), depending on the outcome of the Hungarian matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Failure cases</head><p>The main limitations of our Drive&amp;Segment approach are discussed in Section 4.4 of the main paper. Here, we show some qualitative examples of these failure modes and discuss more thoroughly their roots.</p><p>The first limitation of Drive&amp;Segment is the complete absence of pseudo-labeled training data for the sky class. This is because the LiDAR data do not capture the sky. As a consequence, our models learn to classify the sky pixels as road (see the "sky" row of the confusion matrix in <ref type="figure">Figure 9a</ref>), which is the most dominant (pseudo-)label in the data. We provide examples of this behavior in <ref type="figure">Figure 10</ref>(c).</p><p>The second most common failure mode is inherited from the object proposal method that relies only on geometry-derived features. Specifically, the segment proposal method might over-segment an object, potentially causing different object parts being assigned to different pseudo-labels. The class majority voting in our refinement stage does not always rectify this issue. As a consequence, our models might learn to make predictions that mix multiple pseudo-labels in one object. For example, see <ref type="figure">Figure 10(a)</ref> where the car is mixed with the truck class.</p><p>Finally, our LiDAR-based proposal method groups all points from the ground plane into a single segment, without being able to distinguish the various ground-plane classes (e.g., road, sidewalk and terrain) that are defined in the image domain. <ref type="figure">Figure 10</ref> provides examples of this failure mode. This phenomenon is also well visible in <ref type="figure">Figure 9a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Qualitative comparison to previous work</head><p>We show a qualitative comparison with IIC <ref type="bibr" target="#b29">[30]</ref> and PiCIE <ref type="bibr" target="#b14">[15]</ref> in <ref type="figure">Figure 11</ref>. For a fair comparison, we use the same samples and the visualization protocol as in <ref type="bibr" target="#b14">[15]</ref>. Note that these samples come from the PiCIE and IIC training set, namely from the train set of the Cityscapes <ref type="bibr" target="#b15">[16]</ref> dataset, while for our method these are only test samples. In <ref type="figure">Figure 11</ref>, note how our Drive&amp;Segment is able to segment the person class, while neither IIC nor PiCIE are capable to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Qualitative Results</head><p>In <ref type="figure">Figures 12 and 13</ref>, we report Drive&amp;Segment predictions on Cityscapes validation images. In spite of the domain gap between the training dataset (Waymo Open Dataset [45] with images from US cities) and the Cityscapes test set, our approach produces convincing results. Furthermore, in <ref type="figure">Figure 14</ref>, we report qualitative results of our method pretrained on daytime-only images and evaluated on out-of-trainingdistribution splits of ACDC [43], e.g., night, snow or fog. We discuss the main failure modes in Section B.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Evaluating Drive&amp;Segment with supervised fine-tuning</head><p>The goal of our work is to train image segmentation models without any human annotation. Here, we evaluate with some preliminary experiments the applicability of the <ref type="figure">Fig. 11</ref>. Qualitative comparison of PiCIE <ref type="bibr" target="#b14">[15]</ref>, IIC <ref type="bibr" target="#b29">[30]</ref> and our Drive&amp;Segment approach on PiCIE training samples. For a fair comparison, we use the same visualization procedure as in <ref type="bibr" target="#b14">[15]</ref>. Results are shown on center-cropped Cityscapes training images. Note that our method is able to capture objects' contours much better and to segment categories such as person that are not visible in IIC or PiCIE results. <ref type="table" target="#tab_1">Table 10</ref>. Supervised fine-tuning on the Cityscapes <ref type="bibr" target="#b15">[16]</ref> semantic segmentation task. Results report mean Intersetion over Union (mIoU). We fine-tune the pre-trained ResNet18+FPN networks on either the entire Cityscapes training split ('Full Cityscapes') or only 100 images from the training split ('Low-shot') <ref type="bibr" target="#b20">[21]</ref> and test on the Cityscapes validation split. 'Linear' fine-tunes only the last linear layer, 'Decoder+Linear' fine-tunes the FPN decoder and the last linear layer, and 'End-to-End' fine-tunes the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Cityscapes</head><p>Low proposed Drive&amp;Segment method on a different but related task, that of self-supervised pre-training of semantic segmentation networks (i.e., self-supervised feature learning). Specifically, we take the ResNet18+FPN model trained with Drive&amp;Segment, replace its last linear prediction layer with a new layer that has as many outputs as classes in Cityscapes <ref type="bibr" target="#b18">(19)</ref>, and fine-tune the resulting network on the Cityscapes <ref type="bibr" target="#b15">[16]</ref> dataset using available human annotations. We compare against (a) using PiCIE <ref type="bibr" target="#b14">[15]</ref> for selfsupervised pre-training and (b) supervised pre-training on ImageNet <ref type="bibr" target="#b17">[18]</ref>. We evaluate the different pre-training approaches with three fine-tuning setups. The first setup is to freeze both the ResNet18 backbone and the FPN decoder (i.e., keep their pre-trained weights fixed) and fine-tune only the last linear prediction layer. The second setup is to freeze only the ResNet18 backbone and fine-tune both the FPN decoder and the last linear layer. In both cases, we train on the entire training split (2975 images) of Cityscapes. The goal of these first two setups is to evaluate the quality of the pretrained ResNet18+FPN (1st setup) or ResNet18 (2nd setup) features as they are. The third setup targets the low-shot scenario: the segmentation network is fine-tuned endto-end using only 100 Cityscapes training images (we consider three random splits of 100 images from <ref type="bibr" target="#b20">[21]</ref>). The purpose of this setup is to evaluate the strength of the pretrained network in a regime where only a few annotations are available for fine-tuning.</p><p>In the first two setups we train for 40k iterations, and we train for 4k in the low-shot setup. In all setups, we use SGD with momentum set to 0.9, weight decay to 0.0005 and mini-batches of size 8. During training we use random image scaling (by a ratio in [0.5, 2.0]), random cropping (with size 769) and horizontal flipping. At test time, we use the original image size and horizontal flip augmentations. The learning rates were tuned for each fine-tuning setup and each evaluated method separately.</p><p>We report results in <ref type="table" target="#tab_1">Table 10</ref>. Although our method was not designed or optimized for self-supervised feature pre-training, it still provides promising results that surpass both PiCIE and ImageNet pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth Drive&amp;Segment (Ours) <ref type="figure">Fig. 12</ref>.</p><p>Qualitative results for unsupervised semantic segmentation using our Drive&amp;Segment approach on the validation split of the Cityscapes dataset. The matching between our pseudo-classes and the set of ground-truth classes is obtained using the Hungarian algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth Drive&amp;Segment (Ours) <ref type="figure">Fig. 13</ref>. Qualitative results for unsupervised semantic segmentation using our Drive&amp;Segment approach on the validation split of the Cityscapes dataset. The matching between our pseudo-classes and the set of ground-truth classes is obtained using the Hungarian algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth Drive&amp;Segment (Ours) <ref type="figure">Fig. 14</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2. Overview of Drive&amp;Segment. We first perform cross-modal segment extraction on training dataset by exploiting raw LiDAR point clouds P and raw images I. This yields segments S I projected onto the image space ( ? 3.1). By clustering their self-supervised features, we obtain an unsupervised labeling of these segments ( ? 3.2) and, as a consequence, of their pixels. This provides pixel-wise pseudo ground truth for the next learning step. Finally, given the pseudo-labels and the segments, we perform distillation with cross-modal constraints ( ? 3.3) that conjugates information of the LiDAR and the images to learn a final segmentation model using a teacherstudent architecture. The learnt segmentation model S -highlighted in the figure-is used for inference on unseen datasets, yielding compelling results ( ? 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>yes WO 5 . 9 (</head><label>59</label><figDesc>+1.5) 11.7 (-0.5) 9.6 (-2.9) 9.8 (-2.3) 9.2 (-1.0) PiCIE ? yes WO 4.7 (+0.3) 14.4 (+2.1) 13.7 (+1.2) 14.3 (+2.2) 11.7 (+1.5) Drive&amp;Segment (Ours, S) yes WO 11.2 (+6.8) 14.5 (+2.3) 14.9 (+2.5) 14.6 (+2.6) 13.8 (+3.5) Segmenter, ViT-S/16 Drive&amp;Segment (Ours, S) no WO 13.8 (+9.4) 18.1 (+5.9) 16.4 (+3.9) 18.7 (+6.6) 16.7 (+6.5) the utility of the features learned by our model in other settings, such as linear probing for semantic segmentation (Sec. D), and k-NN evaluation (Sec. B.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>38. Ouali, Y., Hudelot, C., Tami, M.: Autoregressive unsupervised image segmentation. In: ECCV (2020) 4 39. Owens, A., Efros, A.A.: Audio-visual scene analysis with self-supervised multisensory features. In: ECCV (2018) 4 40. Recasens, A., Luc, P., Alayrac, J.B., Wang, L., Strub, F., Tallec, C., Malinowski, M., P?tr?ucean, V., Altch?, F., Valko, M., Grill, J.B., van den Oord, A., Zisserman, A.: Broaden your views for self-supervised video learning. In: ICCV (2021) 4 41. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: MICCAI (2015) 3 42. Sakaridis, C., Dai, D., Van Gool, L.: Map-guided curriculum domain adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation. IEEE TPAMI (2020) 3, 10, 11, 18, 20 43. Sakaridis, C., Dai, D., Van Gool, L.: ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding. In: ICCV (2021) 3, 9, 10, 11, 19, 20, 24, 29 44. Strudel, R., Garcia, R., Laptev, I., Schmid, C.: Segmenter: Transformer for semantic segmentation. In: ICCV (2021) 3, 8, 10 45. Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., et al.: Scalability in perception for autonomous driving: Waymo open dataset. In: CVPR (2020) 3, 9, 10, 13, 21, 23, 24 46. Tian, H., Chen, Y., Dai, J., Zhang, Z., Zhu, X.: Unsupervised object detection with lidar clues. In: CVPR (2021) 4 47. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J?gou, H.: Training dataefficient image transformers &amp; distillation through attention. In: ICML (2021) 12 48. Van Gansbeke, W., Vandenhende, S., Georgoulis, S., Van Gool, L.: Unsupervised semantic segmentation by contrasting object mask proposals. In: ICCV (2021) 1, 4, 12 49. Varma, G., Subramanian, A., Namboodiri, A., Chandraker, M., Jawahar, C.: Idd: A dataset for exploring problems of autonomous navigation in unconstrained environments. In: WACV (2019) 3 50. Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan, M., Wang, X., et al.: Deep high-resolution representation learning for visual recognition. IEEE TPAMI (2020) 3 51. Weston, R., Cen, S., Newman, P., Posner, I.: Probably unknown: Deep inverse sensor modelling radar. In: ICRA (2019) 4 52.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6 )Fig. 7 .</head><label>67</label><figDesc>14.3 (+5.4) 14.4 (+4.9) 13.4 (+5.9) 12.5 (+5.0) Segmenter, ViT-S/16 Drive&amp;Segment (Ours, S) no nuSC 10.6 (+6.3) 13.3 (+4.4) 16.0 (+6.5) 14.8 (+7.3) 13.9 (+6.4) Ablation of the number of clusters. Performance in mIoU, when using the Segmenter model and the ResNet18+FPN model on the Cityscapes dataset, as a function of the number of clusters in the unsupervised labeling step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Row-normalized confusion matrices. GT Ours Input (a) mixed labels (b) good shape, (c) sky ? road confusion wrong class Failure cases. (a) Due to the noise in the training data (discussed in Section B.4; images and LiDAR point clouds come from the Waymo Open [45] dataset), Drive&amp;Segment sometimes predicts multiple pseudo-labels inside the same object (here different shades of blue inside the car on the left). (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>classification task, exactly as all the compared prior methods (PiCIE, DC, and IIC). However, we aim at having a completely unsupervised setup. Therefore, in our experiments with Segmenter, we initializeFig. 6.Qualitative results for unsupervised semantic segmentation using our Drive&amp;Segment approach. To obtain the best matching between our pseudo-classes and the set of groundtruth classes, we use the Hungarian algorithm. The first two rows show samples from the Cityscapes<ref type="bibr" target="#b15">[16]</ref> dataset, and the other three rows show samples from the night and fog splits of the ACDC [43] dataset. Please seeFigures 12, 13 and 14  in the appendix for more qualitative results.</figDesc><table><row><cell>Input</cell><cell>Ground Truth</cell><cell>Drive&amp;Segment (Ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison to the state of the art for unsupervised semantic segmentation on Cityscapes<ref type="bibr" target="#b15">[16]</ref> (CS), DarkZurich [42] (DZ) and Nighttime driving<ref type="bibr" target="#b16">[17]</ref> (ND) datasets measured by the mean IoU (mIoU). The colored differences are reported with respect to the state-of-the-art approach of<ref type="bibr" target="#b14">[15]</ref> denoted by . The sup. init. abbreviation stands for supervised initialization of the encoder, and the column train. data indicates whether Cityscapes (CS) or Waymo Open (WO) dataset was used for training. Please see the appendix for pixel accuracy and for results using the nuScenes dataset for training.</figDesc><table><row><cell></cell><cell cols="3">sup. train. CS19 [16] CS27 [16]</cell><cell>DZ [42]</cell><cell>ND [17]</cell></row><row><cell>architecture, method</cell><cell>init. data</cell><cell>mIoU</cell><cell>mIoU</cell><cell>mIoU</cell><cell>mIoU</cell></row><row><cell>RN18+FPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IIC  ? [30]</cell><cell>yes CS</cell><cell>-</cell><cell cols="2">6.4 (-4.8) -</cell><cell>-</cell></row><row><cell>Modified DC  ? [9]</cell><cell cols="5">yes CS 11.3 (-4.5) 7.9 (-3.3) 7.5 (+2.9) 8.2 (-1.3)</cell></row><row><cell>PiCIE  ? [15]</cell><cell cols="2">yes CS 15.8</cell><cell>11.2</cell><cell>4.6</cell><cell>9.5</cell></row><row><cell>Modified DC ?</cell><cell cols="5">yes WO 11.4 (-4.4) 7.0 (-4.1) 5.9 (+1.3) 8.2 (-1.3)</cell></row><row><cell>PiCIE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? yes WO 13.7 (-2.1) 9.7 (-1.5) 4.9 (+0.3) 9.3 (-0.2) Drive&amp;Segment (Ours, S) yes WO 19.5 (+3.7) 16.2 (+5.1) 10.9 (+6.3) 14.4 (+4.9) Segmenter, ViT-S/16 Drive&amp;Segment (Ours, S) no WO 21.8 (+6.0) 15.3 (+4.1) 14.2 (+9.6) 18.9 (+9.3) ? Results reported in [15].? Models provided by the PiCIE [15] authors.* Trained by PiCIE code base.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison to the state-of-the-art for unsupervised semantic segmentation on the ACDC [43] dataset. Please refer toTable 1for the used symbols. Please see the appendix for pixel accuracy and for results using the nuScenes dataset for training.</figDesc><table><row><cell></cell><cell>sup. train.</cell><cell>night</cell><cell>fog</cell><cell>rain</cell><cell>snow</cell><cell>average</cell></row><row><cell>architecture, method</cell><cell>init. data</cell><cell>mIoU</cell><cell>mIoU</cell><cell>mIoU</cell><cell>mIoU</cell><cell>mIoU</cell></row><row><cell>RN18+FPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">modified DC  6)</cell></row><row><cell>PiCIE  ? [15]</cell><cell cols="2">yes CS 4.4</cell><cell>12.2</cell><cell>12.5</cell><cell>12.1</cell><cell>10.3</cell></row><row><cell>modified DC ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? [9] yes CS 8.1 (+3.7) 8.3 (-4.0) 6.9 (-5.6) 7.4 (-4.7) 7.7 (-2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>when evaluating on the nighttime scenes. For example, on the Dark Zurich [42] dataset, the mIoU of PiCIE [15] decreases by 71% compared to the results on Cityscapes (15.8 ? 4.6), while the mIoU of our Segmenter-based model decreases only by 35% (21.8 ? 14.2). This suggests that our models generalize significantly better to out-of-distribution scenes. These findings hold for PiCIE models trained on both Cityscapes and on Waymo Open Dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablations on the Cityscapes dataset. (a) Benefits of our segment extraction method over segment proposals from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table of Contents</head><label>of</label><figDesc>A Additional quantitative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.1 Results with another training dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2 Ablation of the number of clusters in unsupervised labeling . . . . . . . . . . 19 A.3 Influence of the LiDAR's density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.4 Pixel accuracy results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A.5 Category-wise results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B Analyzing learned representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.1 k-NN evaluation of learned representations . . . . . . . . . . . . . . . . . . . . . . . . 21 B.2 Representation analysis via PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3 Confusion matrices for class mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.4 Failure cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C Additional qualitative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.1 Qualitative comparison to previous work . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.2 Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 D Evaluating Drive&amp;Segment with supervised fine-tuning . . . . . . . . . . . . . . . . . . 24</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Comparative results of unsupervised semantic segmentation methods when trained on nuScenes. Comparison to the state of the art on Cityscapes<ref type="bibr" target="#b15">[16]</ref> (CS), DarkZurich [42] (DZ) and Nighttime Driving<ref type="bibr" target="#b16">[17]</ref> (ND) datasets measured by the mean IoU (mIoU). The colored differences are reported with respect to the state-of-the-art approach of<ref type="bibr" target="#b14">[15]</ref> denoted by ; 'sup. init.' stands for supervised initialization of the encoder and the column 'train. data' indicates the dataset used for training, namely nuScenes<ref type="bibr" target="#b7">[8]</ref> (nuSC).</figDesc><table><row><cell></cell><cell cols="3">sup. train. CS19 [16] CS27 [16]</cell><cell>DZ [42]</cell><cell>ND [17]</cell></row><row><cell>architecture, method</cell><cell>init. data</cell><cell>mIoU</cell><cell>mIoU</cell><cell>mIoU</cell><cell>mIoU</cell></row><row><cell>RN18+FPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PiCIE ? [15]</cell><cell cols="2">yes nuSC 15.8</cell><cell>9.7</cell><cell>4.6</cell><cell>9.9</cell></row><row><cell>Modified DC ? [9]</cell><cell cols="5">yes nuSC 11.6 (-4.2) 7.1 (-2.6) 7.7 (+3.1) 8.3 (-1.6)</cell></row><row><cell cols="6">Drive&amp;Segment (Ours, S) yes nuSC 16.2 (+0.4) 11.4 (+1.7) 7.5 (+2.9) 10.2 (+0.3)</cell></row><row><cell>Segmenter, ViT-S/16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Drive&amp;Segment (Ours, S) no nuSC 19.8 (+4.0) 13.9 (+4.2) 9.7 (+5.1) 14.1 (+4.2)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Our training using PiCIE code base.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Comparative results on ACDC when methods trained on nuScenes. Comparison to the state of the art for unsupervised semantic segmentation on the ACDC [43] dataset. Please refer toTable 4for the symbols.</figDesc><table><row><cell></cell><cell>sup. train.</cell><cell>night</cell><cell>fog</cell><cell>rain</cell><cell>snow</cell><cell>average</cell></row><row><cell>architecture, method</cell><cell>init. data</cell><cell>mIoU</cell><cell>mIoU</cell><cell>mIoU</cell><cell>mIoU</cell><cell>mIoU</cell></row><row><cell>RN18+FPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PiCIE  *  [15]</cell><cell cols="2">yes nuSC 4.3</cell><cell>8.9</cell><cell>9.5</cell><cell>7.5</cell><cell>7.5</cell></row><row><cell>Modified DC  *</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Comparative results using PA metric. Comparison to the state of the art for unsupervised semantic segmentation on Cityscapes<ref type="bibr" target="#b15">[16]</ref> (CS), DarkZurich [42] (DZ) and Nighttime driving<ref type="bibr" target="#b16">[17]</ref> (ND) datasets measured by the pixel accuracy (PA). Same organization asTable 4. For easy reference, rows are colored according to the used training dataset.</figDesc><table><row><cell></cell><cell cols="2">sup. train. CS19 [16]</cell><cell>CS27 [16]</cell><cell>DZ [42]</cell><cell>ND [17]</cell></row><row><cell>architecture, method</cell><cell>init. data</cell><cell>PA</cell><cell>PA</cell><cell>PA</cell><cell>PA</cell></row><row><cell>RN18+FPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PiCIE  ? [15]</cell><cell>yes CS 63.1</cell><cell></cell><cell>62.7</cell><cell>30.7</cell><cell>41.4</cell></row><row><cell>IIC  ? [30]</cell><cell>yes CS</cell><cell>-</cell><cell>47.9 (-14.8)</cell><cell>-</cell><cell>-</cell></row><row><cell>Modified DC  ? [9]</cell><cell cols="5">yes CS 52.4 (-10.7) 52.1 (-10.7) 42.4 (+11.7) 46.2 (+4.8)</cell></row><row><cell>Modified DC ?</cell><cell cols="5">yes nuSc 45.9 (-17.2) 45.7 (-17.0) 41.4 (+10.7) 41.9 (+0.5)</cell></row><row><cell>PiCIE ?</cell><cell cols="5">yes nuSc 61.6 (-1.5) 61.3 (-1.4) 29.6 (-1.1) 45.1 (+3.7)</cell></row><row><cell cols="6">Drive&amp;Segment (Ours, S) yes nuSc 61.4 (-1.7) 61.1 (-1.6) 37.4 (+6.7) 33.6 (-7.8)</cell></row><row><cell>Modified DC ?</cell><cell cols="5">yes WO 55.6 (-7.5) 43.2 (-19.5) 35.8 (+5.1) 33.4 (-8.0)</cell></row><row><cell>PiCIE ?</cell><cell cols="5">yes WO 48.6 (-14.5) 48.3 (-14.4) 31.9 (+1.1) 40.0 (-1.4)</cell></row><row><cell cols="6">Drive&amp;Segment (Ours, S) yes WO 66.4 (+3.3) 67.1 (+4.3) 47.7 (+17.0) 49.0 (+7.6)</cell></row><row><cell>Segmenter, ViT-S/16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Drive&amp;Segment (Ours, S) no nuSc 73.2 (+10.1) 72.8 (+10.1) 50.2 (+19.5) 65.5 (+24.1)</cell></row><row><cell cols="6">Drive&amp;Segment (Ours, S) no WO 69.5 (+6.4) 69.1 (+6.4) 55.9 (+25.1) 60.2 (+18.8)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? Results reported in [15].? Models provided by the PiCIE [15] authors.* Trained by PiCIE code base.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Comparative results on ACDC using PA metric. Comparison to the state-of-the-art approach<ref type="bibr" target="#b14">[15]</ref> for unsupervised semantic segmentation on the ACDC [43] dataset. Same organization asTable 5. For easy reference, rows are colored according to the used training dataset</figDesc><table><row><cell></cell><cell>sup. train.</cell><cell>night</cell><cell>fog</cell><cell>rain</cell><cell>snow</cell><cell>average</cell></row><row><cell>method</cell><cell>init. data</cell><cell>PA</cell><cell>PA</cell><cell>PA</cell><cell>PA</cell><cell>PA</cell></row><row><cell>RN18+FPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PiCIE [15]</cell><cell cols="2">yes CS 25.8</cell><cell>50.0</cell><cell>53.6</cell><cell>50.4</cell><cell>45.0</cell></row><row><cell>MDC [9]</cell><cell cols="6">yes CS 43.0 (+17.3) 43.6 (-6.4) 35.0 (-18.6) 38.8 (-11.5) 40.1 (-4.8)</cell></row><row><cell>Modified DC  *</cell><cell cols="6">yes nuSC 36.5 (+10.7) 44.8 (-5.2) 41.4 (-12.2) 38.5 (-11.9) 40.3 (-4.7)</cell></row><row><cell>PiCIE  *</cell><cell cols="6">yes nuSC 26.9 (+1.1) 33.1 (-16.9) 33.4 (-20.2) 29.1 (-21.3) 30.6 (-14.4)</cell></row><row><cell cols="7">Drive&amp;Segment (Ours, S) yes nuSC 34.5 (+8.7) 59.4 (+9.4) 58.2 (+4.6) 53.9 (+3.5) 51.5 (+6.5)</cell></row><row><cell>MDC ?</cell><cell cols="6">yes WO 32.9 (+7.2) 47.0 (-3.0) 40.3 (-13.3) 44.2 (-6.2) 41.1 (-3.8)</cell></row><row><cell>PiCIE ?</cell><cell cols="6">yes WO 27.2 (+1.4) 56.9 (+6.8) 53.8 (+0.2) 53.0 (+2.6) 47.7 (+2.8)</cell></row><row><cell cols="7">Drive&amp;Segment (Ours, S) yes WO 43.2 (+17.5) 56.5 (+6.5) 54.1 (+0.5) 55.5 (+5.1) 52.3 (+7.4)</cell></row><row><cell>Segmenter, ViT-S/16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Drive&amp;Segment (Ours, S) no nuSC 50.2 (+24.4) 60.2 (+10.2) 62.5 (+8.9) 56.5 (+6.1) 57.5 (+12.5)</cell></row><row><cell cols="7">Drive&amp;Segment (Ours, S) no WO 52.6 (+26.9) 54.2 (+4.2) 50.1 (-3.5) 56.8 (+6.4) 53.4 (+8.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>the per-class IoU results of our Drive&amp;Segment approach on the Cityscapes dataset. We observe that Drive&amp;Segment outperforms the baseline PiCIE on 15 out of 19 classes. IoU gains (w.r.t. PiCIE trained on Waymo Open dataset) are significant for</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .</head><label>8</label><figDesc>Per-class comparative performance on Cityscapes. Per-class IoU is evaluated using the Hungarian algorithm on the 19 validation classes. We can see significant benefits of Drive&amp;Segment ('D&amp;S') over PiCIE in 14 (including all road users and objects) out of 19 classes. Drive&amp;Segment works much worse for sidewalk and sky as we discuss in Sections A.5 and B.4. '(CS)' stands for a model trained on the Cityscapes<ref type="bibr" target="#b15">[16]</ref> dataset, while '(WO)' for models trained on the Waymo Open [45] dataset. The best results per class are highlighted in bold and color. classes that can cover larger image portions, e.g., road (+15.6/+14.2), vegetation (+36.5/+38.5), car (+8.8/-3.8). The results of ResNet18+FPN are slightly worse on the car class because car instances are split into several pseudo-classes. Gains over road and car were expected since LiDAR data provide very good segments for these classes; it is more surprising to see gains on vegetation, a class that is not easily captured by LiDAR.</figDesc><table><row><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell>RN18+FPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="20">PiCIE [15] (CS) 58.2 12.5 63.8 1.0 2.4 1.3 0.1 0.4 55.5 1.7 44.7 1.9 0.5 48.2 1.3 3.9 1.0 0.5 1.6 15.8</cell></row><row><cell cols="20">PiCIE [15] (WO) 58.5 13.8 35.8 6.7 0.7 1.2 0.4 1.2 28.3 1.2 55.8 3.1 0.6 48.5 0.5 1.5 0.3 0.0 2.3 13.7</cell></row><row><cell cols="20">D&amp;S (Ours, WO) 72.7 7.0 56.6 4.5 5.6 16.9 3.6 15.7 66.8 2.2 6.0 40.0 5.0 44.7 0.5 18.5 0.2 1.4 2.1 19.5</cell></row><row><cell>Segmenter, ViT-S/16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="20">D&amp;S (Ours, WO) 74.1 7.0 65.7 6.6 1.0 24.9 4.3 16.6 64.8 1.8 3.7 45.9 4.3 57.3 1.7 19.9 1.3 0.4 12.1 21.8</cell></row><row><cell cols="20">small-object classes such as pole (+23.2/+15.2 with Segmenter and ResNet18+FPN re-</cell></row><row><cell cols="20">spectively), traffic signs (+15.4/+14.5), and person (+42.8/+36.9). They are also sub-</cell></row><row><cell cols="10">stantial for some B Analyzing learned representations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">B.1 k-NN evaluation of learned representations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 .</head><label>9</label><figDesc>Evaluation of learned features using k-NN pixel-wise classification. Results are produced by running k-NN with three different 100-image training sets<ref type="bibr" target="#b20">[21]</ref> and computing the average (over the three runs) pixel accuracy on the Cityscapes validation split. Results are reported with the Pixel Accuracy (PA) metric.Fig. 8. Feature visualization. We do PCA analysis of the pixel-wise decoder features from each image (independenly between the different images) and visualize the three first PCA components as an RGB image. 'Segm.' stands for Segmenter with ViT-S/16 and 'RN' for ResNet18+FPN.</figDesc><table><row><cell>method</cell><cell></cell><cell>k = 1</cell><cell>k = 5</cell><cell>k = 20</cell></row><row><cell cols="2">supervised</cell><cell>76.9</cell><cell>79.4</cell><cell>81.2</cell></row><row><cell cols="2">PiCIE [15]</cell><cell cols="3">74.3 (-2.6) 78.0 (-1.4) 79.1 (-2.1)</cell></row><row><cell cols="5">Drive&amp;Segment 81.1 (+4.2) 83.2 (+3.8) 84.7 (+3.5)</cell></row><row><cell>input</cell><cell cols="2">ours (Segm.)</cell><cell cols="2">ours (RN)</cell><cell>PiCIE (RN)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>. Waymo Open Dataset day ? ACDC [43] {fog, rain, snow, night}. Qualitative results of our Drive&amp;Segment model trained on the daytime images from the Waymo Open Dataset and used to segment samples from the ACDC [43] dataset with various adverse conditions. In rows 2-5 the ground is incorrectly segmented as sky. This failure mode is further discussed in Section B.4.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See project webpage https://vobecant.github.io/DriveAndSegment/ for the code and more.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Range images are depth maps corresponding to the raw LiDAR measurements. Valid measurements are back-projected to the 3D space to form a point cloud.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">These adverse weather datasets<ref type="bibr" target="#b16">[17,</ref>42,43]  are commonly used by domain adaptation approaches that leverage unlabeled images of this type for adaptation. Here, we consider them only for evaluation to assess the generalization of our strategy; we do not have access to any of those images during training.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was partly supported by the European Regional Development Fund under the project IMPACT (reg. no. CZ.02.1.010.00.015 0030000468), and the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90140). Antonin Vobecky was partly supported by the CTU Student Grant Agency (reg. no. SGS21184OHK33T37).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised object detection from audio-visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<idno>arXiv (2021) 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-supervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Lidartouch: Monocular metric depth estimation with a few-beam lidar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bartoccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Zablocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<idno>arXiv (2021) 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emergence of object segmentation in perturbed generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient online segmentation for sparse 3d laser scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bogoslavskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emerging Properties in Self-Supervised Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021) 6</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Localizing visual sounds the hard way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised object segmentation by redrawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arti?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Panopticdeeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PiCIE: Unsupervised semantic segmentation using invariance and equivariance in clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 1, 4, 8, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2016) 3, 9</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ITSC</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In: ICLR (2021) 3, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Obow: Online bag-ofvisual-words generation for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">?</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient visual pretraining with contrastive detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">xmuda: Cross-modal unsupervised domain adaptation for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2019) 4, 8, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised image segmentation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanezaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NRLQ</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">One million scenes for autonomous driving: Once dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rota Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drive&amp;amp;segment</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segmenter</surname></persName>
		</author>
		<idno>9 .01 .02 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .83 .08 .03 .01 .0 .01 .0 .0 .0 .0 .0 .01 .0 .0 .0 .0 .0 .0 .0 .01 .0 .77 .12 .01 .01 .01 .01 .01 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .07 .03 .24 .42 .01 .02 .01 .01 .07 .0 .0 .01 .0 .0 .0 .0 .02 .0 .0 .02 .0 .55 .21 .02 .02 .01 .02 .03 .0 .0 .01 .01 .0 .0 .03 .02 .01 .01 .03 .0 .17 .06 .13 .37 .06 .02 .04 .0 .0 .01 .02 .0 .0 .01 .0 .01 .0 .0 .0 .16 .07 .19 .08 .31 .05 .09 .0 .0 .0 .01 .0 .0 .0 .0 .0 .0 .01 .0 .28 .06 .05 .01 .25 .25 .02 .0 .0 .0 .01 .0 .0 .04 .0 .01 .0 .03 .0 .05 .02 .01 .02 .04 .0 .67 .03 .0 .0 .01 .0 .0 .0 .0 .0 .0 .6 .15 .03 .02 .0 .01 .01 .0 .09 .03 .0 .0 .0 .0 .0 .0 .02 .0 .0 .66 .0 .18 .03 .0 .01 .01 .0 .04 .0 .04 .0 .0 .0 .0 .0 .0 .0 .0 .02 .0 .04 .06 .01 .01 .02 .0 .01 .0 .0 .69 .08 .01 .0 .01 .0 .01 .0 .01 .0 .02 .02 .01 .0 .02 .0 .01 .0 .0 .74 .13 .0 .01 .01 .0 .01 .0 .01 .0 .01 .02 .0 .0 .0 .0 .0 .0 .0 .0 .01 .6 .16 .08 .0 .04 .05 .01 .0 .05 .08 .0 .0 .0 .01 .01 .0 .0 .0 .0 .02 .08 .7 .01 .01 .01 .0 .0 .01 .13 .0 .0 .01 .03 .0 .0 .0 .0 .0 .01 .01 .75 .02 .02 .0 .01 .0 .12 .19 .02 .0 .01 .05 .01 .0 .0 .0 .0 .0 .0 .52 .03 .01 .0 .03 .0 .02 .05 .01 .0 .04 .01 .01 .0 .0 .32 .11 .09 .03 .02 .01 .02 .22 .12 .0 .09 .08 .01 .01 .03 .01 .03 .0 .0 .31 .06 .02 .0 .01 .0 .01 .19</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
