<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lifting Monocular Events to 3D Human Poses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Scarpellini</surname></persName>
							<email>gianluca.scarpellini@iit.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Pattern Analysis &amp; Computer Vision -Istituto Italiano di Tecnologia</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Genova</orgName>
								<address>
									<settlement>Genoa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
							<email>pietro.morerio@iit.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Pattern Analysis &amp; Computer Vision -Istituto Italiano di Tecnologia</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><forename type="middle">Del</forename><surname>Bue</surname></persName>
							<email>alessio.delbue@iit.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Pattern Analysis &amp; Computer Vision -Istituto Italiano di Tecnologia</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Visual Geometry and Modelling</orgName>
								<orgName type="institution" key="instit2">Istituto Italiano di Tecnologia</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lifting Monocular Events to 3D Human Poses</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel 3D human pose estimation approach using a single stream of asynchronous events as input. Most of the state-of-the-art approaches solve this task with RGB cameras, however struggling when subjects are moving fast. On the other hand, event-based 3D pose estimation benefits from the advantages of event-cameras, especially their efficiency and robustness to appearance changes. Yet, finding human poses in asynchronous events is in general more challenging than standard RGB pose estimation, since little or no events are triggered in static scenes. Here we propose the first learning-based method for 3D human pose from a single stream of events. Our method consists of two steps. First, we process the event-camera stream to predict three orthogonal heatmaps per joint; each heatmap is the projection of of the joint onto one orthogonal plane. Next, we fuse the sets of heatmaps to estimate 3D localisation of the body joints. As a further contribution, we make available a new, challenging dataset for eventbased human pose estimation by simulating events from the RGB Human3.6m dataset. Experiments demonstrate that our method achieves solid accuracy, narrowing the performance gap between standard RGB and event-based vision. The code is freely available at https://iit-pavis. github.io/lifting_events_to_3d_hpe.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Natural selection has empowered us with an efficient perception system, enabling our brain to process visual information and respond to threats promptly. Biological evidence suggests that humans and other animals process visual cues differently from traditional cameras <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b61">61]</ref>. Instead of handling frames at fixed time intervals, mammals collect visual cues asynchronously and elaborate information on demand. This observation pushed the research community and engineers to develop new sensors, <ref type="figure">Figure 1</ref>: Our method computes the 3D pose of a subject from event-camera streams. We first aggregate events into meaningful representations that are then used to estimate the final 3D pose of the subject. event-cameras, with a neuromorphic inspiration that provide crucial advantages in time-critical tasks and applications <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Indeed, one of the most important activities we are daily involved in is interacting with other human beings. For this reason, we developed the ability to forecast human motion and adapt our behavior accordingly <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15]</ref>. However, in order to encode asynchronous quick reactions to human activities, a basic but fundamental task to solve is the estimation of human pose from event-based streams. Human pose estimation is already widely adopted in action recognition <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>, human tracking <ref type="bibr" target="#b28">[28]</ref>, sport assistance <ref type="bibr" target="#b56">[56]</ref>, and virtual reality <ref type="bibr" target="#b55">[55]</ref>. Most of the adopted solutions involve using multiple cameras and require the subjects to wear special markers suites <ref type="bibr" target="#b55">[55]</ref>. Despite their efficiency and broad adoption, these techniques rely on delicate synchronization and are difficult to deploy in real environments. For these reasons, monocular human pose estimation represents a fascinating research challenge with growing interests in the industry <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b48">48]</ref>. There are two different families of solutions to solve 3D human pose es-timation: skeleton-based and model-based solutions. The former regresses skeletal 3D joints from a planar image <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref>, while the latter fits a tri-dimensional parametric model of the human body to the subjects in the scene <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3]</ref>. Recently, Xu et al. adapted a model-based approach to event-cameras <ref type="bibr" target="#b57">[57]</ref>. Although they underline an interesting solution, their approach requires RGB images to guide the tracking and cannot be used for real-time applications, as it relies on a time-consuming optimization phase.</p><p>We propose instead an event-only approach to predict skeletal poses from a single stream of events ( <ref type="figure">Figure 1</ref>). Our pipeline consists of two steps. First, a Convolutional Neural Network predicts the projection of each joint of the skeleton onto three orthogonal planes. Instead of predicting the positions directly, we constrain our approach onto estimating intermediate heatmaps of probabilities for each joint. Second, we triangulate the sets of 2D positions of each joint to predict the 3D joint pose. Similar workss adopt raw events to solve pose estimation tasks <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b57">57]</ref>. On the other hand, we aggregate events into tensor-like representations. Although event-representations have been widely investigated and validated <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b12">13]</ref>, no previous work has explored these approaches for monocular human pose estimation. Moreover, differently from standard computer vision, where transfer learning across different tasks has been widely investigated <ref type="bibr" target="#b60">[60]</ref>, it is still unclear whether pretraining on related tasks can improve event-based human pose estimation. To fill these gaps, we compare different pre-training tasks and different event-representations.</p><p>Experiments on natural and synthetic events validate our approach. For validating performance on real eventcamera recordings, we adopt the recent DHP19 dataset <ref type="bibr" target="#b4">[5]</ref>. DHP19 provides recordings of 33 activities from four different points of view. Despite the excellent contribution to event-based vision, DHP19 provides few self-occlusions or hard situations, as most of the activities are conducted on the spot. To fill these gaps, we propose a new, challenging event-based dataset for Human Pose Estimation by simulating events from the standard Human3.6M dataset <ref type="bibr" target="#b20">[21]</ref>. The event-camera community proposed numerous simulation tools to tackle the absence of data <ref type="bibr" target="#b46">[46]</ref>, and these solutions have been successfully adopted in recent work <ref type="bibr" target="#b47">[47]</ref>. Human3.6m provides challenging scenarios, such as people walking and moving extensively in the scene, that are intrinsically harder. In fact, we test our proposal on both DHP19 and Event-Human3.6 and provide different ablations and experiments to support our claims. To summarize, our proposal consists of three main contributions:</p><p>? A pipeline to predict human poses from a single stream of events;</p><p>? A new, synthetic dataset for benchmarking event-based Human Pose Estimation;</p><p>? Extensive experiments to validate transfer learning and pre-training approaches for event-based human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this Section, we discuss skeleton-based approaches for solving monocular Human Pose Estimation and underline their critical points. To solve the limitations, previous works have focused on high-speed cameras; these approaches suffer, however, from high computational and storage limitations. Event-cameras can be a solution to these problem. Indeed, recent works have adopted eventby-event approaches to track objects and subjects in realtime. On the other hand, our approach aggregates events into tensor-like representation, which can be fed to standard Deep Learning models. Moreover, we recognize a gap of challenging datasets for event-based human pose estimation and discuss events simulation and its benefits.</p><p>Monocular Human Pose Estimation. Industry and academia are looking at human pose estimation with increasing interest <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b48">48]</ref>. Commercial solutions usually require special markers suite to track subjects from multiple point of views <ref type="bibr" target="#b55">[55]</ref>. Despite their satisfactory performances, these approaches are extremely costly and require careful setup choices to perform well at high speeds <ref type="bibr" target="#b39">[39]</ref>. For these reasons, monocular approaches have been widely researched <ref type="bibr" target="#b58">[58]</ref>. Along with background, light conditions, texture, and image imperfection, monocular solutions must also handle the intrinsic ambiguity of monocular vision and therefore pose unanswered challenges to the research community. Model-based solutions are an established line of work on this problem. These approaches estimate the full 3D body and shape of the subjects by fitting a model of human body <ref type="bibr" target="#b32">[32]</ref>. Although recent model-based works achieve impressive performances <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18]</ref>, here we will focus on skeleton-based solutions. Skeleton-based approaches aim to regress 3D joints of the skeleton directly from images. As machine learning models deal with probability better than with scalar values, recent solutions predict dense probability maps (denominated heatmaps) of the location of the skeletal joints onto the image plane. In particular, Newell et al. made a break-trough in the field by proposing Stacked-hourglass model <ref type="bibr" target="#b42">[42]</ref>. The authors stack multiple Convolutional Neural Networks to extract expressive heatmaps and apply a differentiable sort-argmax operator to retrieve the 2D pixel location of each joint. Although we can adapt stacked-hourglass models to predict 3D (volumetric) heatmaps <ref type="bibr" target="#b44">[44]</ref>, this path is widely open for improvements, especially since Volumetric Heatmaps are computational and memory demanding <ref type="bibr" target="#b33">[33]</ref>. Indeed, Mehta et al. factorize volumentric heatmaps into three 2D heatmaps to lower computational costs <ref type="bibr" target="#b38">[38]</ref>. The authors train a deep learning model (VNect) to predict x, y, and z axes as dense The recording is an asynchronous train of events; each event is characterized by an image plane coordinate (x, y), a timestamp (t), and a positive or negative polarity (respectively, blue and black in the figure). (c) batches of events are accumulated to build frames. Our model processes frames of events (d) in multiple stages. The model output are three set of independent planes; each subject's joint (i.e., head, left and right wrists, and so on) is onto three independent planes(e). Next, it triangulates the planar predictions (f) and estimates the position of each joint. The output (g) is the subject's skeleton in tridimensional coordinate.</p><p>2D heatmaps and combine the predictions through triangulation. The computational and resources savings of VNect come with a price in terms of accuracy, as this method reaches higher Mean Per-Joint Precision Error (MPJPE) on common benchmarks. Nibali et al. develop this approach further and propose a model (Margipose) to predict xy, zy, and xz heatmaps and regress the final 3D pose <ref type="bibr" target="#b43">[43]</ref>. Others advancements in 3D human pose estimation include GANs <ref type="bibr" target="#b6">[7]</ref> and temporal convolutions <ref type="bibr" target="#b8">[9]</ref>.</p><p>Event-based approaches. Real-time applications require a careful design to meet strong computational, speed, and energy requirements. This premise is especially true when fast-moving human subjects are involved, such as in sport assistance and virtual reality. Monocular solutions involving RGB-D sensors <ref type="bibr" target="#b59">[59]</ref> and high-speed cameras <ref type="bibr" target="#b25">[26]</ref> have been explored, although they cannot meet the computational requirements of real-time applications. On the other hand, event-cameras achieve high recording speed without saturating bandwidth and resources. For these reasons, human pose estimation performed with event-cameras is both interesting and challenging for the community. Approaches that extrapolate information from single events would be ideal, as these methods allow to exploit the interesting advantages of event-cameras. Initial proposals leveraged event-cameras to match events with known objects in the scene <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b22">23]</ref>. Rebecq et al. exploit a similar caveat <ref type="bibr" target="#b45">[45]</ref> to predict semi-dense 3D structure of a scene. More recently, Xu et al. <ref type="bibr" target="#b57">[57]</ref> employ events to (1) track features across frames and (2) enhance the intensity outputs of a DAVIS camera. Next, they predict human-poses with VNect <ref type="bibr" target="#b38">[38]</ref> and Openpose <ref type="bibr" target="#b5">[6]</ref> models and optimize a multi-step optimization scheme to refine the prediction. Despite its efficiency, their approach relies on a heavy preprocessing phase to extract 3D mesh of the subjects and involves multiple components, each with its own hyperparameters. Instead of processing events in small batches, numerous works accumulate events into tensors representations, conducing events in the realm of synchronous deep learning models <ref type="bibr" target="#b11">[12]</ref>. To predict human poses through event-cameras, previous works aggregate events to predict 2D poses from multiple point of views and finally triangulate subjects' 3D poses <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>. On the other hand, our approach is the first attempt to estimate 3D human pose based on a single DVS camera. We prove that human pose estimation from event-only DVS camera is feasible. For an in-depth discussion of event-cameras and their applications, we refer to the excellent event-cameras summary <ref type="bibr" target="#b11">[12]</ref>.</p><p>Datasets for event-based Human Pose Estimation. Few datasets have been recorded using event-cameras, especially if compared with the huge amount of RGB datasets. For human pose estimation, Calabrese et al. released DHP19, a dataset with recordings of 17 subjects and 33 movements. On the other hand, simulating events from RGB videos is a promising path of research, especially since multiple works proved the soundness of training on simulated events. Mueggleret al. <ref type="bibr" target="#b40">[40]</ref> generate synthetic events from RGB images and compare real and synthetic events for ego-pose estimation in various scenarios. More recently, simulated events have been employed for image reconstruction <ref type="bibr" target="#b47">[47]</ref>, depth estimation <ref type="bibr" target="#b13">[14]</ref>, and motion seg-mentation <ref type="bibr" target="#b53">[53]</ref>, especially in high-speed scenarios where RGB ground-truth are hard to collect. In this work, we propose a pipeline to generate synthetic events from the Hu-man3.6m dataset <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> and compare our approach with standard RGB methods to establish a strong benchmark for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal in this paper is to fill the gap between RGBbased and event-based monocular human pose estimation. In particular, we propose an end-to-end pipeline to predict the skeleton of a subject from the stream of a single eventcamera. <ref type="figure" target="#fig_0">Figure 2</ref> provides an overview of our methodology. An event-camera collects an asynchronous stream of events of a subject moving in the scene. Instead of tracking events as previous works <ref type="bibr" target="#b57">[57]</ref>, we aggregate them into tensor-like frames. Next, we predict three heatmaps planes of the cuboid surrounding the subject and finally build his final 3D pose through triangulation.</p><p>Events. Event-cameras have peculiar pixel sensors that capture information asynchronously. In particular, eventcameras have no central clock; each pixel senses the light variations of the scene independently according to</p><formula xml:id="formula_0">?L(x k , t k ) &gt; p k C, where ?L(x k , t k ) . = L(x k , t k ) ? L(x k , t k ? ?t k ),<label>(1)</label></formula><p>where at each pixel x k we compute the difference in light intensity ?L(x k , t k ) between the current and previous time instance every ?t k seconds. If this difference exceeds a fixed threshold C, the pixel emits an event. An eventcamera stream is a sequence of events, each characterized by the image coordinate pair (x, y), a polarity (related to a positive or a negative change of intensity), and a timestamp. Events aggregation. Instead of relying on raw asynchronous events, recent literature has shifted toward aggregating events together to build synchronous events representation. Common approaches range from simply integrating batch of events (constant-count) to representations involving stochastic modelling of events <ref type="bibr" target="#b52">[52]</ref> and temporal sparsity <ref type="bibr" target="#b62">[62]</ref>. As temporal information is critical in 3D human pose estimation <ref type="bibr" target="#b7">[8]</ref>, our first question is to understand if 3D Human Pose Estimation benefits from specific spatiotemporal representations. To provide an answer, we compare constant-count representation with spatio-temporal voxel grids <ref type="bibr" target="#b62">[62]</ref>. While constant-count simply aggregates a constant number of events into an image, spatio-temporal voxel-grid preserves the timestamp contribution of events by building B temporal bins and have been already adopted in image reconstruction <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b49">49]</ref> and depth estimation <ref type="bibr" target="#b13">[14]</ref>. Given a set of N events {(x k , t k , p k )} k=0...N , we compute t * k as the normalized timestamp of event k into range [0, B ? 1]. Each event (x k , t k , p k ) contribute to each bin B  <ref type="figure">Figure 3</ref>: (a) We define the canonical 3D skeleton pose into a normalized cube <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b0">1]</ref> and reproject the cube into the camera image plane using camera calibration parameters.</p><p>(b) For each joint, our method extracts the three orthogonal faces of the cube to generate three marginal heatmaps.</p><p>of voxel V proportionally to its normalized timestamp t * k , as:</p><formula xml:id="formula_1">V(x, t) = N k=0 p k max(0, 1 ? |t ? t * k |), where t * k . = B ? 1 t N ? t 0 .<label>(2)</label></formula><p>We set N = 7500 for both representations and B = 4 for spatio-temporal voxel-grid. Skeleton normalization and projection. Instead of regressing 3D joints directly, our method relies, as a proxy, on their 2D projections onto specific planes <ref type="bibr" target="#b43">[43]</ref>. We generate ground-truth as follows. First, we project the coordinates p xyz of a joint on a plane parallel to the image plane and placed at depth z ref (we adopt the z value of the head joint as z ref ). After that, we map the space to a normalized cube p NDC xyz (Normalized Device Coordinate -NDC <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b0">1]</ref>): the three coordinates assume values in the range [-1, 1], as in <ref type="figure">Figure 3a</ref>. Last, we project p NDC xyz onto the three orthogonal faces of the cube and blur the projection on each face with a Gaussian Filter to generate ground-truth marginal heatmaps H xy , H zy and H xz <ref type="figure">(Figure 3b)</ref>.</p><p>Predicting marginal heatmaps. We design our approach upon marginal heatmaps <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b43">43]</ref> and first predict three 2D heatmaps from our monocular input. <ref type="figure" target="#fig_3">Figure 4</ref> summarizes our model. We first process the event-frame input with a backbone to extract intermediate representations. In particular, we adopt a ResNet-34 <ref type="bibr" target="#b16">[17]</ref> which is cut after the second residual block. The feature extractor initialization is a critical design choice of our approach and we experimentally ablate possible alternatives in Section 4.3, where we compare different initialization and pretraining strategies and provide evidence of the benefits of RGB-to-events transfer learning. The main model consists in three branches, one for each marginal projection (xy, zy, and xz). Each branch is further made of three stages <ref type="figure" target="#fig_3">(Figure 4(a)</ref>), each consisting in a hourglass-like CNN, as de-tailed in <ref type="figure" target="#fig_3">Figure 4(b)</ref>. For each stage we compute an intermediate loss. The result of each stage is also aggregated (summation) with the previous output to feed the next stage, in a residual-like fashion. According to <ref type="bibr" target="#b42">[42]</ref>, intermediate losses help alleviating the problem of vanishing gradients.</p><p>Aggregating marginal heatmaps. Our model is trained jointly to predict the intermediate heatmaps as well as the normalized skeletal coordinates. We apply the soft-argmax operator <ref type="bibr" target="#b41">[41]</ref> to extract the normalized coordinates of each joints onto the xy, xz, and yz planes. We choose the predictions from the xy-plane for the xy coordinate of the final predictionp xyz , as they match naturally with the input image. For z, we average the zy and xz predictions. Eq. 3 summarizes these steps as:  Losses. As the full pipeline is differentiable, we can back-propagate the geometrical error between joints predictions and ground-truths and train our model end-to-end. Moreover, we can interpret marginal heatmaps as probability distributions of joints locations. In this framework, we apply the Jensen-Shannon divergence (Equation 4) between predicted heatmaps? i for stage i and ground-truth heatmaps H. JSD is based on the Kullbeck-Leibler divergence (KL), it is symmetric and has only finite values given by:</p><formula xml:id="formula_2">H i xy , H i xz ,? i yz = Model(x) x i xy , y i xy = soft-argmax(? i xy ) x i xz , z i xz = soft-argmax(? i xz ) y i zy , z i zy = soft-argmax(? i zy ) p i xyz = x i xy , y i xy , z i xz +z i zy 2 .<label>(3)</label></formula><formula xml:id="formula_3">JSD(H,?) = 1 2 KL(H ? ) + 1 2 KL(? H).<label>(4)</label></formula><p>The Jensen-Shannon divergence and the geometrical loss for each stage i are aggregated into the final loss L as:</p><formula xml:id="formula_4">L = i L geometrical (p i xyz , p xyz ) + JSD(H xy ,? i xy )+ JSD(H xz ,? i xz ) + JSD(H zy ,? i zy ),<label>(5)</label></formula><p>where L geometrical (p i xyz , p xyz ) = p i xyz ? p xyz 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We test our approach on our novel Event-Human3.6m dataset and provide extensive comparison to support our claims. Moreover, we experiment on real events from the event-based DHP19 dataset. For both the dataset, we address the scale-depth ambiguity using a ground-truth depth point and calculate the Mean Per-Joint Precision Error (MPJPE) between the de-normalized predictions and the ground-truths <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b33">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>DHP19 dataset. DHP19 <ref type="bibr" target="#b4">[5]</ref> contains 33 recordings of 17 subjects of different sex, age, and size. Each subject is recorded with four DVS cameras from different angles. Nevertheless, the range of movements in the recordings is narrow. Most of the activities, such as legs kicking and arms abductions, are conducted on the spot, with the exception of slow jogging and walking. Moreover, few recordings spot real life activities. These gaps in the data limit its applications in real scenarios.</p><p>Event-Human3.6m dataset. In the previous section we highlight some limitations of the DHP19 dataset <ref type="bibr" target="#b4">[5]</ref>, especially related to the narrowness of movements and activities that it provides. To solve these gaps, we contribute with a new simulated datasets based on the Human3.6m dataset <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Human3.6 recordings include 11 subjects and different activities from real scenarios, such as walking with a dog, talking at the phone, and giving directions. Consequently, extensive research has adopted the standard Hu-man3.6m dataset to evaluate monocular Human Pose Estimation methods <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b44">44]</ref>. We believe event-based re-search will benefit from our Event-Human3.6m, as it extends DHP19 with more challenging scenarios and provides a new benchmark for monocular human pose estimation algorithms. We adopt the ESIM-Py simulator <ref type="bibr" target="#b46">[46]</ref> to convert the RGB recordings of Human3.6m into events and synchronize raw joints ground-truth with events frames through interpolation ( <ref type="figure">Figure 5</ref>). As a result, Event-Human3.6m and DHP19 have comparable ground-truths and event frames. In the following sections we reports extensive experiments on both DHP19 and Event-Human3.6m to test the benefits of our proposal.</p><p>(a) (b) (c) <ref type="figure">Figure 5</ref>: We simulate raw events from Human3.6m recordings (a) with the open-source simulator ESIM-Py <ref type="bibr" target="#b46">[46]</ref>. We set the simulators parameters cp = cn = 0.2, log-eps = 1e?3, and refractory-period = 1e?4, as this setting produces synthetic events similar to DHP19 event-cameras recordings. Next, we accumulate events into event-frames (b) and interpolate ground-truths to match timestamps (c).</p><p>Training details. We explore different hyper-parameters settings empirically. In the following experiments, we train our method on 4 Tesla V100 16Gb GPUs and adopt a batch size (per GPU) of 32. For updating the gradients, we opt for Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with learning rate of 0.0003. We interrupt the train at local convergence through an earlystopping strategy. We evaluate our approach with 1 stage (7M of parameters, 91 MB of storage) and 3 stages (21M parameters, 300MB of storage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Here we discuss the performance of our approach and validate it on the two datasets.</p><p>Evaluation on DHP19. We test 1-stage and 3-stage models with spatio-temporal voxel-grid and constant-count representations. <ref type="table">Table 1</ref> reports the Mean Per Joint Precision Error (MPJPE, in mm) and summarizes the results. As reference, we compare to the stereo approach of Calabrese et al. <ref type="bibr" target="#b4">[5]</ref>. As a first observation, our methodology performs only slightly worse than the stereo approach (13mm difference). In this setting, constant-count representation performs better than voxel-grid. In the ablations, we elaborate on the differences between the two representations when different backbones are adopted as feature extractors.</p><p>Moreover, we provide results for our single stage and 3stages model and compare them. <ref type="table">Table 1</ref> shows that multiple stacked stages and intermediate losses provide sensible performance benefits, at the cost of an increase in computational costs and model size. <ref type="table">Table 1</ref>: Results refer to DHP19 dataset <ref type="bibr" target="#b4">[5]</ref>. We compare our approach with 1 and 3 stack of stages across constantcount and voxel-grid representation. Evaluation on Event-Human3.6m. For each subject, we keep 13 out the 32 provided joints to build skeletons that are compatible with DHP19 ground-truths and evaluate our approach on a cross-subject protocol. We train our models on subjects 1, 3, 5, 7, 8 and test on subjects 9 and 11. Similar works <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b44">44]</ref> evaluate monocular approaches on every 64 th frame of the recordings. We adapt this evaluation protocol to our asynchronous Event-Human3.6m by taking event-frames corresponding to the same testing frames. <ref type="table">Table 2</ref> reports the results of our approach with constantcount and voxel-grid representations. Moreover, we compare our methodology to state-of-the-art RGB approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b33">33]</ref>. Despite the gap with standard computervision techniques, our approach performs fairly against existing RGB approaches. <ref type="table">Table 2</ref>: Comparison between RGB approaches on Hu-man3.6m and our approach on its synthetic counterpart. We adopt a standard cross-subject protocol to validate on the same testing strategy as RGB approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>In this Section, we deepen different aspects of our approach in more detail. In particular, we are interested to explore what movements cause our approach to fail and how backbone initialization impacts performance. In the following, we discuss these questions in more details.</p><p>Transfer learning and pre-training tasks. Event representations and RGB images share some commonalities, especially edges and corners. However, if we compare them closely, we find subtle differences, since event-cameras recordings are highly correlated to the dynamic of the scene. If the RGB/event-frames analogy held, event-based vision could benefit widely from advancements in standard computer vision. As an example, recent computer vision research provides strong evidence in support of transfer learning from large dataset, e.g., the ImageNet dataset <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref>. Further works explore and validate the correlation between 3D Human Pose Estimation and reconstruction tasks <ref type="bibr" target="#b60">[60]</ref>. These insights are supported by common intuition, as both tasks involve an understanding of the structure of the scene. Despite the differences between event and standard cameras, recent works validate the transfer learning hypothesis from RGB to constant-count representation <ref type="bibr" target="#b35">[35]</ref> and learnable representations <ref type="bibr" target="#b12">[13]</ref>. Moreover, Rebecq et al. provide evidence for direct transfer learning by predicting natural images from spatio-temporal event-frames <ref type="bibr" target="#b47">[47]</ref>.</p><p>Our work contributes further to this line of research with two evaluations. First, we compare ImageNet and random initialized models for solving monocular human pose estimation with both constant-count and voxel-grid representations. Second, we attempt to validate if different pretraining tasks help with event-based Human Pose Estimation. For this purpose, we train an auto-encoder consisting of a ResNet-34 as encoder and a small DeconvCNN as decoder. For comparison, we train a ResNet-34 and a ResNet-50 CNN on action recognition task, which has lower correlation with human pose estimation. Next, we test our approach with 4 backbones (random-initialized, action recognition task, reconstruction task, and ImageNet initialized) and compare the results on DHP19 dataset. <ref type="table">Table 3</ref> reports the MPJPE for both constant-count and voxel-grid representations. Constant-count frames benefit more from standard computer vision, especially from ImageNet transferlearning. In fact, our model with ImageNet-pretrained ResNet34 outperforms all others approaches when we adopt constant-count representation.</p><p>Spatio-temporal frames have few similarities with standard RGB images; in fact, it is unclear if this approach can benefit from ImageNet transfer learning. Our experiments reflects these differences, as ImageNet pretrained ResNet-34 and ResNet-50 backbones have lower performance than the random-initialised counterpart.</p><p>We discuss <ref type="table">Table 3</ref> to explore further if recent research in pre-training tasks <ref type="bibr" target="#b60">[60]</ref> is valid in event-based vision. Despite the correlations evidences in RGB settings, we find that auto-encoders backbones are performing worse than the classification counterpart; this conclusion is valid from both representations. Indeed, action-recognition pre-training emerges favorably, especially for spatio-temporal voxelgrid. Our interpretation is that pre-training assumptions fail because of the spatial sparsity of event-representations. Further research is mandatory to unlock better pre-training strategies for event-based vision. <ref type="table">Table 3</ref>: We report the Mean Per Joint Precision Error (MPJPE, in mm) of our 3-stages approach equipped with different initialization strategies. ResNet-34 with ImageNet initialization emerges favorably for constant-count representation. Moreover, we find no benefits in adopting a reconstruction task as pre-training task, although standard computer vision research suggests the opposite <ref type="bibr" target="#b60">[60]</ref>. <ref type="bibr">Repr</ref> Per-movements comparison. Events are highly coupled with the dynamic of the scene. If parts of the body are static, fewer events are recorded. As a consequence, spatial sparsity increases and makes prediction tasks more challenging. To evaluate the impact of static body parts on our approach, we propose a per-movements study for our ImageNet-pretrained method. <ref type="table" target="#tab_3">Table 4</ref> compares our constant-count and spatio-temporal voxel-grid approaches with DHP19 [5] event-based stereo approach. Differently from <ref type="bibr" target="#b4">[5]</ref>, our approach is based upon the more recent state of the art solutions <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b43">43]</ref> and reaches a higher permovement accuracy and lower per-movement standard deviation. As expected, performance decreases when subjects perform movements with only parts of the body (e.g., Punch up forwards left implies static legs). This drop in performance matches the results of the stereo-vision approach (e.g., Punch forwards left/right). On the other hand, we notice above average performances for movements that involve the whole body, such as knee lift and hand movements (during these movements, subjects move on the spot and the whole body generates events). <ref type="figure">Figure 6</ref>: Our approach achieves good performance when subjects are actively moving, as in (a)-(d), but fails to predict the skeletons satisfactorily when some parts of the body remain static during the movements, as in (e)-(g). </p><formula xml:id="formula_5">(a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusions</head><p>We have proposed a deep learning approach for eventbased human pose estimation from a single event-camera. Our method aggregates events into synchronous tensor representations to feed a multi-stage Convolutional Neural Network. Our architecture predicts three orthogonal heatmaps which are triangulated to obtain the final 3D pose. We validated our approach on the event-based DHP19 dataset, where it showed satisfactory per-movement performance against DHP19 stereo approach <ref type="bibr" target="#b4">[5]</ref>. Moreover, we proposed Event-Human3.6m, a new dataset of simulated events from the standard Human3.6m <ref type="bibr" target="#b20">[21]</ref>. Event-Human3.6m extends DHP19 with more challenging movements and actions. We conducted experiments on the synthetic dataset and adopted a cross-subject protocol which is comparable to the standard RGB testing. Although we recognize the differences between synthetic and RGB datasets, our proposal achieved an accuracy comparable to RGB approaches. These experiments demonstrated the effectiveness of our method. <ref type="figure">Figure 6</ref> reports challenging examples where our method underperforms. Static parts of the body generated fewer events and are difficult to predict accurately. We leave this issue for further investigations. Next, we conducted extensive ablations studies to understand how event-based vision can benefit from RGB transfer learning and pre-training. Experiments showed that ImageNet pre-training boosts our approach more than pre-training tasks. Moreover, action recognition pre-training task archived higher performances than reconstruction pre-training, although extensive computer vision research suggests the opposite. Future research should consider closely the relationships between events and RGB cameras in transfer-learning and multi-task learning settings. Further works to answer these open questions can benefit from our synthetic Event-Human3.6m.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) A moving subject is recorded with an event-camera. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Overview of our model (b) Overview of one stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) We process event-frames with a backbone that outputs features of depth d. Next, we adopt three sequential stages to output 3 ? J intermediate heatmaps. We apply an intermediate loss to each stage<ref type="bibr" target="#b41">[41]</ref> and accumulate the losses to solve the vanish gradient problem. (b) Each stage process its input with three deep Convolutional Neural Network through an auto-encoder architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>We compare the per-movement MPJPE between ours and DHP19<ref type="bibr" target="#b4">[5]</ref> stereo approach. Both fail when parts of the body are static and shine when the scene is more dynamic. In bold we highlight worst results per column while with an underline we show best results per column.</figDesc><table><row><cell></cell><cell>Stereo [5]</cell><cell cols="2">Voxel-grid Constant-count</cell></row><row><cell>Left arm abduction</cell><cell>115.04</cell><cell>82.32</cell><cell>80.41</cell></row><row><cell>Right arm abduction</cell><cell>99.65</cell><cell>81.92</cell><cell>79.68</cell></row><row><cell>Left leg abduction</cell><cell>84.65</cell><cell>110.07</cell><cell>105.39</cell></row><row><cell>Right leg abduction</cell><cell>78.35</cell><cell>99.87</cell><cell>93.81</cell></row><row><cell>Left arm bicep curl</cell><cell>103.29</cell><cell>90.49</cell><cell>86.40</cell></row><row><cell>Right arm bicep curl</cell><cell>121.06</cell><cell>80.75</cell><cell>95.73</cell></row><row><cell>Left leg knee lift</cell><cell>74.97</cell><cell>71.60</cell><cell>72.14</cell></row><row><cell>Right leg knee lift</cell><cell>71.95</cell><cell>78.47</cell><cell>72.49</cell></row><row><cell>Walking 3.5 km/h</cell><cell>58.75</cell><cell>86.88</cell><cell>84.74</cell></row><row><cell>Single jump up-down</cell><cell>82.23</cell><cell>80.11</cell><cell>76.73</cell></row><row><cell>Single jump forwards</cell><cell>80.53</cell><cell>89.92</cell><cell>85.10</cell></row><row><cell>Multiple jumps</cell><cell>53.57</cell><cell>99.47</cell><cell>93.83</cell></row><row><cell>Hop right foot</cell><cell>55.56</cell><cell>89.51</cell><cell>84.16</cell></row><row><cell>Hop left foot</cell><cell>54.21</cell><cell>97.86</cell><cell>91.60</cell></row><row><cell>Punch forward left</cell><cell>148.57</cell><cell>114.97</cell><cell>117.87</cell></row><row><cell>Punch forward right</cell><cell>135.92</cell><cell>98.35</cell><cell>93.69</cell></row><row><cell>Punch up forwards left</cell><cell>111.35</cell><cell>124.89</cell><cell>124.81</cell></row><row><cell>Punch up forwards right</cell><cell>131.46</cell><cell>103.01</cell><cell>106.56</cell></row><row><cell>Punch down forwards left</cell><cell>106.92</cell><cell>105.98</cell><cell>105.04</cell></row><row><cell>Punch down forwards right</cell><cell>98.28</cell><cell>90.02</cell><cell>89.90</cell></row><row><cell>Slow jogging</cell><cell>55.16</cell><cell>98.05</cell><cell>89.11</cell></row><row><cell>Star jumps</cell><cell>76.23</cell><cell>108.89</cell><cell>106.77</cell></row><row><cell>Kick forwards left</cell><cell>111.66</cell><cell>117.92</cell><cell>93.07</cell></row><row><cell>Kick forwards right</cell><cell>112.49</cell><cell>117.91</cell><cell>109.85</cell></row><row><cell>Side kick forwards left</cell><cell>118.00</cell><cell>128.38</cell><cell>120.39</cell></row><row><cell>Side kick forwards right</cell><cell>104.67</cell><cell>115.76</cell><cell>111.86</cell></row><row><cell>Hello left hand</cell><cell>96.22</cell><cell>89.08</cell><cell>87.22</cell></row><row><cell>Hello right hand</cell><cell>101.32</cell><cell>71.82</cell><cell>69.83</cell></row><row><cell>Circle left hand</cell><cell>110.59</cell><cell>99.17</cell><cell>95.89</cell></row><row><cell>Circle right hand</cell><cell>112.44</cell><cell>84.00</cell><cell>76.55</cell></row><row><cell>Figure-8 left hand</cell><cell>110.69</cell><cell>90.95</cell><cell>88.10</cell></row><row><cell>Figure-8 right hand</cell><cell>123.59</cell><cell>72.42</cell><cell>72.49</cell></row><row><cell>Clap</cell><cell>122.93</cell><cell>81.03</cell><cell>77.77</cell></row><row><cell>Mean (standard deviation)</cell><cell cols="3">98.06 (?16.60) 95.51 (?15.30) 92.09 (?14.49)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">OpenGL projection onto frustum space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Ho Ahn</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Time-ordered recent event (tore) volumes for event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wes</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Almatrafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayan</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keigo</forename><surname>Hirakawa</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A 240x180 130db 3?s latency global shutter spatiotemporal vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brandli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2333" to="2341" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dhp19: Dynamic vision sensor 3d human pose dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Taverni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Awai Easthope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Skriabine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Corradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Longinotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kynan</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monocular human pose estimation: a survey of deep learningbased methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
		<idno>102897, 2020. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d human pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A real-time tracker for markerless augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Andrew I Comport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaumette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second IEEE and ACM International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Event-based vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Michael Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end learning of representations for asynchronous event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Combining events and frames using recurrent asynchronous multimodal networks for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>R?egg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Hidalgo Carrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Forecasting people trajectories and head poses by jointly reasoning on tracklets and vislets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Setti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsesmelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1267" to="1278" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Resolving 3d human pose ambiguities with 3d scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2282" to="2292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">What makes imagenet good for transfer learning? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi-Young</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1608.08614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision, page nil</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time 3d reconstruction and 6-dof tracking with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanme</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="364" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Nonverbal communication in human interaction. Cengage Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">A</forename><surname>Knapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">G</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The need 4 speed in real-time dense visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">HOTS: A Hierarchy of Event-Based Time-Surfaces for Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryad</forename><forename type="middle">B</forename><surname>Benosman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1346" to="1359" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Skeleton boxes: Solving skeleton based action detection with a single deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huahui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<biblScope unit="page" from="7" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huahui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<biblScope unit="page" from="7" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d skeleton based action recognition by video-domain translation-scale invariant mapping and multi-scale dilated cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="22901" to="22921" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A 128$\times$128 120 dB 15 $\mu$s latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Smpl. ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The silicon retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><forename type="middle">A</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carver</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="76" to="82" />
			<date type="published" when="1991-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Event-based vision meets deep learning on steering prediction for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">I</forename><surname>Maqueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narciso</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Chapter 2 -3d transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mcreynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blythe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Graphics Programming Using OpenGL, The Morgan Kaufmann Series in Computer Graphics</title>
		<editor>TOM McREYNOLDS and DAVID BLYTHE</editor>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">page nil</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo-Hammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rey</forename><forename type="middle">Juan</forename><surname>Carlos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A study of vicon system positioning performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Merriaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Boutteau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Savatier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1591</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">3D Human Pose Estimation with 2D Marginal Heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">EMVS: Event-Based Multi-View Stereo-3D Reconstruction with an Event Camera in Real-Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1394" to="1414" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">ESIM: an Open Event Camera Simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">High speed and high dynamic range video with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Markerless motion capture of man-machine interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schmaltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast image reconstruction with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Scheerlinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Body language and the social order; communication as behavioral control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Albert E Scheflen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Animation of 3d human model using markerless motion capture applied to sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Shingade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archana</forename><surname>Ghotkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.2363</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuele</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bourdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryad</forename><surname>Benosman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Event-Based Motion Segmentation by Motion Compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Stoffregen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsay</forename><surname>Kleeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<idno>. 4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neuromorphic event-based 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>David Reverter Valeiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sio-Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryad</forename><forename type="middle">B</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Motion capture systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vicon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ai coach: Deep human pose estimation and analysis for personalized athletic training assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia, MM &apos;19</title>
		<meeting>the 27th ACM International Conference on Multimedia, MM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Eventcap: Monocular 3d capture of high-speed human motions using an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monoperfcap. ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal upsampling of depth maps using a hybrid camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ze</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1591" to="1602" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A massively asynchronous, parallel brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semir</forename><surname>Zeki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">370</biblScope>
			<biblScope unit="page">20140174</biblScope>
			<date type="published" when="1668-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised event-based optical flow using motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 Workshops</title>
		<editor>Laura Leal-Taix? and Stefan Roth</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
