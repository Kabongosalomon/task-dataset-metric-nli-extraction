<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Person Pose Estimation with Local Joint-to-Person Associations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
							<email>uiqbal@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<email>gall@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Person Pose Estimation with Local Joint-to-Person Associations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite of the recent success of neural networks for human pose estimation, current approaches are limited to pose estimation of a single person and cannot handle humans in groups or crowds. In this work, we propose a method that estimates the poses of multiple persons in an image in which a person can be occluded by another person or might be truncated. To this end, we consider multiperson pose estimation as a joint-to-person association problem. We construct a fully connected graph from a set of detected joint candidates in an image and resolve the joint-to-person association and outlier detection using integer linear programming. Since solving joint-to-person association jointly for all persons in an image is an NP-hard problem and even approximations are expensive, we solve the problem locally for each person. On the challenging MPII Human Pose Dataset for multiple persons, our approach achieves the accuracy of a state-of-the-art method, but it is 6,000 to 19,000 times faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single person pose estimation has made a remarkable progress over the past few years. This is mainly due to the availability of deep learning based methods for detecting joints <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. While earlier approaches in this direction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> combine the body part detectors with tree structured graphical models, more recent methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> demonstrate that spatial relations between joints can be directly learned by a neural network without the need of an additional graphical model. These approaches, however, assume that only a single person is visible in the image and the location of the person is known a-priori. Moreover, the number of parts are defined by the network, e.g., full body or upper body, and cannot be changed. For realistic scenarios such assumptions are too strong and the methods cannot be applied to images that contain a number of overlapping and truncated persons. An example of such a scenario is shown in <ref type="figure">Figure 1</ref>.</p><p>In comparison to single person human pose estimation benchmarks, multi-person pose estimation introduces new challenges. The number of persons in an image is unknown and needs to be correctly estimated, the persons occlude each other and might be truncated, and the joints need to be associated to the correct person. The simplest approach to tackle this problem is to first use a person detector and then estimate the pose for each detection independently <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. This, however, does not resolve the joint association problem of two persons next to each other or truncations. Other approaches estimate the pose of all detected persons jointly <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. In <ref type="bibr" target="#b1">[2]</ref> a person detector is not arXiv:1608.08526v2 [cs.CV] 31 Aug 2016 <ref type="figure">Fig. 1</ref>: Example image from the multi-person subset of the MPII Pose Dataset <ref type="bibr" target="#b15">[16]</ref>.</p><p>required. Instead body part proposals are generated and connected in a large graph. The approach then solves the labeling problem, the joint-to-person association problem and non-maximum suppression jointly. While the model proposed in <ref type="bibr" target="#b1">[2]</ref> can be solved by integer linear programming and achieves state-of-the-art results on a very small subset of the MPII Human Pose Dataset, the complexity makes it infeasible for a practical application. As reported in <ref type="bibr" target="#b4">[5]</ref>, the processing of a single image takes about 72 hours.</p><p>In this work, we address the joint-to-person association problem using a densely connected graphical model as in <ref type="bibr" target="#b1">[2]</ref>, but propose to solve it only locally. To this end, we first use a person detector and crop image regions as illustrated in <ref type="figure">Figure 1</ref>. Each of the regions contains sufficient context, but only the joints of persons that are very close. We then solve the joint-to-person association for the person in the center of each region by integer linear programming (ILP). The labeling of the joints and non-maxima suppression are directly performed by a convolutional neural network. We evaluate our approach on the MPII Human Pose Dataset for multiple persons where we slightly improve the accuracy of <ref type="bibr" target="#b1">[2]</ref> while reducing the runtime by a factor between 6,000 and 19,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Human pose estimation has generally been addressed under the assumption that only a single person is visible. For this, earlier approaches formulate the problem in a graphical model where interactions between body parts are modelled in a tree structure combined with local observations obtained from discriminatively trained part detectors <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. While the tree-structured models provide efficient inference, they struggle to model long-range characteristics of the human body. With the progress in convolutional neural network architectures, more recent works adopt CNNs to obtain stronger part detectors but still use graphical models to obtain coherent pose estimates <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>The state-of-the-art approaches, however, demonstrate that graphical models are of little importance in the presence of strong part detectors since the long-range relationships of the body parts can be directly incorporated in the part detectors <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> multi-staged CNN architectures are proposed where each stage of the network takes as input the score maps of all parts from its preceding stage. This provides additional information about the interdependence, co-occurrence, and context of parts to each stage, and thereby allows the network to implicitly learn image dependent spatial relationships between parts. Similarly, instead of a multi-staged architecture, <ref type="bibr" target="#b4">[5]</ref> proposes to use a very deep network that inherently results in large receptive fields and therefore allows to use contextual information around the parts. All of these methods report impressive results for single person pose estimation without an additional graphical model for refinement.</p><p>In contrast to the single person pose estimation, multi-person pose estimation poses a significantly more complex problem, and only a few works have focused in this direction <ref type="bibr">[2, 5, 11-14, 22, 24-26]</ref>. <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b23">[24]</ref> perform non-maximum suppression on the marginals obtained using a graphical model to generate multiple pose hypotheses in an image. The approaches, however, can only work in scenarios where persons are significantly distant from each other and consider only the fully visible persons. The methods in <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> first detect the persons in an image using a person detector and then estimate the body pose for each person independently. <ref type="bibr" target="#b23">[24]</ref> employs a similar approach for 3D pose estimation of multiple persons in a calibrated multi-camera scenario. The approach first obtains the number of persons using a person detector and then samples the 3D poses for each person from the marginals of a 3D pictorial structure model. For every detected person, <ref type="bibr" target="#b12">[13]</ref> explores a range of tree structured models each containing only a subset of upper-body parts, and selects the best model based on a cost function that penalizes a model containing occluded parts. Since the search space of the models increases exponentially with the number of body parts, the approach is very expensive for full body skeletons. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> define a joint pose estimation model for all detected persons, and utilize several occlusion clues to model interactions between people. All these approaches rely on a standard pictorial structure model with tree structures and cannot incorporate dependencies beyond adjacent joints.</p><p>More recently, <ref type="bibr" target="#b1">[2]</ref> proposed a joint objective function to solve multi-person pose estimation. The approach does not require a separate person detector or any prior information about the number of persons. Unlike earlier works it can tackle any type of occlusion or truncation. It starts by generating a set of class independent part proposals and constructs a densely connected graph from the proposals. It uses integer linear programming to label each proposal by a certain body part and assigns them to unique individuals. The optimization problem proposed in <ref type="bibr" target="#b1">[2]</ref> is theoretically well founded, but is an NP-Hard problem to solve and prohibitively expensive for realistic scenarios. Therefore, it limits the number of part proposals to 100. This means that the approach can estimate the poses of at most 7 fully visible persons with 14 body parts per person. Despite the restriction, the inference takes roughly 72 hours for a single image <ref type="bibr" target="#b4">[5]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, the authors build upon the same model and propose to use stronger part detectors and image dependent spatial models along with an incremental optimization approach that significantly reduces the optimization time of <ref type="bibr" target="#b1">[2]</ref>. The approach, however, is still too slow for practical applications since it requires 8 minutes per image and still limits the number of proposals to a maximum of 150.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview</head><p>Our method solves the problem of joint-to-person association locally for each person in the image. To this end, we first detect the persons using a person detector <ref type="bibr" target="#b26">[27]</ref>. For each detected person, we generate a set of joint candidates using a single person pose estimation model (Section 4). The candidates are prone to errors since the single person models do not take into account occlusion or truncation. In order to associate each joint to the correct person and also to remove the erroneous candidates, we perform inference locally on a fully connected graph for each person using integer linear programming (Section 5). <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Convolutional Pose Machines</head><p>Given a person in an image I, we define its pose as a set X = {x j } j=1...J of J = 14 body joints, where the vector x j ? X represents the 2D location (u, v) of the j th joint in the image. The convolutional pose machines consist of a multi-staged CNN architecture with t ? {1 . . . T } stages, where each stage is a multi-label classifier ? t (x) that is trained to provide confidence maps s j t ? R w?h for each joint j = 1 . . . J and the background, where w and h are the width and the height of the image, respectively.</p><p>The first stage of the architecture uses only the local image evidence and provides the confidence scores</p><formula xml:id="formula_0">? t=1 (x|I) ? {s j 1 (x j = x)} j=1...J+1 .<label>(1)</label></formula><p>Whereas, in addition to the local image evidence, all subsequent stages also utilize the contextual information from the preceding stages to produce confidence score maps</p><formula xml:id="formula_1">? t&gt;1 (x|I, ?(x, s t?1 )) ? {s j t (x j = x)} j=1...J+1 ,<label>(2)</label></formula><p>Input Image <ref type="bibr" target="#b2">[3]</ref>. The first stage (a) utilizes only the local image evidence whereas all subsequent stages (b) also utilize the output of preceding stages to exploit the spatial context between joints. The receptive field of stages t ? 2 is increased by having multiple convolutional layers at the 8 times down-sampled score maps. All stages are locally supervised and a separate loss is computed for each stage. We provide multi-person target score maps to stage 1, and single-person score maps to all subsequent stages.</p><formula xml:id="formula_2">? ? ? 3 9?9 C 2? P 9?9 C 2? P 9?9 C 2? P 5?5 C 9?9 C 1?1 C 1?1 C 9?9 C 2? P 9?9 C 2? P 9?9 C 2? P 5?5 C ? ? + ? ? ( + 1) Loss 1 Target Score Maps = 1 11?11 C 11?11 C 11?11 C 1?1 C 1?1 C ? ? + ? ? ( + 1) Target Score Maps ? 2 Loss ? 2 Input Image ? ? ? 3 (a) Stage 1 (b) Stage ? 2 Fig. 3: CPM architecture proposed in</formula><p>where s t ? R w?h?(J+1) corresponds to the score maps of all body joints and the background at stage t, and ?(x, s t?1 ) indicates the mapping from the scores s t?1 to the context features for location x. The receptive field of the subsequent stages is increased to the extent that the context of the complete person is available. This allows to model complex long-range spatial relationships between joints, and to leverage the context around the person. The CPM architecture is completely differentiable and allows endto-end training of all stages. Due to the multi-stage nature of CPM, the overall CNN architecture consists of many layers and is therefore prone to the problem of vanishing gradients <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. In order to solve this problem, <ref type="bibr" target="#b2">[3]</ref> uses intermediate supervision by adding a loss function at each stage t. The CNN architecture used for each stage can be seen in <ref type="figure">Figure 3</ref>. In this paper we exploit the intermediate supervision of the stages during training for multi-person human pose estimation as we will discuss in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training for Multi-Person Pose Estimation</head><p>Each stage of the CPM is trained to produce confidence score maps for all body joints, and the loss function at the end of every stage computes the l 2 distance between the predicted confidence scores and the target score maps. The target score maps are modeled as Gaussian distributions centered at the ground-truth locations of the joints. For multiperson pose estimation, the aim of the training is to focus only on the body joints of the detected person, while suppressing joints of all other overlapping persons. We do this by creating two types of target score maps. For the first stage, we model the target score maps by a sum of Gaussian distributions for the body joints of all persons appearing in the bounding box enclosing the primary person that appears roughly in the center of the bounding box. For the subsequent stages, we model only the joints of the primary person. An example of target score maps for different stages can be seen in <ref type="figure">Figure 4</ref>. <ref type="figure">Figure 5</ref> shows some examples how the inferred score maps evolve as the number of stages increases. In <ref type="bibr" target="#b2">[3]</ref>, the pose of the person is obtained by taking the maximum of the inferred score maps, i.e., x j = argmax x s j T (x). This, however, assumes that all joints are visible in the image and results in erroneous estimates for invisible joints and can wrongly associate joints of other nearby persons to the primary person. Instead of taking the maximum, we sample N candidates from each inferred score map s j T and resolve the joint-to-person association and outlier removal by integer linear programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Joint-to-Person Association</head><p>We solve the joint-to-person association using a densely connected graphical model as in <ref type="bibr" target="#b1">[2]</ref>. The model proposed in <ref type="bibr" target="#b1">[2]</ref>, however, aims to resolve joint-to-person associations together with proposal labeling globally for all persons, which makes it very expensive to solve. In contrast, we propose to solve this problem locally for each person. We first briefly summarize the DeepCut method <ref type="bibr" target="#b1">[2]</ref> in Section 5.1, and then describe the proposed local joint-to-person association model in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DeepCut</head><p>DeepCut aims to solve the problem of multi-person human pose estimation by jointly modeling the poses of all persons appearing in an image. Given an image, it starts by generating a set D of joint proposals, where x d ? Z 2 denotes the 2D location of the d th proposal. The proposals are then used to formulate a graph optimization problem that aims to select a subset of proposals while suppressing the incompatible proposals, label each selected proposal with a joint type j ? 1 . . . J, and associate them to unique individuals.</p><p>The problem can be solved by integer linear programming (ILP), optimizing over the binary variables x ? {0, 1} D?J , y ? {0, 1} ( D 2 ) , and z ? {0, 1} ( D 2 )?J 2 . For every proposal d, a set of variables {x dj } j=1...J is defined where x dj = 1 indicates that the proposal d is of body joint type j. For every pair of proposals dd , the variable y dd indicates that the proposals d and d belong to the same person. The variable z dd jj = 1 indicates that the proposal d is of joint type j, the proposal d is of joint type j , and both proposals belong to the same person (y dd = 1). The variable z dd jj is constrained such that z dd jj = x dj x d j y dd . The solution of the ILP problem is obtained by optimizing the following objective function:</p><formula xml:id="formula_3">min (x,y,z)?X D ?, x + ?, z<label>(3)</label></formula><p>subject to</p><formula xml:id="formula_4">?d ? D ?jj ? J 2 : x dj + x dj ? 1 (4) ?dd ? D 2 : y dd ? j?J x dj , y dd ? j?J x d j (5) ?dd d ? D 3 : y dd + y d d ? 1 ? y dd (6) ?dd ? D 2 ?jj ? J 2 : x dj + x d j + y dd ? 2 ? z dd jj z dd jj ? min(x dj , x d j , y dd )<label>(7)</label></formula><p>and, optionally,</p><formula xml:id="formula_5">?dd ? D 2 ?jj ? J 2 : x dj + x d j ? 1 ? y dd<label>(8)</label></formula><p>where</p><formula xml:id="formula_6">? dj = log 1 ? p dj p dj (9) ? dd jj = log 1 ? p dd jj p dd jj (10) ?, x = d?D j?J ? dj x dj (11) ?, z = dd ?( D 2 ) j,j ?J ? dd jj z dd jj .<label>(12)</label></formula><p>The constraints (4)- <ref type="bibr" target="#b6">(7)</ref> enforce that optimizing (3) results in valid body pose configurations for one or more persons. The constraints (4) ensure that a proposal d can be labeled with only one joint type, while the constraints (5) guarantee that any pair of proposals dd can belong to the same person only if both are not suppressed, i.e., x dj = 1 and x d j = 1. The constraints (6) are transitivity constraints and enforce for any three proposals dd d ? D 3 that if d and d belong to the same person, and d and d also belong to the same person, then the proposals d and d must also belong to the same person. The constraints <ref type="bibr" target="#b6">(7)</ref> enforce that for any dd ? D 2 and jj ? J 2 , z dd jj = x dj x d j y dd . The constraints <ref type="bibr" target="#b7">(8)</ref> are only applicable for single-person human pose estimation, as they enforce that two proposals dd that are not suppressed must be grouped together. In <ref type="bibr" target="#b8">(9)</ref>, p dj ? (0, 1) are the body joint unaries and correspond to the probability of any proposal d being of joint type j. While in <ref type="bibr" target="#b9">(10)</ref>, p dd jj correspond to the conditional probability that a pair of proposals dd belongs to the same person, given that d and d are of joint type j and j , respectively. In <ref type="bibr" target="#b1">[2]</ref> this ILP formulation is referred as Subset Partitioning and Labelling Problem, as it partitions the initial pool of proposal candidates to unique individuals, labels each proposal with a joint type j, and inherently suppresses the incompatible candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Local Joint-to-Person Association</head><p>In contrast to <ref type="bibr" target="#b1">[2]</ref>, we solve the joint-to-person association problem locally for each person. We also do not label generic proposals as part of the ILP formulation since we use a neural network to obtain detections for each joint as described in Section 4. We therefore start with a set of joint detections D J , where every detection d j at location x dj ? Z 2 has a known joint type j ? 1 . . . J. Our model requires only two types of binary random variables x ? {0, 1} D J and y ? {0, 1} ( D J 2 ) . Here, x dj = 1 indicates that the detection d j of part type j is not suppressed, and y dj d j = 1 indicates that the detection d j of type j, and the detection d j of type j belong to the same person. The objective function for local joint-to-person association takes the form:</p><formula xml:id="formula_7">min (x,y)?X D J ?, x + ?, y<label>(13)</label></formula><p>subject to</p><formula xml:id="formula_8">?d j d j ? D J 2 : y dj d j ? x dj , y dj d j ? x d j (14) ?d j d j d j ? D J 3 : y dj d j + y d j d j ? 1 ? y dj d j (15) ?d j d j ? D J 2 : x dj + x d j ? 1 ? y dj d j<label>(16)</label></formula><p>where</p><formula xml:id="formula_9">? dj = log 1 ? p dj p dj (17) ? dj d j = log 1 ? p dj d j p dj d j (18) ?, x = dj ?D J ? dj x dj (19) ?, y = dj d j ?( D J 2 ) ? dj d j y dj d j .<label>(20)</label></formula><p>The constraints <ref type="bibr" target="#b13">(14)</ref> enforce that detection d j and d j are connected (y dj d j = 1) only if both are not suppressed, i.e., x dj = 1 and x d j = 1. The constraints (15) are transitivity constraints as before and the constraints <ref type="bibr" target="#b15">(16)</ref> guarantee that all detections that are not suppressed belong to the primary person. We can see from <ref type="formula" target="#formula_3">(3)</ref>- <ref type="formula" target="#formula_5">(8)</ref> and <ref type="formula" target="#formula_0">(13)</ref>- <ref type="bibr" target="#b15">(16)</ref>, that the number of variables are reduced from</p><formula xml:id="formula_10">(D ? J + D 2 + D 2 ? J 2 ) to (D J + D J 2 )</formula><p>. Similary, the number of constraints is also drastically reduced.</p><p>In <ref type="formula" target="#formula_0">(17)</ref>, p dj ? (0, 1) is the confidence of the joint detection d j as probability. We obtain this directly from the score maps inferred by the CPM as</p><formula xml:id="formula_11">p dj = f ? (s j T (x dj )), where f ? (s) = s if s ? ? 0 otherwise,<label>(21)</label></formula><p>and ? is a threshold that suppresses detections with a low confidence score. In <ref type="bibr" target="#b17">(18)</ref>, p dj d j ? (0, 1) corresponds to the conditional probability that the detection d j of joint type j and the detection d j of joint type j belong to the same person. For j = j , it is the probability that both detections d j and d j belong to the same body joint. For j = j , it measures the compatibility between two detection candidates of different joint types. Similar to <ref type="bibr" target="#b1">[2]</ref>, we obtain these probabilities by learning discriminative models based on appearance and spatial features of the detection candidates. For j = j , we define a feature vector</p><formula xml:id="formula_12">f dj d j = { x, exp( x), ( x) 2 },<label>(22)</label></formula><p>where x = ( u, v) is the 2D offset between the locations x dj and x d j . For j = j , we define a separate feature vector based on the spatial locations as well as the appearance features obtained from the joint detectors as</p><formula xml:id="formula_13">f dj d j = { x, x , arctan v u , s T (x dj ), s T (x d j )},<label>(23)</label></formula><p>where s T (x) is a vector containing the confidences of all joints and the background at location x. For both cases, we gather positive and negative samples from the annotated poses in the training data and train an SVM with RBF kernel using LibSVM <ref type="bibr" target="#b29">[30]</ref> for each pair jj ? J 2 . In order to obtain the probabilities p dj d j ? (0, 1) we use Platt scaling <ref type="bibr" target="#b30">[31]</ref> to normalize the output of the SVMs to probabilities. After optimizing <ref type="bibr" target="#b12">(13)</ref>, the pose of the primary person is given by the detections with x dj = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate the proposed approach on the Multi-Person subset of the MPII Human Pose Dataset <ref type="bibr" target="#b15">[16]</ref> and follow the evaluation protocol proposed in <ref type="bibr" target="#b1">[2]</ref>. The dataset consists of 3844 training and 1758 testing images with multiple persons. The persons appear in highly articulated poses with a large amount of occlusions and truncations. Since the original test data of the dataset is withheld, we perform all intermediate experiments on a validation set of 1200 images. The validation set is sampled according to the split proposed in <ref type="bibr" target="#b3">[4]</ref> for the single person setup, i.e., we chose all multi-person images that are part of the validation test set proposed in <ref type="bibr" target="#b3">[4]</ref> and use all other images for training. In addition we compare the proposed method with the state-of-the-art approach [2] on their selected subset of 288 images, and also compare with <ref type="bibr" target="#b4">[5]</ref> on the complete test set. The accuracy is measured by average precision (AP) for each joint using the scripts provided by <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation Details</head><p>In order to localize the persons, we use the person detector proposed in <ref type="bibr" target="#b26">[27]</ref>. The detector is trained on the Pascal VOC dataset <ref type="bibr" target="#b31">[32]</ref>. For the quantitative evaluation, we discard detected persons with a bounding box area less equal to 80 ? 80 pixels since small persons are not annotated in the MPII Human Pose Dataset. For the qualitative results shown in <ref type="figure" target="#fig_3">Figure 7</ref>, we do not discard the small detections. For the CPM <ref type="bibr" target="#b2">[3]</ref>, we use the publicly available source code and train it on the Multi-Person subset of the MPII Human Pose Dataset as described in Section 4. As in <ref type="bibr" target="#b2">[3]</ref>, we add images from the Leeds Sports Dataset <ref type="bibr" target="#b32">[33]</ref> during training, and use a 6 stage (T = 6) CPM architecture. For solving <ref type="bibr" target="#b12">(13)</ref>, we use the Gurobi Optimizer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>We first evaluate the impact of the parameter ? in f ? (s) (21) on the pose estimation accuracy measured as mean AP on the validation set containing 1200 images. <ref type="figure" target="#fig_2">Figure  6</ref> shows that the function f ? improves the accuracy when ? is increased until ? = 0.3. For ? &gt; 0.4, the accuracy drops since a high value discards correct detections. For the following experiments, we use ? = 0.2. <ref type="table">Table 1</ref> reports the pose estimation results under different settings of the proposed approach on the validation set. We also report the median run-time required by each setting 1 . Using only the CPM to estimate the pose of each detected person achieves 45.2% mAP and takes only 2 seconds per image. Using the proposed Local Joint-to-Person Association (L-JPA) model with 1 detection candidate per joint (N = 1) to suppress the incompatible detections improves the performance from 45.2% to 49.2% with a very slight increase in run-time. Increasing the number of candidates per joint increases the accuracy only slightly. For the following experiments, we use N = 5. When we compare the numbers with <ref type="figure" target="#fig_2">Figure 6</ref>, we observe that CPM+L-JPA outperforms CPM for any 0 ? ? ? 0.4.</p><p>The accuracy also depends on the used person detector. We use an off-the-shelf person detector without any fine-tuning on the MPII dataset. In order to evaluate the impact of the person detector accuracy, we also estimate poses when the person detections are given by the ground-truth torso (GT Torso) locations of the persons provided with    <ref type="table" target="#tab_2">Table 2</ref> compares the proposed approach with other approaches on a selected subset of 288 test images used in <ref type="bibr" target="#b1">[2]</ref>. Our approach outperforms the state-of-the-art method DeepCut <ref type="bibr" target="#b1">[2]</ref> (54.7% vs. 53.5%) while being significantly faster (10 seconds vs. 57995 seconds). If we use N = 1, our approach requires only 3 seconds per image with a minimal loss of accuracy as shown in <ref type="table">Table 1</ref>, i.e., our approach is more than 19,000 times faster than <ref type="bibr" target="#b1">[2]</ref>. We also compare with a concurrent work <ref type="bibr" target="#b4">[5]</ref>. While the approach <ref type="bibr" target="#b4">[5]</ref> achieves a higher accuracy than our method, our method is significantly faster (10 seconds vs. 230 seconds). In contrast to <ref type="bibr" target="#b4">[5]</ref>, we do not perform fine-tuning of the person detector on the MPII Multi-Person Pose Dataset and envision that doing this will lead to further improvements. We therefore compare with two additional approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> when using GT bounding boxes of the persons. The results for <ref type="bibr" target="#b5">[6]</ref> are taken from <ref type="bibr" target="#b1">[2]</ref>. Our approach outperforms both methods by a large margin. <ref type="table" target="#tab_3">Table 3</ref>, we report our results on all test images of the MPII Multi-Person Pose Dataset. Our method achieves 43.1% mAP. While the approach [2] cannot be evaluated on all test images due to the high computational complexity of the model, <ref type="bibr" target="#b4">[5]</ref> reports a higher accuracy than our model. However, if we compare the run-times in <ref type="table" target="#tab_2">Table  2</ref> and <ref type="table" target="#tab_3">Table 3</ref>, we observe that the run-time of <ref type="bibr" target="#b4">[5]</ref> doubles on the more challenging test set (485 seconds per image). Our approach on the other hand requires only 10 seconds in all evaluation settings and is around 50 times faster. If we use N = 1, our approach is 160 times faster than <ref type="bibr" target="#b4">[5]</ref>. Using the torso annotation (GT Torso) as person detections results again in a significant improvement of the accuracy (62.2% vs. 43.1% mAP). Some qualitative results can be seen in <ref type="figure" target="#fig_3">Figure 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finally in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work we have presented an approach for multi-person pose estimation under occlusions and truncations. Since the global modeling of poses for all persons is impractical, we demonstrated that the problem can be formulated by a set of independent local joint-to-person association problems. Compared to global modeling, these problems can be solved efficiently while still being effective for handling severe occlusions or truncations. Although the accuracy can be further improved by using a better person detector, the proposed method already achieves the accuracy of a state-of-the-art method, while being 6,000 to 19,000 times faster.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the proposed method. We detect persons in an image using a person detector (a). A set of joint candidates is generated for each detected person (b). The candidates build a fully connected graph (c) and the final pose estimates are obtained by integer linear programming (d). (best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Example of target score maps for the head, neck and left shoulder. The target score maps for the first stage include the joints of all persons (left). The target score maps for all subsequent stages only include the joints of the primary person. Examples of score maps provided by different stages of the CPM. The first stage of CPM uses only local image evidence and therefore provides high confidence scores for the joints of all persons in the image. Whereas all subsequent stages are trained to provide high confidence scores only for the joints of the primary person while suppressing the joints of other persons. The primary person is highlighted by a yellow dot in the first row. (best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Impact of the parameter ? in (21) on the pose estimation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Some qualitative results for the MPII Multi-Person Pose Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of pose estimation results (AP) with state-of-the-art approaches on 288 images<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell>Setting</cell><cell cols="4">Head Shoulder Elbow Wrist Hip Knee Ankle Total time (s)</cell></row><row><cell>Ours</cell><cell>58.4</cell><cell>53.9</cell><cell>44.5 35.0 42.2 36.7 31.1 43.1</cell><cell>10</cell></row><row><cell>DeeperCut [5]</cell><cell>78.4</cell><cell>72.5</cell><cell>60.2 51.0 57.2 52.0 45.4 59.5</cell><cell>485</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Using GT ROIs</cell><cell></cell></row><row><cell cols="2">Ours + GT Torso 85.6</cell><cell>79.4</cell><cell>62.9 48.9 62.6 51.9 43.9 62.2</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Pose estimation results (AP) on the withheld test set of the MPII Multi-Person Pose Dataset.the dataset. This results in a significant improvement in accuracy from 49.3% to 76.9% mAP, showing that a better person detector would improve the results further.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Measured on a 2GHz Intel(R) Xeon(R) CPU with a single core and NVidia Geforce GTX Titan-X GPU.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>The work was partially supported by the ERC Starting Grant ARCA (677650).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines. In: CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<editor>BMVC.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thorm?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parsing occluded people by flexible compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">We are family: Joint pose estimation of multiple persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<title level="m">Pictorial structures for object recognition. IJCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved human parsing with a full relational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative appearance models for pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Body parts dependent joint regressors for human pose estimation in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Articulated part-based model for joint object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>3d pictorial structures revisited: Multiple human pose estimation. TPAMI</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>TNN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACM TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
