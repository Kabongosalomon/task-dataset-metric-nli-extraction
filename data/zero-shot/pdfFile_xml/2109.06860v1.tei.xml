<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Da</surname></persName>
							<email>da.yin@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Liunian</surname></persName>
							<email>liunian.harold.li@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
							<email>kwchang@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Commonsense is defined as the knowledge that is shared by everyone. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenarios of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models' ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard multimodal commonsense benchmark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Commonsense reasoning endows machines with high-level reasoning ability to understand situations with implicit commonsense knowledge. Suppose that there is a scene where a woman is wearing a bridal gown at a party. An ideal AI system with commonsense knowledge should be able to infer that this woman is attending a wedding and likely to be the bride.</p><p>Recently, the field of commonsense reasoning is progressing with the development of large-scale benchmark datasets <ref type="bibr" target="#b31">(Zellers et al., 2018;</ref><ref type="bibr" target="#b27">Talmor et al., 2019)</ref>, intended to cover a wide range of commonsense knowledge, such as physical interactions <ref type="bibr" target="#b3">(Bisk et al., 2020)</ref>, social conventions <ref type="bibr" target="#b23">(Sap et al., 2019)</ref>, and commonsense grounded in vision <ref type="bibr" target="#b30">(Zellers et al., 2019)</ref>.</p><p>However, existing benchmarks are often composed by data from sources in certain regions (e.g., Western movies) and overlook the differences across groups in different regions 1 due to factors including cultural differences. In the aforementioned wedding example, while brides are usually in white in Western weddings, they often wear red and their faces are covered with a red cloth in traditional Chinese weddings. If a model is unaware of regional characteristics or incapable of capturing the nuanced regional characteristics, it leads to a disparity in performance across different regions <ref type="bibr">(Acharya et al., 2020)</ref>. This motivates us to study how well a model trained on existing commonsense annotations can generalize to tasks requiring commonsense beyond Western regions.</p><p>In this paper, we mainly focus on regional commonsense with visual scenes. As shown in <ref type="figure">Figure 1</ref>, the three images all describe a wedding but the dresses of the grooms and brides are different, reflecting the regional characteristics of the wedding scenario. In this paper, we introduce a new evaluation benchmark, Geo-Diverse Visual Commonsense Reasoning (GD-VCR), following the settings of the visual commonsense reasoning (VCR) task <ref type="bibr" target="#b30">(Zellers et al., 2019)</ref>. VCR consists of multiple-choice questions paired with images 1 Due to resource constraints, we use regions as a proxy to evaluate commonsense among different groups. We note that groups of individuals in the same region may have different beliefs, cultures, and behaviors. Please see discussion in the section of Ethical Considerations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>West</head><p>East Asia South Asia <ref type="figure">Figure 1</ref>: Examples in GD-VCR. The three images are all about weddings but from different regions (left-toright order: Western, East Asian, South Asian). Current Vision-and-Language models perform well on answering questions about Western weddings but often make mistakes when encountering wedding scenarios in other regions.</p><p>extracted from movies or TV series primarily in Western regions. GD-VCR includes 328 images, which are mainly sourced from movies and TV series in East Asian, South Asian, African, and Western countries. The images are paired with 886 QA pairs, which need to be answered with geo-diverse commonsense and thorough understanding of the images. An example is given in <ref type="figure">Figure 1</ref>. GD-VCR benchmark addresses geo-diverse commonsense, such as "What are [person1] and [person2] participating?". With the help of these questions, it can manifest how models behave differently and reveal potential issues with geographical bias in commonsense reasoning. GD-VCR is one of the first benchmarks to evaluate model's reasoning ability on the task which requires geo-specific commonsense knowledge. We first study how well a model trained on VCR can generalize to questions involving geospecific commonsense. Experimenting with two pre-trained vision-and-language (V&amp;L) models, Vi-sualBERT <ref type="bibr" target="#b15">(Li et al., 2019)</ref> and ViLBERT <ref type="bibr" target="#b18">(Lu et al., 2019)</ref>, we observe that two models achieve 64%-68% accuracy over the QA pairs on images from Western regions, while their accuracy on images from East Asian region ranges around 45%-51%. The significant performance disparity suggests that the commonsense learned in these models cannot be generalized well across different regions.</p><p>We further investigate the reasons why the model exhibits such disparity based on the results of Vi-sualBERT. We first find that the performance gap on the images from Western and non-Western regions is large on the scenarios involving regional characteristics, such as weddings, religion and festivals. We also discover that the disparity is related to the reasoning difficulty of QA pairs. On the QA pairs only requiring basic visual recognition, e.g., "What's [person3] wearing? [person3] is wearing a suit.", the model achieves relatively similar performance over the four regions; however, the gap enlarges when the questions involve higherlevel reasoning with commonsense and rich visual contexts.</p><p>By presenting the GD-VCR benchmark, we call upon the researchers to empower AI systems with geo-diverse commonsense such that they are capable of conducting high-level reasoning on data from different regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Commonsense Reasoning Benchmarks. Recently, there has been an emergence of commonsense reasoning benchmarks <ref type="bibr" target="#b30">(Zellers et al., 2019;</ref><ref type="bibr" target="#b27">Talmor et al., 2019;</ref><ref type="bibr" target="#b23">Sap et al., 2019;</ref><ref type="bibr" target="#b10">Huang et al., 2019;</ref><ref type="bibr" target="#b2">Bhagavatula et al., 2020;</ref><ref type="bibr" target="#b3">Bisk et al., 2020)</ref>, which cover a great variety of commonsense knowledge including visual, social, physical, and temporal commonsense. However, these commonsense benchmarks are mostly constructed by annotators from certain regions (e.g., the US and UK) using specific languages (e.g., English). XCOPA <ref type="bibr" target="#b21">(Ponti et al., 2020</ref>) and X-CSR <ref type="bibr" target="#b16">(Lin et al., 2021)</ref> are two multilingual benchmarks, but most samples in both benchmarks are simply translated from English and cannot reflect the regional characteristics. Different from previous benchmarks, GD-VCR focuses on geo-diverse commonsense instead of viewing commonsense as a universal monolith.</p><p>Vision-and-Language Tasks. A long line of research seeks to build vision-and-language datasets that test a model's ability to understand the visual world and how it is grounded in natural language. The tasks take on various forms, such as phrase grounding <ref type="bibr" target="#b12">(Kazemzadeh et al., 2014;</ref><ref type="bibr" target="#b20">Plummer et al., 2015)</ref>, visual question answering <ref type="bibr" target="#b1">(Antol et al., 2015;</ref><ref type="bibr" target="#b9">Goyal et al., 2017)</ref>, and visual reasoning <ref type="bibr" target="#b30">(Zellers et al., 2019;</ref><ref type="bibr" target="#b26">Suhr et al., 2019)</ref>. To solve these tasks, a wide range of visual grounding skills are required. However, in existing tasks, little consideration is taken into reasoning on the images with regional characteristics.</p><p>Geographic Bias. Geographic bias is a serious issue that may cause harmful effects on certain groups of people. In computer vision, researchers <ref type="bibr" target="#b24">(Shankar et al., 2017;</ref><ref type="bibr" target="#b6">de Vries et al., 2019)</ref> find that most images from two large-scale image datasets, ImageNet <ref type="bibr" target="#b7">(Deng et al., 2009</ref>) and OpenImages <ref type="bibr" target="#b14">(Krasin et al., 2017)</ref>, are amerocentric and eurocentric. When a model trained on these datasets is applied to images from other regions, the performance will drop drastically. There also exists geographic bias in language technology. For example, it underlies natural language processing <ref type="bibr" target="#b4">(Blodgett et al., 2016;</ref><ref type="bibr" target="#b11">Jurgens et al., 2017;</ref><ref type="bibr" target="#b8">Ghosh et al., 2021)</ref> and automatic speech recognition <ref type="bibr" target="#b28">(Tatman, 2017;</ref><ref type="bibr" target="#b13">Koenecke et al., 2020)</ref> models. Our work seeks to reveal and test the geographic bias in the visual commonsense reasoning task and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Benchmark Construction</head><p>To build a geo-diverse visual commonsense reasoning benchmark, we design a three-stage annotation pipeline, following the original VCR dataset. 1) We first ask annotators to collect images from movies and TV series in Western, East Asian, South Asian, and African countries. 2) We request annotators to design questions and write down the right answers according to the collected images. 3) We generate answer candidates automatically and formulate multiple-choice questions. The overview of our pipeline is illustrated in <ref type="figure">Figure 2</ref>. We elaborate on the three stages in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Collection</head><p>In the image collection stage, we request annotators to follow two principles: Images with Regional Characteristics. In our annotation instruction, we require that the collected images should have representative scenarios containing cultural elements of the annotators' regions. We further recommend annotators choose scenarios that are ubiquitous but have specific characteristics across regions, e.g., wedding, funeral, festival, religion events, etc. For the purpose of analysis, during image collection, annotators are required to write down keywords on each of their collected images for categorization. For example, the keywords of the middle image in <ref type="figure">Figure 1</ref> are labeled as "wedding, soldier, bride, groom, couple, war, countryside".</p><p>Sources of Images. Follow the settings of the original VCR dataset, we ask annotators to select diverse and informative images by taking screenshots from movies, TV series, documentaries, and movie trailers from websites including Youtube 2 , BiliBili 3 , IQIYI 4 , etc. These videos usually include various scenarios and rich contents containing a large amount of actions and human interactions. Note that our collected images from Western regions share the same source 5 with those in the original VCR development set. We use them as a control set in the experiments. Details are in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">QA Pair Annotation</head><p>We recruit another batch of annotators who are familiar with the culture in one of the four regions to annotate QA pairs upon the collected images in English. The annotation stage is divided into two parts: 1) designing questions according to the image contents; 2) annotating the correct answers of the questions.</p><p>Following the pre-processing of the original VCR dataset, we first apply the Mask R-CNN object detector <ref type="bibr" target="#b37">6</ref> to mark bounding boxes of objects in each image, and the annotators can use the labels (e.g., person, car, bowl, etc.) to design QA pairs. <ref type="figure">Figure 2</ref>: Overall annotation pipeline. It is divided into three stages: Stage 1 is to collect images with regional characteristics; Stage 2 is to design QA pairs based on the detected objects; Stage 3 is to generate answer candidates to complete the dataset with the help of the answer choices in the original VCR development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Designing Questions</head><p>Annotators are asked to design questions based on the following three instructions.</p><p>Usage of the Detected Objects. Annotators are requested to choose the named objects in the bounding boxes to construct questions. As shown in <ref type="figure">Figure 1</ref>, annotators can design questions such as "What is the relationship between [person1] and [person2] ?". This requirement is aligned with the question design in the original VCR dataset.</p><p>High-Order Cognitive Questions. Following the original VCR dataset, we ask annotators to design high-order cognitive questions which require geo-specific commonsense knowledge and visual understanding to be answered. Take the rightmost image in <ref type="figure">Figure 1</ref> as an example. "Why is [person11] so happy?" is a qualified question because people have to observe the surroundings including [person1] and [person2] 's wearing and others' facial expression, and conclude that it is a wedding. Moreover, [person1] is wearing a wedding dress and others are celebrating for her. Thus, people can infer that [person11] is happy because it is [person1] and [person2] 's wedding. Overall, to answer this question, we need to combine the image context and commonsense knowledge, and reason with multiple turns. A disqualified example of question is "What is [person3] wearing?" in the left image of <ref type="figure">Figure 1</ref>. It is defined as a low-order cognitive question because it can be directly answered by recognizing that the woman is wearing a suit. This type of question does not need commonsense reasoning based on the context information.</p><p>Question Templates. Since models trained on the original VCR dataset will be evaluated on GD-VCR dataset, we attempt to eliminate the discrepancy of questions used between the original VCR dev set and GD-VCR to mitigate the effect of different question formats. Hence, we ask annotators to design questions by referring to the templates summarized from the original VCR development set. To generate the templates, we first replace nouns, verbs, adjectives, and adverbs of the questions in VCR development set with their POS tags (e.g., NN, VB, JJ, etc.) labeled by NLTK 7 , while keeping question words such as "what", "why" and "how", and auxiliary verbs like "is", "do" and "have". In this way, we remove the terms associated with specific questions, while keeping the general patterns.</p><p>We then apply K-Means <ref type="bibr" target="#b19">(MacQueen et al., 1967)</ref> algorithm <ref type="bibr" target="#b38">8</ref> to the question patterns, and manually summarize 17 templates, e.g., "What (n.) is sb. v.+ing sth.?", "What is sb.'s relationship with sb.?". Details of the clustering method and the list of question templates are in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Annotating Correct Answers</head><p>Annotators are required to write down only the right answer for the questions they designed. This is to reduce annotation cost and avoid potential annotation artifacts <ref type="bibr" target="#b30">(Zellers et al., 2019)</ref>. We require that the right answers to the questions should be consistent with the image contents. However, we remind annotators to avoid writing answers that are too specific to the video plots because the answers should be inferred without prior knowledge about the plots. In addition, instead of writing named entities or proper names specific to one region, annotators are required to use common expressions in English. These instructions would help us maintain the difficulty of GD-VCR to a proper extent. Finally, we invite 3 annotators from each QA pair's corresponding regions to validate the correctness of each question and its answer. If 2 of them have an agreement to approve a certain QA pair, we keep it in the final dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Candidate Generation</head><p>In this stage, for each question, we generate three wrong answer candidates (i.e., wrong answer choices), to construct a 4-way multiple-choice QA example. We follow the answer candidate generation algorithm in VCR <ref type="bibr" target="#b30">(Zellers et al., 2019)</ref>:</p><formula xml:id="formula_0">Answer Candidate Pool.</formula><p>Instead of generating answer candidates from scratch by language models, we leverage the right choices in the original VCR development set, and treat them as an answer candidate pool. All the answer candidates of GD-VCR are derived from this pool.</p><p>Answer Candidate Selection. The principles for selecting answer candidates from the pool are two-fold: 1) answer candidates should be relevant to questions; 2) they should be dissimilar with the right choice and other selected answer candidates, so that they would not be the right answer incidentally. Details of the candidate selection algorithm are in Appendix B.2. <ref type="table" target="#tab_0">Table 1</ref> summarizes the statistics of the GD-VCR benchmark and the original VCR development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dataset Statistics</head><p>Texts. We observe that the average lengths of questions and answers in GD-VCR are similar to those in the original VCR development set. Aside from the text lengths, we also consider out-ofvocabulary (OOV) rate with respect to the original VCR training set. This indicates how much unseen knowledge (e.g., entities specific to certain region) are involved in GD-VCR. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we find that the OOV rate of the entire GD-VCR dataset is 6.75%, while that of the original VCR development set is 12.70%. This shows that GD-VCR has a similar distribution of the vocabulary with the original VCR dataset and the difficulty of GD-VCR does not come from the vocabulary gap.</p><p>Images. The average number of the detected objects 10.18 is similar to that of the original VCR development set. Moreover, since the objects mentioned in questions and answers are directly relevant to the reasoning process on each QA pair, we consider statistics of the average number of the relevant objects. The average number of relevant objects in image in GD-VCR is 2.38, which nearly equals that of the VCR development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Performance and Human</head><p>Evaluation over GD-VCR</p><p>We are interested in the following questions: 1) Can a model trained on the original VCR dataset (mostly Western scenarios) generalize well to solve reasoning questions require commonsense specific to other regions? 2) Do humans show similar trend when dealing with questions require regional commonsense that they are not familiar with? Our experiments are conducted with two Visionand-Language models VisualBERT <ref type="bibr" target="#b15">(Li et al., 2019)</ref> and ViLBERT <ref type="bibr" target="#b18">(Lu et al., 2019)</ref>. We fine-tuned the two pre-trained models on the original VCR training set and evaluate them on GD-VCR. All the experimental results are the average of 3 runs with different seeds. Implementation details are listed in Appendix C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Performance</head><p>We apply the two models on GD-VCR benchmark to study how well the two models can generalize. Results are shown in <ref type="table" target="#tab_1">Table 2</ref>. Key observations are summarized as follows:</p><p>Western vs. Non-Western Regions. We find that the models perform significantly worse on the images from non-Western regions. According to VisualBERT's results, we observe that the gap between Western and South Asian regions is 9.43%, while it greatly amplifies to 16.84% and 18.86% when it comes to the comparison with African and East Asian regions, respectively. These results reflect significant differences of models' reasoning ability on the examples from different regions.</p><p>VisualBERT vs. ViLBERT. We find that ViL-BERT outperforms VisualBERT by 6.04% on GD-VCR. We conjecture that the higher performance of ViLBERT partly results from the pre-training data:</p><p>VisualBERT is pre-trained on COCO Captions which includes 80K images <ref type="bibr" target="#b5">(Chen et al., 2015)</ref>, while ViLBERT's pre-training data are from Conceptual Captions containing 3.3M images <ref type="bibr" target="#b25">(Sharma et al., 2018)</ref>. The larger coverage of image contents may help ViLBERT generalize to the images with regional characteristics. It is also shown that the performance gap over the images from Western and non-Western regions shrinks when applying ViL-BERT. However, the gap is still significant, ranging from 3.70% to 17.09%.</p><p>Western v.s. Original VCR Dataset. We observe a performance gap around 2%-6% between images from Western and the original VCR dataset. We speculate that the gap is caused by one main aspect: the requirements in the image collection stage are slightly different. We expect to collect images containing regional characteristics, including cultural elements like customs. It may add to the complexity of the reasoning process as cultural commonsense is needed. However, the gap is much smaller compared with the gap between Western and other regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Evaluation</head><p>Apart from the model performance, we investigate how well human beings perform on GD-VCR. We randomly select 40 QA pairs of each region, and there are 160 QA pairs in total for evaluation. We recruit qualified annotators living in United Kingdom and United States from MTurk 9 to accomplish the evaluation. Assuming them to be familiar with Western culture, we are interested to see their performance on the examples from other regions. Human evaluation results are shown in <ref type="table" target="#tab_1">Table 2</ref>. We notice that human performance is much better than models. More importantly, we observe that the performance gap among regions is much smaller than that of two V&amp;L models. For example, annotators from Western can achieve 87.93% accuracy on East Asian images, and the gap reduces to 8.18% from 18.86% and 17.09%. It implies that human beings are more capable of applying their commonsense knowledge and transferring it to the comprehension in geo-diverse settings, while models are still far away from this level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analyses of Performance Disparity</head><p>As we observe large performance gaps between Western and non-Western data in Section 4.1, in this section, we inspect the pre-trained V&amp;L model to analyze the reasons behind such performance disparity on two aspects, 1) regional differences of scenarios and 2) reasoning level of QA pairs. We analyze the VisualBERT model, since its performance gap is more evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Regional Differences of Scenarios</head><p>As shown in <ref type="figure">Figure 1</ref>, even the same scenarios such as wedding can take different visual forms across regions. Motivated by this, we investigate how large the performance gap is when we apply VisualBERT to the images of the same scenarios happening in different regions.</p><p>We select the scenarios that frequently appear in the annotated keyword set of GD-VCR. Specifically, we choose the scenarios which appear at least 10 times in not only Western images, but also the images from any of the other regions. We visualize each scenario's performance gap between the images from Western and non-Western regions in <ref type="figure">Figure 3</ref>. The scenarios whose gap is above 8% are colored in red; otherwise, they are labeled by blue.</p><p>As shown in <ref type="figure">Figure 3</ref>, we find that on the scenarios which often contain regional characteristics (e.g., wedding, religion, festival), the performance gap is much larger, from 8.28% to 23.69%. One interesting finding is that, aside from festival, wedding and religion, which are generally considered to be different across regions, the gap is considerably large over the scenarios involving customers. We speculate that it is also due to regional characteristics. As shown in <ref type="figure">Figure 5</ref>   <ref type="figure">Figure 3</ref>: Visualization of the performance gap on images of the same scenarios in Western and non-Western regions. The larger the characters, the larger the performance gap over the scenarios. The red and blue words are the scenarios whose performance gap is above and below 8%, respectively. Detailed accuracy on these scenarios is shown in Appendix D.</p><p>Asia and South Asia, many customers would buy things from the local merchants along the streets, while in Western regions, customers typically shop in supermarkets and restaurants. The visual discrepancy may result in errors on the "customer" scenarios in GD-VCR.</p><p>On the other hand, for the scenarios such as party, restaurant and student, the gap is only 0.42%, 1.29% and 1.12%, respectively. We notice that these scenarios are more similar across regions. For example, parties are related to actions like drinking, dancing, and celebration, which are common and take on similar visual forms across regions. Such similarity may contribute to model's high transfer performance on "party".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reasoning Level of QA Pairs</head><p>The QA pairs in GD-VCR are high-order cognitive QA pairs, which require several reasoning steps to be solved. For example, to infer that " [person1]</p><p>and [person2] are in a wedding" in the middle image of <ref type="figure">Figure 1</ref>, human beings must first recognize basic facts such as [person1] is wearing in red and her face is covered by a red cloth. Only by combining the recognized facts and regional commonsense can human make correct predictions. Therefore, the model's failure on these high-order cognitive QA pairs from non-Western regions may be attributed to two reasons: 1) the model fails to recognize the basic facts from the image, 2) or the model succeeds on the basic facts but fails eventually due to lack of geo-specific commonsense.</p><p>To determine at which stage the model fails to generalize, we aim to answer the following two questions: Q1. Can the model perform similarly on recognizing basic visual information in the images from different regions? Q2. Is the performance  <ref type="table">Table 3</ref>: VisualBERT's accuracy (%) on low-order and high-order cognitive QA pairs. "Gap (West)" denotes performance gap over the QA pairs of images from Western and non-Western regions. "|Low ? High|" denotes the performance gap between low-order and high-order cognitive QA pairs from the same regions.</p><p>disparity attributed to the failure of understanding more advanced or basic visual information?</p><p>According to the standard of reasoning level discrimination mentioned in Section 3.2.1, we categorize QA pairs into two types: low-order and high-order cognitive QA pairs. Low-order cognitive QA pairs correspond to the inquiry on basic visual information, while high-order QA pairs involve more advanced information. Our analysis is mainly concerned with the two types of QA pairs. Q1. Can model perform similarly on understanding basic visual information across regions? We evaluate model's performance on loworder cognitive QA pairs to analyze this aspect.</p><p>As mentioned in Section 3.2.1, GD-VCR is composed of high-order cognitive QA pairs but without low-order pairs. Therefore, we additionally annotate low-order cognitive QA pairs on the images of GD-VCR. Specifically, we randomly select 30 images per region and design low-order QA pairs based on these selected images. Finally, we collect 22 QA pairs on Western images, 26 on East Asian images, 16 on South Asian images, and 22 on African images.</p><p>Results are shown in <ref type="table">Table 3</ref>. We observe that the performance over the low-order cognitive QA pairs is all around 60% for the four regions. Performance over Western images is still the highest among the four regions. But note that the performance gap between the images from Western and non-Western regions is not so large as the overall gap shown in <ref type="table" target="#tab_1">Table 2</ref>. For example, the overall performance gap between East Asia and Western is around 19%, but it decreases to 6.41% when the model deals with simpler situations. It demonstrates that, when encountering the QA pairs focusing on simple recognition, VisualBERT can narrow down the gap on the images from different regions.</p><p>In other words, VisualBERT shows more similar ability to process basic visual information, no mat-ter where the images are from.</p><p>Q2. Is the performance disparity attributed to understanding on more advanced or basic visual information? We analyze the performance over low-order and high-order cognitive QA pairs. For a fair comparison, both types of QA pairs share the same images.</p><p>Results are shown in <ref type="table">Table 3</ref>. We observe that VisualBERT's performance over low-order cognitive QA pairs is higher than that over high-order QA pairs on images from East Asia, South Asia, and Africa. Especially, on the images from African regions, the performance gap between these two types of QA pairs is 15.71%. Furthermore, from <ref type="table">Table 3</ref>, we notice that the performance gap between Western and non-Western regions on high-order cognitive QA pairs is much larger than that on low-order QA pairs. For the images from East Asian regions, the performance gap with regard to Western regions on low-order pairs is 6.41%. The gap amplifies to 16.13% when VisualBERT is applied to high-order QA pairs. For African images, the gap changes rapidly from 9.09% to 26.25%. These results show that VisualBERT trained on VCR lacks the ability to perform complex reasoning on the scenarios in non-Western regions. We hope our findings could inspire future work to model high-level reasoning process better with geo-diverse commonsense knowledge in commonsense reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a new benchmark, GD-VCR, for evaluating V&amp;L models' reasoning ability on the QA pairs involving geo-diverse commonsense knowledge. Experimental results show that the V&amp;L models cannot generalize well to the images regarding the regional characteristics of non-Western regions. Based on VisualBERT's results, we find that 1) the scenarios such as wedding, religion and festival, which require geo-diverse commonsense knowledge to be understood, and 2) the reasoning difficulty of QA pairs are highly associated with the performance disparity. For broader impact, we hope that the GD-VCR benchmark could broaden researchers' vision on the scope of commonsense reasoning field and motivate researchers to build better commonsense reasoning systems with more inclusive consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Xiao Liu, Ming Zhong, Te-Lin Wu, Masoud Monajatipoor, Nuan Wen, and other members of UCLANLP and UCLA PlusLab groups for their helpful comments. We also greatly appreciate the help of anonymous annotators for their effort into constructing the benchmark. This work was partially supported by DARPA MCS program under Cooperative Agreement N66001-19-2-4032. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>In this work, we propose a geo-diverse visual commonsense reasoning dataset GD-VCR. Since the paper introduces new dataset, we discuss the potential ethical issues about data collection.</p><p>Intellectual Property and Privacy Rights. We ensure that intellectual property and privacy rights of the original authors of videos and recruited annotators are respected during the dataset construction process with permission of licence 1011 . We also claim that the collected data would not be used commercially.</p><p>Compensation for Annotators. We recruit annotators from Amazon Mechanical Turk platform 12 and college departments of foreign languages and culture. In image collection stage, we paid annotators $0.5-0.7 per collected image. In QA pair annotation stage, the payment is $0.2 per QA pair. For validation and human evaluation, we pay them $0.02-0.03 per QA pair. The pay rate is determined by a preliminary annotation trial to ensure the average hourly pay rate is around $12 per hour. Potential Problems. Although we have considered the potential geographic bias in the benchmark construction process, GD-VCR may still contain unwanted bias. First, due to the resource constraints, GD-VCR dataset is unable to cover diverse regional characteristics at once. For instance, we do not take Southeast Asian, Arabic and Latin American regions into account. Moreover, even groups in the same region may have different beliefs. For the regions like Africa, the regional differences between West Africa, East Africa, and North Africa are evident. However, in GD-VCR, the images from Africa are mainly sourced from East Africa. It inevitably introduces geographic bias into our benchmark. More fine-grained analysis should be conducted to scale up this study, especially before the visual commonsense reasoning model is used in the commercial product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Details of Annotation Pipeline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Image Collection</head><p>In addition to the requirements mentioned in Section 3.1, we have additional requirements on the contents, quality, and sources of images. The image should have at least two people, and should not be grainy and blurry. We require annotators to choose movies, TV series, documentaries and trailers which are free to access and do not have copyright issues. Together with the images and their keywords, we also collect video names, screenshot timestamps, and the links of videos. It is to help the annotators in later stages better understand the image contents with video contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 QA Pair Annotation</head><p>Question Template List. As mentioned in Section 3.2, we recommend annotators to design questions based on question templates. The template list is shown in <ref type="table" target="#tab_7">Table 5</ref>. For clustering methods to summarize templates, we use K-Means algorithm to cluster similar question patterns. Specifically, the maximum number of clusters is at most 20 clusters. The algorithm will automatically stop until the 200-th iteration.</p><p>Other Annotation Details. To pursue diversity of question types, we require annotators to design questions via different question templates. Besides, we ask annotators to avoid annotating too simple answers, such as "yes" and "no".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Answer Candidate Generation Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Relevance Model</head><p>Relevance model is to evaluate the relevance score between questions and answers. Higher relevance scores indicate that the answers are more relevant with the questions. We train the relevance model based on pre-trained BERT-base parameters <ref type="bibr" target="#b29">(Wolf et al., 2020</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Initialization: score ? 0, minscore ? +?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>if Rel(Q, Ai) ? 0.9 then 6:</p><p>for each ansListi in ansList do 7:</p><p>similarity ? Sim(ansListi, Ai) 8:</p><p>if similarity ? 0.2 then 9:</p><p>score ? score + similarity 10:</p><p>else 11:</p><p>score ? score + 10 12:</p><p>end if 13: end for 14:</p><p>if score &lt; minscore then 15:</p><p>minscore ? score 16:</p><p>Wt The pseudo code of the algorithm is shown in Algorithm 1. The two principles for selecting answer candidates are as follows: for each QA pair, 1) they should be relevant with the questions; 2) they should not be similar with the right choices and the selected answer candidates. The model that computes similarity is the "stsb-roberta-base" model <ref type="bibr" target="#b22">(Reimers and Gurevych, 2019)</ref>   D Accuracy on the QA Pairs Involving Specific Scenarios <ref type="table">Table 4</ref> shows VisualBERT's accuracy of the QA pairs involving specific scenarios depicted in <ref type="figure">Figure 3</ref>. Besides the study on GD-VCR, we also make comparison between model performance on GD-VCR and the original VCR development set. We select the scenarios frequently appearing in both GD-VCR and the original VCR development set. Results are shown in <ref type="table">Table 6</ref>. We observe that on the images involving scenarios such as funeral, Vi-sualBERT's performance gap is nearly 25%, which is considerably large. The results further demonstrate that the model is still incapable of tackling the QA pairs which are involving cultural differences behind scenarios well.   <ref type="table">Table 6</ref>: Accuracy (%) on the images involving the same scenarios from the original VCR dataset and non-Western regions from GD-VCR dataset, respectively. <ref type="figure" target="#fig_2">Figure 4</ref> shows the overall statistics of keyword occurrences in GD-VCR benchmark. There are 693 keywords in total, showing the diversity of the scenarios covered by GD-VCR dataset. Besides, we observe that the keywords whose corresponding scenarios have evident regional differences, such as "wedding", "religion", "groom", "bride", appear frequently in GD-VCR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Keywords in GD-VCR Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Examples in GD-VCR Dataset</head><p>In this section, we showcase several examples of GD-VCR in detail. Aside from the images about "wedding" in <ref type="figure">Figure 1</ref>, we manifest the images regarding to "customer" and "funeral" from the four regions we study. In <ref type="figure">Figure 5</ref> and <ref type="figure">Figure 6</ref>, we can observe the regional characteristics from the selected images. Furthermore, we visualize Visual-BERT's prediction on each QA pair.  <ref type="figure">Figure 5</ref>: Examples of the images regarding "customer". Left-to-right order: Western, South Asia, East Asia. We visualize the prediction of the VisualBERT model fine-tuned on the original VCR training set. The blue blocks denote the right answer choices. If red block appears, it means that VisualBERT wrongly predict the answer. The rightmost value indicates the probability of the corresponding choices being selected by VisualBERT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>West</head><p>East Asia South Asia Africa <ref type="figure">Figure 6</ref>: Examples of the images regarding "funeral" or "death". Left-to-right order in the first row: Western, East Asia; Left-to-right order in the second row: South Asia, Africa.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Question:</head><label></label><figDesc>What are [person3] and [person4] participating in? -A. ...... -B. They are in a wedding. -C. ...... -D. ...... Question: What are [person1] and [person2] participating in? -A. ...... -B. They are in a wedding. -C. ...... -D. ...... Question: What are [person1] and [person2] participating in? -A. ...... -B. They are in a wedding. -C. ...... -D. ......</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Statistics of keyword occurrences. "Others" denotes the average occurrences of the keywords appearing less than 20 times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets # Images # QA Pairs Avg. Len. of Ques. Avg. Len. of Ans. Avg. # Obj. Avg. # Relevant Obj. OOV Rate Statistics of the GD-VCR benchmark. The top half of the table is the overall statistics of GD-VCR and the original VCR development set. The bottom half includes the subsets of each region in GD-VCR.</figDesc><table><row><cell>Original VCR</cell><cell>9929</cell><cell>26534</cell><cell>6.77</cell><cell>7.67</cell><cell>10.34</cell><cell></cell><cell>2.39</cell><cell>12.70%</cell></row><row><cell>GD-VCR</cell><cell>328</cell><cell>886</cell><cell>7.38</cell><cell>7.68</cell><cell>10.18</cell><cell></cell><cell>2.38</cell><cell>6.75%</cell></row><row><cell>? West</cell><cell>100</cell><cell>275</cell><cell>7.36</cell><cell>7.19</cell><cell>11.10</cell><cell></cell><cell>2.28</cell><cell>3.44%</cell></row><row><cell>? East Asia</cell><cell>101</cell><cell>282</cell><cell>7.59</cell><cell>7.59</cell><cell>9.57</cell><cell></cell><cell>2.42</cell><cell>4.50%</cell></row><row><cell>? South Asia</cell><cell>87</cell><cell>221</cell><cell>6.85</cell><cell>8.00</cell><cell>10.29</cell><cell></cell><cell>2.12</cell><cell>5.49%</cell></row><row><cell>? Africa</cell><cell>40</cell><cell>108</cell><cell>7.98</cell><cell>8.54</cell><cell>9.29</cell><cell></cell><cell>3.03</cell><cell>7.34%</cell></row><row><cell cols="2">Datasets</cell><cell cols="2">Human Text-only BERT</cell><cell cols="4">VisualBERT Acc. Gap (West) Acc. Gap (West) ViLBERT</cell></row><row><cell cols="2">Original VCR</cell><cell>-</cell><cell>53.8  *</cell><cell>70.10</cell><cell>+5.73</cell><cell>69.84</cell><cell>+2.57</cell></row><row><cell cols="2">GD-VCR</cell><cell>88.84</cell><cell>35.33</cell><cell>53.95</cell><cell>-10.42</cell><cell>59.99</cell><cell>-7.28</cell></row><row><cell cols="2">? West</cell><cell>91.23</cell><cell>37.09</cell><cell>64.37</cell><cell>0.00</cell><cell>67.27</cell><cell>0.00</cell></row><row><cell cols="2">? South Asia</cell><cell>92.98</cell><cell>33.48</cell><cell>54.90</cell><cell>-9.43</cell><cell>63.57</cell><cell>-3.70</cell></row><row><cell cols="2">? Africa</cell><cell>87.93</cell><cell>34.26</cell><cell>47.53</cell><cell>-16.84</cell><cell>59.73</cell><cell>-7.54</cell></row><row><cell cols="2">? East Asia</cell><cell>83.05</cell><cell>35.46</cell><cell>45.51</cell><cell>-18.86</cell><cell>50.18</cell><cell>-17.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Accuracy (%) on the subset of each region in GD-VCR and the original VCR development set. With regard to Western regions, two models' performance gap of the original VCR development set and other regions is shown. We also report human's accuracy (%) over each region subset. Annotators are from United Kingdom and United States according to MTurk.* denotes the reported result in Zellers et al. (2019).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>www.mturk.com annotations on the images from East Asian regions are partly done by the authors of this work.</figDesc><table><row><cell>The</cell></row><row><cell>10 Fair use on YouTube. support.google.com/</cell></row><row><cell>youtube/answer/9783148?hl=en</cell></row><row><cell>11 Copyright Law of the People's Republic of China (Ar-</cell></row><row><cell>ticle 22). http://www.gov.cn/flfg/2010-02/26/</cell></row><row><cell>content_1544458.htm.</cell></row></table><note>12</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Question Q = {q1, q2, ..., qn}, the question's right answer Corr = {c1, c2, ..., cn}, answer candidate pool A = {A1, A2, ..., Am}, relevance model Rel, similarity model Sim. qi and ci indicate tokens. Output: The whole set of four answer choices ansList of the given question Q, including one right choice Corr and three answer candidates W1, W2, and W3.</figDesc><table><row><cell cols="3">vant with a question or not. The relevance score is</cell></row><row><cell cols="2">the probability of being relevant pairs.</cell><cell></cell></row><row><cell cols="3">B.2 Pseudo Code of Answer Candidate</cell></row><row><cell></cell><cell>Generation Algorithm</cell><cell></cell></row><row><cell cols="3">Algorithm 1 Answer Candidate Generation Alg.</cell></row><row><cell cols="2">1: Initialization: ansList ? {Corr}.</cell><cell></cell></row><row><cell cols="2">2: for t = 1, 2, 3 do</cell><cell></cell></row><row><cell>3:</cell><cell>for each Ai in A (t?1)? m 3 +1: t? m 3</cell><cell>do</cell></row><row><cell>). Specifically, the training data are all</cell><cell></cell><cell></cell></row><row><cell>from the original VCR training set and composed</cell><cell></cell><cell></cell></row><row><cell>by relevant and irrelevant QA pairs. The relevant</cell><cell></cell><cell></cell></row><row><cell>QA pairs are the ones consisting of questions and</cell><cell></cell><cell></cell></row><row><cell>their corresponding right answers; the irrelevant</cell><cell></cell><cell></cell></row><row><cell>pairs are the ones consisting of questions and ran-</cell><cell></cell><cell></cell></row><row><cell>dom answers sampled from the whole set of answer</cell><cell></cell><cell></cell></row><row><cell>choices. We build a binary classifier upon these</cell><cell></cell><cell></cell></row><row><cell>training data to classify whether an answer is rele-</cell><cell></cell><cell></cell></row></table><note>Input:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>from github.com/UKPLab/ sentence-transformers.</figDesc><table><row><cell>C Implementation Details of Fine-tuning</cell></row><row><cell>VisualBERT and ViLBERT</cell></row><row><cell>Following VisualBERT (135.07M parameters) con-</cell></row><row><cell>figuration on VCR 13 , we directly use the model pre-</cell></row><row><cell>trained on COCO (Chen et al., 2015) and original</cell></row><row><cell>VCR training set. The experiments of ViLBERT 14</cell></row><row><cell>(252.15M parameters) is based on the model pre-</cell></row><row><cell>trained on Conceptual Captions (Sharma et al.,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Question template list summarized from the original VCR development set.</figDesc><table><row><cell>2018) and original VCR training set. Both mod-</cell></row><row><cell>els are then fine-tuned for 8 epochs on 4 NVIDIA</cell></row><row><cell>GeForce GTX 1080 Ti GPUs, with learning rate</cell></row><row><cell>2e ? 5. The batch size of VisualBERT and ViL-</cell></row><row><cell>BERT is 32 and 16, and fine-tuning one epoch with</cell></row><row><cell>VisualBERT and ViLBERT costs 5.28 and 6.75</cell></row><row><cell>hours, respectively. For both models, we choose</cell></row><row><cell>the epoch which performs the best on the original</cell></row><row><cell>VCR development set among 8 epochs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>person1] is [person1]'s jobQuestion: Why are [person3] and [person5] so sad[person2] hurt [person1] and [person3]'s hand</head><label></label><figDesc>to serve food to the guests. -C. [person1] wants to get a picture of [person1]. -D. [person1] is on vacation. ? -A. Because the cremated body is their friend. -B. They are avoiding the other people at the party. -C. They are scared and worried for their lives thinking of what they leave behind. -D. They just got their food and are happy that they are finally getting a break from classes and eat something. .. -C. [person2] is doing some kind of medical procedure to [person1] and [person3]. -D. A meeting of business people has just found out some bad news that affect [person2].</figDesc><table><row><cell>Question: Why does [person1] come here?</cell><cell>Question: Why is [person19] here?</cell><cell></cell></row><row><cell>-A. [person1] can't believe his friend's death and wants to apologize. -B. [72.69% 10.28% 0.34% 16.70%</cell><cell>-A. [person19]'s father is dead and [person19] is mourning him. -B. Trying to listen to [person19] talk. -C. [person19] need to investigate graveyards at night. -D. [person19] might be a zoo employee.</cell><cell>32.73% 48.38% 0.99% 17.90%</cell></row><row><cell></cell><cell>Question: Why is [person2] crying?</cell><cell></cell></row><row><cell>18.54% 24.23%</cell><cell cols="2">-A. Because [person2] wants to show sympathy to [person1] and [person3]. -B. 84.49% 0.00%</cell></row><row><cell>58.98%</cell><cell></cell><cell>13.41%</cell></row><row><cell>0.95%</cell><cell></cell><cell>2.10%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">www.nltk.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We concatenate the question words and the POS tags of all the other words (e.g., from "What is [person1] doing?" to "What VBZ NNP VBG?"). Then we use sentence representations of the converted sentences as real-valued vectors in K-Means. The representations are obtained from Sentence-Transformers<ref type="bibr" target="#b22">(Reimers and Gurevych, 2019</ref>) based on RoBERTa-base<ref type="bibr" target="#b17">(Liu et al., 2019)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">The annotators should complete at least 1000 HITs, with an approval rate above 95%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">github.com/uclanlp/visualbert 14 github.com/jiasenlu/vilbert_beta</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Kartik Talamadupula, and Mark A. Finlayson. 2020. An Atlas of Cultural Commonsense for Machine Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acharya</surname></persName>
		</author>
		<idno>abs/2009.05664</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abductive Commonsense Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PIQA: Reasoning about Physical Commonsense in Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Demographic Dialectal Variation in Social Media: A Case Study of African-American English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan O&amp;apos;</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Connor</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1120</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1119" to="1130" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xinlei Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1504.00325</idno>
	</analytic>
	<monogr>
		<title level="m">Microsoft COCO Captions: Data Collection and Evaluation Server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Does Object Recognition Work for Everyone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cross-geographic Bias Detection in Toxicity Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinodkumar</forename><surname>Prabhakaran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the Role of Image Understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2391" to="2401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating Dialectal Variability for Socially Equitable Language Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="51" to="57" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1086</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Racial Disparities in Automated Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><surname>Koenecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Nudell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minnie</forename><surname>Quartey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zion</forename><surname>Mengesha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Toups</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>John R Rickford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharad</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="7684" to="7689" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">OpenImages: A Public Dataset for Large-scale Multi-label and Multi-class Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhyanesh</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://github.com/openimages" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">VisualBERT: A Simple and Performant Baseline for Vision and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyeon</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1274" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="j">RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Visionand-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Some Methods for Classification and Analysis of Multivariate Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Flickr30k Entities: Collecting Region-to-phrase Correspondences for Richer Image-to-sentence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Edoardo Maria Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianchu</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2362" to="2376" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Social IQa: Commonsense Reasoning about Social Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1454</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4463" to="4473" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreya</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimbo</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08536</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1238</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Corpus for Reasoning about Natural Language Grounded in Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1644</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6418" to="6428" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gender and Dialect Bias in YouTube&apos;s Automatic Captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachael</forename><surname>Tatman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-1606</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</title>
		<meeting>the First ACL Workshop on Ethics in Natural Language Processing<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From Recognition to Cognition: Visual Commonsense Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going for a walk&quot;: A Study of Temporal Commonsense Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1332</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3363" to="3369" />
		</imprint>
	</monogr>
	<note>Going on a vacation&quot; takes longer than</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Regions Wedding Festival Religion Bride Groom Restaurant Family Student Customer Party West</title>
		<idno>62.22</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accuracy (%) on the images involving the same scenarios from different regions</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">What (n.) is sb. v.+ing prep. PHRASE?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">What (n.) is sb. v.+ing? 5. What is sb.&apos;s job/occupation?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">What is sb.&apos;s relationship with sb.? 7. Why is sb. v.+ing sth. CLAUSE?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Why is sb. adj</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Why is sb. here? 10. Why is sb. acting adv.? 11. How does sb. feel/look? 12. Where are sb. (v.+ing)? will sb</title>
		<imprint/>
	</monogr>
	<note>go? 17. Where was sb. previously</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
