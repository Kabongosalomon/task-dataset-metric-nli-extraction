<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Video Object Segmentation by Motion-Aware Mask Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Miao</surname></persName>
							<email>bo.miao@research.uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsheng</forename><surname>Gao</surname></persName>
							<email>yongsheng.gao@griffith.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Griffith University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
							<email>ajmal.mian@uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Video Object Segmentation by Motion-Aware Mask Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a self-supervised spatio-temporal matching method, coined Motion-Aware Mask Propagation (MAMP), for video object segmentation. MAMP leverages the frame reconstruction task for training without the need for annotations. During inference, MAMP extracts high-resolution features from each frame to build a memory bank from the features as well as the predicted masks of selected past frames. MAMP then propagates the masks from the memory bank to subsequent frames according to our proposed motion-aware spatio-temporal matching module to handle fast motion and long-term matching scenarios. Evaluation on DAVIS-2017 and YouTube-VOS datasets show that MAMP achieves stateof-the-art performance with stronger generalization ability compared to existing self-supervised methods, i.e., 4.2% higher mean J &amp;F on DAVIS-2017 and 4.85% higher mean J &amp;F on the unseen categories of YouTube-VOS than the nearest competitor. Moreover, MAMP performs at par with many supervised video object segmentation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Video object segmentation (VOS) is a fundamental problem in visual understanding where the aim is to segment objects of interest from the background in unconstrained videos. VOS enables machines to sense the motion pattern, location, and boundaries of the objects of interest in videos, which is useful in a wide range of applications. For example, in video editing, manual frame-wise segmentation is laborious and does not maintain temporal consistency whereas VOS can segment all frames automatically using the mask of one frame as a guide. The problem of segmenting objects of interest in a video using the ground truth object masks provided for only the first frame is referred to as semisupervised VOS. This is challenging because the appearance of objects in a video change significantly due to fast motion, occlusion, scale variation, etc. Moreover, other similar looking non-target objects may confuse the model to segment incorrect objects.</p><p>Semi-supervised VOS techniques fall into two categories: supervised and self-supervised. Supervised approaches <ref type="bibr" target="#b22">(Oh et al. 2019;</ref><ref type="bibr" target="#b43">Yang, Wei, and Yang 2020)</ref> use rich annotation information from training data to learn the model achieving <ref type="bibr">Copyright ? 2022</ref>.  <ref type="figure">Figure 1</ref>: Comparison on DAVIS-2017 validation set with other methods. MAMP outperforms existing self-supervised methods, and is at par with some supervised methods trained with large amounts of annotated data.</p><p>great success in VOS. However, these methods are unattractive given their reliance on accurate pixel-level annotations for training (see <ref type="figure">Fig. 1</ref>), which are expensive to generate. Moreover, supervised approaches struggle to maintain the same performance in the wild. In contrast, self-supervised methods <ref type="bibr" target="#b10">Lai, Lu, and Xie 2020)</ref> learn feature representations based on the intrinsic properties of the video frames, and thus do not require any annotations and can better generalize to unseen objects. Even though the motivations behind existing self-supervised methods are different, they share the same objective of learning to extract general feature representations and construct precise spatiotemporal correspondences to propagate the object masks in video sequences. Exploiting the spatio-temporal coherence in videos, the pretext tasks of self-supervised methods can be designed to either maximize the temporal cyclecorrespondence consistency  or minimize the reconstruction or prediction loss <ref type="bibr" target="#b10">(Lai, Lu, and Xie 2020)</ref>. Once trained, the models are able to extract general feature representations and build spatio-temporal correspondences between the reference and query frames. Therefore, pixels in the query frames can be classified according to the mask labels of their corresponding region of interests (ROI) in the reference frames. Despite their simplicity, existing selfsupervised methods perform poorly in cases of fast motion and long-term matching scenarios. To overcome the above challenges, we propose a selfsupervised method coined motion-aware mask propagation (MAMP). Similar to previous self-supervised methods, MAMP learns image features and builds spatio-temporal correspondences without any annotations during training. During inference, MAMP first leverages the feature representations and the given object masks for the first frame to build a memory bank. The proposed motion-aware spatiotemporal matching module in MAMP then exploits the motion cues to mitigate the issues caused by fast motion and long-term correspondence mismatches, and propagates the mask from the memory bank to subsequent frames. Moreover, the proposed size-aware image feature alignment module fixes the misalignment during mask propagation and the memory bank is constantly updated by the past frames to provide the most appropriate spatio-temporal guidance. We evaluate MAMP on the DAVIS-2017 and YouTube-VOS benchmarks to verify its effectiveness as well as generalization ability. Our contributions are summarized as follows:</p><p>? We propose Motion-Aware Mask Propagation <ref type="bibr">(MAMP)</ref> for VOS that trains the model end-to-end without any annotations and effectively propagates the masks across frames. ? We propose a motion-aware spatio-temporal matching module to mitigate errors caused by fast motion and longterm correspondence mismatches. This module improves the performance of MAMP on YouTube-VOS by 6.4%. ? Without any bells and whistles (e.g., fancy data augmentations, online adaptation, and external datasets), MAMP significantly outperforms existing self-supervised methods by 4.2% on DAVIS-2017 and 4.85% on the unseen categories of YouTube-VOS. ? Experiment on YouTube-VOS dataset shows that MAMP has the best generalization ability compared to existing self-supervised and supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Semi-supervised VOS aims to leverage the ground truth object mask given (only) in the first frame to segment the objects of interest in subsequent frames. Existing semisupervised VOS methods can be divided into online-learning and offline-learning methods depending on whether online adaptation is needed during inference. The former usually update the networks dynamically during inference based on the first frame <ref type="bibr" target="#b19">Maninis et al. 2018;</ref><ref type="bibr" target="#b24">Perazzi et al. 2017;</ref><ref type="bibr" target="#b6">Huang et al. 2020)</ref>, synthetic frames <ref type="bibr" target="#b7">(Khoreva et al. 2019;</ref><ref type="bibr" target="#b18">Luiten, Voigtlaender, and Leibe 2018)</ref>, or multiple selected past frames <ref type="bibr" target="#b32">(Voigtlaender and Leibe 2017;</ref><ref type="bibr" target="#b20">Meinhardt and Leal-Taixe 2020)</ref> of each video making the networks object-specific. Literature has shown the effectiveness of online-learning <ref type="bibr" target="#b38">(Wang et al. 2019c</ref>), however, it is time consuming and adversely affects the models' generalization ability. Offline-learning methods usually propagate the given object mask or features of the first frame either explicitly or implicitly to subsequent frames, making the expensive online-adaptation no longer necessary <ref type="bibr" target="#b36">Wang et al. 2019b</ref>). Recurrent neural networks <ref type="bibr" target="#b30">(Ventura et al. 2019;</ref><ref type="bibr" target="#b13">Li and Loy 2018</ref>) and recurrent training strategies <ref type="bibr" target="#b21">(Oh et al. 2018)</ref> can implicitly propagate the spatio-temporal features from past frames to the current frame. Memory networks with attention mechanism <ref type="bibr" target="#b22">(Oh et al. 2019;</ref><ref type="bibr" target="#b15">Liang et al. 2020;</ref><ref type="bibr" target="#b14">Li, Shen, and Shan 2020;</ref><ref type="bibr" target="#b27">Seong, Hyun, and Kim 2020;</ref><ref type="bibr" target="#b40">Xie et al. 2021</ref>), correlation mechanism <ref type="bibr" target="#b5">(Hu, Huang, and Schwing 2018;</ref><ref type="bibr" target="#b31">Voigtlaender et al. 2019;</ref><ref type="bibr" target="#b43">Yang, Wei, and Yang 2020;</ref><ref type="bibr" target="#b10">Lai, Lu, and Xie 2020)</ref>, or clustering methods  can explicitly propagate the spatio-temporal features from multiple different past frames to the current frame. Existing offline-learning methods usually perform local or non-local spatio-temporal matching for temporal association and mask propagation. However, non-local matching is noisy and has a large memory footprint, while local matching struggles to cope with problems from fast motion and long-term correspondence mismatches. The proposed MAMP is an offline-learning method and does not require time-consuming online adaptation. MAMP leverages a dynamically updated memory bank to store features and masks from selected past frames, and propagates masks effectively according to our proposed motion-aware spatio-temporal matching module. Unlike previous matching methods, our motion-aware spatio-temporal matching module not only excludes the noisy matching results but also mitigates the problems caused by fast motion and long-term correspondence mismatches. Memory Networks aim to capture the long-term dependencies by storing temporal features or different categories of features in a memory module. LSTM <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber 1997)</ref> and GRU <ref type="bibr" target="#b2">(Cho et al. 2014</ref>) implicitly represent spatio-temporal features with local memory cells in a highly compressed way limiting the representation ability. Memory networks <ref type="bibr" target="#b39">(Weston, Chopra, and Bordes 2014)</ref> were introduced to explicitly store the important features. A common memory network in VOS is STM <ref type="bibr" target="#b22">(Oh et al. 2019)</ref> which incrementally adds the features of past frames to the memory bank, and leverages the non-local spatio-temporal matching to provide spatio-temporal features. However, the incremental memory bank updates are impractical when segmenting long videos due to the growing memory cost. In this work, we divide the memory into long-term and short-term memory. The former is fixed, whereas the latter is updated dynamically using the past few frames making our MAMP memory efficient. Self-supervised Learning can learn general feature representations and spatio-temporal correspondences based on the intrinsic properties of videos. It has shown promising capacity on various downstream tasks as it does not require annotations and can better generalize <ref type="bibr" target="#b33">(Vondrick et al. 2018;</ref><ref type="bibr" target="#b3">Han, Xie, and Zisserman 2019;</ref><ref type="bibr" target="#b12">Li et al. 2019;</ref><ref type="bibr" target="#b8">Kim, Cho, and Kweon 2019;</ref><ref type="bibr" target="#b35">Wang, Jiao, and Liu 2020;</ref><ref type="bibr" target="#b28">Tao, Wang, and Yamasaki 2020;</ref><ref type="bibr" target="#b23">Pan et al. 2021)</ref>. Many pretext tasks have been explored for self-supervised learning such as future frame prediction <ref type="bibr" target="#b16">(Liu et al. 2018)</ref>, query frame reconstruction <ref type="bibr" target="#b11">(Lai and Xie 2019;</ref><ref type="bibr" target="#b9">Kim et al. 2020;</ref><ref type="bibr" target="#b10">Lai, Lu, and Xie 2020)</ref>, patch The frames are converted to Lab color space and channel dropout is used only on the ab channels to generate the reconstruction target for self-supervision. During inference, a parameter sharing encoder is used to encode the frames into feature maps. The proposed motion-aware spatio-temporal matching module is used to compute spatio-temporal correspondences between the past frames in the memory bank and the current frame and propagate the masks across frames.</p><p>re-localization <ref type="bibr" target="#b37">(Wang, Jabri, and Efros 2019;</ref><ref type="bibr" target="#b17">Lu et al. 2020)</ref>, and motion statistics prediction <ref type="bibr" target="#b34">(Wang et al. 2019a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Overview <ref type="figure" target="#fig_1">Fig. 2</ref> shows an overview of our method for VOS. MAMP is trained with the reconstruction task to learn feature representations and construct robust spatio-temporal correspondences between pairs of frames from the same video. Hence, zero annotation is required to train the model. During inference, MAMP segments the frames in a sequential manner. The parameter sharing encoder is used to extract framewise features and the motion-aware spatio-temporal matching module propagates the masks from the past frames to the current frame according to the spatio-temporal affinity matrix. The size-aware image feature alignment module also facilitates the mask propagation to prevent misalignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-supervised Feature Representation Learning</head><p>We use the reconstruction task for self-supervised feature representation learning and robust spatio-temporal matching. Since the channel correlation in Lab color space is smaller than that of RGB <ref type="bibr" target="#b26">(Reinhard et al. 2001)</ref>, we randomly dropout one of the ab channels and make the Lab color as the reconstruction target. Dropout preserves enough information for the input while avoiding trivial solutions. Therefore, the model learns the general feature representations and the spatio-temporal correspondences between the reference and query frames instead of learning how to predict the missing channel from the observed channels. To minimize the reconstruction loss, semantically similar pixels between reference and query frames are forced to have highly correlated feature representations, while semantically dissimilar pixels are forced to have weakly correlated feature representations. Finally, the reconstruction target of the query frames is predicted according to the highly correlated ROIs in the reference frames. Specifically, given a reference and query frame {I r , I q } ? R H?W ?3 from a video, a parameter-sharing convolutional encoder ?(g(I); ?) is used to extract their feature represen-</p><formula xml:id="formula_0">tations {F r , F q } ? R h?w?c , where g(?)</formula><p>is the information bottleneck to prevent trivial solutions. The dropped channels of the two frames {C r , C q } ? R H?W ?1 are downsampled to the resolution of the features {C r,d , C q,d } ? R h?w?1 based on the size-aware image feature alignment module.</p><p>To enable C r to represent and reconstruct C q , the spatiotemporal affinity matrix A q,r ? R hw?R that represents the strength of the correlation between F q and F r is:</p><formula xml:id="formula_1">A i,j q,r = exp( F i q , F j r / ? c) j?R exp( F i q , F j r / ? c) ,<label>(1)</label></formula><p>where i and j are the locations in F q and F r , respectively. ?, ? is the dot product between two vectors, and c refers to the number of channels to re-scale the correlation value and R is the ROI of i. Next, a location i in C q,d is represented by the weighted sum of the corresponding ROI in C r,d :</p><formula xml:id="formula_2">C i q,d = j?R A i,j q,r C j r,d<label>(2)</label></formula><p>Finally, C i q,d is upsampled to C i q , and Huber Loss L is used to force C i q to be close to C i q :</p><formula xml:id="formula_3">L = 1 n N i=1 Z i<label>(3)</label></formula><p>where </p><formula xml:id="formula_4">Z i = 0.5( C i q ? C i q ) 2 , if | C i q ? C i q |&lt;1 | C i q ? C i q | ? 0.5, otherwise.<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion-aware Spatio-temporal Matching</head><p>To account for moving objects, VOS models should be able to retrieve the corresponding ROIs from the reference frames for mask propagation under motion. To meet this constraint, non-local spatio-temporal matching methods <ref type="bibr" target="#b22">(Oh et al. 2019</ref>) consider all locations in the reference frames as potential ROIs. However, non-local matching generates many noisy matches and has a large memory footprint. Local spatio-temporal matching methods <ref type="bibr" target="#b10">(Lai, Lu, and Xie 2020)</ref> retrieve the ROIs in the reference frames based on the location coordinates in the query frame and a pre-defined retrieval radius. Although local matching is more efficient, existing local methods have limited receptive fields limiting their ability to localize the most correlated ROIs when they encounter fast motion or after long-term matching.</p><p>We propose a motion-aware spatio-temporal matching module that leverages optical flow to enable the query locations to retrieve the most correlated ROIs from the reference frames, see <ref type="figure">Fig. 3(a)</ref>. We use RAFT <ref type="bibr" target="#b29">(Teed and Deng 2020)</ref> which costs only about 18ms and 13ms to compute the motion offsets between frame pairs in DAVIS-2017 and YouTube-VOS datasets, respectively. As shown in <ref type="figure">Fig. 3(b)</ref>, the vanilla local spatio-temporal matching method cannot retrieve the most correlated ROIs for the query point. However, with our motion-aware spatio-temporal matching, the query point can find its most correlated ROIs even if the ROI pixels are not consecutive in raw space.</p><p>Our motion-aware spatio-temporal matching module takes the features of a query frame F q ? R h?w?c , the features of reference frames F r ? R h?w?c?n , the downsampled masks of reference frames M r ? R h?w?o?n , and the downsampled motion offsets M O(?x, ?y) ? R h?w?n?2 between the query and reference frames as inputs, where ?x and ?y are the displacement vectors along the horizontal and vertical directions, respectively. F r and M r are first warped according to M O(?x, ?y) making the locations with the same coordinates in F q and the warped F r and M r to be the most similar pairs. Therefore, for one location i ? F q (x, y) in the query frame, the initial corresponding ROIs can be retrieved, i.e., R = {?j ? F r , |w(j) ? i| ? r}, where r is the radius of ROIs and w(j) ? F r (x + ?x, y + ?y). Due to different input resolutions, we empirically set r to 6 during training and 12 during inference. Subsequently, the initial spatio-temporal affinity matrix Corr q,r ? R h?w?n(2r+1) 2 is computed and the TopK selection block is used to filter out the weakly correlated locations in the ROIs to save the Softmax operation from being affected by noise. k is set to 36 for all experiments (see <ref type="figure">Supplementary Material)</ref>. Finally, mask propagation is achieved by multiplying the selected spatio-temporal affinity matrix with the corresponding ROIs in M r .</p><p>During inference, with all the ROIs being dynamically sampled from different reference frames in the memory bank based on the motion-aware spatio-temporal matching module, the mask propagation becomes more accurate and the problems caused by fast motion and long-term correspondence mismatches are alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size-aware Image Feature Alignment</head><p>To reduce memory consumption, previous methods perform bilinear downsampling on the supervision signals (masks) of the reference frames and propagate these signals at the feature resolution. However, this operation introduces misalignment (see <ref type="figure">Fig. 4(b)</ref>) between the strided convolution layers and the supervision signals from na?ve bilinear downsampling and upsampling. MAST <ref type="bibr" target="#b10">(Lai, Lu, and Xie 2020)</ref> has an image feature alignment module to deal with this problem but it does not cater for the misalignment caused at the upsampling stage. To solve this problem, we propose a sizeaware image feature alignment module where the supervision signals are automatically padded (see <ref type="figure">Fig. 4(a)</ref>) so that the input size can be divisible by the size after downsampling, and the supervision signals are sampled at the convolution kernel centers. Hence, the misalignment is removed at both downsampling and upsampling stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Training: We modify ResNet-18 and use it as the encoder to extract image features with a spatial resolution of 1/4 of the input images (see Supplementary Material for details). The encoder parameters are randomly initialized without pre-training. A pair of nearby video frames are randomly sampled as the reference and query frame, and the reconstruction task with Huber Loss is used to train the model. For   <ref type="table">Table 2</ref>: Evaluation on YouTube-VOS 2018 validation set for "seen" and "unseen" categories ("unseen" object category does not appear in training). For overall, J , and F, higher values are better and for Gen. (Generalization) Gap, lower values are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We use Region Similarity J and Countour Accuracy F for evaluation. We also report the Generalization Gap as in <ref type="bibr" target="#b10">(Lai, Lu, and Xie 2020)</ref> to evaluate the generalization ability of MAMP on YouTube-VOS. Generalization Gap computes the model's performance gap between inference on seen and unseen categories. Its value is inversely proportional to the generalization ability of a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head><p>We compare MAMP with existing methods on DAVIS-2017 and YouTube-VOS 2018. Results are obtained using the official evaluation code and server. We do our best to compare the results as fairly as possible. For example, multistage training strategies, external datasets, data augmentations, and online adaptation are not used in this work. <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table">Table 2</ref> summarize the performance of the state-of-the-art methods and MAMP on DAVIS-2017 and YouTube-VOS 2018. MAMP significantly outperforms benchmark self-supervised methods by over 4.2% on DAVIS-2017, 4% on YouTube-VOS 2018, and by 4.85% on the unseen categories of YouTube-VOS 2018. Moreover, MAMP is also comparable to some supervised methods. These results demonstrate the effectiveness of MAMP.</p><p>To evaluate the generalization ability of MAMP, we evaluate it on both "seen" and "unseen" categories of YouTube-VOS 2018. Objects in "unseen" categories do not appear in the training set. <ref type="table">Table 2</ref> shows that MAMP performs well on "unseen" categories and has the best generalization ability. Surprisingly, it performs better on "unseen" categories than on "seen" categories because of the better boundary segmentation performance on "unseen" objects. These results indicate that MAMP can learn general feature representations that are not restricted by the specific object categories in the training set. The most comparable supervised method in generalization ability is GC (Li, Shen, and Shan 2020) (1.8 vs -1.2). However, GC is trained with several external datasets with precise ground truth annotations.</p><p>In addition to YouTube-VOS 2018, we also evaluate MAMP on YouTube-VOS 2019 which has more videos and object instances. The results are included in supplementary material and show that MAMP achieves the best performance and generalization ability compared to other selfsupervised methods on YouTube-VOS 2019 as well. Qualitative Results <ref type="figure" target="#fig_2">Figure 5</ref> shows qualitative results of MAMP under various challenging scenarios, e.g., occlusion, fast motion, large deformations, and scale variations. MAMP is able to handle these challenging scenarios effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Motion-aware Spatio-temporal Matching. In <ref type="table" target="#tab_3">Table 3</ref>, we replaced the vanilla local spatio-temporal matching module of <ref type="bibr" target="#b10">(Lai, Lu, and Xie 2020)</ref> with the proposed motion-aware spatio-temporal matching module and the performance increased by 3.1% on DAVIS-2017 and 6.4% on YouTube-VOS 2018. Vanilla local spatio-temporal matching module retrieves the corresponding ROIs according to the predefined radius and ROI localization method which is not accurate enough as the most correlated ROIs are prone to be outside the search radius under fast motion and long-term matching scenarios. Without these most correlated ROIs, the label of one location in the query frame will be determined by the labels of several uncorrelated or weakly-correlated locations in the reference frames. Therefore, the segmentation results could be further improved if we can retrieve the most correlated ROIs for each location in the query frame. Our motion-aware spatio-temporal matching module leverages the motion cues to register the reference frames in the memory bank to the query frame before computing the local spatio-temporal correspondences and filters out weakly correlated noise before mask propagation. Therefore, the above issues are alleviated even if the reference frames are temporally far from the query frame. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the motion-aware spatio-temporal matching module brings more performance gains in YouTube-VOS 2018, as this dataset has longer video clips and lower frame rates compared to DAVIS-2017. To further demonstrate the effectiveness of our motion-aware spatio-temporal matching module, we choose different k values to select the TopK correlated locations in the ROIs for mask propagation, the results show that MAMP still achieves the best performance compared to previous benchmarks even if only one of the 3125 locations in the ROIs is used for mask propagation (details in Supplementary Material).</p><p>Size-aware Image Feature Alignment. We removed the image feature alignment module of <ref type="bibr" target="#b10">(Lai, Lu, and Xie 2020)</ref> to replace it with our size-aware image feature alignment module. The performance increased by 1.6% on DAVIS-2017 and remained unchanged on YouTube-VOS 2018 (see <ref type="table" target="#tab_3">Table 3</ref>). This is because our size-aware image feature alignment module fixes the misalignment at the upsampling stage caused by the improper input size. After computing the statics, we found that the number of videos that have an improper input size is 96.7% for DAVIS-2017 validation set and only 1.9% for the YouTube-VOS 2018 validation set.</p><p>Long-term Memory and Short-term Memory. In <ref type="table">Table 4</ref>, we compare results for different memory settings. We can see that all memory settings have reasonable performance. Long-term memory provides accurate ground truth information for query frames, while short-term memory offers upto-date information from past neighboring frames. The results show that MAMP with short-term memory performs better than using long-term memory. This is because the appearance and scale of objects usually change significantly  <ref type="table">Table 4</ref>: Ablation study for long and short-term memory.</p><p>over time and long-term memory alone is unable to adapt to these changes. Furthermore, it can be seen that MAMP using both memory types has the best performance as both memories are complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with MAST</head><p>Our nearest competitor is MAST <ref type="bibr" target="#b10">(Lai, Lu, and Xie 2020)</ref> which leverages local spatio-temporal matching module with ROI localization for long-term mask propagation. The framework of MAMP is inspired from MAST, however, MAMP is different in various aspects: (1) The local spatiotemporal matching in MAST is sub-optimal for handling fast motions and long-term matching scenarios whereas the proposed motion-aware spatio-temporal matching in MAMP can better handle (see <ref type="table" target="#tab_3">Table 3</ref>) such situations by exploiting motion cues and noise filters.</p><p>(2) The local spatio-temporal matching module of MAST has a larger memory footprint compared to the proposed motion-aware spatio-temporal matching in MAMP. One 3090 GPU supports only 5 reference frames in the memory bank for MAST, but 18 reference frames for MAMP. <ref type="formula" target="#formula_3">(3)</ref> The image feature alignment module of MAST does not fix the misalignment at the upsampling stage, whereas the size-aware image feature alignment module of MAMP alleviates this problem as shown in <ref type="figure">Fig. 4(b)</ref> and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed MAMP that enables general feature representation and motion-guided mask propagation. MAMP model can be trained without the need for annotations, and outperforms existing self-supervised methods by a large margin. Moreover, MAMP demonstrates the best generalization ability compared to previous methods. We believe that MAMP has the potential to propagate spatiotemporal features and masks in practical video segmentation tasks. In the future, we will develop more effective pretext tasks and adaptive memory selection methods to further improve the performance of MAMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone Architecture</head><p>We modify ResNet-18 and use it as the encoder to extract image features with a spatial resolution of 1/4 of the input images, the specific architecture is shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TopK Correlated Locations for Mask Propagation</head><p>If we retrieve ROIs with a radius of 12 on 5 reference frames, the corresponding ROIs for one location in the query frame will include 3125 locations. However, noisy matches to 3125 locations may adversely affect the model's performance. Hence, the proposed motion-aware spatio-temporal matching filters out redundant and noisy ROIs by selecting TopK correlated locations only for mask propagation. As shown in <ref type="table" target="#tab_9">Table 6</ref>, leveraging the top 36 or top 9 correlated locations in the ROIs for mask propagation improves the performance of MAMP compared to using all 3125 locations. Using the top 36 correlated locations obtains the best performance. Moreover, compared to other state-of-the-art self-supervised methods, MAMP still maintains the best performance even if only one of the 3125 locations in the ROIs is used for mask propagation. These results further demonstrate the effectiveness of the proposed motion-aware spatio-temporal matching module.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on YouTube-VOS 2019</head><p>We also evaluate the proposed MAMP on YouTube-VOS 2019. YouTube-VOS 2019 includes more videos and object instances compared to YouTube-VOS 2018. As shown in  <ref type="table" target="#tab_10">Table 7</ref>: Evaluation on YouTube-VOS 2019 validation set for "seen" and "unseen" categories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Framework of the proposed MAMP. During training, a random pair of neighboring video frames is sampled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of MAMP. The frames are sampled at fixed intervals and the first frame in each video is assigned index 0. MAMP performs well in challenging scenarios of occlusion/dis-occlusion, fast motion, large deformations, and scale variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>F q , F r are the features of query, reference frames. M r is the masks of reference frames, and M q is the predicted mask. (b) Comparison of the matching results between the vanilla local spatio-temporal matching module and our motion-aware spatio-temporal matching module.</figDesc><table><row><cell cols="12">(a) Motion-Aware Spatio-Temporal Matching Module</cell></row><row><cell>? ? ? 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ? ? ?n</cell><cell>W</cell><cell>? ? ? ?</cell><cell></cell><cell>I</cell><cell cols="3">ROIs: ? ? (2 + 1) 2 ?</cell><cell></cell><cell></cell><cell>C</cell></row><row><cell></cell><cell>Motion Offsets</cell><cell>? ? ? ? 2</cell><cell>Index Template</cell><cell cols="2">Search Window</cell><cell>(2 + 1) ? (2 + 1)</cell><cell>Index</cell><cell>TopK</cell><cell>? ?</cell><cell>, I</cell><cell>? ? (2 + 1) 2</cell><cell>A ? ? ? ?</cell></row><row><cell>? ? ? ?</cell><cell>W</cell><cell>? ? ? ?</cell><cell></cell><cell>I</cell><cell cols="3">ROIs: ? ? (2 + 1) 2 ?</cell><cell>I</cell><cell cols="3">? ? ? S ? ? ? ?</cell><cell>? ? ?</cell></row><row><cell>W Warp</cell><cell></cell><cell cols="5">C Local Window Correlation</cell><cell cols="4">Element-wise Product</cell><cell>A</cell><cell>Argmax</cell></row><row><cell cols="2">I Index Sampling</cell><cell>S Softmax</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Dimension-wise Sum</cell></row><row><cell></cell><cell></cell><cell cols="8">(b) Comparison of Matching Results</cell><cell></cell></row><row><cell cols="3">Reference Frame</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ROI</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Query Point</cell><cell cols="3">w/o MSMM</cell><cell></cell></row><row><cell cols="2">Query Frame</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Motion-aware ROI</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">w/ MSMM</cell><cell></cell></row><row><cell cols="12">Figure 3: (a) Proposed motion-aware spatio-temporal</cell></row><row><cell cols="3">matching module.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>To be consistent with benchmarks, MAMP is evaluated on the YouTube-VOS validation set at half resolution and DAVIS-2017 validation set at full resolution. Results on DAVIS-2017 and YouTube-VOS are obtained using the official evaluation code and server. During testing, MAMP leverages the size-aware image feature alignment module to fix the misalignment, and uses the trained encoder to extract features from each frame. After that, MAMP uses the proposed motion-aware spatio-temporal matching to propagate the masks from the memory bank to subsequent frames. The memory bank of MAMP is updated dynamically to include I 0 and I 5 as long-term memory and I t?5 , I t?3 , and I t?1 as short-term memory.</figDesc><table><row><cell cols="2">Downsampling</cell><cell>Upsampling</cell><cell></cell></row><row><cell></cell><cell>4 ? 5</cell><cell></cell><cell></cell></row><row><cell>Padding 8 ? 10 Divisible by n? No. 8 ? 9</cell><cell></cell><cell></cell><cell>8 ? 10</cell><cell>Unpadding</cell></row><row><cell>Divisible by n? Yes.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(b) Comparison of Different Sampling Strategies</cell><cell></cell></row><row><cell>Bilinear Sampling</cell><cell cols="4">Image Feature Alignment Size-Aware Image Feature Alignment</cell></row><row><cell>CNN Kernel Centre</cell><cell>Sampling Centre</cell><cell>Misalignment</cell><cell cols="2">Alignment</cell></row><row><cell cols="5">Figure 4: Size-aware image feature alignment module in comparison to bilinear sampling and image feature align-</cell><cell>Experiments</cell></row><row><cell cols="5">ment. n is the ratio of the input to downsampled size. The</cell><cell>Datasets</cell></row><row><cell cols="5">proposed size-aware image feature alignment fixes the mis-alignment in both downsampling and upsampling stages.</cell><cell>DAVIS-2017 (Pont-Tuset et al. 2017) is commonly for VOS in short video clips and complex scenes. It contains 150</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>videos with over 200 objects. The validation set of DAVIS-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2017 contains 30 videos.</cell></row><row><cell cols="5">pre-processing, the frames are resized to 256 ? 256, and no</cell><cell>YouTube-VOS (Xu et al. 2018) is the largest dataset for</cell></row><row><cell cols="5">data augmentation is used. We train our model with pairwise</cell><cell>VOS with long video clips. It contains over 4000 high-</cell></row><row><cell cols="5">frames for 33 epochs on YouTube-VOS using a batch-size of</cell><cell>resolution videos with 7000+ objects. The validation set of</cell></row><row><cell cols="5">24 for all experiments. We adopt Adam optimizer with the</cell><cell>YouTube-VOS 2018 contains 474 videos. Unlike previous</cell></row><row><cell cols="5">base learning rate of 1e-3, and the learning rate is divided</cell><cell>methods (Oh et al. 2019; Xie et al. 2021) that leverage sev-</cell></row><row><cell cols="5">by 2 after 0.4M, 0.6M, 0.8M, and 1.0M iterations, respec-</cell><cell>eral external datasets to train the model, we only train our</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>model on YouTube-VOS and test our model on both DAVIS-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2017 and YouTube-VOS. Unless specified otherwise, the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>YouTube-VOS dataset in this paper refers to the 2018 ver-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sion which is consistent with previous benchmarks.</cell></row></table><note>Evaluation on DAVIS-2017 validation set. Note that each method modifies vanilla backbone models to suit their framework. Training Dataset (T. Data) notations: C=COCO, D=DAVIS, E=ECSSD, H=HKU-IS, I=ImageNet, K=Kinetics, M=Mapillary, O=OxUvA, P=PASCAL-VOC, S=MSRA10K, V=VLOG, Y=YouTube-VOS.(a) Size-Aware Image Feature Alignmenttively. Our model is trained end-to-end without any multi- stage training strategies. The training takes about 11 hours on one NVIDIA GeForce 3090 GPU. Testing: The proposed MAMP does not require time- consuming online adaption to fine-tune the model during testing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Ablation experiment for motion-aware spatio-</cell></row><row><cell>temporal matching module (M) and size-aware image fea-</cell></row><row><cell>ture alignment module (A).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>(4) MAST leverages multi-stage training strategies for training, i.e., finetunes the model using multiple reference frames. However, MAMP is trained end-to-end with pairwise frames once only. (5) MAST does not converge well using new releases of PyTorch and only obtains about 50 mean J &amp;F on the validation set of DAVIS-2017 when training with PyTorch 1.9. MAMP solves this issue by normalizing the spatio-temporal affinity matrix with the channel number and achieves 69.7 mean J &amp;F on the validation set of DAVIS-2017. Finally, qualitative comparisons of MAST and MAMP further demonstrate the superiority of MAMP (see videos in Supplementary Material).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Layer Name Output Size</cell><cell cols="2">Configuration</cell></row><row><cell>Conv1</cell><cell cols="3">H/2 ? W/2 7 ? 7, 64, stride 2</cell></row><row><cell>Conv2</cell><cell>H/2 ? W/2</cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell>? 2</cell></row><row><cell>Conv3</cell><cell>H/4 ? W/4</cell><cell>3 ? 3, 128 3 ? 3, 128</cell><cell>? 2</cell></row><row><cell>Conv4</cell><cell>H/4 ? W/4</cell><cell>3 ? 3, 256 3 ? 3, 256</cell><cell>? 2</cell></row><row><cell>Conv5</cell><cell>H/4 ? W/4</cell><cell>3 ? 3, 256 3 ? 3, 256</cell><cell>? 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Architecture of the modified ResNet-18.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablation of TopK correlated locations in the ROIs for mask propagation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 ,</head><label>7</label><figDesc>MAMP still significantly outperforms other selfsupervised methods and has the best generalization ability.</figDesc><table><row><cell>Method</cell><cell>Sup. Overall</cell><cell></cell><cell>Seen</cell><cell cols="2">Unseen</cell><cell>Gen. Gap</cell></row><row><cell></cell><cell></cell><cell>J</cell><cell>F</cell><cell>J</cell><cell>F</cell></row><row><cell>Vid. Color.</cell><cell>39.0</cell><cell cols="4">43.3 38.2 36.6 37.5</cell><cell>3.7</cell></row><row><cell>CorrFlow</cell><cell>47.0</cell><cell cols="4">51.2 46.6 44.5 45.9</cell><cell>3.7</cell></row><row><cell>MAST</cell><cell>64.9</cell><cell cols="4">64.3 65.3 61.5 68.4</cell><cell>0.15</cell></row><row><cell>Ours</cell><cell>68.2</cell><cell cols="4">66.3 67.5 65.4 73.7</cell><cell>-2.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the ARC Industrial Transformation Research Hub IH180100002.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>References Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast Video Object Segmentation With Temporal Aggregation Network and Dynamic Template Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8879" to="8889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1175" to="1197" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8545" to="8552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rpm-net: Robust pixel-level matching networks for selfsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2057" to="2065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MAST: A memoryaugmented self-supervised tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervised learning for video correspondence flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint-task self-supervised learning for temporal correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast Video Object Segmentation using the Global Context Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="735" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video Object Segmentation with Adaptive Feature Bank and Uncertain-Region Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3430" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8960" to="8970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1515" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Make One-Shot Video Object Segmentation Efficient Again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9226" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Videomoco: Contrastive video representation learning with temporally adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11205" to="11214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kernelized Memory Network for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="629" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning using inter-intra contrastive framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2193" to="2201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RVOS: End-to-End Recurrent Network for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giro-I</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5277" to="5286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9481" to="9490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="391" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4006" to="4015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning by pace prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="504" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2566" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3978" to="3987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient Regional Memory Network for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1286" to="1295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<title level="m">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="332" to="348" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
