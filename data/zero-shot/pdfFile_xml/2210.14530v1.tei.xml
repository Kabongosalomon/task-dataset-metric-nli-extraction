<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 1 RGB-T Semantic Segmentation with Location, Activation, and Sharpening</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongyang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yike</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Zhi</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xinpeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Dan</forename><surname>Zeng</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 1 RGB-T Semantic Segmentation with Location, Activation, and Sharpening</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-RGB-T semantic segmentation</term>
					<term>discriminative treatment</term>
					<term>collaborative location</term>
					<term>complementary activation</term>
					<term>edge sharpening</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation is important for scene understanding. To address the scenes of adverse illumination conditions of natural images, thermal infrared (TIR) images are introduced. Most existing RGB-T semantic segmentation methods follow three cross-modal fusion paradigms, i.e., encoder fusion, decoder fusion, and feature fusion. Some methods, unfortunately, ignore the properties of RGB and TIR features or the properties of features at different levels. In this paper, we propose a novel feature fusion-based network for RGB-T semantic segmentation, named LASNet, which follows three steps of location, activation, and sharpening. The highlight of LASNet is that we fully consider the characteristics of cross-modal features at different levels, and accordingly propose three specific modules for better segmentation. Concretely, we propose a Collaborative Location Module (CLM) for high-level semantic features, aiming to locate all potential objects. We propose a Complementary Activation Module for middle-level features, aiming to activate exact regions of different objects. We propose an Edge Sharpening Module (ESM) for lowlevel texture features, aiming to sharpen the edges of objects. Furthermore, in the training phase, we attach a location supervision and an edge supervision after CLM and ESM, respectively, and impose two semantic supervisions in the decoder part to facilitate network convergence. Experimental results on two public datasets demonstrate that the superiority of our LASNet over relevant state-of-the-art methods. The code and results of our method are available at https://github.com/MathLee/LASNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S EMANTIC segmentation <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>, also known as scene parsing, is a fundamental topic in computer vision, and focuses on assigning a category label to each pixel in a natural image, which is a dense prediction task. It plays an important role in autonomous driving <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, medical analysis <ref type="bibr" target="#b6">[7]</ref>, remote sensing scene understanding <ref type="bibr" target="#b7">[8]</ref> and so on. In the past decade, many effective solutions <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b16">[17]</ref> for semantic segmentation have been proposed. However, there are still some hard scenes that unimodal image-based methods cannot handle well, such as cluttered backgrounds, adverse illumination conditions (even darkness), and occlusions by fog or smoke. Therefore, researchers additionally introduce thermal <ref type="bibr">All</ref>    <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b28">[29]</ref>, decoder fusion <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and feature fusion <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>. infrared (TIR) images, and combine them with natural images to handle the above hard scenes, forming an emerging task called RGB-T semantic segmentation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. TIR images reflect the surface temperature of objects collected by the TIR sensor, and are insensitive to illumination changes, making up for the shortcomings of unimodal natural images <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b23">[24]</ref>. For multi-modal inputs, the key is how to effectively make them complement each other. Therefore, researchers strive to develop effective cross-modal fusion strategies for RGB images and TIR images in RGB-T semantic segmentation. In general, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, there are currently three typical cross-modal fusion paradigms for RGB-T semantic segmentation, including encoder fusion <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b28">[29]</ref>, decoder fusion <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and feature fusion <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>. Specifically, encoder fusion paradigm directly adopts element-wise summation to achieve cross-modal feature fusion on the feature extraction stage. Decoder fusion paradigm usually adopts the same module to process cross-modal features at different levels on the inference stage. Feature fusion paradigm focuses on fusing multi-level cross-modal features between encoder and decoder, and usually employs the same module.</p><p>The above three fusion paradigms have greatly promoted the development of RGB-T semantic segmentation. However, the disadvantages of them are obvious. The cross-modal feature fusion operation of the encoder fusion paradigm is the simple element-wise summation, ignoring the properties of RGB and TIR features. The other two paradigms design specific modules to fuse cross-modal features, and have the same problem of using the unified module to process features at different arXiv:2210.14530v1 [cs.CV] 26 Oct 2022 levels. In fact, features at different levels of convolutional neural networks (CNNs) <ref type="bibr" target="#b34">[35]</ref> have unique characteristics. The indiscriminate treatment ignores the properties of features at different levels and is suboptimal.</p><p>Driven by the aforementioned observation, in this paper, we propose a novel LASNet for RGB-T semantic segmentation, aiming to explore the characteristics of features at different levels and to mine the common valuable content contained in cross-modal features. We implement LASNet through the feature fusion paradigm. Different from previous methods, we achieve accurate segmentation via three steps of object location, region activation, and edge sharpening. We arrange these three steps reasonably according to the characteristics of features at different levels. Furthermore, we design three dedicated modules for these three steps instead of the unified modules used by previous methods.</p><p>Specifically, we divide the basic cross-modal features extracted by the backbone into three levels, i.e., high, middle, and low levels. For high-level features, we propose a Collaborative Location Module (CLM), which builds pixellevel correlations <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> among semantic representations to locate all potential objects. For middle-level features, we propose a Complementary Activation Module (CAM), which builds on the attention mechanism <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b37">[38]</ref> to generate informative features for exact region activation. For low-level features, we propose an Edge Sharpening Module (ESM), which extracts edges using multiple receptive fields to sharpen objects of different sizes. In addition, we impose the location supervision on CLM and the edge supervision on ESM to improve the accuracy of object location and edge extraction. In this way, the above three modules make good use of different levels of features, facilitating the proposed LASNet to generate satisfactory segmentation results.</p><p>Our main contributions are summarized as follows:</p><p>? We fully explore the characteristics of cross-modal features at different levels, and propose a novel LASNet for RGB-T semantic segmentation, following three steps of location, activation, and sharpening. ? We propose specific CLM, CAM, and ESM for high-, middle-, and low-level cross-modal features, respectively, which are responsible for object location, region activation, and edge sharpening. These three modules mine the common valuable content contained in cross-modal features at different levels to generate discriminative features, facilitating accurate segmentation. ? We evaluate the proposed LASNet on MFNet dataset and PSTNet dataset. The results demonstrate that our LASNet outperforms state-of-the-art methods and show the reasonableness of discriminative treatment of crossmodal features at different levels in our LASNet.</p><p>The rest of this paper is organized as follows. In Sec. II, we review the related work of RGB and multi-modal semantic segmentation. In Sec. III, we elaborate the proposed LASNet. In Sec. IV, we present comprehensive experiments and analyses. Finally, in Sec. V, we conclude this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. RGB Semantic Segmentation</head><p>Recently, with the help of CNNs, RGB semantic segmentation (i.e., unimodal semantic segmentation) has made amazing progress. Long et al. <ref type="bibr" target="#b0">[1]</ref> proposed the milestone Fully Convolutional Network (FCN), which is the first endto-end CNN-based semantic segmentation method. Subsequently, Noh et al. <ref type="bibr" target="#b1">[2]</ref> and Badrinarayanan et al. <ref type="bibr" target="#b3">[4]</ref> proposed the Deconvolution Network (DeconvNet) and the encoderdecoder architecture-based SegNet, respectively, which are two other pioneering works. Inspired by the above works, many technologies have been developed and applied for RGB semantic segmentation, such as multi-scale strategy, contextual dependency, and transformers.</p><p>As a representative work of multi-scale strategy, Chen et al. proposed a series of methods, including DeepLabV1 <ref type="bibr" target="#b2">[3]</ref>, DeepLabV2 <ref type="bibr" target="#b38">[39]</ref>, DeepLabV3 <ref type="bibr" target="#b39">[40]</ref>, and DeepLabV3+ <ref type="bibr" target="#b40">[41]</ref>. They introduced atrous convolutions to capture multi-scale information. Furthermore, they proposed atrous spatial pyramid pooling (ASPP), and made the network deeper with atrous convolutions. Yang et al. <ref type="bibr" target="#b41">[42]</ref> proposed the Densely connected ASPP (DenseASPP) to generate features with very large receptive fields. Zhao et al. <ref type="bibr" target="#b8">[9]</ref> adopted four pooling layers of different sizes to capture the multi-scale global information, and proposed the Pyramid Scene Parsing Network (PSPNet). Ji et al. <ref type="bibr" target="#b42">[43]</ref> adopted the spatial pyramid pooling to ensemble multi-scale features in the encoding stage, and learned boundary information in the decoding stage.</p><p>Some researchers focus on exploring contextual dependency for RGB semantic segmentation. For example, Yuan et al. <ref type="bibr" target="#b43">[44]</ref> introduced the self-attention mechanism to learn the pixelwise similarity maps to identify objects belonging to the same category as the target pixel. Moreover, Yuan et al. <ref type="bibr" target="#b44">[45]</ref> built the pixel-region relation, and enhanced the representation of each pixel by the object-contextual representation for accurate segmentation. Fu et al. <ref type="bibr" target="#b11">[12]</ref> proposed Dual Attention Network (DANet) to model the global dependencies in channel and spatial dimensions via self-attention. Huang et al. <ref type="bibr" target="#b9">[10]</ref> proposed the criss-cross attention to capture contextual information in an efficient way. Similar to <ref type="bibr" target="#b9">[10]</ref>, Liu et al. <ref type="bibr" target="#b45">[46]</ref> and Weng et al. <ref type="bibr" target="#b14">[15]</ref> focused on the efficiency of algorithms, and developed real-time and accurate segmentation methods. To better mine global contextual dependencies, Strudel et al. <ref type="bibr" target="#b10">[11]</ref> built the Segmenter on ViT <ref type="bibr" target="#b46">[47]</ref>. Xie et al. <ref type="bibr" target="#b13">[14]</ref> proposed a cutting-edge Transformer framework, named SegFormer, with a lightweight All-MLP decoder.</p><p>Overall, semantic segmentation in unimodal images has achieved great success, but the above segmentation methods often fail in challenging scenes, such as low contrast, cluttered backgrounds, adverse illumination conditions, and occlusions by fog or smoke. The researchers therefore introduce additional information to handle these difficult scenes, proposing multi-modal semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-modal Semantic Segmentation</head><p>In this paper, multi-modality data generally refers to RGB images and depth maps or TIR images, i.e., RGB-D or RGB-T. In addition to semantic segmentation, these two types of multi-modal data have a wide range of applications in computer vision, such as salient object detection <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b47">[48]</ref>- <ref type="bibr" target="#b49">[50]</ref>, tracking <ref type="bibr" target="#b23">[24]</ref>, and glass segmentation <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Obviously, depth maps and TIR images are different and have their own characteristics. Depth maps contain rich geometric distance information, while TIR images reflect the surface temperature of objects.</p><p>1) RGB-D Semantic Segmentation. Depth maps are useful for handling scenes with low contrast and disturbing backgrounds. Cao et al. <ref type="bibr" target="#b53">[54]</ref> directly concatenated RGB image and depth map as the network input, and proposed the shape-aware convolutional layer to construct a segmentation network. Hazirbas et al. <ref type="bibr" target="#b54">[55]</ref> fused RGB features and depth features through element-wise summation in the two-stream encoder. Differently, some researchers focus on the fusion of different levels of features. For example, Wang et al. <ref type="bibr" target="#b55">[56]</ref> extracted RGB features and depth features independently, and then learned common features from the extracted high-level features to improve the segmentation accuracy. Hu et al. <ref type="bibr" target="#b56">[57]</ref> proposed an attention complementary module to enhance the independently extracted features, and then adopted elementwise summation for simple fusion. Moreover, Chen et al. <ref type="bibr" target="#b57">[58]</ref> proposed a separation-and-aggregation gate to fuse features at multiple levels, and integrated the fused features into the encoder. There are other approaches that do not follow the above structures. Wang et al. <ref type="bibr" target="#b58">[59]</ref> proposed depth-aware convolution and depth-aware average pooling in a depth-aware CNN, integrating geometry into CNN. Lin et al. <ref type="bibr" target="#b59">[60]</ref> proposed the zig-zag architecture to construct context features of RGB image, and then matched the image region and discrete depth map to achieve the eventual segmentation.</p><p>2) RGB-T Semantic Segmentation. TIR images are useful for handling scenes with adverse illumination conditions and occlusions by fog or smoke. We review existing RGB-T semantic segmentation methods according to the three paradigms in <ref type="figure" target="#fig_0">Fig. 1</ref>. Sun et al. <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref> proposed the encoder fusion paradigm, which integrates TIR features into RGB features through element-wise summation and is similar to <ref type="bibr" target="#b54">[55]</ref>. Following this paradigm, Guo et al. <ref type="bibr" target="#b25">[26]</ref> proposed a specific decoder with multi-level skip connections and the auxiliary decoding module for segmentation. Deng et al. <ref type="bibr" target="#b26">[27]</ref> introduced a feature-enhanced attention module to fuse RGB and thermal information at multiple levels in the encoder part. In the decoder, Zhou et al. <ref type="bibr" target="#b28">[29]</ref> concatenated all features extracted from encoder, and proposed multi-label supervision to assist semantic segmentation. Differently, Ha et al. <ref type="bibr" target="#b17">[18]</ref> proposed the decoder fusion paradigm, which adopts two independent networks to extract RGB features and TIR features and integrates them in the decoder. Zhang et al. <ref type="bibr" target="#b29">[30]</ref> reduced multimodality difference in the independent encoders, and achieved cross-modal feature fusion via the channel weighted fusion module in the decoder. Zhou et al. <ref type="bibr" target="#b30">[31]</ref> extracted semantic information from high-level RGB and TIR features via concatenation-attention operation, and proposed a hierarchical multimodal fusion module for multiscale fusion in the decoder. Zhou et al. <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref> separated the cross-modal feature fusion from the encoder and decoder, proposing the feature fusion paradigm. They adopted the multi-modal fusion module to process RGB and TIR features at different levels, and extracted global information from the high-level fused features to guide the segmentation process of objects.</p><p>The above methods almost indiscriminately process features at different levels. Only a few methods <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b33">[34]</ref> designed specific modules for high-level features, but these modules are based on some simple fusion strategies. In this paper, we fully explore the characteristics of cross-modal features at different levels, and propose CLM, CAM, and ESM for high-, middle-, and low-level cross-modal features, respectively. Our three modules are specially tailored to the characteristics of different levels of features. Based on three specific modules and the feature fusion paradigm, we propose a complete solution, i.e., LASNet, for RGB-T semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we present the proposed LASNet in detail. In Sec. III-A, we introduce the network overview of LASNet. In Sec. III-B, Sec. III-C, and Sec. III-D, we elaborate the proposed CLM, CAM, and ESM, respectively. In Sec. III-E, we present the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Overview</head><p>As depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>, the proposed LASNet is based on the feature fusion paradigm, including a feature extractor, three specific modules, and a decoder. For feature extractor, we adopt two parallel ResNet-152 <ref type="bibr" target="#b52">[53]</ref> backbones, named RGB branch and TIR branch, to extract cross-modal features from RGB image and TIR image. We denote the five convolution blocks in RGB branch and TIR branch as R i and T i (i = 1, 2, 3, 4, 5), respectively, and the corresponding output RGB and TIR features as</p><formula xml:id="formula_0">{f i r ,f i t } ? R hi?wi?ci . The input size is denoted as H ? W , so h i is H 2 i , w i is W 2 i , and c i ? {64, 256, 512, 1024, 2048}.</formula><p>Here, the RGB branch and the TIR branch share parameters, which not only keeps the extracted features in the same feature space, but also reduces the number of parameters. Furthermore, we employ two identical convolutional layers (i.e., shared parameters) to project the same level of RGB and TIR features,f i r andf i t , to f i r and f i t , respectively, with fewer channels (i.e., c i ? {64, 128, 256, 256, 512}) to reduce computational cost.</p><p>We divide the extracted basic cross-modal features into three levels, that is,</p><formula xml:id="formula_1">{f 5 r , f 5 t } are the high level, {f 2?4 r , f 2?4 t</formula><p>} are the middle level, and {f 1 r , f 1 t } are the low level. We arrange CLM, CAM, and ESM for these three levels of features to achieve object location, region activation, and edge sharpening, respectively. Concretely, CLM can enhance the location representation of all potential objects with the help of location supervision. CAM can activate multi-level exact regions of different objects. ESM can extract edge information of objects with the help of edge supervision. These three specific modules are the core components of our LASNet, implementing three important segmentation-friendly functions. With the informative features generated by the above modules RGB Feature Extractor  from the five-level features, we achieve the accurate segmentation result S sem in a general decoder, which is composed of five decoder blocks, and each decoder block includes a dropout layer, two convolutional layers, and an upsampling operation.</p><formula xml:id="formula_2">R 1 h 1 ?w 1 ?c 1 Supervision ? Element-wise Summation R 2 h 2 ?w 2 ?c 2 R 3 h 3 ?w 3 ?c 3 R 4 h 4 ?w 4 ?c 4 R 5 h 5 ?w 5 ?c 5 TIR T 1 h 1 ?w 1 ?c 1 T 2 h 2 ?w 2 ?c 2 T 3 h 3 ?w 3 ?c 3 T 4 h 4 ?w 4 ?c 4 T 5 h 5 ?w 5 ?c 5 Location Activation Sharpening CAM CAM CAM EdgeHead ESM LocHead ? D 3 ? D 4 ? D 5 CLM ? D 2 D 1 GT sem GT eg GT loc S sem SemHead GT sem SemHead Conv. Detector</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Collaborative Location Module</head><p>As we all know, high-level features contain rich semantic information, and have strong representations for object location. And cross-modal RGB and TIR features are complementary, which is more conducive to locating objects. Inspired by the object segmentation works which explore the correlation (i.e., co-attention <ref type="bibr" target="#b60">[61]</ref>) of target objects in consecutive video frames <ref type="bibr" target="#b35">[36]</ref> (i.e., cross-frame) and features of successive levels <ref type="bibr" target="#b36">[37]</ref> (i.e., cross-level), we propose Collaborative Location Module to model the pixel-level correlation of cross-modal high-level semantic features to collaboratively determine the object location. In addition, we mine the valuable representation from two types of feature combinations (i.e., summation and multiplication) in the CLM. We illustrate the CLM in <ref type="figure" target="#fig_2">Fig. 3</ref>. The input features of the CLM are f 5 r and f 5 t . The whole process of CLM can be divided into cross-modal correlation modeling and correlation combinations. <ref type="bibr">RGB</ref> TIR f 5 r on RGB f 5 t on TIR f 5 mul on GT <ref type="figure">Fig. 4</ref>. Feature visualization of f 5 r , f 5 t , and f 5 mul in CLM. Here, we superimpose f 5 r , f 5 t , and f 5 mul on RGB image, TIR image, and ground truth (GT), respectively. Please zoom-in for details. 1) Cross-modal Correlation Modeling. In low illumination environments, objects in the RGB image are drowned in the background, as the input RGB image in <ref type="figure" target="#fig_1">Fig. 2</ref>, but are evident in the TIR image. We believe that the multiplication operation can highlight drowned regions of RGB features with the evident content of TIR features in the product features, and simultaneously extract the coexistence information of two features. Therefore, we first perform the element-wise multiplication operation on f 5 r and f 5 t , generating f 5 mul ? R h5?w5?c5 . In <ref type="figure">Fig. 4</ref>, we visualize f 5 r , f 5 t , and f 5 mul . In the first two cases of nighttime, the persons and objects are almost invisible in RGB images, but are obvious in TIR images. This makes the object regions in f 5 r indistinguishable, but the object regions in f 5 t can be easily highlighted, resulting in the object regions in the product features f 5 mul are highlighted more accurately than those in f 5 r . In the last two cases, the object regions in f 5 t and f 5 r are relatively clear. Through the multiplication operation, the common objects in f 5 t and f 5 r can be extracted, resulting in that f 5 mul can highlight the objects. Then, we model the pixel-level correlation of {f 5 mul , f 5 r } and {f 5 mul , f 5 t }, respectively, to collaboratively identify objects in cross-modal features, which can be formulated as follows:</p><formula xml:id="formula_3">f 5 r?corr = Corr(f 5 mul , f 5 r ), f 5 t?corr = Corr(f 5 mul , f 5 t ),<label>(1)</label></formula><p>where {f 5 r?corr , f 5 t?corr } ? R h5?w5?c5 are the cross-modal correlation features, and Corr(?) is the co-attention operation <ref type="bibr" target="#b60">[61]</ref>. Here, taking f 5 mul and f 5 r as an example, the coattention operation computes the spatial dependencies of f 5 mul and f 5 r through the matrix multiplication, producing an affinity matrix, and then transfers the valuable information of the affinity matrix to f 5 r . 2) Correlation Combinations. f 5 r?corr and f 5 t?corr have a strong representation of the object location, and there are differences between them. How to effectively fuse them is important. In addition to the multiplication operation mentioned before, we believe that the summation operation can extract comprehensive features and reduce information leakage. Therefore, we adopt a hybrid scheme to combine the cross-modal correlation features as follows:</p><formula xml:id="formula_4">f 5 clm = (f 5 r?corr ? f 5 t?corr ) (f 5 r?corr ? f 5 t?corr ), (2) where f 5 clm ? R h5?w5?c5</formula><p>is the output feature of the CLM, ?/? and are the element-wise multiplication/summation and concatenation, respectively, and we omit the convolutional layer.</p><p>Through the above collaborative multiplication and summation operations, we can obtain informative features of object location. Furthermore, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we attach a location head (LocHead) after the CLM, and impose the location supervision to achieve more accurate object location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Complementary Activation Module</head><p>Middle-level cross-modal features (i.e., {f 2?4 r , f 2?4 t }) occupy the majority and play the role of gradually refining objects and restoring the resolution. Since we locate all potential objects in high-level features, we try to specifically activate exact regions of objects in middle-level features with different scales and propose Complementary Activation Module to achieve it. Concretely, our CAM is based on the attention mechanism <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b37">[38]</ref>, which can activate specific feature regions <ref type="bibr" target="#b37">[38]</ref> and establish stronger connections between features <ref type="bibr" target="#b11">[12]</ref>. We continue to take advantage of the two types of complementary feature combinations, i.e., summation and multiplication, to mine the common valuable content.</p><p>We illustrate the CLM in <ref type="figure" target="#fig_3">Fig. 5</ref>, whose inputs are f i r and f i t . In general, the CLM contains two steps of feature combinations and feature activation. First, we achieve feature combinations by element-wise multiplication and summation, generating {f i mul , f i sum } i=2,3,4 ? R hi?wi?ci . Since f i mul represents the coexistence information and f i sum represents the comprehensive information without omission, we make an attempt to enhance the broad f i sum with the pithy f i mul for fine region activation. Notably, to preserve the high-response regions of f i mul , we forgo smoothing it with the convolutional layer. We apply a convolutional layer to f i sum for feature normalization.</p><p>Second, we apply the spatial attention <ref type="bibr" target="#b37">[38]</ref> to f i mul to obtain the pithy spatial attention map, and use this map to highlight the target regions in f i sum , generating f i sa ? R hi?wi?ci . In addition, we introduce the channel self-attention <ref type="bibr" target="#b11">[12]</ref> to model the channel-wise dependencies of f i sa , generating the output feature of CAM, f i cam ? R hi?wi?ci . Using two types of attention mechanisms in spatial and channel domains can enhance the robustness of discriminative feature representations.</p><p>To sum up, we briefly formulate the above process as follows:</p><formula xml:id="formula_5">f i cam = CSA SA(f i r ? f i t ) ? conv(f i r ? f i t ) , i = 2, 3, 4,<label>(3)</label></formula><p>where SA(?) and CSA(?) are spatial attention and channel self-attention, respectively, and conv(?) is the convolutional layer. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the CAM is arranged on three levels of cross-modal features, which is conducive to mining the multi-level complementary information and can activate the same regions of objects at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Edge Sharpening Module</head><p>In semantic segmentation, there are usually multiple objects of different categories in an RGB-T pair, and their sizes are varied. In CLM and CAM, we successfully explore the common valuable content of high-and middle-level features. Low-level features contain rich texture information, which can describe the details of objects. Therefore, we propose Edge Sharpening Module for low-level features to explore the common valuable content and perceive details with different receptive fields.</p><p>We illustrate the ESM in <ref type="figure" target="#fig_4">Fig. 6</ref>, whose inputs are f 1 r and f 1 t . Similar to CLM, the ESM contains feature combinations and multi-head dilated convolutions. Feature combinations are achieved by element-wise multiplication and summation (accompanied by the convolutional layer), producing {f 1 mul , f 1 sum } ? R h1?w1?c1 . Then, we add f 1 mul and f 1 sum , and design a parallel structure with dilated convolutions <ref type="bibr" target="#b61">[62]</ref> to extract multi-scale detail information. Finally, we aggregate multi-scale features by concatenation, generating the output feature of ESM, f 1 esm ? R h1?w1?c1 . We briefly summarize the above process as follows:</p><formula xml:id="formula_6">f 1 esm = M HDC conv(f 1 r ? f 1 t ) ? conv(f 1 r ? f 1 t ) ,<label>(4)</label></formula><p>where M HDC(?) represents the multi-head dilated convolutions.</p><p>As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, similar to CLM, we attach an edge head (EdgeHead) after the ESM, and impose the edge supervision to achieve more accurate edge information. Moreover, considering that f 1 esm is used to sharpen the edges of objects, we attach a semantic head (SemHead) after the second decoder block (i.e., D 2 ) and impose the semantic supervision to improve the accuracy of feature representation for object regions, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In this way, f 1 esm can better sharpen the edges of object regions in the output feature of D 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Function</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, in addition to the semantic supervision on D 2 , the decoder also has a semantic supervision on the final segmentation result S sem . Therefore, during the training phase, our LASNet has a total of four supervisions, including a location supervision, an edge supervision, and two semantic supervisions. Specifically, for the location supervision (L loc ), the edge supervision (L eg ) and the semantic supervision on D 2 (L sem2 ), we adopt the widely used weighted cross-entropy loss. For the final semantic supervision (L sem ), we adopt a hybrid loss, including the weighted cross-entropy loss and the Lovasz-softmax loss <ref type="bibr" target="#b64">[65]</ref>. To make the network more focused on the semantic segmentation result, we reduce the attention of the location supervision by empirically setting a coefficient of 0.5 for it. Compared to the semantic supervision, the edge supervision has a small loss that does not affect the semantic segmentation result, so we do not adjust it.</p><p>Here, we summarize the total training loss function L total as follows: ? ? ? L total = 0.5L loc + L eg + L sem2 + L sem , L loc/eg/sem2 = wbce (up(S loc/eg/sem2 ), GT loc/eg/sem ),</p><formula xml:id="formula_7">L sem = wbce (S sem , GT sem ) + Lovasz (S sem , GT sem ),<label>(5)</label></formula><p>where S loc /S eg is generated by the LocHead/EdgeHead, S sem2 and S sem are generated by the SemHead, and GT is the corresponding ground truth. We implement the above predicted heads using a convolutional layer. Notably, existing RGB-T semantic segmentation datasets only provide the ground truth of semantic segmentation, so we make GT loc and GT eg algorithmically. We assign the background class of GT sem as background with value of zero and the other classes as foreground with value of one, obtaining the binary location ground truth GT loc . We extract the edges of all classes except the background class, and set the pixel values in the edges to one and others to zero, composing the binary edge ground truth GT eg . With the effective training loss, our LASNet can converge well and produce satisfactory segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Experimental Protocol 1) Datasets. We train and evaluate the proposed method on MFNet dataset <ref type="bibr" target="#b17">[18]</ref> and PST900 dataset <ref type="bibr" target="#b18">[19]</ref>. 2) Evaluation Metrics. We adopt two widely used quantitative evaluation metrics to evaluate the segmenattion performance of our method and all compared methods, including mean accuracy (mAcc) and mean intersection over union (mIoU), which can be respectively calculated as follows:</p><formula xml:id="formula_8">mAcc = 1 C C k=1 T P k T P k + F N k ,<label>(6)</label></formula><formula xml:id="formula_9">mIoU = 1 C C k=1 T P k T P k + F P k + F N k ,<label>(7)</label></formula><p>where C is the total number of classes including background class, which is respectively set to nine and five in the MFNet dataset and PST900 dataset, and T P k , F P k and F N k represent the true positives, false positives and false negatives of class k, respectively. The higher the two metrics, the better.</p><p>3) Implementation Details. We implement the proposed LASNet on the PyTorch <ref type="bibr" target="#b65">[66]</ref> with an NVIDIA GTX 3090 GPU (24GB RAM). For the MFNet dataset and PST900 dataset, we train and test the proposed LASNet according to the dataset partition described earlier. During the training phase, we keep the original resolution of RGB image and TIR image for network input, use the random flipping and cropping for data augmentation, and adopt the Ranger optimizer with the batch size of 4 and the initial learning rate of 5e ?5 . The parameters of the feature extractor are initialized by the pre-trained ResNet-152 model <ref type="bibr" target="#b52">[53]</ref>, and the parameters of other newly added convolutional layers are initialized through the "Kaiming" method <ref type="bibr" target="#b66">[67]</ref>. For both datasets, we train the proposed LASNet for 200 epochs. During the testing phase, we directly input the RGB-T image pair with original resolution into the trained LASNet without any post-processing to obtain the segmentation result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with State-of-the-arts</head><p>We compare our LASNet with relevant state-of-the-art RGB/RGB-D/RGB-T semantic segmentation methods. For a fair comparison, following previous works <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>, we also modify some RGB semantic segmentation methods to adapt to RGB-T image pairs. We retrain the RGB, RGB-D, and modified RGB methods with their default parameter settings on the same training set as our method on two datasets. We obtain the segmentation maps of other RGB-T methods through the public benchmarks or codes. 1) Comparison on the MFNet Dataset. On the MFNet dataset, we compare our LASNet with 14 state-of-the-art methods, including two RGB semantic segmentation methods (i.e., DANet <ref type="bibr" target="#b11">[12]</ref> and HRNet <ref type="bibr" target="#b62">[63]</ref>) and their modified RGB-T versions, four RGB-D semantic segmentation methods (i.e., FuseNet <ref type="bibr" target="#b54">[55]</ref>, D-CNN <ref type="bibr" target="#b58">[59]</ref>, ACNet <ref type="bibr" target="#b56">[57]</ref>, and SA-Gate <ref type="bibr" target="#b57">[58]</ref>), and eight RGB-T semantic segmentation methods (i.e., MFNet <ref type="bibr" target="#b17">[18]</ref>, two versions of RTFNet <ref type="bibr" target="#b24">[25]</ref>, PSTNet <ref type="bibr" target="#b18">[19]</ref>, MLFNet <ref type="bibr" target="#b25">[26]</ref>, FuseSeg <ref type="bibr" target="#b27">[28]</ref>, ABMDRNet <ref type="bibr" target="#b29">[30]</ref>, MMNet <ref type="bibr" target="#b63">[64]</ref>, and EGFNet <ref type="bibr" target="#b33">[34]</ref>).</p><p>We report the quantitative performance of our method and all compared methods on the MFNet dataset in Tab. I, including performance of eight classes and overall performance. Overall, our method achieves the best performance in terms of two metrics, especially on mAcc. Across all 18 metrics, our method achieves seven first places and four second places, demonstrating a competitive adaptability to different scenes. Concretely, on mAcc, our method outperforms the second best method (i.e., EGFNet) by a large margin, reaching 2.7%. On mIoU, our method outperforms the second best method (i.e., EGFNet and ABMDRNet) by 0.1%. Among all classes, our method performs very well on the class of car stop, surpassing EGFNet by 8.1% and 5.8% on mAcc and mIoU, respectively.</p><p>In addition, we report the quantitative performance of some available methods on the test set of MFNet dataset in daytime and nighttime in Tab. II. Our method shows its superiority in both scenes, especially in the nighttime scene. This means that the three steps (i.e., object location, region activation, and edge sharpening) of our method is successful and can make full use of TIR images to handle low illumination scenes. The above quantitative analysis clearly demonstrates the effectiveness of our LASNet on the MFNet dataset.</p><p>Furthermore, to visually compare the segmentation results, we show the segmentation results of our method and eight representative state-of-the-art methods in daytime and nighttime on the test set of MFNet in <ref type="figure">Fig. 7</ref>. The the first four cases belong to daytime, and the the last four belong to nighttime. In these four daytime cases with cluttered backgrounds, our method can accurately identify and locate all categories of objects in the first three cases, while other methods miss or mis-segment objects. This benefits from the CLM and CAM in our method, where the CLM is responsible for accurate location and the CAM is responsible for exact region activation. In the last case of the daytime scene, the objects segmented by our method are more complete than those of other methods, which benefits from the precise activation of CAM and the edge sharpening of ESM. In these four nighttime cases with adverse illumination conditions, it is obvious that the objects are mostly barely visible in the RGB images, but clearly in the TIR images. Thanks to the valuable information mined by the three proposed modules from cross-modal features, our method achieves satisfactory segmentation results in all difficult nighttime scenes even in small object scenes (the last case). In summary, our method can handle both challenging scenes to generate good segmentation results, showing strong generalization.</p><p>2) Comparison on the PST900 Dataset. On the PST900 dataset, we compare our LASNet with 10 state-of-theart methods, including three RGB semantic segmentation methods (i.e., ERFNet <ref type="bibr" target="#b67">[68]</ref>, CCNet <ref type="bibr" target="#b9">[10]</ref>, and Efficient-FCN <ref type="bibr" target="#b45">[46]</ref>) and their modified RGB-T versions, two RGB- D semantic segmentation methods (i.e., ACNet <ref type="bibr" target="#b56">[57]</ref> and SA-Gate <ref type="bibr" target="#b57">[58]</ref>), and five RGB-T semantic segmentation methods (i.e., MFNet <ref type="bibr" target="#b17">[18]</ref>, PSTNet <ref type="bibr" target="#b18">[19]</ref>, MFFENet <ref type="bibr" target="#b28">[29]</ref>, EGFNet <ref type="bibr" target="#b33">[34]</ref>, and MTANet <ref type="bibr" target="#b30">[31]</ref>). We report the quantitative performance of our method and all compared methods on the PST900 dataset in Tab. III, including performance of five classes and overall performance. Overall, our method achieves competitive performance on the PST900 dataset. Across all 12 metrics, our method achieves four first places and six second places. Specifically, our method ranks first on mIoU, outperforming the second-place SA-Gate by 5.35%. Our method ranks second on mAcc, 2.46% lower than EGFNet, but 5.89% ahead of EGFNet on mIoU. Notably, our method improves the IoU score for the fireextinguisher class by an astonishing 9.00%. The above analysis demonstrates the effectiveness of our LASNet on the PST900 dataset, as well as its applicability on different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>We conduct comprehensive ablation experiments to evaluate the effectiveness of each module of our LASNet on the MFNet dataset. Specifically, we evaluate the individual and joint contributions of the three modules, the effectiveness of each component of ESM, CAM and CLM, and the validity of auxiliary location supervision and edge supervision. For all ablation experiments, we train the variant with the same parameter and dataset settings as described in Sec. IV-A, and adopt mIoU to evaluate the performance.</p><p>1) The individual and joint contributions of the three modules. We propose ESM, CAM and CLM to achieve three steps of location, activation and sharpening with discriminative feature processing. Here, we provide four variants to assess  2) The effectiveness of each component of ESM. To assess the effectiveness of each component of ESM, we provide three variants of ESM in the upper part of Tab. V: 1) removing the feature combination of multiplication, i.e., w/o mul, 2) removing the feature combination of summation, i.e., w/o sum, and 3) replacing multi-head dilated convolutions with a convolutional layer, i.e., w/o MHDC. We observe that mining multi-scale details information in only one feature combination (multiplication or summation) is unsatisfactory, resulting in performance degradation. The similar performance of w/o mul and w/o sum, i.e., 53.3% and 53.2%, indicates these two types of feature combinations are equally important in ESM. w/o MHDC means that only the single-scale detail information can be extracted, which is not conducive to detecting the edges of objects of different scales. Therefore, the performance of w/o MHDC drops.</p><p>3) The effectiveness of each component of CAM. In CAM, we enhance f i sum with f i mul for fine region activation. To assess the effectiveness of this process, we provide a variant, which removes the feature combination of multiplication and employs self-enhancement on f i sum , i.e., w/o mul2. The results shown in Tab. V illustrate that the self-enhancement of f i sum is sub-optimal, and that the multiplication operation can extract more compact information that is conducive to accurate region activation. Besides, we remove the spatial attention, i.e., w/o SA, to prove the effectiveness of the pithy spatial attention map in region activation. Moreover, we remove the channel self-attention, i.e., w/o CSA, to illustrate the importance of establishing channel-wise dependencies for region activation. The dropped performance of w/o SA and w/o CSA in Tab. V means that using one kind of attention alone is insufficient, and that combining the attention of two different domains is powerful for exploring discriminative information.</p><p>4) The effectiveness of each component of CLM. To assess the effectiveness of each component of CLM, we provide three variants of CLM in the bottom part of Tab. V: 1) removing the cross-modal correlation modeling, i.e., w/o Corr, 2) removing the correlation combination of multiplication, i.e., w/o mul3, and 3) removing the correlation combination of summation, i.e., w/o sum3. w/o Corr discards building cross-modal pixellevel correlations that can collaboratively locate objects, so its performance drops by 2.6%, which is the worst in Tab. V. This indicates that the cross-modal correlation model is extremely important. It is sub-optimal to use only one combination operation to fuse cross-modal correlations, resulting in a 1.7% performance drop for w/o mul3 and a 1.9% performance drop for w/o sum3. This phenomenon is the same as the performance degradation of w/o mul and w/o sum of ESM, which justifies the rationality of our LASNet in exploring the common valuable content of cross-modal features at different levels through multiplication and summation.</p><p>5) The advantages of the combination of the element-wise multiplication and the element-wise summation in our three modules. To assess the advantages of the combination of the element-wise multiplication and the element-wise summation in our three modules, we provide five variants for feature integration in Tab. VI (M, S, and C means the elementwise multiplication, the element-wise summation, and the concatenation, respectively): 1) C+S means replacing M with C and keeping S, 2) M+C, 3) S+M, 4) S+C, and 5) C+M. Here, we define our original structure as M+S. We observe that the performance of all five variants is inferior to our M+S, which illustrates the optimality of our combination of M and S. We believe that the better performance of M+S can be attributed to 1) the reasonable use of the coexistence information mined by M and the comprehensive information mined by S, and 2) successful extraction of complementary information of the above information for segmentation. Besides, the reasons for the poor performance of other variants may be as follows: 1) the concatenation is a general operation for feature integration, but its generated features cannot effectively express the complementary information of the cross-model features, and 2) the position of operations affects the extraction of complementary information, and the exchange of position will lead to the failure to extract effective complementary information.</p><p>6) The validity of auxiliary location supervision and edge supervision. It is important to employ proper supervision in the training phase, which can improve the segmentation accuracy without increasing the computational cost in the inference phase. To assess the validity of auxiliary location supervision and edge supervision attached to CLM and ESM, we provide three combinations of supervision with default coefficients in Tab. VII: 1) removing both supervisions, 2) keeping only location supervision, and 3) keeping only edge supervision.</p><p>We observe that discarding one or all of the supervision is not good for the final segmentation performance. Under the effective guidance of both supervisions, CLM and ESM can enhance the representation of object location and fine edges, contributing to the final segmentation results of our LASNet. Moreover, to prove the rationality of coefficient setting of location supervision, we provide a variant of the location supervision with a coefficient of 1. As shown in Tab. VII, we observe a 1.0% decrease in the performance of the last variant, which means that it is reasonable to reduce the weight of location supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we follow the feature fusion paradigm, and propose LASNet for RGB-T semantic segmentation. The main idea of LASNet is to process RGB and TIR features at different levels with specific modules. Therefore, we propose three plug-and-play modules for LASNet, namely CLM, CAM, and ESM, which are responsible for object location, region activation and edge sharpening, respectively. In these three modules, we focus on extracting the complementary information between the two feature combinations of multiplication and summation rather than directly extracting the complementary information between RGB and TIR features, which is different from previous methods. The tight integration of the paradigm and the three modules results in our complete solution. Comprehensive ablation studies demonstrate the effectiveness of the above three modules, and extensive comparisons demonstrate the superiority of our LASNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Three typical cross-modal fusion paradigms for RGB-T semantic segmentation, including encoder fusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Pipeline of the proposed LASNet. Overall, LASNet follows the feature fusion paradigm, and consists of three parts, including a feature extractor, three specific modules, and a decoder. Here, we adopt the ResNet-152<ref type="bibr" target="#b52">[53]</ref> as the backbone, and construct a parallel ResNet-152 structure for feature extraction, generating five-level basic cross-modal features. Then, we arrange Collaborative Location Module (CLM), Complementary Activation Module (CAM), and Edge Sharpening Module (ESM) for the high-, middle-, and low-level features, respectively. CLM enhances the location representation of all potential objects. CAM activates exact regions of different objects in three intermediate levels of features. ESM sharpens the edges of objects. Finally, according to the output features of the above three modules, we generate the segmentation result Ssem in the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?Fig. 3 .</head><label>3</label><figDesc>Element-wise Multiplication Corr. Correlation ? Concatenation Illustration of the Collaborative Location Module (CLM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of the Complementary Activation Module (CAM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of the Edge Sharpening Module (ESM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>MFNet dataset 1</head><label>1</label><figDesc>contains 1,569 RGB-T image pairs captured by the InfReC R500 camera and corresponding pixellevel semantic annotations. 820 RGB-T image pairs are taken at daytime, and the remaining 749 pairs are at nighttime. The resolution of all RGB and TIR images is 480?640. In addition to the background class, this dataset contains eight classes, including car, person, bike, curve, car stop, guardrail, color cone, and bump. All RGB-T image pairs are divided into three parts, of which 784 pairs (410 daytime images and 374 nighttime images) are used for training, 392 pairs (205 daytime images and 187 nighttime images) for validation, and 393 pairs (205 daytime images and 188 nighttime images) for testing. PST900 dataset 2 contains 894 aligned RGB and TIR image pairs, where RGB images are captured by a Stereolabs ZED Mini stereo camera and TIR images are captured by a FLIR Boson 320 camera. The resolution of all RGB and TIR images is 1280?720. This dataset contains four classes of handdrill, backpack, fire-extinguisher and survivor, and a default background class. This dataset is divided into two parts for training and testing, of which 597 pairs 3 constitute the training set and 288 pairs constitute the testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>authors are with Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication, Shanghai Institute for Advanced Communication and Data Science, Shanghai University, Shanghai 200444, China, and School of Communication and Information Engineering, Shanghai University, Shanghai 200444, China (email: ligongyang@shu.edu.cn; shuwangyike@shu.edu.cn; liuzhisjtu@163.com; xzhang@shu.edu.cn; dzeng@shu.edu.cn).</figDesc><table /><note>Corresponding authors: Zhi Liu and Xinpeng Zhang.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>COMPARISONS (%) ON THE TEST SET OF MFNET DATASET. THE VALUE 0.0 REPRESENTS THAT THERE ARE NO TRUE POSITIVES. '-' MEANS THAT THE AUTHORS DO NOT PROVIDE THE CORRESPONDING RESULTS. THE TOP TWO RESULTS INEACH COLUMN ARE HIGHLIGHTED IN RED AND BLUE.Acc IoU Acc IoU Acc IoU Acc IoU Acc IoU Acc IoU Acc IoU Acc IoU DANet19<ref type="bibr" target="#b11">[12]</ref> RGB 89.8 84.5 65.2 55.0 76.5 62.6 44.2 33.4 32.7 27.4 2.8 0.9 46.6 41.9 56.0 44.5 57.0 49.7 DANet19 [12] RGB-T 91.3 71.3 82.7 48.1 79.2 51.8 48.0 30.2 25.5 18.2 5.2 0.7 47.6 30.3 19.9 18.8 55.2 41.3 HRNet19 [63] RGB 91.1 84.9 66.6 55.4 76.6 60.3 42.6 33.3 37.9 28.3 11.5 2.5 44.8 40.3 62.6 46.9 59.2 49.9 HRNet19 [63] RGB-T 90.8 86.9 75.1 67.3 70.2 59.2 39.1 35.3 28.0 23.1 12.1 1.7 50.4 46.6 55.8 47.3 57.9 51.7 87.9 81.4 71.7 78.5 64.6 68.4 44.8 29.1 22.7 63.7 6.4 55.8 46.9 66.4 47.9 70.6 54.5 ABMDRNet21 [30] RGB-T 94.3 84.8 90.0 69.6 75.7 60.3 64.0 45.1 44.1 33.1 31.0 5.1 61.7 47.4 66.2 50.0 69.5 54.8 RGB-T 95.8 87.6 89.0 69.8 80.6 58.8 71.5 42.8 48.7 33.8 33.6 7.0 65.3 48.3 71.1 47.1 72.7 54.8 LASNet (Ours) RGB-T 94.9 84.2 81.7 67.1 82.1 56.9 70.7 41.1 56.8 39.6 59.5 18.9 58.1 48.8 77.2 40.1 75.4 54.9</figDesc><table><row><cell>Methods Type</cell><cell>Car</cell><cell>Person</cell><cell>Bike</cell><cell>Curve</cell><cell cols="3">Car Stop Guardrail Color Cone Bump</cell><cell>mAcc mIoU</cell></row><row><cell cols="8">FuseNet16 [55] RGB-D 81.0 75.6 75.2 66.3 64.5 51.9 51.0 37.8 28.7 15.0 0.0 0.0 31.1 21.4 51.9 45.0 52.4 45.6</cell></row><row><cell cols="8">D-CNN18 [59] RGB-D 85.2 77.0 61.7 53.4 76.0 56.5 40.2 30.9 9.9 29.3 22.8 6.4 32.9 30.1 36.5 32.3 55.1 46.1</cell></row><row><cell cols="8">ACNet19 [57] RGB-D 93.7 79.4 86.8 64.7 77.8 52.7 57.2 32.9 51.5 28.4 7.0 0.8 57.5 16.9 49.8 44.4 64.3 46.3</cell></row><row><cell cols="8">SA-Gate20 [58] RGB-D 86.0 73.8 80.8 59.2 69.4 51.3 56.7 38.4 24.7 19.3 0.0 0.0 56.9 24.5 52.1 48.8 58.3 45.8</cell></row><row><cell cols="8">MFNet17 [18] RGB-T 77.2 65.9 67.0 58.9 53.9 42.9 36.2 29.9 19.1 9.9 0.1 8.5 30.3 25.2 30.0 27.7 45.1 39.7</cell></row><row><cell cols="8">RTFNet5019 [25] RGB-T 91.3 86.3 78.2 67.8 71.5 58.2 69.8 43.7 32.1 24.3 13.4 3.6 40.4 26.0 73.5 57.2 62.2 51.7</cell></row><row><cell cols="8">RTFNet15219 [25] RGB-T 93.0 87.4 79.3 70.3 76.8 62.7 60.7 45.3 38.5 29.8 0.0 0.0 45.5 29.1 74.7 55.7 63.1 53.2</cell></row><row><cell cols="8">PSTNet20 [19] RGB-T -76.8 -52.6 -55.3 -29.6 -25.1 -15.1 -</cell><cell>39.4</cell><cell>-45.0</cell><cell>-</cell><cell>48.4</cell></row><row><cell cols="8">MLFNet21 [26] RGB-T -82.3 -68.1 -67.3 -27.3 -30.4 -15.7 -</cell><cell>55.6</cell><cell>-40.1</cell><cell>-</cell><cell>53.8</cell></row><row><cell cols="6">FuseSeg21 [28] RGB-T 93.1 MMNet22 [64] RGB-T -83.9 -69.3 -59.0 -43.2 -24.7 -</cell><cell>4.6</cell><cell>-</cell><cell>42.2</cell><cell>-50.7 62.7 52.8</cell></row><row><cell>EGFNet22 [34]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II QUANTITATIVE</head><label>II</label><figDesc>COMPARISONS (%) ON THE TEST SET OF MFNET DATASET IN DAYTIME AND NIGHTTIME. THE BEST RESULT IN EACH COLUMN IS HIGHLIGHTED IN RED.</figDesc><table><row><cell>Car</cell><cell></cell><cell>Person</cell><cell>Bike</cell><cell>Curve</cell><cell>Car Stop</cell><cell cols="2">Guardrailar</cell><cell cols="2">Color Cone</cell><cell>Bump</cell></row><row><cell>RGB</cell><cell>TIR</cell><cell>GT</cell><cell>Ours</cell><cell>EGFNet</cell><cell cols="2">MMNet ABMDRNet RTFNet</cell><cell>MFNet</cell><cell>SA-Gate</cell><cell>ACNet</cell><cell>HRNet-T</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Methods</cell><cell>Type</cell><cell cols="2">Daytime Acc IoU</cell><cell>Nighttime Acc IoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DANet19 [12]</cell><cell>RGB</cell><cell>61.0</cell><cell>46.3</cell><cell>52.6</cell><cell>47.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DANet19 [12]</cell><cell>RGB-T</cell><cell>50.9</cell><cell>37.5</cell><cell>52.4</cell><cell>40.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HRNet19 [63]</cell><cell>RGB</cell><cell>64.7</cell><cell>46.7</cell><cell>54.0</cell><cell>47.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HRNet19 [63]</cell><cell>RGB-T</cell><cell>54.4</cell><cell>46.1</cell><cell>55.1</cell><cell>50.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FuseNet16 [55]</cell><cell>RGB-D</cell><cell>49.5</cell><cell>41.0</cell><cell>48.9</cell><cell>43.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>D-CNN18 [59]</cell><cell>RGB-D</cell><cell>50.6</cell><cell>42.4</cell><cell>50.7</cell><cell>43.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ACNet19 [57]</cell><cell>RGB-D</cell><cell>60.7</cell><cell>41.6</cell><cell>63.9</cell><cell>47.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SA-Gate20 [58]</cell><cell>RGB-D</cell><cell>49.3</cell><cell>37.9</cell><cell>56.9</cell><cell>45.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MFNet17 [18]</cell><cell>RGB-T</cell><cell>42.6</cell><cell>36.1</cell><cell>48.9</cell><cell>43.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RTFNet5019 [25]</cell><cell>RGB-T</cell><cell>57.3</cell><cell>44.4</cell><cell>59.4</cell><cell>52.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RTFNet15219 [25]</cell><cell>RGB-T</cell><cell>60.0</cell><cell>45.8</cell><cell>60.7</cell><cell>54.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MLFNet21 [26]</cell><cell>RGB-T</cell><cell>-</cell><cell>45.6</cell><cell>-</cell><cell>54.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FuseSeg21 [28]</cell><cell>RGB-T</cell><cell>62.1</cell><cell>47.8</cell><cell>67.3</cell><cell>54.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ABMDRNet21 [30]</cell><cell>RGB-T</cell><cell>58.4</cell><cell>46.7</cell><cell>68.3</cell><cell>55.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LASNet (Ours)</cell><cell>RGB-T</cell><cell>73.3</cell><cell>45.2</cell><cell>72.8</cell><cell>58.7</cell></row></table><note>Fig. 7. Visual comparisons of our method and eight representative state-of-the-art methods in daytime (the first four cases) and nighttime (the last four cases) on the test set of MFNet. HRNet-T represents the modified HRNet for RGB-T image pairs. Please zoom-in for the best view.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>COMPARISONS (%) ON THE TEST SET OF PST900 DATASET. '-' MEANS THAT THE AUTHORS DO NOT PROVIDE THE CORRESPONDING RESULTS. THE TOP TWO RESULTS IN EACH COLUMN ARE HIGHLIGHTED IN RED AND BLUE.</figDesc><table><row><cell>Methods Type</cell><cell cols="8">Background Hand-Drill Acc IoU Acc IoU Acc IoU Acc Backpack Fire-Extinguisher IoU</cell><cell cols="2">Survivor Acc IoU</cell><cell cols="2">mAcc mIoU</cell></row><row><cell>ERFNet18 [68] RGB</cell><cell>-</cell><cell>98.69</cell><cell>-</cell><cell>42.40</cell><cell>-</cell><cell>65.28</cell><cell>-</cell><cell>61.18</cell><cell>-</cell><cell>41.69</cell><cell>-</cell><cell>61.85</cell></row><row><cell>ERFNet18 [68] RGB-T</cell><cell>-</cell><cell>98.73</cell><cell>-</cell><cell>52.76</cell><cell>-</cell><cell>68.08</cell><cell>-</cell><cell>58.79</cell><cell>-</cell><cell>34.38</cell><cell>-</cell><cell>62.55</cell></row><row><cell cols="8">CCNet19 [10] RGB 99.86 99.05 51.77 32.27 68.30 66.42 67.79</cell><cell>51.84</cell><cell cols="4">60.84 57.50 69.71 61.42</cell></row><row><cell cols="8">CCNet19 [10] RGB-T 99.59 98.74 54.09 51.01 75.96 72.95 88.06</cell><cell>73.80</cell><cell cols="4">49.45 33.52 73.43 66.00</cell></row><row><cell cols="8">EfficientFCN20 [46] RGB 99.81 98.63 32.08 30.12 60.06 58.15 78.87</cell><cell>39.96</cell><cell cols="4">32.76 28.00 60.72 50.98</cell></row><row><cell cols="8">EfficientFCN20 [46] RGB-T 99.80 98.85 48.75 38.58 69.90 67.59 76.45</cell><cell>46.28</cell><cell cols="4">38.86 35.06 66.75 57.27</cell></row><row><cell cols="8">ACNet19 [57] RGB-D 99.83 99.25 53.59 51.46 85.56 83.19 84.88</cell><cell>59.95</cell><cell cols="4">69.10 65.19 78.67 71.81</cell></row><row><cell cols="8">SA-Gate20 [58] RGB-D 99.74 99.25 89.88 81.01 89.03 79.77 80.70</cell><cell>72.97</cell><cell cols="4">64.19 62.22 84.71 79.05</cell></row><row><cell>MFNet17 [18] RGB-T</cell><cell>-</cell><cell>98.63</cell><cell>-</cell><cell>41.13</cell><cell>-</cell><cell>64.27</cell><cell>-</cell><cell>60.35</cell><cell>-</cell><cell>20.70</cell><cell>-</cell><cell>57.02</cell></row><row><cell>PSTNet20 [19] RGB-T</cell><cell>-</cell><cell>98.85</cell><cell>-</cell><cell>53.60</cell><cell>-</cell><cell>69.20</cell><cell>-</cell><cell>70.12</cell><cell>-</cell><cell>50.03</cell><cell>-</cell><cell>68.36</cell></row><row><cell>MFFENet22 [29] RGB-T</cell><cell>-</cell><cell>99.40</cell><cell>-</cell><cell>72.50</cell><cell>-</cell><cell>81.02</cell><cell>-</cell><cell>66.38</cell><cell>-</cell><cell>75.60</cell><cell>-</cell><cell>78.98</cell></row><row><cell cols="8">EGFNet22 [34] RGB-T 99.48 99.26 97.99 64.67 94.17 83.05 95.17</cell><cell>71.29</cell><cell cols="4">83.30 74.30 94.02 78.51</cell></row><row><cell>MTANet22 [31] RGB-T</cell><cell>-</cell><cell>99.33</cell><cell>-</cell><cell>62.05</cell><cell>-</cell><cell>87.50</cell><cell>-</cell><cell>64.95</cell><cell>-</cell><cell>79.14</cell><cell>-</cell><cell>78.60</cell></row><row><cell cols="8">LASNet (Ours) RGB-T 99.77 99.46 92.36 77.75 90.80 86.48 91.81</cell><cell>82.80</cell><cell cols="4">83.43 75.49 91.63 84.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV QUANTITATIVE</head><label>IV</label><figDesc>RESULTS (%) OF ASSESSING THE INDIVIDUAL AND JOINT CONTRIBUTIONS OF THE THREE MODULES IN LASNET. THE BEST ONE IS RED. Baseline+CLM. The quantitative results are reported in Tab. IV. "Baseline" only achieves 48.3% on mIoU, which is 6.6% lower than our full LASNet, demonstrating that the three modules can indeed improve the segmentation accuracy. With the help of ESM, CAM or CLM, No.2, No.3, and No.4 variants improve the performance compared to "Baseline", respectively.Furthermore, we provide three variants to assess the joint contribution of the three modules in Tab. IV: 5) Base-line+CAM+CLM, 6) Baseline+ESM+CLM, and 7) Base-line+ESM+CAM. With the cooperation of two modules, the</figDesc><table><row><cell>No.</cell><cell>Baseline</cell><cell>ESM</cell><cell>CAM</cell><cell>CLM</cell><cell>MFNet [18]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mIoU</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.3</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.9</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.5</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.4</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.6</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.0</cell></row><row><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.0</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>54.9</cell></row><row><cell cols="6">the individual contribution of the three modules: 1) Baseline,</cell></row><row><cell cols="6">where we employ element-wise summation to fuse RGB and</cell></row><row><cell cols="6">TIR features instead of three modules, 2) Baseline+ESM,</cell></row><row><cell cols="3">3) Baseline+CAM, and 4)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V QUANTITATIVE</head><label>V</label><figDesc>RESULTS (%) OF ASSESSING THE EFFECTIVENESS OF EACH COMPONENT OF ESM, CAM, AND CLM. THE BEST ONE IS RED. three modules makes the excellent full LASNet, reaching 54.9%. The above analysis shows that the proposed modules are valid and contribute to the final segmentation performance.</figDesc><table><row><cell>Aspects</cell><cell>Models</cell><cell>MFNet [18] mIoU</cell></row><row><cell></cell><cell>LASNet (Ours)</cell><cell>54.9</cell></row><row><cell>ESM</cell><cell>w/o mul w/o sum</cell><cell>53.3 53.2</cell></row><row><cell></cell><cell>w/o MHDC</cell><cell>53.5</cell></row><row><cell>CAM</cell><cell>w/o mul2 w/o SA</cell><cell>53.7 52.7</cell></row><row><cell></cell><cell>w/o CSA</cell><cell>52.9</cell></row><row><cell>CLM</cell><cell>w/o Corr w/o mul3</cell><cell>52.3 53.2</cell></row><row><cell></cell><cell>w/o sum3</cell><cell>53.0</cell></row><row><cell cols="3">performance of the above three variants is further improved</cell></row><row><cell cols="3">compared to that of a single module. The perfect coopera-</cell></row><row><cell>tion of the</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI QUANTITATIVE</head><label>VI</label><figDesc>RESULTS (%) OF ILLUSTRATING THE ADVANTAGES OF THE COMBINATION OF THE ELEMENT-WISE MULTIPLICATION AND THE ELEMENT-WISE SUMMATION IN OUR THREE MODULES. M MEANS THE ELEMENT-WISE MULTIPLICATION, S MEANS THE ELEMENT-WISE SUMMATION, AND C MEANS THE CONCATENATION. THE BEST ONE IS RED.</figDesc><table><row><cell>Models</cell><cell>M+S (Ours)</cell><cell>C+S</cell><cell>M+C</cell></row><row><cell>mIoU</cell><cell>54.9</cell><cell>54.2</cell><cell>53.6</cell></row><row><cell>Models</cell><cell>S+M</cell><cell>S+C</cell><cell>C+M</cell></row><row><cell>mIoU</cell><cell>53.3</cell><cell>53.6</cell><cell>53.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII QUANTITATIVE</head><label>VII</label><figDesc>RESULTS (%) OF ASSESSING THE VALIDITY OF AUXILIARY LOCATION SUPERVISION (LOCSUP) AND EDGE SUPERVISION (EDGESUP). THE NUMBER IN PARENTHESES IS THE COEFFICIENT OF LOCSUP. THE BEST ONE IS RED.</figDesc><table><row><cell cols="2">No. LocSup EdgeSup</cell><cell>MFNet [18] mIoU</cell></row><row><cell>1</cell><cell></cell><cell>53.0</cell></row><row><cell>2</cell><cell>(0.5)</cell><cell>53.9</cell></row><row><cell>3</cell><cell></cell><cell>53.4</cell></row><row><cell>4</cell><cell>(1)</cell><cell>53.9</cell></row><row><cell>5</cell><cell>(0.5)</cell><cell>54.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.mi.t.u-tokyo.ac.jp/static/projects/mil multispectral/ 2 https://github.com/ShreyasSkandanS/pst900 thermal rgb<ref type="bibr" target="#b2">3</ref> Notably, there are actually only 597 pairs in the training set of previous works<ref type="bibr" target="#b18">[19]</ref>,<ref type="bibr" target="#b28">[29]</ref>,<ref type="bibr" target="#b30">[31]</ref>,<ref type="bibr" target="#b33">[34]</ref> rather than 606 pairs.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning hybrid semantic affinity for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4599" to="4612" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Partial domain adaptation on semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3798" to="3809" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2015-10" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Entropy guided adversarial domain adaptation for aerial image semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CCNet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="7242" to="7252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation by a class-level multiple group cosegmentation and foreground fusion strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4823" to="4836" />
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SegFormer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021-12" />
			<biblScope unit="page">90</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stage-aware feature alignment network for real-time semantic segmentation of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4444" to="4459" />
			<date type="published" when="2022-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring cross-image pixel contrast for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="7303" to="7313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation: A prototype view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MFNet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ IROS</title>
		<meeting>IEEE/RSJ IROS</meeting>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="5108" to="5115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PST900: RGB-thermal calibration, dataset and segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICRA</title>
		<meeting>IEEE ICRA</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9441" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RGBT salient object detection: Benchmark and a novel cooperative ranking approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4421" to="4433" />
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ECFFNet: Effective and consistent feature fusion network for RGB-T salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1224" to="1235" />
			<date type="published" when="2022-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">APNet: Adversarial learning assistance and perceived importance fusion network for all-day RGB-T salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerg. Top. Comput. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="957" to="968" />
			<date type="published" when="2022-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CGMDRNet: Cross-guided modality difference reduction network for RGB-T salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6308" to="6323" />
			<date type="published" when="2022-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RGBT tracking by trident fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="579" to="592" />
			<date type="published" when="2022-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">RTFNet: RGB-thermal fusion network for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2576" to="2583" />
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust semantic segmentation based on RGB-thermal in variable lighting scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiminxu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meas</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page">110176</biblScope>
			<date type="published" when="2021-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FEANet: Feature-enhanced attention network for RGB-thermal real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ IROS</title>
		<meeting>IEEE/RSJ IROS</meeting>
		<imprint>
			<date type="published" when="2021-09" />
			<biblScope unit="page" from="4467" to="4473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FuseSeg: Semantic segmentation of urban scenes based on RGB and thermal data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1000" to="1011" />
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MFFENet: Multiscale feature fusion and enhancement network for RGB-thermal urban road scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2526" to="2538" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ABM-DRNet: Adaptive-weighted bi-directional modality difference reduction network for RGB-T semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="2633" to="2642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MTANet: Multitask-aware network with hierarchical multimodal fusion for RGB-T urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/zero-shot/arxiv_pdf/10.1109/TIV.2022.3164899</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Veh</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GMNet: Gradedfeature multilabel-learning network for RGB-thermal urban scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="7790" to="7802" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GCNet: Grid-like contextaware network for RGB-thermal semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">508</biblScope>
			<biblScope unit="page" from="60" to="67" />
			<date type="published" when="2022-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Edge-aware guidance fusion network for RGB-thermal scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yaguan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2022-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Zero-shot video object segmentation with co-attention siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2228" to="2242" />
			<date type="published" when="2022-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lightweight salient object detection in optical remote sensing images via feature correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DenseASPP for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Encoder-decoder with cascaded crfs for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1926" to="1938" />
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">OCNet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">EfficientFCN: Holisticallyguided decoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="561" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-modal weighting network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="665" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ICNet: Information conversion network for RGB-D based salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4873" to="4884" />
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hierarchical alternate interaction network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3528" to="3542" />
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Glass segmentation with RGB-Thermal image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05453</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Depth-aware glass surface detection with cross-modal context mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11250</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ShapeConv: Shape-aware convolutional layer for indoor RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="7068" to="7077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">FuseNet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning common and specific features for RGB-D semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="664" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">ACNET: Attention based network to exploit complementary features for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICIP</title>
		<meeting>IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-andaggregation gate for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="561" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for RGB-D segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="144" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Zig-Zag network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2642" to="2655" />
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="5686" to="5696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">MMNet: Multi-modal multi-stage network for RGB-T image semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5817" to="5829" />
			<date type="published" when="2022-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The Lovasz-softmax loss: A tractable surrogate for the optimization of the intersection-overunion measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
