<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fully Self-Supervised Learning for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wang</surname></persName>
							<email>wangyuan19@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
							<email>weizhuo@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Wang</surname></persName>
							<email>wangzhi@sz.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ju</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ShenZhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent ShenZhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Tencent ShenZhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University ShenZhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Tencent ShenZhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fully Self-Supervised Learning for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Corresponding authors. This work was done while Yuan Wang was a Research Intern in Ten-cent.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present a fully self-supervised framework for semantic segmentation(F S 4 ). A fully bootstrapped strategy for semantic segmentation, which saves efforts for the huge amount of annotation, is crucial for building customized models from end-to-end for open-world domains. This application is eagerly needed in realistic scenarios. Even though recent self-supervised semantic segmentation methods have gained great progress, these works however heavily depend on the fully-supervised pretrained model and make it impossible a fully self-supervised pipeline. To solve this problem, we proposed a bootstrapped training scheme for semantic segmentation, which fully leveraged the global semantic knowledge for selfsupervision with our proposed PGG strategy and CAE module. In particular, we perform pixel clustering and assignments for segmentation supervision. Preventing it from clustering a mess, we proposed 1) a pyramid-global-guided (PGG) training strategy to supervise the learning with pyramid image/patch-level pseudo labels, which are generated by grouping the unsupervised features. The stable global and pyramid semantic pseudo labels can prevent the segmentation from learning too many clutter regions or degrading to one background region; 2) in addition, we proposed context-aware embedding (CAE) module to generate global feature embedding in view of its neighbors close both in space and appearance in a non-trivial way. We evaluate our method on the large-scale COCO-Stuff dataset and achieved 7.19 mIoU improvements on both things and stuff objects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a task that gives each pixel in one image a class label. Fully supervised segmentation has gained great success due to deep learning on massive annotations. Pixel-level annotation is however extremely expensive. It is merely impossible to annotate pixels for the increasing open-world applications. This inspires us to design a fully unsupervised semantic segmentation scheme that can automatically recognize pixels belonging to different classes without human annotations.</p><p>The target of self-supervised semantic segmentation is to bootstrapped discover the categories of pixels. It requires the model can automatically discover existing semanticmeaningful categories in the dataset and group pixels to the corresponding categories. Just recently, the pioneer works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> gave their attempts on this task by clustering on local elements, i.e., patches or pixels. These methods have achieved promising results based on ImageNet pretrained model, which is learned on 1k-class image annotations. For short, we name the supervised ImageNet pretrained model as ImageNet model afterwards. Altering the ImageNet model from an unsupervised pretrained model, which can be acquired by any instance-level selfsupervised (ISS) methods such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, the performance drops to nearly half. The reason for this fact is analyzed in <ref type="figure">Figure 2</ref>. The fully self-supervised capacity of segmentation, however, is important, since it is crucial for end-to-end customized model building for intensive open-world scenarios. In this work, we target at the fully unsupervised semantic segmentation. To this end, we propose a novel method that fully makes use of the global knowledge and raises the performance twice the mIoU, and matches the results based on a well-supervised pretrained model.</p><p>To build a fully supervised pipeline, a straightforward <ref type="bibr">Figure 1</ref>. Example of segmentation results, where (a) is the original image, (b) is the ground truth, (c) is the result of PiCIE <ref type="bibr" target="#b6">[7]</ref> and (d) is our result. We use the same color to visualize one semantic class in all above sub-images. Our method performs much better than the PiCIE conterpart, where we predict the correct semantic on river part while the PiCIE predicts a clutter of mess regions there.</p><p>way is to utilized an instance-level unsupervised method <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> to provide an initial model and then perform local(pixellevel) bootstrapped clustering and learning on it, such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. Grouping based on pixel/local-patches features itself is however unstable. Due to lack of global imagelevel guidance, its segmentation is easy to have too many small regions or too large background/stuff regions that cross boundaries. Starting from an unsupervised pretrained model makes the issue more serious, as shown in <ref type="figure">Fig 1.</ref> This could be because the local features of the supervised pretrained model are well-tuned for the object labels and its strong objective embedding in local features can work as global guidance during dataset clustering, while the local features from unsupervised methods are not welltuned, as shown in <ref type="figure">Figure.</ref> 2. Inspired by the above assumption, we propose a novel fully self-supervised segmentation method which fully leverages the global semantic knowledge with our pyramid global guidance(PGG) strategy for global supervision and our context-aware embedding(CAE) for global feature embedding.</p><p>In our work, we build a novel framework that performs two-level clustering, i.e. image-level and pixel-level, in two stages separately. The image-level clustering leverages the existent success of ISS methods and assigns image-level pseudo labels to pyramid views of images. The pseudo labels contain high-level semantics which is consistent in the dataset. The pyramid image pseudo labels keep unchanged during subsequent training on pixel-level clustering. Here we need to mention that even though instance self-supervised methods, such as <ref type="bibr">[2-6, 11, 14]</ref> lack capacity for direct dense recognition, such as pixel-wise segmentation, their global features is semantic meaningful, and they can work as the global signal for guidance. We generate the labels on pyramids instead of one label for the whole image to provide finer supervision on images with multiple instances.</p><p>For the pixel-clustering stage, we follow <ref type="bibr" target="#b6">[7]</ref> to bootstrap discover semantic clusters on pixel features and segment them. Improved on <ref type="bibr" target="#b6">[7]</ref>, we 1) used the obtained pyramid image pseudo labels to supervise image labeling through a CAM module <ref type="bibr" target="#b26">[27]</ref> on pixel labeling during the whole learning. In this way, our training intrinsically forces the pixel clustering to pay more attention to high-level semantics that defines the image class. This PGG strategy effectively bridges the gap raised by the suboptimal pretrained model; 2) In addition, we proposed a CAE module to enable the pixel clustering to be aware of its spatial neighbors in the image. We found that two close parts which are apparently the same semantic class depending on color can be segmented apart in pixel clustering of <ref type="bibr" target="#b6">[7]</ref>, such as the river part in <ref type="figure">Figure 1</ref>. Inspired by this observation, we attempt to use raw image features for segmentation, such as the color and position cues used in classical segmentation, such as graph cut <ref type="bibr" target="#b22">[23]</ref>, etc. The raw features, however, has been discovered to easily lead to collapsing solution in previous works. In our work, we found a non-trivial way to utilize both semantic features and raw features to improve the segmentation but avoid learning collapsing.</p><p>Here we keep the above two-level clusters disjoint to avoid global instance features being infected by unstable local ones.</p><p>In summary, our contributions are listed as follows,</p><p>? We designed a novel and effective pipeline for the pioneer task of fully self-supervised semantic segmentation, which has great practical value for the widely open-world recognition scenarios.</p><p>? We found a effective way using pyramid image-level pseudo-labels on the F S 4 task. The module we proposed is robust to process general images with multiple instances.</p><p>? We proposed a novel context-aware embedding module that improve the features by both semantic features and raw image features, and found a non-trivial way to avoid collapse results while using raw features. <ref type="figure">Figure 2</ref>. Pixel feature distribution on the supervised model <ref type="bibr" target="#b8">[9]</ref> and unsupervised model <ref type="bibr" target="#b2">[3]</ref> visualized using t-SNE. Here we grabbed the officially released pretrained models and extracted the features of their last layers on COCO-stuff dataset individually. We then resized them to size 80x80 using bilinear operation for pixel-wise grouping. We can see that the supervised pretrained model, i.e. ImageNet model, has better clusters on pixels. Since the whole learning is a bootstrapped process <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>, a good clustering at the initial state is crucial for globally guiding the subsequent pixel grouping. Better view in color.</p><p>Here note that our method does not require any kinds of labels. After training, the pixel cluster centers are used to segment the images in the validation dataset by assigning each pixel a label according to the distances between the pixel features and cluster centers. We show that in COCOstuff <ref type="bibr" target="#b0">[1]</ref> dataset, our method can outperform the previous methods nearly twice on mIoU when both things and stuff parts are counted in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Self-supervised learning. self-supervised learning, or unsupervised learning in another name, develops very quickly recently. Most of these works focus on learning representations on an image level. Among these works, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b10">11]</ref> learn the representations by forcing the models to learn instances from different images uniquely, and they have no cluster concepts. Some other works, such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, try to introduce class semantics by clustering procedure. In this way, they naturally assign a pseudo label for each image. Note here, to avoid trivial solution, <ref type="bibr" target="#b1">[2]</ref> conducts offline clustering, and <ref type="bibr" target="#b2">[3]</ref> uses the Sinkhorn-Knopp algorithm to prevent all images grouped to one large cluster. All the above methods can be categorized into instance-level selfsupervised learning, which focus on distinguishing images. To this end, only the most discriminative parts in each image may be focused on, and many stuff information is ignored. This is contradicting with the targets of dense prediction, such as object detection and segmentation, which care about the boundaries of both stuff(background structures) and things(objects). As a consequence, the instance selfsupervised models can hardly segment an image directly.</p><p>To make the unsupervised model adapted to dense downstream target, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> designs contrast training strategy with dense correspondence. These methods, however, still cannot realize automatically recognize pixels.</p><p>Self-supervised semantic segmentation. Very recently the self-supervised semantic segmentation has gained increasing attention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. These methods aim to recognize classes of dense pixels. Clustering is a straightforward way to discover semantic classes and perform recognition on pixels or patches. IIC <ref type="bibr" target="#b13">[14]</ref> an invariant information clustering method via maximizing the mutual information between encoded image pairs. A later work <ref type="bibr" target="#b6">[7]</ref> conducts alternative offline pixel-wise clustering and online training, where the training is led by the invariance and equivariance objective on the assigned clusters. It also introduces latent contrast learning when the pixel is trained to be assigned to the cluster center/prototype which is assigned offline ahead. Reference <ref type="bibr" target="#b6">[7]</ref> however only depends on local features, and it performs much worse when the training starts from a sub-optimal point. Our method attempts to improve it via introducing image-level pseudo labels, which is related to weakly supervised learning.</p><p>Weakly supervised segmentation. In a weakly supervised task, it utilizes weak labels of images to facilitate training supervision. Generally, these labels can be bound-ing boxes, image-level labels, etc. For the target of segmentation, class activation maps (CAM) <ref type="bibr" target="#b26">[27]</ref>, which is the response map generated from a pretrained image classifier, are widely used as pseudo labels. Later works, such as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>, are proposed to refine the CAM via leveraging the consistency among segments on different geometric or photometric transformations of the same image. They proved that when learning the refined pseudo labels we can get better segmentation. The existent weak-supervised segmentation methods depend on an image-level classifier, whose classes are usually defined by the foreground objects. In our case, the semantics of the image pseudo labels are mixed, which is bootstrapped popped up depending on the objects, background contents, scene types and etc, through the learning and discovering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Baseline Method</head><p>Before we step on our method, we first introduce a baseline method <ref type="bibr" target="#b6">[7]</ref> that our method builds on. This method builds the unsupervised semantic segmentation learning by clustering pixel features based on their invariance and equivariance regulation. We will format the training procedure mathematically in the following part.</p><p>In the baseline work, it builds a siamese network with two branches. For image x, on each branch, it applies random photometric transformations, such as Gaussian blur and color distortion, on the image independently and obtains an image view. On one branch, it first applies geometric transform such as cropping and flipping on its image view and then fed it to the convolutional neural network (CNN) model to get the feature set F <ref type="bibr" target="#b0">(1)</ref> . For the other branch, it first feeds the image view to CNN model and then performs random geometric transform to get the final feature set F <ref type="bibr" target="#b1">(2)</ref> .</p><p>We then apply K-means clustering method to the feature maps from the two branches seperately. On the two branches, it maintains two sets of clusters individually, that is, two sets of labels Y (1) = {y</p><formula xml:id="formula_0">(1) 1 , y (1) 2 , ..., y (1) N }, Y (2) = {y (2) 1 , y (2) 2 , ..., y<label>(2)</label></formula><p>N } for each pixels and two sets of cluster centers ? (1) = {?</p><formula xml:id="formula_1">(1) 1 , ? (1) 2 , ..., ? (1) K }, ? (2) = {? (2) 1 , ? (2) 2 , ..., ?<label>(2)</label></formula><p>K }. According to the PiCIE assumption that pixel labels should be invariant to its color transformation and equivariance to its geometric transformation, the cluster labels from two branches should be equivariance. The loss is formulated as following:</p><formula xml:id="formula_2">L p = L within + L cross (1) L within = 1 N i L clust (F (1) i , y<label>(1)</label></formula><formula xml:id="formula_3">i , ? (1) ) +L clust (F (2) i , y (2) i , ? (2) )<label>(2)</label></formula><formula xml:id="formula_4">L cross = 1 N i L clust (F (1) i , y (2) i , ? (2) ) +L clust (F (2) i , y (1) i , ? (1) ) (3) L clust (F i , y i , ?) = ? log( exp(?d(F i , ? yi )) k exp(?d(F i , ? k )) )<label>(4)</label></formula><p>L within represents the pixel feature vectors in one view should be closer to its cluster centers. L cross represents the pixel feature vectors in one view should be closer to the assigned cluster centers in the other view as well.</p><p>Note here, the K-means procedure is done after one epoch training on the whole dataset, and then its cluster center and label assignments are fixed during the training of the next epoch. This offline and disjoint design is crucial to prevent trivial solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>Our fully unsupervised semantic segmentation scheme is shown in <ref type="figure">Fig. 3</ref>. As a fully unsupervised scheme, the model is first trained by an instance-level self-supervised(ISS) method on a large-scale dataset. To guide the training procedure by high-level semantics, we generate pyramid pseudo labels for each image in the training set based on the ISS features using our Pyaramid-global-guided(PGG) strategy. The pyramid pseudo labels keep unchanged and they are used as one of the supervised signals during training. We assume the global semantic acquired by the ISS model is stable and consistent among the dataset. Note that pseudo labels are generated on image pyramids to process images with multiple objects.</p><p>During training, we supervised the training based on both the generated pyramid pseudo labels and its pixelcluster consistency <ref type="bibr" target="#b6">[7]</ref>. In the training step, following the baseline method in section 3, we first need to generate pixellevel cluster centers and assignment labels to the dataset. We apply different photometric transformations to one image to get two views of the image. Each view is fed a Siamese network but one view is applied geometric transformations firstly. Different from the baseline <ref type="bibr" target="#b6">[7]</ref>, to help the pixel features catch the information of the whole image, the output feature map of the Siamese network is refined by our novel CAE module, which leverages the neighborhood relationship based on both features and image raw information. Acquired the refined pixel features, we conduct pixel clustering and dataset training alternatively in a disjoint manner.</p><p>To train the network, we supervise the model with both pixel level and our image label loss. In the pixel level, we use the invariance and equivariance loss L p in Eq. <ref type="bibr" target="#b0">(1)</ref> In the image level, the pyramid pseudo labels provide the loss L w of the PGG strategy in a weakly-supervision man- <ref type="figure">Figure 3</ref>. The overview of our framework. The green parts are novel modules proposed in this work. The whole framework is guided by our pyramid global guidance(PGG) strategy, which includes the pyramid view and label generation, active selection, and the global-level guidance in the clustering process. In particular, the approach consists of two phases, which are our pyramid view and label generation with active selection, and the pixel clustering process. We generate a pyramid of images and assign each subimage in the pyramid of an image a pseudo label, and the subimages selected by active selection are treated as independent images for training. During the clustering process, the feature learning and clustering processes are guided by pseudo labels. Specifically, we feed each training image to two branches shown above. In the top branch, we apply geometric transformations on the feature map, while we apply the same geometric transformation on the image directly in the bottom branch. The two branches process different views but share the CNN network. Features of each view are extracted and collected for both pixel clustering and image classification. Note that the cross in the figure means each view needs to go through both the clustering and classification branches. ner, where L w is a cross-entropy classification loss. This loss encourages the dominant representation to match its image pseudo label. The overall loss is formulated as follows:</p><formula xml:id="formula_5">L = L w + L p .<label>(5)</label></formula><p>In the test stage, the cluster centers generated from the training data are used to label pixels in the test data. The test images are fed to the Siamese network to get feature vectors for pixels, then distances to each cluster center are computed. A post-process CRF model <ref type="bibr" target="#b14">[15]</ref> is applied to the class scores to each cluster center to refine the segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pyramid-global-guided strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Pseudo label generation</head><p>Labeling method. We generate pseudo labels by clustering method based on an existent ISS model, such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. Specifically, we first extract image features, which are acquired by global average pooling on the feature map from the last layer of an ISS pretrained neural network. We then perform K-means on the dataset where the ISS model is trained to get cluster centers. The cluster center is then used as a classifier to assign other images. Here we normalize all the features and cluster centers to unit norm. Here note that, for some ISS methods, such as SwAV and DeepCluster, the cluster centers, or prototypes in another name, is available directly. Nevertheless, we still need to group these prototypes into smaller cluster numbers for space-saving and dataset adaptation. Given the cluster centers, an image is labeled by its nearest cluster center based on the cosine distance of image global average features. In our experiment, to compare with PiCIE <ref type="bibr" target="#b6">[7]</ref>, we also adopt the supervised pretrained models as the initialization. In such a situation, we directly use the fully connected layer of the model to generate pseudo labels.</p><p>Pyramid views. In our strategy, we apply the labeling to the pyramid of image views as shown in <ref type="figure">Fig 3,</ref> where each subimage, i.e. a cropped view, in the pyramid is treated as an independent image for labeling and training afterwards. We introduce the pyramid labeling based on the observation that a scene image could contain multiple objects and each subimage has a dominant one. One label for one image may not cover all the information in that image. This inspires us to divide the images into several subimages and assign each subimage a pseudo label. In our setting, we divide the image into 5 small crops of four on corners and one on the center of the image, then we have 6 views in all for one image, considering the original one. With the help of the ISS model, we obtain a set of pyramid labels for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Active selection.</head><p>To generate stable pseudo labels, we also proposed an active selection procedure to pruning the views of ambiguous semantics. Since the small subimages of images are only part of the image and its object may be incomplete. This scenario would lead to ambiguous semantic, and a lower score to its closest cluster center. In the end, we design an active selection mechanism to decide whether a label in the pyramid is used for training. We calculate the probabilities of the labels in pyramid views. In details,we assume that the distance between the global feature and the cluster center reflects the probability that the view contains distinguishable contents. Based on this assumption, we rank the view images according to their feature distance with their cluster centers, and then we only select the top 40% images for training. Here the parameter 40% is chosen based on experimental performance.</p><p>Once the image pseudo labels are generated, we fix them during the whole training procedure. Global information and local pixel mining are both important for the segmentation task and they are complementary to each other. We keep the image clustering and pixel mining disjoint to prevent the stable global information from the ISS model distracted by pixel local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Global guidance</head><p>Given the image pseudo-labels, during training, we add a classification module after the feature map to learn the image-level information from the pseudo labels.</p><p>Similar to class activation map (CAM) <ref type="bibr" target="#b26">[27]</ref>, we apply a softmax function to the output feature map of the backbone to calculate the probabilities of different classes. The crossentropy loss is computed between the pseudo labels and the probabilities. The loss function is formulated as follows:</p><formula xml:id="formula_6">L w = ? log( exp(q l ) m exp(q m ) ),<label>(6)</label></formula><p>where q is output of the classification module and q m is the score on the m-th cluster center or class. In Eq.(6), l is the pseudo label. In this loss, we encourage the dominant semantic pooled on the image to match a pre-defined image pseudo-label which is explicitly learned on instances. The classical CAM <ref type="bibr" target="#b26">[27]</ref> compute q by q = GAP(g(F )), where g is a 1 ? 1 convolution layer for classification and F ? R C?H?W is the feature map after the CAE module. GAP indicates global average pooling here. In our work, we modify this module to q = g 1 (ReLU(GAP(g 2 (F )))), where g 1 and g 2 are linear layers and 1 ? 1 convolution layer respectively. The g 1 is for classification. We add a ReLU layer and a linear layer after the original output. This modification makes the classification module can consider all the pixels by a non-linear function before finally deciding the category of the image. We experimentally find that the non-linear operation improves the ability of classification and summarizing. Here we use g or g 1 to classify the image or one of the pyramid views to one of global semantics generated by PGG. Note that, the PGG and pixel-level clustering have separate cluster pools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Context-aware embedding module</head><p>To help the embedding vector of each pixel learn the knowledge of the whole image, we proposed a contextaware embedding (CAE) module which uses a self-attention mechanism. The CAE module is shown in <ref type="figure" target="#fig_0">Fig. 4</ref>. We build the communication among pixels based on their similarities. The similarity among pixels is defined by three types of features: 1) the high-level embedding from CNN features; 2) the color feature on the raw image; 3) the geometrical feature, i.e, the spatial coordinates in the image. We calculate the cosine similarity on CNN features, that is</p><formula xml:id="formula_7">s ij = d(f i , f j ) = fi?fj fi fj , where f i is the feature vector for pixel i in feature map f .</formula><p>Inspired by <ref type="bibr" target="#b14">[15]</ref>, we formulate the similarity on color and position features via a Gaussian kernel as follows.</p><formula xml:id="formula_8">k ij = ? 1 k 1ij + ? 2 k 2ij ,<label>(7)</label></formula><formula xml:id="formula_9">k 1ij = exp(? |p i ? p j | 2 2? 2 1 ? |I i ? I j | 2 2? 2 ),<label>(8)</label></formula><formula xml:id="formula_10">k 2ij = exp(? |p i ? p j | 2 2? 2 3 ),<label>(9)</label></formula><p>where p i denotes the position of pixel i, I i is the color vector of pixel i, ? 1 and ? 2 are two positive linear combination weights. The k 1 shows that the closer distance from one pixel to the other in color space, the more attention should be paid while considering the position. The k 2 indicates one pixel should pay more attention to the nearer pixels. The non-local factor for pixel j to pixel i is then calculated as P ij = s ij k ij . This procedure is also similar to a graph model <ref type="bibr" target="#b14">[15]</ref>, in which the parameters are manually defined. In our situation, however, it is hard to pick suitable values. We set these parameters as differentiable variables and use backpropagation to find the proper values. Finally, the feature map is refined by F i = h(f i ) + j P ij f i , where F i is the new feature vector for pixel i and h is a projector which is a 1 ? 1 convolution layer. In our work, we surprisedly find that encoding raw information on the graph edge is better than using CNN features only. It could be because in an unsupervised scenario CNN feature can collapse locally that the features learn to one point even though they belong to different objects and their appearances are of great difference. The raw information on relations plays a complementary role here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Post process</head><p>After the clustering procedure, we refine the segmentation to follow object boundaries leveraging a CRF model <ref type="bibr" target="#b14">[15]</ref>. We calculate the probabilities of each pixel to the cluster centers as the unary of the graphical model. We model the edge on the graph by the color and positional information. The CRF model actually has a similar function with the proposed CAE module, but we found that an additional CRF model can still make the segmentation rough 0.5 mIoU better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In our method, we perform the fully self-supervised training for segmentation without any kinds of annotations. In experimental evaluation, we show comparison with previous methods, ablation study and extension evaluations on fully-supervised initialization and the semi-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>Dataset. Here we evaluate our methods on two largescale datasets, i.e. COCO-stuff dataset <ref type="bibr" target="#b7">[8]</ref> and Cityscapes <ref type="bibr" target="#b7">[8]</ref>. The COCO-stuff dataset is a dataset containing 80 things categories and 91 stuff categories. In <ref type="bibr" target="#b6">[7]</ref>, its classes are merged into 27 categories (15 stuff and 12 things), with 49629 for training and 2175 for testing. We follow <ref type="bibr" target="#b6">[7]</ref> for training and testing. In the evaluation, we evaluate our method both on stuff (background contents) and things (foreground objects). Cityscapes <ref type="bibr" target="#b7">[8]</ref> is a dataset with street scenes collected from fifty cities. We process the dataset exactly the same way as <ref type="bibr" target="#b6">[7]</ref> for fair comparison. Follow <ref type="bibr" target="#b6">[7]</ref>, We use the same 7 out of the 8 groups for training and test-ing. The ground truth testing cluster number is set as 27. Its train, train extra and test subsets are used for training and test on the val subset. To prove that our method has ability to leverage additional data, we train our model with and without additional data of a similar domain, such as the ImageNet dataset <ref type="bibr" target="#b8">[9]</ref>, individually. Note that, even though we use additional data, we not use their annotations and our model is still fully self-supervised.</p><p>Model architectures. We mainly adopt the ResNet-50 <ref type="bibr" target="#b11">[12]</ref> with FPN <ref type="bibr" target="#b17">[18]</ref> as our backbone unless otherwise mentioned. The FPN decoder, which is consists of a convolutional layer and an upsampling module, uses the output feature map from the residual layer-1 to layer-4. the FPN output dimension is set as 256. For the CAE module, we downsample the raw image and feature map to 1/16 of the original resolution to calculate the relation map. After the relationship modeling, the refined feature map is upsampled to 1/8 of the image resolution to match the projector output which is a linear process on the FPN feature. Here we by default use our full model except that in table. 1 we use the classical CAM structure.</p><p>Training details. In our work, we first pretrain a model using instance-level unsupervised methods, i.e. SwAV <ref type="bibr" target="#b2">[3]</ref> and MoCoV2 <ref type="bibr" target="#b4">[5]</ref>, for initialization. The model is then used for both the subsequent training and global imagelevel pseudo label generation by our PGG. When we train our fully self-supervised model with additional data, i.e. ImageNet dataset, we just use their official released models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> and then perform the above-mentioned process on the target dataset. Here we use the same hyperparameters such as learning rate, pixel-level cluster number, batch size with <ref type="bibr" target="#b6">[7]</ref> except that we set FPN dimension as 256. In our work, we perform clustering on two levels, i.e. local and global, and they have separate cluster pools. For the local level, the group number is decided by their ground truth semantic class number, which is 27 on both COCO-stuff and Cityscape datasets. For the global level, we group 50 clusters by default.</p><p>Evaluation details. After training, we run the trained model on the training images again and cluster the pixellevel features using the K-mean algorithm. The clustering centers are then used to segment the testing images. According to the distance between the pixel features and the cluster center, each pixel is assigned a label. Depending on the predicted segmentation, we then use Hungarianmatching <ref type="bibr" target="#b15">[16]</ref> to match the cluster centers with the ground truth labels. Note that, for a fair comparison, all our statistical results in experiments are achieved on models without the post process. We only visualize the post process in <ref type="figure" target="#fig_1">Fig. 1 and Fig. 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with SOTA on F S 4 task</head><p>Comparison on COCO-stuff. For the fully selfsupervised semantic segmentation, there are only a few works explored on the task. Here we trained our model depending on only the COCO-stuff dataset without additional data and fully-supervised pretrained model. In particular, following our pipeline we first train an initial model using ISS methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, and then apply our PGG strategy and pixel-clustering training. We compare our methods with previous methods trained on the same setting in <ref type="table">Table.</ref> 1, where results on previous works are adopted from <ref type="bibr" target="#b6">[7]</ref>. For a fair comparison, we also re-implement PiCIE by altering their initial model with a better-pretrained model, which is trained by MoCoV2 on the COCO-stuff dataset. With an ISS method pretraining, the PiCIE performs better. From the results, we can see that our method performs much better than previous works. We outperform the better version of PiCIE by 15.5% accuracy which attests to the superiority of our method. Here we compare accuracy results due to that most of the previous methods only report the accuracy results.</p><p>Comparison on models trained with additional data. To demonstrate the stability of our method and our ability of leveraging additional data which has close domains with the target scene, we also train our model based on additional data, such as the ImageNet dataset, for the COCO-stuff and Cityscapes target scenes. Here we grabbed the officially released pretrained model trained by <ref type="bibr" target="#b4">[5]</ref> on the ImageNet dataset for initialization. We assume the pretrained model has contained the knowledge of new data and we can leverage the additional data through the pretrained model. We then feed the pretrained model to the PiCIE and our pipeline individually for segmentation. The results are shown in <ref type="table">Table 2</ref> and <ref type="table">Table 3</ref>. Note that we re-implement the PiCIE with its official code. From the results, we can see that our method stably outperforms PiCIE on both category splits on different initializations. We nearly double the mIOU on All categories and reach the upper bound shown in <ref type="table">Table.</ref> [7], which demonstrates the effectiveness of our method again. For futher study, we trained our model on SwAV ImageNet pretrained model as well, and we get 13.56 mIoU while PiCIE get 6.78 mIoU. This result evidences that our work performs stable and robust on different self-supervised ini-Method Accuracy Random CNN 19.4 K-means <ref type="bibr" target="#b20">[21]</ref> 14.1 SIFT <ref type="bibr" target="#b18">[19]</ref> 20.2 Doersch 2015 <ref type="bibr" target="#b9">[10]</ref> 23.1 Isola 2016 <ref type="bibr" target="#b12">[13]</ref> 24.3 Deep cluster <ref type="bibr" target="#b1">[2]</ref> 19.9 IIC <ref type="bibr" target="#b13">[14]</ref> 27.7 AC <ref type="bibr" target="#b19">[20]</ref> 30.8 PiCIE <ref type="bibr" target="#b6">[7]</ref> 31.48</p><p>PiCIE* 33.0 Ours 48.5  <ref type="table">Table 2</ref>. COCO-Stuff dataset -Comparison on models trained with additional data. To prove our ability of leveraging additional data of similar domains, we trained the models with initialization which is trained on the ImageNet dataset with an ISS method. Note here, we still keep the model fully self-supervised since we do not use any annotations.</p><p>tialization methods. We also show the visualization comparison with the baseline method <ref type="bibr" target="#b6">[7]</ref>. From visualized results, we can see that our segmentation can focus on higher-level semantics, i.e. objects, and prevent segmentation of too many small regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation study</head><p>Effectiveness of different components. We perform the ablation study on the COCO-stuff dataset by incrementally adding our modules or training operations to our model, which includes 1) additional finetune on COCOstuff dataset with ISS methods, denoted as COCO-stuff  finetune; 2) the CAE module with classical CAM structure; 3) the proposed pseudo-labeling and global guidance strategy with one label per image, denoted as GG.; 4) its improved version with pyramid labels for each image, i.e. our PGG strategy; 5) and our modified CAM structure denoted as Mod.. We evaluate our method with the ResNet-50 backbone that is trained by MoCoV2 on additional data of the ImageNet dataset. The results are shown in <ref type="table">Table 4</ref>.</p><p>The results indicate that each of our proposed modules contributes to the final improvements, among them, our GG. module gains the most dramatic improvement. In addition, for the CAE module, we attempt to model the relationship with only the low-level raw information and with only the high-level CNN feature individually. On these two settings, our models achieve the mIoU of 12.01 and 9.83 respectively. It indicates that with our non-trivial design modeling raw relationships is helpful.</p><p>Study on global cluster number. In Tab. 5, we show how the class numbers in our GG strategy influence the results. The results present that our method is insensitive to the cluster/class number. The performances are stable on a set of cluster numbers before 100.  <ref type="table">Table 6</ref>. Compassion on fully-supervised initialization. To compare with the PiCIE, we add the same over-clustering loss as described in <ref type="bibr" target="#b6">[7]</ref>, which is denoted as +H.</p><p>number close to the true class number of the dataset, that is 27 for the COCO-stuff dataset, can give better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison on fully-supervised initialization.</head><p>To compare with PiCIE, we also follow its setting using ResNet-18 as the backbone and initializing the training from the fully-supervised pretrained model on ImageNet <ref type="bibr" target="#b8">[9]</ref>. Results are shown in <ref type="table">Table 6</ref>. Although our method is designed for a totally unsupervised manner, we can still outperform the prior art, which indicates the global guidance is still helpful when the learning starts from a good state. We produce the pseudo labels for training images from the fully connected classification layer of the supervised models. The results here serve as an upper bound for our fully unsupervised setting. We can see that our method bridges the gap raised by suboptimal initialization and its result nearly reaches the upper bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a novel method that is able to train a segmentation model in a fully unsupervised manner. The proposed pyramid global guidance(PGG) strategy and context-aware embedding(CAE) module encourage pixel features to pay attention to high-level image semantics while learning their own concept. We show that our method can effectively and automatically discover high-level semantics without any human labels. The method we propose is also robust to the different training starting points. Nevertheless, unsupervised semantic segmentation, due to lacking strong and stable supervision signal, can easily run into local collapsing, we will keep making effort to solve this problem in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment details</head><p>The base training settings closely follow PiCIE <ref type="bibr" target="#b6">[7]</ref>. We describe these parameters as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Data processing</head><p>During training, we pre-process the images by resizing and center cropping to 320 ? 320. We apply random photometric transformations including color jitter, gray scale, and Gaussian blur with the probability 0.8, 0.2, 0.5. The color jitter contains jittering brightness, contrast, saturation and hue, whose control factors are 0.3, 0.3, 0.3, 0.1. The control factor of Gaussian blur is randomly chosen from [0.1, 2.0]. For geometric transformation, we use random crop and random horizontal flip. The crop scale is randomly chosen from [0.5, 1.0]. The probability for the horizontal flip is 0.5.</p><p>When assigning the pseudo labels, we follow <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> to resize images to 256?256 and center crop them to 224?224 for inference. We then use the features from projectors for clustering. For pyramid pseudo labels, we crop five views for each image from its four corners and the center. These views are resized to 640?640 and then processed following the same procedure for image-level pseudo label generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training parameters</head><p>We train the network for 10 epochs using ADAM optimizer with the learning rate 1e-3 and no weight decay. The batch size is 128.</p><p>In our experiment, we find out that unbalanced loss leads to a better result for PiCIE <ref type="bibr" target="#b6">[7]</ref>. The result of unbalanced loss is 6.78 mIoU, while the result of balanced loss is 6.23, when PiCIE method uses SwAV <ref type="bibr" target="#b2">[3]</ref> initialization. We thus report the results on unbalanced loss in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Segmentation comparison</head><p>Here we present more segmentation results in <ref type="figure">Fig. 6</ref>. Our results are processed with our full model with the CRF <ref type="bibr" target="#b14">[15]</ref>. From these figures, we can see that our method can better understand the high-level semantics of images, while for the foreground objects PiCIE <ref type="bibr" target="#b6">[7]</ref> is more concentrated on the low-level cues such as the edges and the colors.</p><p>Since our method leverages the global pseudo labels, we pay much attention on dominant objects and scene stuff, and in some cases neglect small objects in the scene. This issue can be solved to some extent by our pyramid labels. We will also take it as our future work to solve this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Image-level pseudo labels</head><p>We present the images with the same pseudo labels in <ref type="figure">Fig. 7</ref>, where the images are from both the original views and crop views. From the visualization, we can conclude that our pseudo label generation strategy can effectively group the images with similar semantics together. <ref type="figure">Figure 6</ref>. Segmentation results. <ref type="figure">Figure 7</ref>. Image grouping results. We show the images and crop views in cluster 1 and cluster 2. Cluster 1 mainly contains images with glass. Cluster 2 mainly contains images with bird. The crop views are more concentrated on these objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>The structure of CAE module. The in the figure represents Hadamard product.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>The results of semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison on COCO-stuff dataset.</figDesc><table><row><cell>We compare</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Cityscapes dataset -Comparison on models trained with additional data.We trained the models with initialization which is trained on the ImageNet dataset with an ISS method. Ablation study -Effectiveness of different modules. We show the results when using the MoCoV2 ISS pretrain method. COCO-stuff finetune means we use the MoCoV2 algorithm to train the model further on the COCO-stuff dataset. GG. means global guidance that we generate one pseudo label for each image for image-level supervision and do not use the pyramid views for training.</figDesc><table><row><cell>COCO-stuff finetune</cell><cell>GG.</cell><cell>CAE module</cell><cell>PGG strategy</cell><cell>Mod. mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>7.57</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>7.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>9.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>12.61</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>13.43</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>14.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on global cluster number. For imagelevel supervision when using MoCoV2 initialization on COCOstuff dataset.</figDesc><table><row><cell>It may indicate that a cluster</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors</editor>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Big selfsupervised models are strong semi-supervised learners. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PiCIE: Unsupervised semantic segmentation using invariance and equivariance in clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Jang Hyun Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo?vila</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno>abs/2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06811</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Neural Information Processing Systems, NIPS&apos;11</title>
		<meeting>the 24th International Conference on Neural Information Processing Systems, NIPS&apos;11<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc. 5</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pseudo-mask matters in weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="6964" to="6973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Autoregressive unsupervised image segmentation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python. the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised learning of dense visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">Y</forename><surname>Benmalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph cut based image segmentation with connectivity priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="3024" to="3033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12272" to="12281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="16684" to="16693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
