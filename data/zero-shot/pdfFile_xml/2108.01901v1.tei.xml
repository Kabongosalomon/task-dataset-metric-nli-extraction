<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FPB: Feature Pyramid Branch for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">FPB: Feature Pyramid Branch for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-person re-identification</term>
					<term>feature pyramid branch</term>
					<term>attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High performance person Re-Identification (Re-ID) requires the model to focus on both global silhouette and local details of pedestrian. To extract such more representative features, an effective way is to exploit deep models with multiple branches. However, most multi-branch based methods implemented by duplication of part backbone structure normally lead to severe increase of computational cost. In this paper, we propose a lightweight Feature Pyramid Branch (FPB) to extract features from different layers of networks and aggregate them in a bidirectional pyramid structure. Cooperated by attention modules and our proposed cross orthogonality regularization, FPB significantly prompts the performance of backbone network by only introducing less than 1.5M extra parameters. Extensive experimental results on standard benchmark datasets demonstrate that our proposed FPB based model outperforms stateof-the-art methods with obvious margin as well as much less model complexity. FPB borrows the idea of the Feature Pyramid Network (FPN) from prevailing object detection methods. To our best knowledge, it is the first successful application of similar structure in person Re-ID tasks, which empirically proves that pyramid network as affiliated branch could be a potential structure in related feature embedding models. The source code is publicly available at https://github.com/anocodetest1/FPB.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P ERSON Re-Identification (Re-ID) has been extensively applied as a retrieval technique in large scale person tracking and related scenarios of intelligent video surveillance. Given a query sample of specific person, it aims to match the same person in the gallery set of samples which may be captured by cameras from different viewpoints in different backgrounds <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Due to the increasing demand of public safety, the practical importance of person Re-ID leads to more and more attention from community. Although significant progress has been witnessed during the last decade, there still exist some challenging problems in the study of person Re-ID, e.g., partial occlusions <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, drastic deformations of human pose <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, complex environment and background clutter <ref type="bibr" target="#b6">[7]</ref>, etc.</p><p>Recently, deep learning based models have been proven to be quite effective to tackle aforementioned problems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr">[9]</ref>. Pre-trained Convolutional Neural Network (CNN) models such as ResNet <ref type="bibr">[10]</ref> and InceptionNet <ref type="bibr">[11]</ref>  learning based feature embedding. These low-dimensional but representative features instead of original images of person can be exploited for high efficient matching as well as retrieval of unknown person in new set of instances.</p><p>In contrast to conventional vision tasks with large scale datasets, existing datasets for person Re-ID normally require algorithm to learn a classification model over a relatively large number of classes (&gt; 700 persons) with limited samples (&lt; 20, 000 images). Also, due to the slight difference between the final goal of feature embedding and learning of classification, simply using existing backbone models in Re-arXiv:2108.01901v1 [cs.CV] 4 Aug 2021 ID tends to deliver intermediate features without sufficient generalization. Many efforts have been devoted to alleviate this problem, including 1) particularly designed loss function to discover discriminative features, e.g., sphere loss <ref type="bibr">[12]</ref>, triplet loss <ref type="bibr" target="#b8">[13]</ref> and center loss <ref type="bibr" target="#b9">[14]</ref>; 2) adopting affiliated structure such as local part <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[15]</ref> or multiple branches <ref type="bibr" target="#b11">[16]</ref>, <ref type="bibr" target="#b12">[17]</ref>, <ref type="bibr" target="#b13">[18]</ref> to learn fine-grained features for higher diversity; 3) introducing attention mechanisms to emphasize feature correlations and prompt the efficiency of model <ref type="bibr" target="#b14">[19]</ref>. However, recent works focus on tediously adding extra structures to backbone networks for the gain of performance, which raises an interesting question. Are these structures really effective or performance can also be prompted by simply increasing the number of parameters, e.g., model with larger backbones or ensemble techniques <ref type="bibr" target="#b15">[20]</ref>.</p><p>In this paper, we comprehensively consider the performance as well as efficiency of person Re-ID models, proposing a compact model consisting of a backbone and a Feature Pyramid Branch (FPB). FPB is mainly inspired by the Feature Pyramid Network (FPN) structure in the field of object detection. As a common structure in prevailing object detectors <ref type="bibr" target="#b16">[21]</ref>, <ref type="bibr" target="#b17">[22]</ref>, <ref type="bibr" target="#b18">[23]</ref>, FPN has been proven to be quite effective in aggregating features at different scales. Although many works in the literature have proven that person Re-ID also requires feature extraction at different granularities <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[24]</ref>, due to the difference between tasks of Re-ID and object detection, it is still challenging to exploit FPN into person Re-ID architecture.</p><p>Here we address this problem, proposing a bidirectional pyramid structure cooperated by attentive auxiliary modules as a lightweight branch solution. In <ref type="figure" target="#fig_0">Fig. 1</ref>, we compare our proposed model with other ResNet based state-of-theart methods. We choose ResNet50 as a typical backbone instance since it is extensively adopted by existing Re-ID methods and shows high compatibility with various structures. Based on ResNet50, a backbone with 25.56M parameters, our proposed FPB based method achieves the best performance on all benchmarks by only introducing less than 1.5M extra parameters.</p><p>From <ref type="figure" target="#fig_0">Fig. 1</ref> one can see that, 1) for methods with parallel amounts of parameters as FPB (&lt;28M), e.g., Bag-Of-Tricks, PCB and KPM, the mean Average Precision (mAP) is significantly prompted over 5% on all of three datasets. 2) Other leading methods with approaching performance normally require models with more than 32M parameters, which implies at least 4? times extra parameters are added to the backbone. 3) Furthermore, FPB also outperforms methods with much higher complexity such as Pyramid, ABD and LocalCNN, which retain models with more than 50M learnable parameters. Our main contributions in this paper can be summarized as follows:</p><p>1) We propose a lightweight FPB which can be plugged into backbone network to form an asymmetrical multibranch architecture. Diverse features from different scales are extracted and integrated for final matching. As far as we know, we are the first to successfully exploit feature pyramid network into the model of person Re-ID.</p><p>2) To further prompt the performance of FPB, self-attention modules as auxiliary modules are carefully evaluated and inserted into different positions of network. Also, we propose an extra cross orthogonality regularization over features from two layers of FPB. This penalty effectively reduces the correlation of feature maps especially after attention modules, and thus improves the diversity of resulting features. 3) Extensive validations on different person Re-ID datasets demonstrate that FPB structure can deliver significant improvement with trivial increase of computation cost (&lt;1.5M extra parameters). It outperforms other leading methods and achieves new state-of-the-art on all prevailing benchmarks with an efficient implementation.</p><p>Our results also empirically prove that FPN could be a potential structure in related feature embedding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Person Re-Identification</head><p>Recently, deep learning based person Re-ID methods show clear advantages to conventional methods which still rely on handcrafted features <ref type="bibr" target="#b20">[25]</ref>, <ref type="bibr" target="#b21">[26]</ref>. There also exist two paradigms to learn deep models with the feature embedding capability for final matching. The first one is to adopt pre-trained backbone, e.g., ResNet, and fine-tune parameters of specific architecture on person Re-ID datasets <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr">[9]</ref>, <ref type="bibr" target="#b11">[16]</ref>, <ref type="bibr" target="#b12">[17]</ref>, <ref type="bibr" target="#b14">[19]</ref>, <ref type="bibr" target="#b22">[27]</ref>. The other one is to design novel architecture specifically for the task of person Re-ID and train the model from scratch <ref type="bibr" target="#b23">[28]</ref>, <ref type="bibr" target="#b24">[29]</ref>, <ref type="bibr" target="#b25">[30]</ref>, <ref type="bibr" target="#b26">[31]</ref>. Although specific models such as Omni-Scale Network (OSNet) <ref type="bibr" target="#b24">[29]</ref> and Harmonious Attention CNN (HA-CNN) <ref type="bibr" target="#b23">[28]</ref> normally imply higher efficient models with much less parameters, we still focus on the former paradigm based on generic backbones in this paper. This is because we observe that these models deliver higher performance especially on large scale benchmarks, e.g., MSMT17 <ref type="bibr" target="#b27">[32]</ref>. Also, they demonstrate higher feasibility for further modification as well as distribution in realistic scenarios.</p><p>On the other hand, different mechanisms are also introduced into the stage of model training to prompt the performance. Data augmentation methods, such as random erasing <ref type="bibr" target="#b28">[33]</ref>, random patch <ref type="bibr" target="#b24">[29]</ref>, are commonly adopted by various methods. Based on essential random erasing at every single sample, randomly dropping block <ref type="bibr" target="#b29">[34]</ref>, <ref type="bibr" target="#b30">[35]</ref> further proposed to randomly drop the same part at all samples within a batch for learning more attentive local features. Besides augmentation, other strategies such as stochastic weight averaging <ref type="bibr" target="#b31">[36]</ref>, extra regularization <ref type="bibr" target="#b32">[37]</ref> have also been proven to be effective to prompt the capability of learned models. In this paper, since we focus on the structure of FPB, only essential strategies during training are adopted for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Diversity of Features</head><p>Due to aforementioned reasons, to prompt the generalization capacity of feature embedding models is a critical issue in person Re-ID. An effective way is to increase the diversity of extracted features. Conventional output feature of deep models is normally resulted by direct averaging operation over the whole feature map. Conversely, <ref type="bibr" target="#b7">[8]</ref> proposed to extract local features from different parts for higher diversity in final matching. Besides part based models, to extract features from multiple levels <ref type="bibr" target="#b33">[38]</ref>, <ref type="bibr" target="#b34">[39]</ref> is another potential way to deliver more representative features. Features from shallow layers of networks can naturally reflect detailed local information at images. On the other side, recent research demonstrates that a complementary feature consisting of both global and local features could be more diverse for feature matching. Thus multi-branch structure becomes a prevailing architecture in the literature of person Re-ID <ref type="bibr" target="#b14">[19]</ref>, <ref type="bibr" target="#b11">[16]</ref>, <ref type="bibr" target="#b26">[31]</ref>, <ref type="bibr" target="#b12">[17]</ref>.</p><p>An inevitable problem brought by multi-branch structure is the increase of complexity. As an essential way, <ref type="bibr" target="#b14">[19]</ref>, <ref type="bibr" target="#b22">[27]</ref> construct the dual-branch structure by duplicating part of backbone with separated learnable parameters. For more branches, since most of existing works tend to design symmetrical multi-branch structure, multiple duplications of the last part of backbone apparently lead to significant increase of parameters. For some lightweight backbones, multiple branches could even introduce extra parameters more than original backbones <ref type="bibr" target="#b25">[30]</ref>, <ref type="bibr" target="#b26">[31]</ref>. Our proposed approach follows the paradigm of multi-branch networks. However, we carefully restrain the complexity of local part branch by a lightweight pyramid network structure rather than direct duplication of backbone. The pyramid network can also extract features from multiple levels of backbone, thus detailed information from shallow layers are naturally retained. Finally, complementary feature embedding at different granularities are learned jointly in an end-to-end framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Pyramid Network</head><p>The FPN structure is extensively exploited in the field of object detection. The first successful application of FPN is <ref type="bibr" target="#b16">[21]</ref>. It proposed a top-down pathway to output detection results at multiple scales simultaneously. In this way, conventional pyramid of original input image is replaced by a CNN structure to achieve multi-scale object detection more efficiently. Following the idea, PANet <ref type="bibr" target="#b35">[40]</ref> further proposed a bidirectional FPN consisting of a top-down as well as a bottom-up path to aggregate features at each layer of FPN. M2det [41] adopted a block of alternating joint U-shape module to fuse multi level features. EfficientDet <ref type="bibr" target="#b17">[22]</ref> introduced the down-sampling structure from ResNet and proposed a Bidirectional FPN BiFPN. State-of-the-art object detectors including both onestage approaches (SSD <ref type="bibr" target="#b37">[42]</ref>, YOLO <ref type="bibr" target="#b18">[23]</ref>, EfficientDet <ref type="bibr" target="#b17">[22]</ref>) and two-stage approaches (Mask RCNN <ref type="bibr" target="#b38">[43]</ref>, DetNet <ref type="bibr" target="#b39">[44]</ref>) all exploit FPN structure to tackle the scale variation problem. In this paper, we introduce the FPN into the task of person Re-ID since it also requires feature matching at both global and local scales. Despite difference exists between person Re-ID and object detection, our results empirically prove that with delicate design, FPN can bring significant benefit with trivial increase of model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Attention Mechanisms</head><p>There are various implementations of attention modules in the literature. The original attention module is proposed for the Natural Language Processing (NLP) tasks <ref type="bibr" target="#b40">[45]</ref>, <ref type="bibr" target="#b41">[46]</ref>, which is normally referred as the multi-head attention. It focuses on reducing the ambiguity of input features by their context information. Recently, multi-head attention as well as the transformer architecture have also been proven to be effective in various vision tasks <ref type="bibr" target="#b42">[47]</ref>. Differing from original multi-head attention module in NLP, the most extensively applied attention module in vision tasks is the so-called Non-Local Neural Networks structure <ref type="bibr" target="#b43">[48]</ref>. It aims to encode the correlation between features at different positions to output more attentive features as well. Differing from Non-Local Neural Networks, another design of attention, namely Position Attention Module (PAM) is also extensively adopted in person Re-ID tasks <ref type="bibr" target="#b14">[19]</ref>. PAM can be treated as a simplified version of multi-head attention without dimension reduction or multi-head structure on the value branch. It also focuses on attending features at different positions, extracting correlation information as position affinity matrix to reweigh features.</p><p>Besides PAM, there exists another series of attention modules, namely Channel Attention Module (CAM), aim to extract correlation information over different channels of feature maps. Some typical implementations of CAM can be referred to Squeeze-and-Excitation block <ref type="bibr" target="#b44">[49]</ref> and Efficient Channel Attention <ref type="bibr" target="#b45">[50]</ref>. Since no extra parameter is required in the implementation of CAM, it can be deployed as an efficient mechanism to extract channel-wise response over features with trivial cost. Many variants of CAM attend as building block in different architectures of CNNs, specifically in lightweight designs such as OSNet <ref type="bibr" target="#b24">[29]</ref> and EfficientNet <ref type="bibr" target="#b46">[51]</ref>. Also, CAM is extensively deployed in existing person Re-ID frameworks such as HA-CNN <ref type="bibr" target="#b23">[28]</ref> and Attentive but Diverse Network (ABD-Net) <ref type="bibr" target="#b14">[19]</ref>.</p><p>In this paper, our implementation of attention module consists of the concatenation of a PAM and a CAM. This structure is analogous to the design of attention module in <ref type="bibr" target="#b14">[19]</ref>. We observe that the PAM can deliver slightly better performance than prevailing Non-Local Neural Networks in our architecture. Meanwhile, the CAM can also bring explicit gain via negligible increase of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, our proposed method is a dualbranch framework, consisting of a global branch and a feature pyramid branch. The global branch is mainly based on the Bag-Of-Tricks method [9], which contains a modified version of ResNet50 [10] as backbone network. Differing from standard ResNet50 based ID-Discriminative Embedding (IDE) <ref type="bibr" target="#b0">[1]</ref>, here the last down-sampling operation in layer 4 of ResNet50 is removed to increase the size of the output. After the Global Average Pooling (GAP) operation over the output, a 2048dimension vector is delivered as the global feature f g . A BNNeck [9] is also adopted to diliver the normalized version of global feature f b here. More details can be referred to the description of loss fuctions in Section III-D.</p><p>Based on the backbone network, we propose a lightweight Feature Pyramid Branch (FPB) to enrich the diversity of features for person Re-ID. The outter structure of this branch is  inspired by the Part-based Convolutional Baseline (PCB) <ref type="bibr" target="#b7">[8]</ref>. Output feature map is devided into parts to emphasize local information. The difference is we take shallower features from the layer 2 and layer 3 of backbone as input here. These shallow features can retain more local details from image. Simultaneously, features with lower dimensions reduce the number of learnable parameters within the branch.</p><p>Specificially, the output feature map of FPB is processed by different strategies during training and inference respectively. Average pooling operation is adopted here to archive N 1024dimension vectors as f p n s (n ? {1, . . . , N }). A Dimension Reduction (DM) layer consisting of convolutional filter followed by Batch Normalization (BN) and Rectified Linear Unit (ReLU) activation compresses f p n s to N 256-dimension vectors as f s n s to optimize the classification loss on the branch during training. The concatenation of f p n s and output feature from backbone forms the final resulting feature of the whole model. This feature can represent the original image for an efficient matching and retrieval of images from the same person in numbers of candidates with unknown identities.</p><p>The integration of multiple branches here is a reminiscent of AsNet <ref type="bibr" target="#b22">[27]</ref>, which also prompted the diversity of resulting features with asymmetrical branches. However, our implementation here is much more compact since we propose FPB as the replacement of simple duplication of layer 4 of ResNet in <ref type="bibr" target="#b22">[27]</ref>. Note that we choose ResNet50 as backbone in this work only because its popularity and flexibility with other structures. FPB can also be exploited as compatible plugin to other common feature extraction backbones, such as Densenet <ref type="bibr" target="#b47">[52]</ref>, InceptionNet <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Pyramid Branch</head><p>The pivotal structure of FPB is a two layers FPN as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. This affiliated branch network begins with lateral convolutional filters to convert feature maps with different channel numbers to unified 256. The lateral filters consist of standard 1 ? 1 convolutional filters followed by BN and ReLU. Within the FPB, four low dimensional convolutional filters fullfil the aggregation of features at different scales. The structures of these filters are similar to lateral filters, except that they adopt 3 ? 3 kernel.</p><p>There exist two cross-scale connections between feature maps with different spatial resolutions in FPB. One top-down connection is implemented by nearest interpolation to increase the size of feature map. Conversely, one bottom-up connection is implemented by max pooling with 2 ? 2 kernel. There also exist two down-sampling operations as extra edges from input to output node at each layer. As illustrated by curved arrows in <ref type="figure" target="#fig_2">Fig. 2</ref>, we implement down-sampling analogously to the residual structure in ResNet. As the output of FPB, we take the feature at the deeper layer of FPB and recover the channel to 1024 for consequent processing. Hence, each layer within FPB can actually be treated as a small bottleneck structure, which extracts information by filters with relatively larger kernel as well as fewer channels.</p><p>The design of FPB is inspired by the so-called BiFPN in <ref type="bibr" target="#b17">[22]</ref>, which was originally proposed for the task of object detection. Differing from conventional FPN <ref type="bibr" target="#b16">[21]</ref>, <ref type="bibr" target="#b35">[40]</ref>, BiFPN proposed to add an extra edge from the input to output node at the same scale. Our empirical results also prove the efficacy and simplicity of this structure. However, note that there exist two essential differences between the structures of FPB and BiFPN. First, FPN aims to tackle the problem of occurring objects with different sizes by aggregating features at different scales, while FPB aims to integrate diverse features from different scales into final matching. Therefore FPB aggregates features at a single output to average pooling operation, rather than multiple outputs at each layer as BiFPN. Second, we implement inner nodes as well as down-sampling connections with wider filters than BiFPN. This is because, the task of Re-ID requires complicated information from different scales for the classification over a relatively larger amount of identities. In contrast, object detection focuses on a classification problem only over tens of categories, taking COCO detection datasets <ref type="bibr" target="#b48">[53]</ref> as a typical instance. Thus filters with more channels here are adopted to fit the complexity of problem. Moreover, there also exist some other subtle modifications in FPB according to our empirical results. For example, we found that weighted feature fusion with learnable parameters at each node is inappropriate in Re-ID. More detailed analysis can be referred to the ablation study in Section IV-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-Attention</head><p>Attention module has been proven as an effective mechanism in various machine learning scenarios. It exploits the correlation between features to help model focus on more related features and reduce the ambiguity of features for final tasks. In the case of person Re-ID, the main goal is to train the model to discriminate persons with samples in training set, and to extract representative features from images of unknown identities in testing set for final matching. This divergence between training and inference requires the model to extract more generic features rather than baised features related to specific instances in training set. The exploitation of selfattention modules can effectively prompt the generalization capacity of model by forcing it to focus on the relationship of features. Based on this observation, we insert two self-attention modules at backbone and feature pyramid branch respectively. Both self-attention modules consist of a PAM followed by a CAM. Their structures are shown in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>Position Attention Module: Our implementation of PAM is analogous to ABD-Net <ref type="bibr" target="#b14">[19]</ref>, which can be treated as a simplification of the extensively applied multi-head attention mechanism in Natural Language Processing (NLP) <ref type="bibr" target="#b41">[46]</ref>. Given input feature maps X ? R C?(H?W ) , where C, H and W are channel number, height and width, respectively. The PAM projects and reshapes feature at every position onto two lower dimensional subspaces, resulting in query Q ? R C r ?S and key K ? R C r ?S . Here S = H ? W is the spatial size of feature map and r is a hyper parameter to control the dimension of subspace. We follow the usual choice and simply set r as 8 for all experiments. Specifically, the projections from X to Q and K are implemented by 2D convolution with kernel size as 1?1. Then the attention of X can be calculated from query Q, key K and value V as</p><formula xml:id="formula_0">attention p (X) = V ?(A p ) = V ?(Q T K),<label>(1)</label></formula><p>where ?(?) is the Softmax function, value V ? R C?S is another projection of X with equivalent dimension. Note that if we ignore all learnable parameters here, the position affinity matrix A p can be simplified as a Gram matrix, which can measure the correlation between features at different positions of X. From this perspective, the essential goal of position attention is to reweigh each feature by its correlations with other features. As shown in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>, we also adopt a residual structure as well as a learnable parameter ? to adjust the impact of attention.</p><p>Channel Attention Module: Based on similar motivation, we also implement CAM to extract attention over different channels of features in X. As illustrated in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>, without projection, the channel affinity matrix A c is directly calculated as</p><formula xml:id="formula_1">attention c (X) = (?(A c )X) T = (?(XX T )X) T .<label>(2)</label></formula><p>A learnable parameter is also adopted here to adjust the impact of attention in the final sum operation with original features. Comparing with the PAM, the implementation of CAM only requires a few parameters and brings explicit improvement to performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross Orthogonality Regularization</head><p>On top of self-attention modules, we further enforce the diversity of features by orthogonality regularization. As suggested in <ref type="bibr" target="#b14">[19]</ref>, the orthogonality regularization aims to prompt the representative efficiency of features by reducing the feature correlation between different channels. The influence is especially obvious on features after attention modules. Given feature map X ? R C?S , conventional hard regularization normally relies on the Singular Value Decomposition (SVD), which is computationally expensive especially for high dimensional features. A substitute is the soft orthogonality regularization which optimizes the conditional number of XX T as</p><formula xml:id="formula_2">L or = ?||? 1 (XX T ) ? ? 2 (XX T )|| 2 2 ,<label>(3)</label></formula><p>where ? is a small constant, ? 1 and ? 2 denote the largest and smallest eigenvalues of matrix, respectively. With the fast iterative algorithm for solving eigenvalues, the orthogonality regularization can be implemented efficiently during training.</p><p>Rather than merely applying orthogonality regularization on single feature map, we propose the Cross Orthogonality Regularization (COR) as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Two feature maps, f h and f l after attention modules are taken into account jointly here. Feature maps with different resolutions are unified by max pooling operation, and then concatenated into one higher dimensional feature map. The orthogonality regularization is then applied to enforce the orthogonality over all channels of features from different positions. The motivation of COR instead of single orthogonality regularization on different feature maps respectively is that we observe standard back propagation can naturally reduce the correlation over features. It intrinsically ensures the efficiency of deep models. In this case, the effectiveness of simple orthogonality regularization on feature map is partially overlapped by the learning of entire model, while COR can be more complementary since it affects multiple branches simultaneously. Without any extra computation increase to inference, COR brings small by obvious improvement to final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Functions</head><p>At the training stage, we can get four types of output features as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, f g , f b , f p n s and f s n s. Here notation f p n s and f s n s mean multiple features from each part after average pooling. The following loss function is optimized to learn all parameters within the model:</p><formula xml:id="formula_3">L total = ?L triplet (f g i f p in s, y i , f g j f p jn s, y j )<label>(4)</label></formula><formula xml:id="formula_4">+ L ce (W b f b i , y i ) + N n=1 L ce (W s n f s in , y i ) + L cor ,</formula><p>where i is the index of training sample (x i , y i ). L triplet (?) is the hard mining triplet loss <ref type="bibr" target="#b8">[13]</ref> between sample i and another sample j within a batch. represents the concatenation operation of all vectors in f g and f p n s. L ce (?) is the cross entropy loss, W b and W s n are the FC layers after f b and each f s n , respectively. L cor is the COR version of L or in Equation <ref type="bibr" target="#b2">3</ref>. A hyperparameter ? is adopted as a balance between different losses. The utilization of L ce (?) follows the conventional framework of learning person Re-ID models <ref type="bibr" target="#b0">[1]</ref>, [9], while the L triplet (?) prompts the generalization capacity of model by ensuring a larger distance between output features f g f p n s from samples of different identities than the same one.</p><p>As listed in Equation 4, two kinds of features f g and f b , delivered by the BNNeck [9] mechanism on the global branch, are included in the loss function. The BNNeck deploys a BN layer after f g to get the normalized version f b . f b is utilized for the opization of the L ce (?) on global branch during training as well as part of the final feature during inference. While original f g is utilized as part of the output feature to optimize the L triplet (?) during training. Our experimental results demonstrate that merely deploying the BNNeck on the global branch rather than both branches can deliver the optimal performance since it naturally forces an asymmetrical structure over outputs and thus ensures the diversity of features from different paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EMPIRICAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>In this section, we conduct a series of experiments to analyze the performance of our proposed FPB and compare it with other state-of-the-art works. Four prevailing person Re-ID datasets are considered here, Market1501 <ref type="bibr" target="#b49">[54]</ref>, DukeMTMC <ref type="bibr" target="#b50">[55]</ref> CUHK03 <ref type="bibr" target="#b51">[56]</ref> and MSMT17 <ref type="bibr" target="#b27">[32]</ref>.</p><p>Market1501 <ref type="bibr" target="#b49">[54]</ref> consists of 32,668 images from 1501 identities captured by six cameras, in which each identity is at least captured by two cameras with multiple images. For the training set, 12,936 images from 751 identities are considered, leading to an average of 17.2 training samples for one person are available. For the testing set, 19,732 images from 750 other identities are considered, in which 3,368 images are used as probe set while the rest are used as gallery set.</p><p>DukeMTMC <ref type="bibr" target="#b50">[55]</ref> consists of 36,411 images from 1,404 identities captured by more than two cameras, and 408 identities captured by only one camera as distractors. For the training set, 16,522 images from 702 identities are considered. For the testing set, 17,661 images from 702 other identities are considered, in which 2,228 images are used as probe set while the rest images from the 702 identities as well as distractors are used as gallery set.</p><p>CUHK03 <ref type="bibr" target="#b51">[56]</ref> consists of images from 1467 identities captured by five cameras, in which 767 identities are used as training set and 700 other identities are used as testing set. The dataset contains two tasks, person Re-ID with labeled images and with detected images. The labeled dataset has 7,368 images for training and 6,728 images for testing. The detected dataset has 7,365 images for training and 7,732 images for testing.</p><p>MSMT17 <ref type="bibr" target="#b27">[32]</ref> is relatively larger than aforementioned three datasets. It consists of 126,441 images from 4,101 identities captured by a 15-camera network (12 outdoor, 3 indoor). For the training set, 32,621 images from 1,041 identities are considered. For the testing set, 93,820 images from 3,060 other identities are considered, in which 11,659 images are used as probe set while the rest are used as gallery set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Since we choose ResNet as the backbone, the training framework of our proposed model mainly follows conventional person Re-ID methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>. First, the training starts with the pre-trained backbone from ImageNet <ref type="bibr">[10]</ref>. For the rest part of the architecture in <ref type="figure" target="#fig_2">Fig. 2</ref>, we adopt the widely applied He method <ref type="bibr" target="#b61">[66]</ref> as initialization. Standard augmentation methods including random horizontal flip, random crop, random erasing <ref type="bibr" target="#b28">[33]</ref> and random patch <ref type="bibr" target="#b24">[29]</ref> are also adopted during training. We fine-tune the model with Adam optimizer for 120 epochs. The linear warmup strategy [9] is used, in which the learning rate is initialized at 3.5e-5 and increased to 3.5e-4 in 20 epochs. Then the learning rate is decayed after 60 and 90 epochs with a rate of 0.1, respectively.</p><p>For Market1501, DukeMTMC and CUHK03, the size of input image is resized to 384 ? 128. Experiments are executed with a hardware environment as Intel E5-2680CPU at 2.4GHz and a single NVidia Tesla P40 GPU. The model is trained with a batch size of 64 from 16 identities. For MSMT17, to prompt the efficiency of learning this massive dataset, we adopt multiple GPUs and keep the batch size at single GPU still to 64.</p><p>Evaluation Protocol: For quantitative comparison over different methods, we consider the Cumulative Matching Characteristics (CMC) at mean Average Precision (mAP) and top-1 accuracy (rank-1) as standard metrics. All results are obtained without any re-ranking <ref type="bibr" target="#b62">[67]</ref> or multi-query fusion <ref type="bibr" target="#b63">[68]</ref> techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State-of-the-art Methods</head><p>In this section, we compare our proposed approach with other state-of-the-art methods. In <ref type="table" target="#tab_0">Table I</ref>, we list the performance of different methods on four tasks, Market1501, DukeMTMC, CUHK03 (Labeled) and CUHK03 (Detected). One can see that our proposed scheme outperforms other approaches with obvious margins. The only exception is the rank-1 accuracy of HOReID <ref type="bibr" target="#b13">[18]</ref> at Market1501. Here we adopt the result of HOReID <ref type="bibr" target="#b13">[18]</ref> with ResNet50 as backbone and an extra Global Hard Identity Searching (GHIS) <ref type="bibr" target="#b64">[69]</ref> method during training. Without this augmentation, the rank-1 accuracy of HOReID reduces to 95.74%, which is worse than FPB. For the mAP, FPB substantially exceeds the second best approaches by 0.6%, 1.8%, 2.9% and 3.8%. From our observations in experiments, we treat mAP as a more reliable indicator in scenarios of person Re-ID. This observation also agrees with <ref type="bibr" target="#b14">[19]</ref>. Note that the identical architecture with a number of 27.04M learnable parameters is used for all experiments on different datasets. It only brings an increase of less than 1.5M extra parameters to Resnet50 baseline, which demonstrates the significant efficacy of FPB on extracting representative features.</p><p>MSMT17: For the more challenging large scale person Re-ID dataset MSMT17, we compare different configurations of our proposed FPB with other state-of-the-art methods in <ref type="table" target="#tab_0">Table II</ref>. Here we implement two versions of FPB with ResNet50 and ResNet101 as backbones respectively. From the perspective of mAP, one can see that the ResNet50 based implementation of FPB outperforms other methods with obvious gap. It even exceeds the second best approach, ResNet152 based Adaptive L2 <ref type="bibr" target="#b32">[37]</ref> with more than 60M parameters, from 62.2% to 63.5%. Meanwhile, ResNet101 based FPB retains a parallel performance here with larger backbone. However from the perspective of rank-1 accuracy, ResNet50 based FPB is worse than the best performance from ABD-Net <ref type="bibr" target="#b14">[19]</ref>. This situation is mitigated by the larger ResNet101 backbone. In this case, the FPB dilivers the best performance of rank-1 accuracy. Note that, even with ResNet101 backbone, the parameters of our proposed FPB is still comparable to ABD-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We carefully construct the final structure of proposed FPB step by step from the IDE as baseline. The Influences of each attempt are listed in <ref type="table" target="#tab_0">Table III</ref>. First, we empirically prove the effectiveness of proposed tricks in [9], then we focus on configurations of the feature pyramid structure here. Typical FPNs in the frameworks of object detection, e.g., Single Shot Detector (SSD) <ref type="bibr" target="#b37">[42]</ref> and PANet <ref type="bibr" target="#b35">[40]</ref>, normally retain a structure with more than three layers. Therefore we construct a three-layers FPB as initialization. It takes feature maps after layer 1, 2 and 3 of backbone as input. As listed in <ref type="table" target="#tab_0">Table III</ref>, our final comparison demonstrates that FPB with two layers after layer 2 and 3 of backbone can deliver the optimal performance, which implies that too detailed local features from small respective field might not be useful for feature matching in Re-ID. Due to the variation of view and pose, these features could be too unstable to be fixed in specific positions in resulting f p . This is a major difference between person Re-ID and object detection, in which detailed edge information is required for localization of object.</p><p>We also compared other configurations in <ref type="table" target="#tab_0">Table III</ref>. Here the down-sampling mechanism refers to the extra edges from input to output node at each layer in FPB. This operation also bring obvious improvement based on simple connections. In contrast to the observation in <ref type="bibr" target="#b17">[22]</ref>, the weighted feature fusion at each node in FPB causes reduction of both mAP nd top-1 accuracy of models. At last, we compare different widths of convolutional filters including 256 and 512 channel numbers in the FPB. It turns out wider filters with extra computation consumptions can merely brings trivial impact here. Hence, filters with 256 channels become to our final choice in subsequent experiments.</p><p>Self-Attention Module: Although attention module has been proven as an effective mechanism in various vision tasks. We observe that only deploying them at carefully selected positions can bring positive influence to the whole model. In FPB, two self-attention modules are injected into the architecture at positions as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. We list the improvements brought by them in <ref type="table" target="#tab_0">Table IV</ref>. The deployment of attention modules after the layer 2 of backbone and the lateral filter at the shallower layer of FPB finally delivers the optimal performance. This observation agrees with strategy of attention modules in <ref type="bibr" target="#b22">[27]</ref>, although there is only one attention module in AsNet. The extra attention module at FPB further prompts the performance with trivial increase of parameters, since channel number is severely reduced by lateral filters.</p><p>Cross Orthogonality Regularization: In <ref type="table" target="#tab_0">Table IV</ref>, we also list the improvement brought by our proposed COR, which delivers the final performance of FPB. One can see that this joint constraint on features from two layers brings small but non-trivial improvement to the performance of whole model. We also illustrate the learning curves of L or s in <ref type="figure" target="#fig_4">Fig. 4</ref>, which reflect the variation of correlations over feature maps during training. Here we take feature maps at the two positions in  <ref type="figure" target="#fig_2">Fig. 2</ref> into account, comparing three different strategies of orthogonality regularization during training. For the first case (No OR), we simply output the sum of correlations without any optimization of regularization. Then we adopt conventional OR on the two feature maps respectively. At last, we deploy our proposed COR on the two feature maps to deliver three learning curves. Note that without any optimization, the learning curve of correlation can still decline during the training. To reduce the correlation of resulting feature maps is a natural function of standard back propagation learning. It results in more efficient model as well as more representative output features. The OR mechanism accelerates this process, helping the feature correlation reach a lower point more efficiently at the beginning of training. However, one can see that despite due to different calculation method our proposed COR starts from a higher point than other strategies, it finally delivers the lowest correlation between different channels of resulting feature maps.</p><p>This observation also helps to understand why injection of attention modules into relatively shallow levels of networks is more effective. The forward inference of CNNs naturally reduces the feature correlation layer by layer. Thus available information in Equations 1 and 2 for the calculation of attention is getting less and less as well.</p><p>From <ref type="figure" target="#fig_5">Fig. 5</ref>, we can observe this process more inituitively from the correlation of outpuyt feature maps. From the correlation matrices at the first and second rows, one can see that self-attention modules introduce extra correlation while enhancing the related features for embedding. This observation agrees with the effect of attention in <ref type="bibr" target="#b14">[19]</ref>. At <ref type="figure" target="#fig_5">Fig. 5(c)</ref>, the OR mechanism significantly reduces the correlation of both f h and f l respectively, improving the efficiency of these features. However, from the correlation matrix of f h f l we can see that correlation between f h and f l still exist. Our proposed COR finally reduces the co-redundancy and further prompts the diversity between different branches of the model. Another observation from <ref type="figure" target="#fig_5">Fig. 5(b)</ref> is that correlation of f h is larger than f l . This observation also agrees with our aforementioned assumption, the correlation of features is naturally shrinked during inference of CNNs to ensure a more efficient extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of Part Number:</head><p>For the output features f p of our proposed FPB, we adopt a similar structure as <ref type="bibr" target="#b22">[27]</ref>, in which several features represent corresponding parts of image respectively. In <ref type="figure">Fig. 6</ref>, we study the variations of mAP and top-1 accuracy alongside the change of part numbers as well. It can be shown that on three different datasets, the tendency is consistent and explicit. The configuration of three parts in f p has been proven as optimal and is adopted in all of our experiments. This configuration is the same as <ref type="bibr" target="#b22">[27]</ref>, while differing from six parts in the original part-based framework for person Re-ID <ref type="bibr" target="#b7">[8]</ref>. It implies that a smaller part number as part branch can work better with the help from global branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization</head><p>To further study the influence of our proposed dual-branch structure, we illustrate some feature embedding samples with their activation maps <ref type="bibr" target="#b11">[16]</ref> in <ref type="figure">Fig. 7</ref>. For each input image, we compare activation maps from three different configurations. The first one is generated by IDE as baseline. The second one is activation map of output feature map after the layer 4 of global branch in <ref type="figure" target="#fig_2">Fig. 2</ref>. The third one is at the output of the feature pyramid branch before average pooling. <ref type="figure">Fig. 7</ref> shows two observations: (1) Rather than small highlighted points in IDE, the output feature maps of global branch correspond to higher activation over the main part of person. This is mainly because of the utilization of selfattention modules. (2) The attentive feature branch serves as complementary function, focusing on more detailed local parts of person. Such diversity is ensured by two mechanisms. An asymmetrical architecture between branches naturally lead to different processses of features, help branches focus on different parts during training. Also, the triplet loss function over the concatenated output features from both branches encourages more diverse information being extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>We propose a novel structure called feature pyramid branch for the task of Person Re-ID. The structure can server as an affiliated branch to complement the backbone. Features at different scales can be extracted and aggregated in this branch for the final embedding with higher diversity. Cooperated with attention mechanism as well as orthogonality regularization, our proposed FPB structure can significantly prompt the model performance with trivial increase of computational complexity. For future work, we will continure to investigate the potentiality of the feature pyramid structure in various scenarios of feature embedding, specifically its compatibility with more lightweight backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correla on Correla on</head><p>Correla on  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>serve as a strong backbone for extracting representative visual features from images. The generic framework of such approaches mainly follows a fine-tuning stage on labeled samples to deliver a model which can discriminate person from each other within training set. Then intermediate features before final classification layer are retained to accomplish the deep Performance comparison of our proposed FPB and other state-of-theart Re-ID methods. Results on the same dataset are presented by the same color. Results of the same method are presented by the same shape. Top: mAP vs. number of model parameters. Bottom: Rank-1 accuracy vs. number of model parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The overall architecture of our proposed method. Here the stride of the last down-sampling layer is changed from 2 to 1 and denoted as last stride=1. The residual structure in FPB is illustrated as curved arrows here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The structures of (a) Position Attention Module and (b) Channel Attention Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Learning curves of Lors with different configurations of orthogonality regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of correlation matrices over channels of output feature maps: f h as the first column, f l as the second column and concatenation of f h and f l (f h f l ) as the third column. Here we compare four different configurations: (a) no attention or OR as baseline, (b) attention without OR, (c) attention + OR and (d) attention + COR. The relationship between color and correlation is illustrated in the colorbar at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>[ 9 ]Fig. 6 .Fig. 7 .</head><label>967</label><figDesc>H. Luo, Y. Gu, X. Liao, S. Lai, and W. Jiang, "Bag of tricks and a strong baseline for deep person re-identification," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 0-0. [10] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778. [11] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, "Inception-v4, inception-resnet and the impact of residual connections on learning," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no. 1, 2017. [12] X. Fan, W. Jiang, H. Luo, and M. Fei, "Spherereid: Deep hypersphere manifold embedding for person re-identification," Journal of Visual Communication and Image Representation, vol. 60, pp. 51-58, 2019. The (a) mAP and (b) rank-1 vs. part number over three person Re-ID datasets. Visualization of samples and corresponding attention maps. Columns one and five are original images. Columns two and six are activation maps of output of IDE as baseline. Others are activation maps of feature maps at different positions of FPB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF OUR PROPOSED METHOD WITH STATE-OF-THE-ART METHODS ON MARKET1501, DUKEMTMC, CUHK03 (LABELED) AND CUHK03 (DETECTED). THE BEST PERFORMANCES ARE HIGHLIGHTED BY BOLD FONT.</figDesc><table><row><cell>Method</cell><cell cols="2">Market1501 mAP(%) rank-1(%)</cell><cell cols="2">DukeMTMC mAP(%) rank-1(%)</cell><cell cols="2">CUHK03-Labeled mAP(%) rank-1(%)</cell><cell cols="2">CUHK03-Detected mAP(%) rank-1(%)</cell></row><row><cell>KPM [57]</cell><cell>75.3</cell><cell>90.1</cell><cell>63.2</cell><cell>80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PCB+RPP [8]</cell><cell>81.6</cell><cell>93.8</cell><cell>69.2</cell><cell>83.3</cell><cell>-</cell><cell>-</cell><cell>57.5</cell><cell>63.7</cell></row><row><cell>Mancs [58]</cell><cell>82.3</cell><cell>93.1</cell><cell>71.8</cell><cell>84.9</cell><cell>63.9</cell><cell>69.0</cell><cell>60.5</cell><cell>65.5</cell></row><row><cell>MGN [59]</cell><cell>86.9</cell><cell>95.7</cell><cell>78.4</cell><cell>88.7</cell><cell>67.4</cell><cell>68</cell><cell>66.0</cell><cell>68.0</cell></row><row><cell>MHN [60]</cell><cell>85.0</cell><cell>95.1</cell><cell>77.2</cell><cell>89.1</cell><cell>72.4</cell><cell>77.2</cell><cell>65.4</cell><cell>71.7</cell></row><row><cell>CAMA [16]</cell><cell>84.5</cell><cell>94.7</cell><cell>72.9</cell><cell>85.8</cell><cell>66.5</cell><cell>70.1</cell><cell>64.2</cell><cell>66.6</cell></row><row><cell>Bag-Of-Tricks [9]</cell><cell>85.9</cell><cell>94.5</cell><cell>76.4</cell><cell>86.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ABD-Net [19]</cell><cell>88.28</cell><cell>95.6</cell><cell>78.59</cell><cell>89.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BDB [34]</cell><cell>86.7</cell><cell>95.3</cell><cell>76.0</cell><cell>89.0</cell><cell>76.7</cell><cell>79.4</cell><cell>73.5</cell><cell>76.4</cell></row><row><cell>Pyramid [17]</cell><cell>88.2</cell><cell>95.7</cell><cell>79.0</cell><cell>89.0</cell><cell>76.9</cell><cell>78.9</cell><cell>74.8</cell><cell>78.9</cell></row><row><cell>SONA [61]</cell><cell>88.67</cell><cell>95.68</cell><cell>78.05</cell><cell>89.25</cell><cell>79.23</cell><cell>81.85</cell><cell>76.35</cell><cell>79.1</cell></row><row><cell>AsNet [27]</cell><cell>89.13</cell><cell>95.50</cell><cell>81.10</cell><cell>89.93</cell><cell>80.87</cell><cell>83.23</cell><cell>77.17</cell><cell>81.43</cell></row><row><cell>Adaptive L2 [37]</cell><cell>88.9</cell><cell>95.6</cell><cell>81.0</cell><cell>90.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RGA-SC [62]</cell><cell>88.4</cell><cell>96.1</cell><cell>-</cell><cell>-</cell><cell>77.4</cell><cell>81.1</cell><cell>74.5</cell><cell>79.6</cell></row><row><cell>CDNet [63]</cell><cell>86.0</cell><cell>95.1</cell><cell>76.8</cell><cell>88.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>L3DS [64]</cell><cell>87.3</cell><cell>95.0</cell><cell>76.1</cell><cell>88.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PAT [65]</cell><cell>88.0</cell><cell>95.4</cell><cell>78.2</cell><cell>88.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HOReID [18]</cell><cell>90.00</cell><cell>96.45</cell><cell>81.03</cell><cell>89.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>FPB</cell><cell>90.6</cell><cell>96.1</cell><cell>82.9</cell><cell>91.2</cell><cell>83.8</cell><cell>85.9</cell><cell>81.0</cell><cell>83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF OUR PROPOSED METHOD WITH STATE-OF-THE-ART METHODS ON MSMT17. THE BEST PERFORMANCES ON DIFFERENT BACKBONES ARE HIGHLIGHTED BY BOLD FONT.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mAP(%)</cell><cell cols="2">rank-1(%) rank-5(%)</cell></row><row><cell>PDC [70]</cell><cell>GoogLeNet</cell><cell>29.7</cell><cell>58.0</cell><cell>73.6</cell></row><row><cell>GLAD [71]</cell><cell>ResNet50</cell><cell>34.0</cell><cell>61.4</cell><cell>76.8</cell></row><row><cell>IANet [72]</cell><cell>ResNet50</cell><cell>46.8</cell><cell>75.5</cell><cell>85.5</cell></row><row><cell>BFE [72]</cell><cell>ResNet50</cell><cell>51.5</cell><cell>78.8</cell><cell>89.1</cell></row><row><cell>DGNet [73]</cell><cell>ResNet50</cell><cell>52.3</cell><cell>77.2</cell><cell>87.4</cell></row><row><cell>OSNet [29]</cell><cell>OSNet</cell><cell>52.9</cell><cell>78.7</cell><cell>-</cell></row><row><cell>CDNet [63]</cell><cell>CDNet</cell><cell>54.7</cell><cell>78.9</cell><cell>-</cell></row><row><cell>ABD-Net [19]</cell><cell>ResNet50</cell><cell>60.8</cell><cell>82.3</cell><cell>90.6</cell></row><row><cell>RGA-SC [62]</cell><cell>ResNet50</cell><cell>57.5</cell><cell>80.3</cell><cell>-</cell></row><row><cell cols="2">Adaptive L2 [37] ResNet50</cell><cell>59.4</cell><cell>79.6</cell><cell>-</cell></row><row><cell>HOReID [18]</cell><cell>ResNet50</cell><cell>52.97</cell><cell>76.24</cell><cell>-</cell></row><row><cell>MGN+PS [15]</cell><cell>ResNet50</cell><cell>62.4</cell><cell>84.0</cell><cell>-</cell></row><row><cell>FPB</cell><cell>ResNet50</cell><cell>63.5</cell><cell>79.8</cell><cell>85.4</cell></row><row><cell cols="2">Adaptive L2 [37] ResNet101</cell><cell>61.9</cell><cell>81.3</cell><cell>-</cell></row><row><cell cols="2">Adaptive L2 [37] ResNet152</cell><cell>62.2</cell><cell>81.7</cell><cell>-</cell></row><row><cell>HOReID [18]</cell><cell>ResNet101</cell><cell>54.77</cell><cell>78.42</cell><cell>-</cell></row><row><cell>FPB</cell><cell>ResNet101</cell><cell>63.6</cell><cell>82.5</cell><cell>90.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III INFLUENCES</head><label>III</label><figDesc>OF EACH MECHANISM IN THE PATH FROM IDE AS BASELINE TO OUR FINAL CHOICES OF FPB. THE RESULTS ARE REPORTED AS MAP(%)/RANK-1(%).</figDesc><table><row><cell>Method</cell><cell>IDE</cell><cell></cell><cell></cell><cell>FPB</cell></row><row><cell>last stride=1?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>bnneck?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>one-layer FPB?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>two-layers FPB?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>three-layers FPB?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>down-sampling?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>weighted fusion?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>256 channels?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>512 channels?</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Market1501 81.1/90.8 83.7/92.4 83.9/92.7 82.1/91.6 86.2/94.1 86.1/94.2 88.2/95.0 85.0/94.2 88.2/95.1 88.2/95.0</cell></row><row><cell></cell><cell cols="2">TABLE IV</cell><cell></cell></row><row><cell cols="5">THE INFLUENCES OF SELF-ATTENTION MODULES AND CROSS</cell></row><row><cell cols="4">ORTHOGONALITY REGULARIZATION.</cell></row><row><cell>Method</cell><cell cols="4">mAP(%) rank-1(%) rank-5(%)</cell></row><row><cell>baseline</cell><cell></cell><cell>88.2</cell><cell>95.0</cell><cell>98.4</cell></row><row><cell cols="2">+ Attention on backbone</cell><cell>89.3</cell><cell>95.2</cell><cell>98.3</cell></row><row><cell>+ Attention on FPB</cell><cell></cell><cell>90.2</cell><cell>95.6</cell><cell>98.7</cell></row><row><cell>+ COR</cell><cell></cell><cell>90.6</cell><cell>96.1</cell><cell>98.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarially occluded samples for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5098" to="5107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vrstc: Occlusion-free video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7176" to="7185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="907" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A posesensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improve person re-identification with part awareness learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7468" to="7481" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards rich feature discovery with class activation maps augmentation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="1389" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Horeid: Deep high-order mapping enhances pose alignment for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Boulgouris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2908" to="2922" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abd-net: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Proceedings on International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1846" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Parameter-free spatial attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12150</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint learning for attribute-consistent person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="134" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?stinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Asnet: Asymmetrical network for learning rich features in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="850" to="854" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3701" to="3711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attention network robustification for person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07038</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning diverse features with part-level resolution for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Computer Vision</title>
		<editor>Y. Peng, Q. Liu, H. Lu, Z. Sun, C. Liu, X. Chen, H. Zha, and J. Yang</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="16" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Proceedings on International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3691" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Diversityachieving slow-dropblock network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adaptive l2 regularization in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient and deep person re-identification using multi-level similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-scale deep learning architectures for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">M2det: A single-shot object detector based on multi-level feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9259" to="9266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2011" to="2023" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Eca-net: Efficient channel attention for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end deep kronecker-product matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6886" to="6895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Mixed high-order attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05819</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Second-order nonlocal attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Proceedings on International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3760" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Relation-aware global attention for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Combined depth space based architecture search for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6729" to="6738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning 3d shape feature for texture-insensitive person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8146" to="8155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Diverse part discovery: Occluded person re-identification with part-aware transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2898" to="2907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3652" to="3661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3356" to="3365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning incremental triplet margin for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9243" to="9250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3980" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Glad: Global-localalignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9309" to="9318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2133" to="2142" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
