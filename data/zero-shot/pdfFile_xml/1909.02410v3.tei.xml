<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic-Aware Scene Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>L?pez-Cifuentes</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Video Processing and Understanding Lab</orgName>
								<orgName type="institution">Universidad Aut?noma de Madrid</orgName>
								<address>
									<postCode>28049</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Escudero-Vi?olo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Video Processing and Understanding Lab</orgName>
								<orgName type="institution">Universidad Aut?noma de Madrid</orgName>
								<address>
									<postCode>28049</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s</forename><surname>Besc?s</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Video Processing and Understanding Lab</orgName>
								<orgName type="institution">Universidad Aut?noma de Madrid</orgName>
								<address>
									<postCode>28049</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?lvaro</forename><surname>Garc?a-Mart?n</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Video Processing and Understanding Lab</orgName>
								<orgName type="institution">Universidad Aut?noma de Madrid</orgName>
								<address>
									<postCode>28049</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic-Aware Scene Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scene recognition</term>
					<term>deep learning</term>
					<term>convolutional neural networks</term>
					<term>semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene recognition is currently one of the top-challenging research fields in computer vision. This may be due to the ambiguity between classes: images of several scene classes may share similar objects, which causes confusion among them. The problem is aggravated when images of a particular scene class are notably different. Convolutional Neural Networks (CNNs) have significantly boosted performance in scene recognition, albeit it is still far below from other recognition tasks (e.g., object or image recognition). In this paper, we describe a novel approach for scene recognition based on an end-to-end multi-modal CNN that combines image and context information by means of an attention module. Context information, in the shape of a semantic segmentation, is used to gate features extracted from the RGB image by leveraging on information encoded in the semantic representation: the set of scene objects and stuff, and their relative locations. This gating process reinforces the learning of indicative scene content and enhances scene disambiguation by refocusing the receptive fields of the CNN towards them. Experimental results on three publicly available datasets show that the proposed approach outperforms every other state-of-the-art method while significantly reducing the number of network parameters. All the code and data used along this paper is available at: https://github.com/vpulab/Semantic-Aware-Scene-Recognition</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Motivation and Problem Statement</head><p>Scene recognition is a hot research topic whose complexity is, according to reported performances <ref type="bibr" target="#b0">[1]</ref>, on top of image understanding challenges. The paradigm shift forced by the advent of Deep Learning methods, and specifically, of Convolutional Neural Networks (CNNs), has significantly enhanced results, albeit they are still far below those achieved in tasks as image classification, object detection and semantic segmentation <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref>.</p><p>The complexity of the scene recognition task lies partially on the ambiguity between different scene categories showing similar appearances and objects' distributions: inter-scene boundaries can be blurry, as the sets of objects that define a scene might be highly similar to another's. Therefore, scene recognition should cope with the classical tug-off-war between repeatable (handle intra-class variation) and distinctive (discriminate among different categories) characterizations. Whereas CNNs have been proved to automatically yield trade-off solutions with success, the complexity of the problem increases with the number of categories, and specially when training datasets are unbalanced.</p><p>Recent studies on the semantic interpretability of CNNs, suggest that the learning of scenes is inherent to the learning of the objects they include <ref type="bibr" target="#b5">[5]</ref>. Object detectors somehow arise as latent-variables in hiddenunits within networks trained to recognize scenes. These detectors operate without constraining the networks to decompose the scene recognition problem <ref type="bibr" target="#b6">[6]</ref>. The number of detectors is, to some extent, larger for deeper and wider (with a larger number of units) networks. During the last years, the main trend to enhance scene recognition performance has focused on increasing the number of CNN units. However, performance does not increase linearly with the increase in the number of network parameters <ref type="bibr" target="#b0">[1]</ref>. For instance, DenseNet-161 is twenty-times deeper than AlexNet, and VGG-16 has twenty-times more units than GoogleNet, but performances of DenseNet-161 and VGG-16 for scene recognition are only a 2.9% and a 1.6% better than those of AlexNet and GoogleNet, respectively (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>This paper presents a novel strategy to improve scene recognition: to use object-level information to guide scene learning during the training process. It is well-known that-probably due to the ImageNet finetuning paradigm <ref type="bibr" target="#b7">[7]</ref>, widely used models are biased towards the spatial center of the visual representation (see <ref type="figure" target="#fig_1">Figure 2</ref>) <ref type="bibr" target="#b8">[8]</ref>. The proposed approach relies on semantic-driven attention mechanisms to prioritize the learning of common scene objects. This strategy achieves the best reported results on the validation set of the Places365 Standard dataset <ref type="bibr" target="#b0">[1]</ref> (see <ref type="figure" target="#fig_0">Figure 1</ref>) without increasing the network's width or deepness, i.e. using 67.13% less units and 62.73% less layers than VGG-16 and DenseNet-161 respectively.</p><p>Similar strategies have been recently proposed to constrain scene recognition through histograms of patchwise object detections <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>. Compared to these strategies, the proposed method naturally exploits the spatial relationships between objects and emphasizes interpretability by relying on semantic segmentation.</p><p>Semantic segmentation can be understood as a dense object detection task. In fact, scene recognition, object detection and semantic segmentation are interrelated tasks that share a common Branch in the recently proposed taskonomies or task-similarity branches <ref type="bibr" target="#b13">[13]</ref>. However, the performance achieved by semantic segmentation methods is generally lower than that of object detection ones mainly due to the difference in size and variety of their respective datasets. Howbeit, the proposed method surmounts this gap and yields a higher scene recognition performance than object-constrained methods <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>, using a substantially smaller number of units (see <ref type="bibr">Section 4)</ref>.</p><p>In essence, the proposed work aims to enhance scene recognition without increasing network's capacity or depth leading to lower training and inference times, as well as to smaller data requirements and training resources. The specific contributions of these paper are threefold:</p><p>1. We propose an end-to-end multi-modal deep learning architecture which gathers both image and context information using a two-branched CNN architecture.</p><p>2. We propose to use semantic segmentation as an additional information source to automatically create, through a convolutional neural network, an attention model to reinforce the learning of relevant contextual information.</p><p>3. We validate the effectiveness of the proposed method yielding state-of-the-art results for the MIT Indoor 67 <ref type="bibr" target="#b14">[14]</ref>, SUN 397 <ref type="bibr" target="#b15">[15]</ref> and Places365 <ref type="bibr" target="#b0">[1]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Scene Recognition</head><p>A variety of works for scene recognition have been proposed during the last years: it is a hot topic.</p><p>Existing methods can be broadly organized into two different categories: those based on hand-crafted feature representations and those based on CNN architectures.</p><p>Among the first group, earliest works proposed to design a holistic feature representations. Generalized Search Trees (GIST) <ref type="bibr" target="#b16">[16]</ref> were used to generate a holistic low dimensional representation for each image.</p><p>However, precisely due to the holistic approach, GIST lacks of scene's local structure information. To overcome this problem, local feature representations were used to exploit the pattern of local patches and combine their representations in a concatenated feature vector. Census Transform Histogram (CENTRIST) <ref type="bibr" target="#b17">[17]</ref> encodes the local structural properties within an image and suppresses detailed textural information to boost scene recognition performance. To further increase generalization, Oriented Texture Curves (OTC) <ref type="bibr" target="#b18">[18]</ref> captures patch textures along multiple orientations to be robust to illumination changes, geometric distortions and local contrast differences. Overall, despite reporting noteworthy results, these hand-crafted features, either holistic or local, exploit low-level features which may not be discriminative enough for ambiguous or highly related scenes <ref type="bibr" target="#b0">[1]</ref>. Besides, the hand-craft nature of features may hinder their scalability as ad hoc designs may be required for new domains.</p><p>Solutions based on CNNs generally result in higher performances. Recent network architectures, together with multi-million datasets such as Places-365, crush the results obtained by hand-crafted features <ref type="bibr" target="#b0">[1]</ref>.</p><p>CNNs exploit multi-scale feature representations using convolutional layers and do not require the design of hand-crafted features as they are implicitly learned through the training process. Besides, CNNs combine low-level latent information such as color, texture and material with high-level information, e.g., parts and objects to obtain better scene representations and boost scene recognition performance <ref type="bibr" target="#b5">[5]</ref>. Well-consolidated network architectures, such as AlexNet, GoogLeNet, VGG-16, ResNet-152 and DenseNet-161 have reported accuracies of 53.17%, 53.63%, 55.24%, 54.74% and 56.10% respectively when classifying images from the challenging Places-365 Standard dataset <ref type="bibr" target="#b0">[1]</ref>.</p><p>Features extracted from these networks have been also combined with different scene classification methods. For instance, Xie et al. <ref type="bibr" target="#b19">[19]</ref> proposed to use features from AlexNet and VGG with Mid-level Local Representation (MLR) and Convolutional Fisher Vector representation (CFV) dictionaries to incorporate local and structural information. Cimpoi et al. <ref type="bibr" target="#b20">[20]</ref> used material and texture information extracted from a CNN-filter bank and from a bag-of-visual-words to increase generalization for enhanced scene recognition.</p><p>Yoo et al. <ref type="bibr" target="#b21">[21]</ref> benefited from multi-scale CNN-based activations aggregated by a Fisher kernel framework to perform significantly better on the MIT Indoor Dataset <ref type="bibr" target="#b14">[14]</ref>.</p><p>While accuracies from this networks and methods are much higher than those reported by methods based on hand-crafted features, they are far from those achieved by CNNs in other image classification problems (e.g., ImageNet challenge best reported accuracy is 85.4% <ref type="bibr" target="#b22">[22]</ref> while best reported accuracy on Person Re-Identification task is 95.4% <ref type="bibr" target="#b23">[23]</ref>).</p><p>As described in Section 1, the increment in capacity (number of units) of CNNs does not, for scene recognition, lead to a linear rise in performance. This might be explained by the inability of the networks to handle inter-class similarities <ref type="bibr" target="#b11">[11]</ref>: unrelated scene classes may share objects that are prone to produce alike scene representations weakening the network's generalization power. To cope with this problem, some recent methods incorporate context and discriminative object information to constrain scene recognition.</p><p>Based on context information, Xie et al. <ref type="bibr" target="#b24">[24]</ref> proposed to enhance fine-grained recognition by detecting part candidates based on saliency detection and by constructing a CNN architecture with local parts and global discrimination. Zhao et al. <ref type="bibr" target="#b25">[25]</ref>, similarly, proposed a discriminative discovery network (DisNet) that generates a discriminative map (Dis-Map) for the input image. This map is further used to select scale-aware discriminative locations which are then forwarded to a multi-scale pipeline for CNN feature extraction.</p><p>In terms of discriminative object information, Herranz-Perdiguero et al. <ref type="bibr" target="#b26">[26]</ref> proposed an extension of the DeepLab semantic segmentation network by introducing SVMs classifiers to perform scene recognition based on object histograms. In the same vein, Wang et al. <ref type="bibr" target="#b10">[10]</ref> designed an architecture where patch-based features are extracted from customized object-wise and scene-wise CNNs to construct semantic representations of the scene. A scene recognition method built on these representations (Vectors of Semantically Aggregated Descriptors (VSAD)) yields excellent performance on standard scene recognition benchmarks.</p><p>VSAD's performance has been recently enhanced by measuring correlations between objects among different scene classes <ref type="bibr" target="#b11">[11]</ref>. These correlations are then used to reduce the effect of common objects in scene missclassification and enhance the effect of discriminative objects through a Semantic Descriptor with Objectness (SDO).</p><p>Even though these methods constitute state-of-the-art in scene recognition, their dependency of object information, obtained by using patch-based object classification techniques, entails severe and reactive parametrization (scale, patch-size, stride, overlapping...). Moreover, their descriptors are object-centered and lack of information on the spatial-interrelations between object instances. We instead propose to use an end-to-end CNN that exploits spatial relationships by using semantic segmentation instead of object information to guide network's attention. This proves to provide equal or better performance while significantly reducing the number of network units and hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic Segmentation</head><p>Semantic segmentation is the task of assigning a unique object (or stuff) label to every pixel of an image. As reported in several benchmarks (MIT Scene Parsing Benchmark <ref type="bibr" target="#b3">[3]</ref>, CityScapes Dataset <ref type="bibr" target="#b4">[4]</ref>, and COCO Challenge <ref type="bibr" target="#b27">[27]</ref>), top-performing strategies, which are completely based on end-to-end deep learning architectures, are year by year getting closer to human accuracy.</p><p>Among the top performing strategies, Wang et al. proposed to use a dense up-sampling CNN to generate pixel-level predictions within a hybrid dilated convolution framework <ref type="bibr" target="#b28">[28]</ref>. In this vein, the Unified Perceptual Parsing <ref type="bibr" target="#b29">[29]</ref> benefits from a multi-task framework to recognize several visual concepts given a single image (semantic segmentation, materials, texture). By regularization, when multiple tasks are trained simultaneously, their individual results are boosted. Zhao et al. modelled contextual information between different semantic labels proposing the Pyramid Pooling Module [30]-e.g., an airplane is likely to be on a runway or flying across the sky but rarely over the water. These spatial relationships allow reducing the complexity associated with large sets of object labels, generally improving performance. Guo et al. <ref type="bibr" target="#b31">[31]</ref> proposed the Inter-Class Shared Boundary (ISB) Encoder which aims to encode the level of spatial adjacency between object-class pairs into the segmentation network to obtain higher accuracy for classes associated with small objects.</p><p>Motivated by the success of modelling contextual information and spatial relationships, we decided to explore the use of semantic segmentation as an additional modality of information to confront scene recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-Modal Deep Learning Architectures</head><p>The use of complementary image modalities (e.g., depth, optical flow or heat maps) as additional sources of information to multi-modal classification models has been recently proposed to boost several computer vision tasks: pedestrian detection <ref type="bibr" target="#b32">[32]</ref><ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref>, action recognition <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> and hand-gesture recognition <ref type="bibr" target="#b38">[38]</ref>.</p><p>Regarding pedestrian detection, a plethora of solutions have been proposed to include additional sources of information besides the RGB one <ref type="bibr" target="#b32">[32]</ref><ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref>  The effectiveness of modelling temporal features using multi-modal networks has been also studied <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>. Simonyan et al. <ref type="bibr" target="#b36">[36]</ref> and Park et al. <ref type="bibr" target="#b37">[37]</ref> proposed to incorporate optical flow as an additional input for action recognition. These methods lead to state-of-the-art performance when classifying actions in videos.</p><p>Similarly, hand-gesture recognition has also benefited from the use of optical-flow <ref type="bibr" target="#b38">[38]</ref>.</p><p>In this paper, we propose to combine RGB images and their corresponding semantic segmentation. Additionally, instead of using traditional combinations-linear combination, feature concatenation or averaging <ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref>, we propose to use a CNN architecture based on a novel attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Attention Networks</head><p>CNNs with architectures based on attention modules have been recently shown to improve performances on a wide and varied set of tasks <ref type="bibr" target="#b39">[39]</ref>. These attention modules are inspired by saliency theories on the human visual system, and aim to enhance CNNs by refocusing them onto task-relevant image content, emphasizing the learning of this content <ref type="bibr" target="#b40">[40]</ref>.</p><p>For image classification, the Residual Attention Network <ref type="bibr" target="#b41">[41]</ref>, based on encoder-decoder stacked attention modules, increases classification performance by generating attention-aware features. The Convolutional Block Attention Module (CBAM) <ref type="bibr" target="#b40">[40]</ref> infers both spatial and channel attention maps to refine features at all levels while obtaining top performance on ImageNet classification tasks and on MS COCO and VOC2007 object detection datasets. An evaluation benchmark proposed by Kiela et al. <ref type="bibr" target="#b42">[42]</ref>, suggested that the incorporation of different multi-modal attention methods, including additive, max-pooling, and bilinear models is highly beneficial for image classification algorithms.</p><p>In terms of action recognition and video analysis, attention is used to focus on relevant content over time. Yang et al. proposed to use soft attention models by combining a convolutional Long Short-Term Memory Network (LSTM) with a hierarchical architecture to improve action recognition metrics <ref type="bibr" target="#b43">[43]</ref>.</p><p>Guided by the broad range and impact of attention mechanisms, in this paper we propose to train an attention mechanism based on semantic information. For the semantic segmentation of a given image, this mechanism returns a map based on prior learning. This map promises to reinforce the learning of features derived from common scene objects and to hinder the effect of features produced by non-specific objects in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Architecture</head><p>This paper proposes a multi-modal deep-learning scene recognition system composed of a two-branched CNN and an Attention Module (see <ref type="figure" target="#fig_4">Figure 3</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture Legend</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivations</head><p>The basic idea behind the design is that features extracted from RGB and semantic segmentation domains complement each other for scene recognition. If we train a CNN to recognize scenes just based on the semantic segmentation of an input RGB image (see top Branch in <ref type="figure" target="#fig_4">Figure 3</ref>), this network tends to focus on representative and discriminative scene objects, yielding genuine-semantic representations based on objects and stuff classes. We propose to use these representations to train an attention map. This map is then used to gate RGB-extracted representations (see bottom Branch in <ref type="figure" target="#fig_4">Figure 3</ref>), hence refocusing them on the learned scene content.</p><p>The effect of this attention mechanism is that the receptive field of the most probable neuron at the final classification layer-known as class-activation map <ref type="bibr" target="#b9">[9]</ref>-is enlarged and displaced towards scene discriminative elements that have been learned by the Semantic Branch. An example of this behavior is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>: the RGB Branch is focused on the center of the image; the Semantic Branch mainly focuses on the oven for predicting the scene class; the combined network includes the oven and other "Kitchen" discriminative objects as the stove and the cabinet for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preliminaries</head><p>The proposed network architecture is presented in <ref type="figure" target="#fig_4">Figure 3</ref>. Let I ? R w?h?3 be a normalized RGB image and let M ? R w?h?L be a score tensor representing I's semantic segmentation, where M i,j ? R 1?1?L</p><p>represents the probability distribution for the i, j-th pixel on the L-set of learned semantic labels. Their output is a pair of feature tensors, F M and F I respectively, which are fed to the Attention Module to obtain the final feature tensor F A . This is fed to a linear classifier to obtain the final prediction for a K-class scene recognition problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">RGB Branch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Semantic Branch</head><p>Given I, a semantic segmentation network is used to infer a semantic segmentation score tensor M which is then fed to the Semantic Branch. M encodes information of the set of meaningful and representative objects, their spatial relations and their location in the scene. This Branch returns a set of semantic-based features F M ? R wo?ho?co that model the set of semantic labels and their semantic inter-dependencies in spatial and channel dimensions.</p><p>The architecture of the Semantic Branch is a shallow network whose output size exactly matches that of a ResNet-18 feature map (i.e, before the linear classifier). The use of a shallow network is here preferred, as its input M lacks of texture information. Nonetheless, we compare the effect of using deeper networks for this Branch in section 4.4.</p><p>To reinforce the semantic classes that are relevant for a given image, three Channel Attention Modules (ChAM) <ref type="bibr" target="#b40">[40]</ref> are introduced between convolutional blocks in the Semantic Branch. The ChAM module (see <ref type="figure">Figure 4</ref>) exploits the inter-channel relationships between features and results on a per-channel attention map. In this map, some feature channels are reinforced and some inhibited after the sigmoid layer. As each channel represents the probability of a semantic class, ChAM forces the network to focus on certain </p><formula xml:id="formula_0">f 1 = W C2 ? ?(W C1 ? f avg ) f 2 = W C2 ? ?(W C1 ? f max ) m c (F) = ?(f 1 + f 2 ),<label>(1)</label></formula><p>where ? and ? are Sigmoid and ReLU activation functions respectively, W C1 ? R c/r?c and W C2 ? R c?r/c are the weights of the fully connected layers, and r is a reduction ratio parameter.</p><p>This channel-attention map is used to weight F by:</p><formula xml:id="formula_1">F = m c (F) F,<label>(2)</label></formula><p>where ( ) represents a Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Attention Module</head><p>The Attention Module is used to obtain a set of semantic-weighted features F A ? R wa?ha?ca which are forwarded to a linear classifier to obtain the final scene posterior probabilities. The architecture and design parameters of the proposed Semantic Attention Module are depicted in <ref type="figure" target="#fig_7">Figure 5</ref> and detailed in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Alternative attention mechanisms have been evaluated (see Section 4.4). Hereinafter, we describe the one that yields the highest performance. Semantic gating representations F M,A are obtained from the output of the Semantic Branch F M by a double convolutional block followed by a sigmoid module:</p><formula xml:id="formula_2">F M,A = ? ? W A2 M ? W A1 M F M + b A1 M + b A2 M ,<label>(3)</label></formula><p>where ? and ? are again Sigmoid and ReLU activation functions respectively,</p><formula xml:id="formula_3">W A1 M , b A1 M and W A2 M , b A1 M</formula><p>are the weights and biases of the two convolutional layers.</p><p>Similarly, RGB features to be gated (F I,A ) are obtained from the output of the RGB Branch F I by:</p><formula xml:id="formula_4">F I,A = ? W A2 I ? W A1 I F I + b A1 I + b A2 I ,<label>(4)</label></formula><p>where W A1 I , b A1 I and W A2 I , b A1 I are the weights of the two convolutional layers of this Branch. Gating is performed by simply multiplying these two representations:</p><formula xml:id="formula_5">F A = F I,A F M,A .<label>(5)</label></formula><p>F A is then forwarded to a classifier composed of an average pooling, a dropout, and a fully connected layer, generating a feature vector f ? R K . The inference of scene posterior probabilities y ? R K from f is achieved by using a logarithmic normalized exponential (logarithmic softmax ) function ?(f) : R K ? R K :</p><formula xml:id="formula_6">y k = ?(f) = log exp(f k ) i exp(f i ) , ?i = 1, ..., K,<label>(6)</label></formula><p>where y k is the posterior probability for class k given the feature vector f.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training Procedure and Loss</head><p>A two-stage learning procedure has been followed paying particular attention to prevent one of the branches dominating the training process.</p><p>Observe that some training examples might be better classified using either RGB or semantic features. In our experiments, when both branches were jointly trained, and one of the branches was more discriminative than the other, the overall loss was small, hindering the optimization of the less discriminative Branch. To avoid this situation, during the first stage, both RGB and Semantic Branches are separately trained for a given scenario (both branches are initialized with Places pretrained weights). At the second training step, trained RGB and semantic branches are frozen while the Attention Module and the linear classifier are fully trained from scratch.</p><p>Both training stages are optimized minimizing the following logistic function:</p><formula xml:id="formula_7">arg min ? 1 N N i=1 ?log(?(f)),<label>(7)</label></formula><p>where ? denotes the trainable parameters, ?log(?) is the Negative Log-Likelihood (NLL) loss and N is the number of training samples for a given batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>In this section, we evaluate the proposed scene recognition network on four well-known and publicly available datasets: ADE20K <ref type="bibr" target="#b3">[3]</ref>, MIT Indoor 67 <ref type="bibr" target="#b14">[14]</ref>, SUN 397 <ref type="bibr" target="#b15">[15]</ref> and Places365 <ref type="bibr" target="#b0">[1]</ref>. To ease the reproducibility of the method, the implementation design of the proposed method is first explained and common evaluation metrics for all the experiments are presented. Then, to assess the effect of each design decision, we perform a varied set of ablation studies. The section closes with a comparison against state-of-the-art approaches on three publicly available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Design</head><p>Data Dimensions: Each input RGB image I is spatially adapted to the network input by re-sizing the smaller edge to 256 and then cropping to a square shape of 224 ? 224 ? 3 dimension. In the training stage, this cropping is randomized as part of the data augmentation protocol, whereas in the validation stage, we follow a ten-crop evaluation protocol as described in <ref type="bibr" target="#b0">[1]</ref>. The Semantic Branch inherits this adaptation.</p><p>RGB and Semantic Branches produce features F I and F M respectively, both with dimensions 512 ? 7 ? 7.</p><p>After the Attention Module, a 1024 ? 3 ? 3 attention representation F A is obtained. This representation is fed to the linear classifier, yielding a probability vector y with dimensions K ? 1, being K the number of scene classes of a given dataset.</p><p>Semantic Segmentation: It is performed by an UPerNet-50 network <ref type="bibr" target="#b29">[29]</ref> fully trained on the ADE20K dataset with L = 150 objects and stuff classes. This fixed Semantic Segmentation Network (see <ref type="figure" target="#fig_4">Fig. 3</ref>) is used throughout these experiments without further adaptation to any dataset.     to reduce the influence of semantic segmentation quality in the evaluation results. Additionally, the dataset includes ground truth for the classification of K = 1055 scene classes. Therefore, it can also be used as a scene recognition benchmark, albeit data composition of scene classes is notably unbalanced. The top row of <ref type="figure" target="#fig_3">Figure 6</ref> depicts example images and scene classes of this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E20K</head><p>MIT Indoor 67 Dataset: MIT Indoor 67 dataset <ref type="bibr" target="#b14">[14]</ref> contains 15620 RGB images of indoor places arranged onto K = 67 scene classes. In the reported experiments, we followed the division between train   with the dataset <ref type="bibr" target="#b15">[15]</ref>. Following such protocol, the dataset is divided into 50 training and validation images </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>Scene recognition benchmarks are generally evaluated via the Top@k accuracy metric with k ? [1, K].</p><p>In this paper, Top@{k = 1, 2, 5} accuracy metrics have been chosen. The Top@1 accuracy measures the percentage of validation/testing images whose top-scored class coincides with the ground-truth label. Top@2</p><p>and Top@5 accuracies, are the percentage of validation images whose ground-truth label corresponds to any of the 2 and 5 top-scored classes respectively.</p><p>The Top@k accuracy metrics are biased to classes over-represented in the validation set; or, in other words, under-represented classes barely affect these metrics. In order to cope with unbalanced validation sets (e.g., ADE20K for scene recognition), we propose to use an additional performance metric, the Mean Class Accuracy (MCA):</p><formula xml:id="formula_8">M CA = i T op i @1 K ?i = 1, ..., K,<label>(8)</label></formula><p>where Top i @1 is the Top@1 metric for scene class i. Note that MCA equals Top@1 for perfectly balanced datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>The aim of this section is to gauge the influence of different design elements that constitute significant aspects of the proposed method. First, the effect of different Semantic Branch architectures is evaluated.</p><p>Second, the effect of variations in both the attention mechanism and the attention module architecture is assessed and analyzed. Third, the influence of the multi-modal architecture and the attention module with respect to the RGB Branch is quantified. As stated in Section 4.2, all the ablation studies are carried out using the ADE20K dataset.</p><p>Influence of the Semantic Branch Architecture  <ref type="bibr" target="#b40">[40]</ref>, and use of pre-trained models) have been explored.</p><p>Results from <ref type="table" target="#tab_5">Table 2</ref> suggest that the inference capabilities of deeper and wider CNNs are not fully exploited, as similar results can be achieved with shallower networks. For instance, when trained from scratch, there is not a significant difference in terms of Top@k and MCA metrics between using ResNet-18 or a shallower network, e.g., a relative improvement of 0.49% with respect to the 4-convolutional layers configuration. Configurations using 3 or 4 convolutional layers yield similar performance. However, for a given input, a network with 3 convolutional layers (with its associated 3 spatial down-sampling steps) requires larger kernels than a network with 4 convolutional layers (4 down-sampling steps) to obtain a feature output of the same size due to the kernels' size, hence leading to a higher number of parameters. Using a pre-trained network moderately improves performance: ResNet-18 pre-trained on ImageNet performs a 3.00% (Top@1) and a 21.6% (MCA) better than ResNet-18 from scratch, but not drastically, due to the non-RGB nature of the semantic domain. The use of ChAM modules (as in <ref type="figure" target="#fig_4">Figure 3</ref> for the shallow networks and as in <ref type="bibr" target="#b40">[40]</ref> for ResNet-18) slightly improves between a 2.6% and 5.4% in terms of MCA, without implying a significant increase in complexity (0.1 Million additional units on average).</p><p>In the light of these experiments, we opted for using the 4 convolutional layer architecture for the Semantic Branch as a trade-off solution between performance and complexity. This architecture yields a comparable performance to ResNet-18 (?3.61% and ?14.86% for Top@1 and MCA respectively), given that substantially less units are used (?78.3%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of the Attention Module</head><p>We have performed two ablation studies on the influence of the attention module. First, we evaluated the effect of using different attention mechanisms all of them sharing a similar network architecture: two convolutional layers for each Branch using 3 ? 3 ? 512 and 3 ? 3 ? 1024 kernels. Then, once selected an attention mechanism, we further explore the use of alternative network architectures to drive the attention.  <ref type="table" target="#tab_8">Table 3</ref> presents an extensive study on attention mechanisms to validate their effectiveness and performance, which is compared with the RGB Baseline. To ease the analysis, <ref type="figure" target="#fig_13">Figure 8</ref> graphically depicts each of the evaluated attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanisms</head><p>Results from <ref type="table" target="#tab_8">Table 3</ref>  Moreover, gating may also serve to nullify not-relevant RGB information, easing the learning process. Additionally, we tested gating attention contrariwise, i.e. using RGB features to gate semantic features (see <ref type="figure" target="#fig_13">Figure   8</ref> right and <ref type="table" target="#tab_8">Table 3</ref> bottom). In this case, feature representations were obtained from the semantic domain, which lacks texture and color information, and the attention map was inferred from RGB domain-which lacks objects' labels and clear boundaries. These representation problems deteriorate performance a 9.59% and a 25.51% for Top@1 and MCA respectively. However, this Gated Sem combination improves the performance of the Semantic Branch itself (compare <ref type="table" target="#tab_5">Table 2 third row and Table 3</ref> last row). In conclusion,  we opted for the top-performing G-RGB-H attention mechanism. Results indicate that the inclusion of convolutional layers to adapt F I and F M before the attention mechanism improves performance. The lack of convolutional layers compared with a 2-layer configuration leads to a decrease in performance of a 2.73% for Top@1 and a 10.37% for MCA. In our opinion, this owes to the two-stage learning procedure that we follow (see Regarding kernel nature, the use of 1 ? 1 channel-increasing layers improves performance with respect to using no layers at all, but its performance is still behind to that obtained by considering also the width and height dimensions (3 ? 3 spatial kernels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture of the G-RGB-H Module</head><p>In conclusion, we opted for a G-RGB-H attention strategy based on two convolutional layers composed of 3 ? 3 ? 512 and 3 ? 3 ? 1024 kernels at each Branch.   In order to extend this result, we have further evaluated the impact of decreasing the number of semantic classes (L, initially 150 classes) in the method's performance. To this aim, two randomly selected sets of semantic classes (L = 50 and L = 100) have been used to train the proposed method. The selected sets were incremental, i.e. the set of L = 50 semantic classes was contained within the set of L = 100 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of Semantic Segmentation in Scene Recognition</head><p>In order to reduce a potential bias of the random selection, three different experiments with three different random seeds have been carried out. For these experiments, <ref type="figure" target="#fig_16">Figure 9</ref> represents average results by bars' heights and standard deviations via error bars. These indicate that a decrease in the number of semantic classes reduces the performance of the proposed method. If the number of semantic classes is reduced to L = 100, Top@1 and MCA metrics reduce by 15.28% and by 16.59%, respectively, compared to considering all classes, but still outperforms the baseline in terms of MCA by a 8.26%. When the number of semantic classes is further reduced to L = 50, performance reduces by 2.00% in terms of MCA compared with the ResNet-18 baseline. These results indicate that the performance of the proposed method is reduced if we use few semantic classes, but also suggest that performance might increase if we had more than L = 150 semantic classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">State-of-the-art Comparison</head><p>Along this section the proposed method is compared with 19 state-of-the-art approaches, ranging from common CNN architectures trained for scene recognition to methods using objects to drive scene recognition.</p><p>Comparison is performed on three datasets: MIT Indoor 67 <ref type="bibr" target="#b14">[14]</ref>, SUN 397 <ref type="bibr" target="#b15">[15]</ref> and Places365 <ref type="bibr" target="#b0">[1]</ref>. Unless agree with the neuron interpretability profiles discussed on <ref type="bibr" target="#b5">[5]</ref>.</p><p>Results on MIT Indoor 67 dataset (see <ref type="table">Table 6</ref>) are in line with those of  <ref type="table">Table 6</ref>: State-of-the-art results on MIT Indoor 67 dataset. All stated results with no reference have been extracted from <ref type="bibr" target="#b11">[11]</ref>.</p><p>Methods using objects to drive scene recognition include: <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>, Semantic Branch, Ours, Ours* and Ours**.</p><p>plementarity between the RGB and the semantic representations. The proposed method increases Top@1 performance of the RGB Branch by a 3.50% and a 3.19% for ResNet-18 and ResNet-50 backbones respectively. These relative increments are smaller than those reported in <ref type="table" target="#tab_10">Table 5</ref> as the semantic segmentation model, trained with the ADE20K dataset, is less tailored to the MIT scenarios. The proposed method (Ours) performs better than most of state-of-the-art algorithms while using a substantially smaller number of parameters. e.g., a 1.90% improvement over the single scale SDO <ref type="bibr" target="#b11">[11]</ref> (a method similar in spirit) while reducing a 82.97% the number of parameters. The ResNet-50 backbone version of our method (Ours*) outperforms every other state-of-the-art method, providing relative performance increments of 1.10% and 0.40% with respect to VSAD and multi-scale SDO, still with a significant reduction in the model's complexity-69.20% less parameters. Additionally, whereas multi-scale patch-based algorithms require severe parametrization of the size, stride and scale of each of the explored patches, the proposed algorithm is only parametrized in terms of training hyper-parameters, highly simplifying its learning process.</p><p>A similar discussion applies to the SUN 397 Dataset (see <ref type="table" target="#tab_15">Table 7</ref>). The proposed method (Ours)      <ref type="table" target="#tab_16">Table 8</ref>) and those we obtained after the downloading and evaluation of the publicly available network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on Places 365</head><p>The proposed method (Ours) obtains the best results while maintaining relatively low complexity. Its performance improves those of the deepest network, DenseNet-161, by a 0.73% in terms of Top@1 accuracy and it surpasses the highest-capacity network, VGG-19, by a 2.29% reducing the number of parameters a 67.13%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Interpretation of the Attention Module</head><p>Class Activation Maps: To qualitatively analyze the benefits of the attention module, <ref type="figure" target="#fig_0">Figure 10</ref> depicts intermediate results of our method, including, from left to right: the original color image, its semantic segmentation and Class Activation Maps (CAMs) <ref type="bibr" target="#b9">[9]</ref> at the output of the RGB Branch, through the F I features, the Semantic Branch, via the F M features, and after the attention module, using the F A features.</p><p>CAMs are here represented normalized in color with dark-red and dark-blue representing maximum (1) and minimum (0) activation respectively. Top 3 predicted classes for each Branch are included at the top-left corner of each image.</p><p>The automatic refocusing capability of the attention module can be observed at first glance. Whereas CAMs of the RGB Branch are clearly biased towards the image center, after the attention module, the attention is focused on human-accountable concepts that can be indicative of the scene class, e.g., the microwave for the kitchen, the animals for the chicken farm or the mirror for the bathroom. This refocusing is specially useful for the disambiguation between similar classes-e.g., the runway to correct the airport prediction at the second row, and to drive attention towards discriminative objects in conflicting scenes-e.g., the mirror can be used to recognize the bathroom at the last row.</p><p>Owing to the trained convolutional layers in the attention module (see Section 3.5) the final CAM is not derived from the straight multiplication of the RGB and the Semantic CAMs. Instead, the effect of the Semantic CAM-together with refocusing, is generally an enlargement of the focus area (compare third and fifth columns in the <ref type="figure" target="#fig_0">Figure 10)</ref>; hence, increasing the amount of information that may be used to discriminate between scene classes. This may be a consequence of the larger CAMs yielded by the Semantic Branch: as the semantic segmentation contains less information than the color image, the Semantic Branch tends to focus on either small discriminative objects or on large areas containing objects' transitions.</p><p>Correlation between scene and objects: Using the Class Activation Maps described in the previous section we can obtain the correlation between objects and scene concepts, by taking into account the activation received by each object/stuff class in both the MIT Indoor 67 and the SUN 397 datasets. Accumulated attentions along the dataset are depicted in <ref type="figure" target="#fig_0">Figure 11</ref>. The horizontal axis represents the set of semantic classes assigned a larger focus-according to the CAMs-for both datasets. Objects/stuff are divided into 3 separate subsets: purple color is used to represent objects that are common in Indoor scenarios, green objects are those that can be found in Outdoor scenarios, and black ones are those that may be found in Both scenarios.</p><p>The top graph in <ref type="figure" target="#fig_0">Figure 11</ref> suggests that, for MIT Indoor 67 dataset, the proposed network (blue bars) is essentially focused on stuff/objects associated with the Indoor (purple) and Both (black) subsets.</p><p>Comparing with the Baseline (orange bars), the attention received by Outdoor semantic classes decreases, e.g. see "road", "railing", "bus" or "land". These results are consistent with the MIT Indoor dataset which only includes indoor scenarios. Moreover, the correlation between objects and scene concepts for the SUN 397 dataset <ref type="figure" target="#fig_0">(Figure 11</ref> bottom graph) suggests a more evenly distributed attention towards stuff/objects from Indoor, Outdoor and Both sets-disregarding objects' sizes.</p><p>Results for MIT Indoor and SUN 397 datasets suggest that the most attended semantic classes depend on whether the dataset's scenes are from Indoor, Outdoor or Both scenarios. We believe that this preliminary analysis sets a base for forthcoming interpretability studies that will be part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Limitations of the Proposed Method</head><p>According to the reported results, semantic segmentation is useful to gate the process of scene recognition on RGB images. However, if semantic segmentation is flawed or imprecise, the proposed method may not be able to surpass the erroneous information. <ref type="figure" target="#fig_0">Figure 12</ref> depicts two qualitative examples of these problems. In the top row, the semantic image lacks information on discriminative objects e.g., the computer. In the absence of these objects, the cabinet object, which is correctly segmented, dominates the gating process and drives an erroneous recognition: a "Kitchen" instead of an "Office". The bottom row contains another problematic situation. The semantic segmentator miss-classifies the court as water (probably due to its color and texture). The proposed network, guided by the primary presence of water, infers that the 3 most probable scenes classes are those in which water is preeminent, hence failing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper describes a novel approach to scene recognition based on an end-to-end multi-modal convolutional neural network. The proposed method gathers both image and context information using a two-branched CNN (the traditional RGB branch and a complementary semantic information branch) whose Results on publicly available datasets (ADE20K, MIT Indoor 67, SUN 397 and Places 365) confirm that the proposed method outperforms every reported state-of-the-art method while significantly reducing the number of network parameters. This model simplification decreases training and inference times and the amount of necessary data for the learning stage. Overall, results confirm that the combination of both RGB and semantic segmentation modalities generally benefits scene recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Scene recognition performance of CNN-based solutions on the Places-365 Standard Dataset [1]. Each network is represented by its Top@1 accuracy (vertical axis), its number of layers (horizontal axis) and its number of units (radius of each circle). Circles' colors are only for better identification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Scene recognition results depending on the input data. (a) RGB image corresponding to the "Kitchen" scene class. (b) Semantic segmentation from (a) obtained by a state-of-the-art CNN-based algorithm. (c) Class Activation Map (CAM) [9] just using the (a) RGB image. (d) CAM just using the (b) semantic segmentation. (e) CAM for the proposed approach, using both (a) and (b). Top@3 predicted classes are included in the top-left corner of images (c) to (e). Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. Park et al. proposed the incorporation of optical-flow-based features to model motion through a decision-forest detection scheme [32]. Convolutional Channel Features (CCF) [33] rely on extracted low-level features from VGG-16 network as an additional input channel to enhance pedestrian detection. Daniel et al. [34] used short range and long range multiresolution channel features obtained by eight different individual classifiers to propose a detector, obtaining state-of-the-art results at a higher speed. An extensive study of the features that may help pedestrian detection has been carried out by Mao et al. [35]. Authors suggest that mono-modal pedestrian detection results are improved when depth, heat-maps, optical flow, or segmentation representations are integrated as additional information modalities into a CNN-based pedestrian detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>6</head><label>6</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the multi-modal deep-learning scene recognition model. The architecture is composed of a Semantic Branch, a RGB Branch and an Attention Module. The semantic branch aims to extract meaningful features from a semantic segmentation score map. This Branch aims to convey an attention map only based on meaningful and representative scene objects and their relationships. The RGB Branch extracts features from the color image. These features are then gated by the semantic-based attention map in the Attention Module. Through this process, the network is automatically refocused towards the meaningful objects learned as relevant for recognition by the Semantic Branch. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8 Dise?o 3 Figure 4 :</head><label>834</label><figDesc>Architecture of a Channel Attention Module (ChAM)<ref type="bibr" target="#b40">[40]</ref>. ChAM is fed with an arbitrary size feature map F ? R w ?h ?c and infers a 1D channel attention map mc ? R 1?1?c . Better viewed in color.The network is composed of a Semantic Branch(Figure 3 top)and a RGB Branch(Figure 3 bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>It consists of a complete ResNet-18 architecture. This Branch is fed with I and returns a set of RGBbased features F I ? R wo?ho?co where w o , h o and c o are the width, the height and the number of channels of the output feature map. This Branch includes the original ResNet-18's Basic Blocks with three of them (Basic Blocks 2, 3 and 4) implementing residual connections. F I , which is obtained from Basic Block 4, is then forwarded into the Attention Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Architecture of the Attention Module. It aims to obtain a set of semantic-weighed features F A ? R wa?ha?ca , based on RGB and semantic representations F M and F I . Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Image examples extracted from ADE20K<ref type="bibr" target="#b3">[3]</ref> (top row) and MIT Indoor 67<ref type="bibr" target="#b14">[14]</ref> (bottom row) datasets. Notice the large inter-class similarity between some of the scenes in both datasets. Better viewed in color.Its output (a 150 vector per pixel, indicating its probability distribution) is the score tensor M. To ease storage and convergence, this tensor is sparsified, i.e. all the M i,j -th probability distributions which are not among the three top for pixel (i, j) are set to zero.Data Augmentation: These techniques are used in order to reduce model overfitting and to increase network generalization. Specifically, for the RGB image, we use regular random crop, horizontal flipping, Gaussian blur, contrast normalization, Gaussian noise and bright changes. Due to their nature, for the semantic segmentation tensors, we only apply regular random crop and horizontal flipping. The training dataset is shuffled at each epoch. Hyper-parameters: To minimize the loss function in equation 7 and optimize the network's trainable parameters ?, the Deep Frank-Wolfe (DFW)<ref type="bibr" target="#b44">[44]</ref> algorithm is used. DFW is a first-order optimization algorithm for deep neural learning which, contrary to the Stochastic Gradient Descend (SGD), only requires the initial learning rate hyper-parameter and not a hand-crafted decay schedule. In all our experiments the initial learning rate was set to 0.1. The use of alternative values did not improve learning performance. The use of DFW resulted in similar performance to using SGD with a learning-rate decay schedule suited for each dataset in around the same number of epochs. However, by suppressing the decay policy hyper-parameter, the training process was lightened, as the number of parameters to sweep was simplified. Momentum was set to 0.9 and weight decay was set to 0.0001 for the two stages in the training procedure (Section 3.6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Image examples extracted from SUN 397 [15] (top row) and Places 365 [1] (bottom row) datasets. Notice the large inter-class similarity between some of the scenes in both datasets. Better viewed in color.the training computational time in a 21.4% with respect to the RGB baseline: a 6.78% due to the training of the semantic branch and a 14.62% due to the training of the attention module. When evaluating the proposed network, inference time is increased by a 37.5% with respect to the RGB baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4. 2 .</head><label>2</label><figDesc>DatasetsADE20K Scene Parsing Dataset: The scene parsing dataset ADE20K<ref type="bibr" target="#b3">[3]</ref> contains 22210 scenecentric images exhaustively annotated with objects. The dataset is divided into 20210 images for training and 2.000 images for validation. ADE20K is a semantic segmentation benchmark with ground-truth for L = 150 semantic categories provided for each image, hence enabling its use for training semantic segmentation algorithms (Section 4.1-Semantic Segmentation). This dataset is used in our ablation studies (Section 4.4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>and validation sets proposed by the authors<ref type="bibr" target="#b14">[14]</ref>: 80 training images and 20 images for validation per scene class. The bottom row ofFigure 6represents example images and scene classes of the dataset.SUN 397 Dataset: SUN 397 Dataset [15] is a large-scale scene recognition dataset composed by 39700 RGB images divided into K = 397 scene classes, covering a large variety of environmental scenes both indoor and outdoor. An evaluation protocol to divide the full dataset into both train a validation sets is provided 14</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>per scene class. The top row of Figure 7 reproduces examples images and scene classes of this dataset. Places 365 Database: Places365 dataset [1] is explicitly designed for scene recognition. It is composed of 10 million images comprising 434 scene classes. There are two versions of the dataset: Places365-Standard with 1.8 million train and 36000 validation images from K = 365 scene classes, and Places365-Challenge-2016, in which the size of the training set is increased up to 6.2 million extra images, including 69 new scene classes (leading to a total of 8 million train images from 434 scene classes). In this paper, experiments are carried out using the Places365-Standard dataset. The bottom row of Figure 7, presents example images and scene classes of this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Different attention mechanisms architectures used for the attention module ablation study. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>suggest that attention mechanisms generally outperform the RGB baseline in terms of MCA. Additionally, those based on Hadamard combination, either gated or not, perform better than additive and concatenation mechanisms. Additive combination results in a 4.94% and a 6.46% decrease in terms of Top@1 and MCA with respect to the Gated RGB Hadamard combination (G-RGB-H). Similarly, attention based on feature concatenation performs a 1.13% (Top@1) and a 6.38% (MCA) worse than G-RGB-H. Besides, concatenation increases the complexity of the linear classifier by generating a larger input to this module. In our opinion, the decreases in performance may be indicative of the inability of additive and concatenation mechanisms to favorable scale scene-relevant information.Focusing on Hadamard combination, the use of a gated RGB combination (G-RGB-H) slightly improves the non-gated one (a 4.31% for Top@1 and a 2.35% for MCA) thanks to the normalized semantic attention obtained after the sigmoid activation layer. The effect of this normalized map is to gate RGB information, hence maintaining its numerical range instead of scaling it as in the non-gated Hadamard combination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Section 3.6): if no convolutional layers are used, features from the semantic and the RGB Branches are not adapted, hindering the learning process. On the other hand, the use of a deeper attention module (using 3 instead of 2 layers) produces a decrease in performance of 2.15% and 7.77% in terms of Top@1 and MCA respectively, suggesting training over-fitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 9 :</head><label>9</label><figDesc>Influence of the number of semantic classes in the performance of the proposed method. For comparison, ResNet-18 RGB baseline is also included in the figure. Average Top@1 metric for the three different random seeds is represented by blue bars and the blue axis. Average Mean Class Accuracy (MCA) is represented by orange bars and the orange axis. Error bars in models with L = 50 and L = 100 semantic classes are representing standard deviation valuess.explicitly mentioned, results of all the approaches are extracted from their respective papers.Evaluation on MIT Indoor 67 and SUN 397Tables 6 and 7 agglutinate performances of scene recognition methods for the MIT Indoor 67 and the SUN 397 datasets respectively. As these datasets are balanced, we only include the Top@1 metric. All the compared algorithms are based on CNNs (see the Backbone column for details). For this evaluation, and just to offer a fair comparison in terms of network complexity, we also present results for a version of the proposed method (Ours) using a ResNet-50 network, instead of a ResNet-18 one, for the RGB Branch (Ours*). Note that, ResNet-50 is preferred over a higher-capacity network as VGG-19. The reason for this choice lies on the distribution of the discriminative power in each architecture. Regarding only the learnable layers, VGG-19 is composed of 16 convolutional layers, followed by a powerful classification module made up of three fully connected layers that constitute around 86% of the learnable capacity of the network. Consequently, during the training process a relevant fraction of the discriminative power is learned by the classifier. In our experiments, we noticed that this unbalanced distribution conveys features of a lower interpretability at the output of the convolutional layers that, in turn, are less useful for the proposed scheme. Differently, the convolutional layers of ResNet-50 represent around 97% of the learnable capacity of the network, providing a high interpretability power to the features entering the attention module. The results of these experiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative results. First and second column represent the RGB and semantic segmentation images for selected examples of the ADE20K, the SUN 397 and the Places 365 validation sets. The third, fourth and fifth columns depict the Class Activation Map (CAM) [9] obtained by using features extracted from: the RGB Branch used baseline (ResNet-18), the Semantic Branch and the proposed method (Ours). CAM represents the image areas that produce a greater activation of the network (the redder the larger). CAM images also indicate the ground-truth label and the Top 3 predictions. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 :</head><label>11</label><figDesc>Correlation between objects and scene concepts in both MIT Indoor 67 (top bar graph) and SUN 397 datasets (bottom bar graph) for ResNet-18 Baseline and for the proposed method (Ours). Objects/stuff are divided into 3 separate subsets. Purple color is used to represent objects that are Indoors, green objects are Outdoors, and black ones are those that lie in Both indoors and outdoors. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 12 :</head><label>12</label><figDesc>Examples of limitations of the proposed method. In some cases, when semantic segmentation is insufficient (top row) or incorrect (bottom row) network predictions are erroneous. In the top row, the presence of a cabinet and the absence of the computer in the semantic segmentation leads to a kitchen prediction. In the bottom row, the erroneous segmentation of the floor as water leads to water-related scene predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>features are combined via an attention module. Several attention strategies have been explored including classical additive and concatenating strategies as well as novel strategies based on fully trained convolutional layers followed by a Hadamard product. Among these last, the top performing strategy relies on a softmax transformation of the convolved Semantic Branch features. These transformed features are used to gate convolved features of the RGB Branch, which results in the reinforcement of the learning of relevant context information by changing the focus of attention towards human-accountable concepts indicative of 26 scene classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Layers and output sizes for the Attention Module.</figDesc><table><row><cell>Name</cell><cell>Output Size</cell><cell>Blocks</cell></row><row><cell>att conv I</cell><cell>512 ? 5 ? 5</cell><cell>3 ? 3, 512, stride 1</cell></row><row><cell>att conv M</cell><cell>512 ? 5 ? 5</cell><cell>3 ? 3, 512, stride 1</cell></row><row><cell>att conv2 I</cell><cell>1024 ? 3 ? 3</cell><cell>3 ? 3, 1024, stride 1</cell></row><row><cell cols="2">att conv2 M 1024 ? 3 ? 3</cell><cell>3 ? 3, 1024, stride 1</cell></row><row><cell>attention</cell><cell>1024 ? 3 ? 3</cell><cell>Hadamard product</cell></row><row><cell>avg pool</cell><cell>1024 ? 1 ? 1</cell><cell>Average Pooling</cell></row><row><cell>classifier</cell><cell>K ? 1</cell><cell>Dropout, K-dimensional FC, Softmax</cell></row></table><note>classes. Specifically, ChAM is fed with an arbitrary size feature map F ? R w ?h ?c and infers a 1D channel attention map m c ? R 1?1?c . F is squeezed along the spatial dimension by using both average-pooling and max-pooling operations, obtaining feature vectors f avg and f max . These vectors are combined via two shared fully connected networks separated by a ReLu activation layer. Resulting vectors f 1 and f 2 are then added and regularized via a sigmoid layer to yield the 1D channel attention map m c :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Hardware and Software: The model design, training and evaluation has been implemented using PyTorch 1.1.0 Deep Learning framework [45] running on a PC using a 12 Cores CPU and a NVIDIA TITAN</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Ring Indoor</cell><cell></cell><cell></cell></row><row><cell>T Indoor 67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Movie Theatre</cell><cell>Concert Hall</cell><cell>Class Room</cell><cell>Computer Room</cell><cell>Restaurant</cell><cell>Fast food Restaurant</cell></row><row><cell>N 397</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swimming Pool Indoor</cell><cell>Swimming Pool Outdoor</cell><cell>Stadium Baseball</cell><cell>Stadium Football</cell><cell>Bedroom</cell><cell>Hotel Room</cell></row><row><cell>ces 365</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Field</cell><cell>Forest</cell><cell>Lagoon</cell><cell>Coast</cell><cell>Kitchen</cell><cell>Kitchenette</cell></row></table><note>Xp 12GB Graphics Processing Unit. Given this hardware specifications, the proposed architecture increases</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Ablation results for different architectures for the Semantic Branch.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 presents</head><label>2</label><figDesc></figDesc><table /><note>results when semantic representations (F M ) are solely used for scene-recognition. Seven configurations for the Semantic Branch that result from the combination of three design parameters (varia- tion of the network architecture, employment of ChAM modules</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Ablation results for different attention mechanisms.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>presents comparative results for several variations of this module's architecture: straightly gating</cell></row><row><cell>without adaptation (No Conv layers), using 2 or 3 convolutional layers, and using 1 ? 1 channel-increasing</cell></row><row><cell>layers.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc>gauges the effectiveness of the proposed architecture when compared to the solely use of either the RGB Branch or the Semantic Branch. Comparative performances show that the RGB and semantic branches highly complement each other when used for scene recognition. The proposed architecture (Table 5 last row) outperforms the RGB baseline (Table 5 first row): a 9.92% in terms of Top@1 performance and a 29.80% in terms of MCA. Note that, due to the unbalanced dataset, the MCA metric is more adequate for comparison.</figDesc><table><row><cell>Conv Layers</cell><cell>Kernel Size</cell><cell cols="3">Top@1 Top@2 Top@5</cell><cell>MCA</cell></row><row><cell cols="2">No Conv Layers</cell><cell>60.84</cell><cell>70.55</cell><cell>80.22</cell><cell>24.20</cell></row><row><cell></cell><cell>1 ? 1 ? 512</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell>61.00</cell><cell>72.30</cell><cell>81.95</cell><cell>24.92</cell></row><row><cell></cell><cell>1 ? 1 ? 1024</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3 ? 3 ? 512</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell>62.55</cell><cell>73.25</cell><cell>82.75</cell><cell>27.00</cell></row><row><cell></cell><cell>3 ? 3 ? 1024</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3 ? 3 ? 512</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>3 ? 3 ? 1024</cell><cell>61.20</cell><cell>71.150</cell><cell>81.750</cell><cell>24.90</cell></row><row><cell></cell><cell>3 ? 3 ? 1024</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Ablation results for different G-RGB-H architectures.</figDesc><table><row><cell cols="3">RGB Semantic Top@1 Top@2 Top@5</cell><cell>MCA</cell></row><row><cell>56.90</cell><cell>67.25</cell><cell>78.00</cell><cell>20.80</cell></row><row><cell>50.60</cell><cell>60.45</cell><cell>72.10</cell><cell>12.17</cell></row><row><cell>62.55</cell><cell>73.25</cell><cell>82.75</cell><cell>27.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Scene recognition results on ADE20K.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>, suggesting a high com-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>State-of-the-art results on SUN 397 dataset. All stated results with no reference have been extracted from<ref type="bibr" target="#b11">[11]</ref>.Methods using objects to drive scene recognition include:<ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>, Semantic Branch, Ours, Ours* and Ours**.</figDesc><table><row><cell>Network</cell><cell>Number of</cell><cell cols="3">Top@1 Top@2 Top@5</cell><cell>MCA</cell></row><row><cell></cell><cell>Parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AlexNet</cell><cell>? 62 M</cell><cell>47.45</cell><cell>62.33</cell><cell>78.39</cell><cell>49.15</cell></row><row><cell>AlexNet @[1]</cell><cell>? 62 M</cell><cell>53.17</cell><cell>-</cell><cell>82.89</cell><cell>-</cell></row><row><cell>GoogLeNet @[1]</cell><cell>? 7 M</cell><cell>53.63</cell><cell>-</cell><cell>83.88</cell><cell>-</cell></row><row><cell>ResNet-18</cell><cell>? 12 M</cell><cell>53.05</cell><cell>68.87</cell><cell>83.86</cell><cell>54.40</cell></row><row><cell>ResNet-50</cell><cell>? 25 M</cell><cell>55.47</cell><cell>70.40</cell><cell>85.36</cell><cell>55.47</cell></row><row><cell>ResNet-50 @[1]</cell><cell>? 25 M</cell><cell>54.74</cell><cell>-</cell><cell>85.08</cell><cell>-</cell></row><row><cell>VGG-19 @[1]</cell><cell>? 143 M</cell><cell>55.24</cell><cell>-</cell><cell>84.91</cell><cell>-</cell></row><row><cell>DenseNet-161</cell><cell>? 29 M</cell><cell>56.12</cell><cell>71.48</cell><cell>86.12</cell><cell>56.12</cell></row><row><cell>Semantic Branch</cell><cell>? 2.6 M</cell><cell>36.20</cell><cell>50.11</cell><cell>68.48</cell><cell>36.20</cell></row><row><cell>Ours</cell><cell>? 47 M</cell><cell>56.51</cell><cell>71.57</cell><cell>86.00</cell><cell>56.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>State-of-the-art results on Places-365 Dataset (%). (@[1] stands for performance metrics reported in [1]).outperforms most of state-of-the-art approaches, maintaining lower complexity; and if we go for a more complex network for the RGB Branch (Ours*), the proposed method outperforms all reported methods still using less than 1/3 of parameters.</figDesc><table><row><cell>RGB Image</cell><cell>Semantic Segmenation</cell><cell>RGB Branch (ResNet-18) Class Activation Map</cell><cell>Semantic Branch Class Activation Map</cell><cell>Ours Class Activation Map</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 8 presents</head><label>8</label><figDesc>results for the challenging Places365 Dataset. This Table includes both results from the original Places365 paper (@[1] in</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This study has been partially supported by the Spanish Government through its TEC2017-88169-R MobiNetVideo project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Scene parsing through ade20k dataset</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Network dissection: Quantifying interpretability of deep visual representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6541" to="6549" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly supervised patchnets: Describing and aggregating local patches for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2028" to="2041" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scene recognition with objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="474" to="487" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep patch representations with shared codebook for scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1s</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
	<note>Taskonomy: Disentangling task transfer learning</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gist of the scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurobiology of Attention</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="251" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Centrist: A visual descriptor for scene categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1489" to="1501" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Otc: A novel local descriptor for scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="377" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hybrid cnn and dictionary-based models for scene recognition and domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1263" to="1274" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1628</idno>
		<title level="m">Fisher kernel for deep neural activations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lg-cnn: From local parts to global discrimination for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="118" to="131" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reproducible experiments on adaptive discriminative region discovery for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1076" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In pixels we trust: From pixel labeling to object localization and scene categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Herranz-Perdiguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Redondo-Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>L?pez-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Small object sensitive segmentation of urban street scene with spatial adjacency between object classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2643" to="2653" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring weak stabilization for motion feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2882" to="2889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
	<note>Convolutional channel features</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic channels for fast pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Costea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2360" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6034" to="6043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Combining multiple sources of knowledge in deep cnns for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving the performance of unimodal dynamic hand-gesture recognition with multimodal training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abavisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R V</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient large-scale multi-modal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cham: action recognition using convolutional hierarchical attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3958" to="3962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep frank-wolfe for neural network optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Berrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
