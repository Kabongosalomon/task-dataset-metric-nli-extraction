<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransNet: A deep network for fast detection of common shot transitions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Sou?ek</surname></persName>
							<email>tomas.soucek1@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering Faculty of Mathematics and Physics</orgName>
								<orgName type="laboratory">SIRET Research Group</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslav</forename><surname>Moravec</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering Faculty of Mathematics and Physics</orgName>
								<orgName type="laboratory">SIRET Research Group</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Loko?</surname></persName>
							<email>lokoc@ksi.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering Faculty of Mathematics and Physics</orgName>
								<orgName type="laboratory">SIRET Research Group</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TransNet: A deep network for fast detection of common shot transitions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Shot boundary detection</term>
					<term>deep learning</term>
					<term>video processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Shot boundary detection (SBD) is an important first step in many video processing applications. This paper presents a simple modular convolutional neural network architecture that achieves stateof-the-art results on the RAI dataset with well above real-time inference speed even on a single mediocre GPU. The network employs dilated convolutions and operates just on small resized frames. The training process employed randomly generated transitions using selected shots from the TRECVID IACC.3 dataset. The code and a selected trained network will be available at https: //github.com/soCzech/TransNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A popular way to structure a video is by making use of a shot composition, where shots are delimited by transitions. Since information about the transitions is not available in the video format, automated shot boundary detection is an important step for video management and retrieval systems. For example, information about shots can be employed for video summarization, advanced browsing and filtering in known-item search tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Shot changes can be either immediate (hard cuts) or gradual, the later spanning from basic linear interleaving of two shots over a certain number of video frames to more exotic geometric transformations from one shot to another one. To make matters worse shot boundary detectors must distinguish between shot transitions and sudden changes in a video caused by partial occlusion of the scene by an object passing closer to the camera. Fast camera motion or motion of an object in the scene also should not be mistaken for a shot transition. This may indicate that some semantic representation of a scene is necessary to correctly segment a video.</p><p>In this work, we propose TransNet, a scalable architecture with multiple dilated 3D convolutional operations per layer (instead of only one as is usual) resulting in the greater field of view with less trainable parameters. Even though the architecture is trained on just two common types of transitions (hard cuts and dissolves), it achieves state-of-the-art results on the RAI dataset <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The goal of the shot boundary detection is to temporally segment a video into shots. To determine the shot boundary, one of the first methods utilized thresholded pixel differences <ref type="bibr" target="#b16">[16]</ref> effective for stationary shots with a small number of moving objects. Since then, more robust techniques to compare images were developed based on local color histograms, color coherence vectors <ref type="bibr" target="#b12">[12]</ref> or SIFT features. The work of Shao et al. <ref type="bibr" target="#b13">[13]</ref> utilizes HSV and gradient histograms for shot boundary detection, Apostolidis et al. <ref type="bibr" target="#b1">[2]</ref> use not only the histogram but also a set of SURF descriptors to detect the differences between a pair of frames. Other approaches revolve around edge information <ref type="bibr" target="#b7">[8]</ref> or motion vectors <ref type="bibr" target="#b0">[1]</ref>.</p><p>With the advent of deep learning, new methods for shot detection using convolutional neural networks (CNN) emerged. Baraldi et al. <ref type="bibr" target="#b3">[4]</ref> utilize spectral clustering given a set of features for every frame extracted by a deep siamese network. Recently, Gygli <ref type="bibr" target="#b5">[6]</ref> used a relatively shallow neural network with 3D convolutions with the third dimension over time. Even though 3D convolutions significantly increase computational complexity and memory requirements over standard 2D convolutions due to the added dimension, Gygli has beaten the previous approach in accuracy and speed as well. Another approach by Hassanien et al. <ref type="bibr" target="#b6">[7]</ref> also uses 3D CNN however its output is fed through SVM classifier and further postprocessing is done to reduce false alarms of gradual transitions through a histogram-driven temporal differencing. Our work partially overcomes problem of computationally hungry 3D convolutions when a large field of view is required to cope with long gradual transitions by using dilated convolutions over the time dimension, which had been proven useful in speech generation task <ref type="bibr" target="#b15">[15]</ref>.</p><p>The deep learning approaches revolve around the need for large annotated datasets. Until recently <ref type="bibr" target="#b14">[14]</ref>, the size of publicly available datasets for SBD was the limiting factor. Fortunately, synthetic training data can be easily generated from virtually any video content by interleaving randomly selected sequences from different videos as is done in <ref type="bibr" target="#b5">[6]</ref> and others. The downside of this method is, however, that the real data can contain cuts between shots of the same scene which rarely occur in the synthetic data sets due to the nature how they are generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL ARCHITECTURE</head><p>The proposed TransNet architecture ( <ref type="figure" target="#fig_0">Figure 1</ref>) follows the work of Gygli <ref type="bibr" target="#b5">[6]</ref> and other standard convolutional architectures. As an input, the network takes a sequence of N consecutive video frames and applies series of 3D convolutions returning a prediction for every frame in the input. Each prediction expresses how likely a given frame is a shot boundary.</p><p>The main building block of the model (Dilated DCNN cell) is designed as four 3D 3?3?3 convolutional operations. The convolutions employ different dilation rates for the time dimension and their outputs are concatenated in the channel dimension. This approach significantly reduces the number of trainable parameters compared to standard 3D convolutions with the same field of view.  multiple SDDCNN blocks, every next block operating on smaller spatial resolution but a greater channel dimension, further increasing the expressive power and the receptive field of the network. Two fully connected layers refine the features extracted by the convolutional layers and predict a possible shot boundary for every frame representation independently (layers' weights are shared). ReLU activation function is used in all layers with the only exception of the last fully connected layer with softmax output. Stride 1 and the 'same' padding is employed in all convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRAINING</head><p>This section describes the employed dataset and training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The TRECVID IACC.3 dataset <ref type="bibr" target="#b2">[3]</ref> was utilized as it is provided with a set of predefined temporal segments. Hence, pairs of the predefined segments can be randomly selected from the pool for automatic creation of transitions for training purposes. More specifically, we considered segments of 3000 IACC.3 randomly selected videos. Furthermore, segments with less than 5 frames were excluded and from the remaining set only every other segment was picked, resulting in selected 54884 segments.</p><p>The training examples were generated on demand during training by randomly sampling two shots and joining them by a random type of a transition. Only hard cuts and dissolves were considered for training. Position of the transition was generated randomly. For dissolves, also its length was generated randomly from the interval <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">30]</ref>. The length N of each training sequence was selected to be 100 frames. The size of the input frames was set to 48 ? 27 pixels.</p><p>In order to validate the models, additional 100 IACC.3 videos (i.e., different from the training set) were manually labeled, resulting in 3800 shots. For testing, the RAI dataset <ref type="bibr" target="#b3">[4]</ref> was considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training details</head><p>The proposed architecture provides the following meta-parameters that were investigated by a grid search:</p><p>(1) S, the number of DDCNN cells in a SDDCNN layer, (2) L, the number of SDDCNN layers, (3) F , the number of filters in the first set of DDCNN layers (doubled in each following SDDCNN layer), (4) D, the number of neurons in the dense layer. For training, batch size of 20 was used for all investigated networks. In order to prevent overfitting, only 30 epochs were considered, each with 300 batches. Adam optimizer <ref type="bibr" target="#b8">[9]</ref> with the default learning rate 0.001 and cross entropy loss function were used. According to our preliminary evaluations, dropout did not improve results. Nevertheless, we plan to investigate advanced forms of regularization and training data augmentation in the future. Depending on the architecture, the whole training took approximately two to four hours to complete on one Tesla V100 GPU.</p><p>Even in the case of dissolves, when the transition is over multiple frames, the network was trained to predict only the middle frame as a shot boundary. This creates a discrepancy between the number of 'transition' frames (each sequence contains only one) and frames without a transition (99 in our case). Increasing the weight of the transitions in the loss function did not produce better results than lowering the acceptance threshold ? under commonly used 0.5; therefore, the latter approach is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>During validation and testing, the list of shots is constructed in the following way: The shot starts at the first frame when the prediction drops below a threshold ? and ends at the first frame when the prediction exceeds ? . The evaluation metric described in Section 5.1 compares the generated shot list with the ground truth. Note that only predictions for frames 25-75 are used due to incomplete temporal information for the first/last frames. Therefore, when processing a video, the input window is shifted by 50 frames between individual forward passes through the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation metric</head><p>The F1 score is used as an evaluation metric which is the same metric as in <ref type="bibr" target="#b3">[4]</ref>. Reported F1 score is computed as an average of individual F1 scores for each video. Based on our analysis of the evaluation script 1 , <ref type="figure">Figure 2</ref> shows cases when detected shots are considered to be true positive, false positive, or false negative. A true positive is detected only if the detected shot transition overlaps with the ground truth transition (3, 4 in green). A false positive is detected if the predicted transition has no overlap with the ground truth (1, 4 in red) or the transition is detected for the second time (3 in red). A false negative is detected if there is no transition overlapping with the ground truth (1, 2 dotted) -the ground truth transition is missed.   Recall / Threshold Precision / F1 P/R curve F1/Thr curve most of the models. The effect of ? on precision, recall and F1 score is depicted in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Based on the evaluations presented in <ref type="figure">Figure 3</ref>, the best performing model is considered the one with 16 filters in the first layer, two stacked DDCNN cells in every one of the three SDDCNN blocks and with 256 neurons in the dense layer (F=16, L=3 S=2, D=256). The average F1 score 0.94 of the top performing model on the RAI dataset (see <ref type="table" target="#tab_2">Table 1</ref>) is on par with the score reported by Hassanien et al. <ref type="bibr" target="#b6">[7]</ref>. The overall F1 score even slightly outperforms the work of Hassanien et al., even though they proposed a network with more than 40 times as many parameters trained for a larger set of transition types. Furthermore, our model has the advantage that no additional post-processing is needed.</p><p>Since the validation dataset contains various sequences of frames where even annotators are not sure whether there is a shot transition, the reported scores for the validation data are lower. In addition, even the top performing TransNet model faces problems with detection of some transitions, for example, false positives in dynamic shots and false negatives in gradual transitions.</p><p>The model detected 1058 false positives and 679 false negatives with respect to the annotation. After closer inspection, for about 20% of false negatives there was one very close false positive (shifted by one frame). This is in contrast to the RAI dataset results <ref type="table">(Table  2)</ref> where the network achieves a lower number of false positives than false negatives. Based on manual inspection of the videos we conclude that RAI videos do not contain many highly dynamic shots (i.e. resulting in false positives) compared to the IACC.3 validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present the TransNet neural network, the first shot detection model based on dilated 3D convolutions. The effectiveness of dilated 3D convolutions has been shown on RAI dataset with the TransNet performing on par with the current state-of-theart approach without any additional post-processing and with a fraction of learnable parameters. The network also runs more than 100x faster than real-time on a single powerful GPU 2 .</p><p>In the future, we plan to do further evaluation and improvements to enable deeper and more robust models. The source code and our trained model will be available at https://github.com/soCzech/ TransNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This paper has been supported by Czech Science Foundation (GA?R) project Nr. 19-22071Y. <ref type="bibr" target="#b1">2</ref> It took just 50s to detect shot boundaries of preprocessed frames from the whole RAI dataset (about 98 minutes of video) using Tesla V100 GPU. <ref type="bibr">Baraldi</ref>   <ref type="table">Table 2</ref>: Per video results on the RAI dataset. For each video the total number of transitions (#T), true positives (TP), false positives (FP), false negatives (FN), precision (P), recall (R) and F1 score (F1) are shown.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>TransNet shot boundary detection network architecture for S = 1 and L = 1. Note that N represents length of video sequence, not batch size. In our case N = 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 Figure 2 :</head><label>32</label><figDesc>presents the F1 scores of investigated models for validation and test datasets. Note that the top performing weights for each model configuration were selected based on results on validation dataset after each epoch. The confidence threshold ? indicating transition was set to ? = 0.1 as it performed reasonably well for Visualization of the evaluation approach. Predicted transitions shown with solid and missed with dotted rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Precision/Recall curve for the best performing model with corresponding thresholds ? next to the points (in red) and F1 score dependency on threshold (in blue). Measured on RAI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Multiple DDCNN cells on top of each other followed by spatial max pooling form a Stacked DDCNN block. The TransNet consists of Input N ? widt h ? hei?ht ? 3</figDesc><table><row><cell>Conv 3?3?3</cell><cell>Conv 3?3?3</cell><cell>Conv 3?3?3</cell><cell>Conv 3?3?3</cell></row><row><cell>dilation 1</cell><cell>dilation 2</cell><cell>dilation 4</cell><cell>dilation 8</cell></row><row><cell cols="2">DDCNN cell, each conv with 2 i ?1 F channels</cell><cell>Concat</cell><cell>stack S times</cell></row><row><cell></cell><cell cols="2">Max pooling</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1?2?2</cell><cell></cell></row><row><cell>SDDCNN block</cell><cell></cell><cell></cell><cell>stack L times</cell></row><row><cell></cell><cell></cell><cell>Dense D</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Dense 2</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Softmax</cell><cell></cell></row><row><cell></cell><cell></cell><cell>N ? 2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Average and overall F1 scores for the RAI test dataset of the best architectures. The overall F1 scores are computed by calculating precesion and recall over the whole dataset, not just single video.</figDesc><table><row><cell></cell><cell></cell><cell>et al.</cell><cell cols="2">Gygli</cell><cell cols="2">Hassanien et al. ours</cell></row><row><cell>average</cell><cell cols="2">0.84 [4]</cell><cell cols="2">0.88 [6]</cell><cell></cell><cell>0.94 [7]</cell><cell>0.94</cell></row><row><cell>overall</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell cols="2">0.934 [7]</cell><cell>0.943</cell></row><row><cell cols="5">Video #T TP FP FN</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>V1</cell><cell>80</cell><cell>57</cell><cell>2</cell><cell cols="3">23 0.966 0.713 0.820</cell></row><row><cell>V2</cell><cell cols="2">146 132</cell><cell>5</cell><cell cols="3">14 0.964 0.904 0.933</cell></row><row><cell>V3</cell><cell cols="2">112 111</cell><cell>4</cell><cell>1</cell><cell cols="2">0.965 0.991 0.978</cell></row><row><cell>V4</cell><cell>60</cell><cell>59</cell><cell>5</cell><cell>1</cell><cell cols="2">0.922 0.983 0.952</cell></row><row><cell>V5</cell><cell cols="2">104 101</cell><cell>8</cell><cell>3</cell><cell cols="2">0.927 0.971 0.948</cell></row><row><cell>V6</cell><cell>54</cell><cell>53</cell><cell>3</cell><cell>1</cell><cell cols="2">0.946 0.981 0.964</cell></row><row><cell>V7</cell><cell cols="2">109 103</cell><cell>1</cell><cell>6</cell><cell cols="2">0.990 0.945 0.967</cell></row><row><cell>V8</cell><cell cols="2">196 181</cell><cell>4</cell><cell cols="3">15 0.978 0.923 0.950</cell></row><row><cell>V9</cell><cell>61</cell><cell>55</cell><cell>2</cell><cell>6</cell><cell cols="2">0.965 0.902 0.932</cell></row><row><cell>V10</cell><cell>63</cell><cell>57</cell><cell>0</cell><cell>6</cell><cell cols="2">1.000 0.905 0.950</cell></row><row><cell cols="7">Overall 985 909 34 76 0.964 0.923 0.943</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code of the evaluation method is available at http://imagelab.ing.unimore.it/ imagelab/researchActivity.asp?idActivity=19</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Video shot boundary detection using motion activity descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelati</forename><surname>Malek Amel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdessalem</forename><surname>Ben Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdellatif</forename><surname>Mtibaa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1004.4605</idno>
		<ptr target="http://arxiv.org/abs/1004.4605" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast shot segmentation combining global and local visual descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Evlampios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Apostolidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mezaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6583" to="6587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating Ad-hoc and Instance Video Search, Events Detection, Video Captioning and Hyperlinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asad</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Qu?not</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roeland</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TRECVID 2017</title>
		<meeting>TRECVID 2017<address><addrLine>NIST, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shot and Scene Detection via Hierarchical Clustering for Re-using Broadcast Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costantino</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Analysis of Images and Patterns</title>
		<editor>George Azzopardi and Nicolai Petkov</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interactive video search tools: a detailed analysis of the video browser showdown 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Cob?rzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>H?rst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bla?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Loko?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Uwe</forename><surname>Barthel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Rossetto</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-016-3661-2</idno>
		<ptr target="https://doi.org/10.1007/s11042-016-3661-2" />
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="5539" to="5571" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<idno type="DOI">10.1109/CBMI.2018.8516556</idno>
		<ptr target="https://doi.org/10.1109/CBMI.2018.8516556" />
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Content-Based Multimedia Indexing, CBMI 2018</title>
		<meeting><address><addrLine>La Rochelle, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large-scale, Fast and Accurate Shot Boundary Detection through Spatio-temporal Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hassanien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">A</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Selim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Hefeeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03281</idno>
		<ptr target="http://arxiv.org/abs/1705.03281" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shot Boundary Detection Based on Mutual Information and Canny Edge Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiuhuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lilei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CSSE.2008.939</idno>
		<ptr target="https://doi.org/10.1109/CSSE.2008.939" />
	</analytic>
	<monogr>
		<title level="m">2008 International Conference on Computer Science and Software Engineering</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1124" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On Influential Trends in Interactive Video Retrieval: Video Browser Showdown</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Loko?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>M?nzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Awad</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2018.2830110</idno>
		<ptr target="https://doi.org/10.1109/TMM.2018.2830110" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3361" to="3376" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive Search or Sequential Browsing? A Detailed Analysis of the Video Browser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Loko?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Koval??k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>M?nzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Sch?ffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput</title>
		<editor>Stefanos Vrochidis, Phuong Anh Nguyen, Sitapa Rujikietgumjorn, and Kai Uwe Barthel</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<idno type="DOI">10.1145/3295663</idno>
		<ptr target="https://doi.org/10.1145/3295663" />
	</analytic>
	<monogr>
		<title level="j">Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparing Images Using Color Coherence Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Pass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/244130.244148</idno>
		<ptr target="https://doi.org/10.1145/244130.244148" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth ACM International Conference on Multimedia (MULTIMEDIA &apos;96)</title>
		<meeting>the Fourth ACM International Conference on Multimedia (MULTIMEDIA &apos;96)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="65" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencheng</forename><surname>Cui</surname></persName>
		</author>
		<title level="m">Shot boundary detection algorithm based on HSV histogram and HOG feature. 5th International Conference on Advanced Engineering Materials and Technology</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast Video Shot Transition Localization with Deep Structured Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04234</idno>
		<ptr target="http://arxiv.org/abs/1808.04234" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">WaveNet: A Generative Model for Raw Audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<ptr target="http://arxiv.org/abs/1609.03499" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic partitioning of full-motion video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atreyi</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Smoliar</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01210504</idno>
		<ptr target="https://doi.org/10.1007/BF01210504" />
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="28" />
			<date type="published" when="1993-01-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
