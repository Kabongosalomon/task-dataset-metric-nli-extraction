<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransCG: A Large-Scale Real-World Dataset for Transparent Object Depth Completion and A Grasping Baseline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Automation Letters</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Version</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>June</surname></persName>
						</author>
						<title level="a" type="main">TransCG: A Large-Scale Real-World Dataset for Transparent Object Depth Completion and A Grasping Baseline</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Perception for Grasping and Manipulation</term>
					<term>Data Sets for Robotic Vision</term>
					<term>Deep Learning in Grasping and Manipulation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transparent objects are common in our daily life and frequently handled in the automated production line. Robust vision-based robotic grasping and manipulation for these objects would be beneficial for automation. However, the majority of current grasping algorithms would fail in this case since they heavily rely on the depth image, while ordinary depth sensors usually fail to produce accurate depth information for transparent objects owing to the reflection and refraction of light. In this work, we address this issue by contributing a large-scale real-world dataset for transparent object depth completion, which contains 57,715 RGB-D images from 130 different scenes. Our dataset is the first large-scale, real-world dataset that provides ground truth depth, surface normals, transparent masks in diverse and cluttered scenes. Cross-domain experiments show that our dataset is more general and can enable better generalization ability for models. Moreover, we propose an end-to-end depth completion network, which takes the RGB image and the inaccurate depth map as inputs and outputs a refined depth map. Experiments demonstrate superior efficacy, efficiency and robustness of our method over previous works, and it is able to process images of high resolutions under limited hardware resources. Real robot experiments show that our method can also be applied to novel transparent object grasping robustly. The full dataset and our method are publicly available at www.graspnet.net/transcg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Transparent materials are widely used in modern industry, and robots inevitably need to process transparent objects no matter in manufacturing, logistics, or household services. Recently, much progress has been made in the field of robot grasping and manipulation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>. However, many of the advances are not directly applicable in scenes with transparent objects since these methods heavily rely on the depth information collected by the RGB-D cameras, yet ordinary depth sensors usually fail to construct a complete depth image in scenes that include transparent objects. The physical properties of transparent objects would lead to the distortion of light <ref type="figure">Fig. 1</ref>. The methodology for building our dataset. We first perform objectlevel annotation to enable the real-time object pose tracking for our collecting system. With the assistance of online optical tracking, we are able to collect and annotate data automatically. Our dataset consists of RGB images, raw depth information collected by the depth sensor of RGB-D camera, the generated ground-truth depth images, transparent ground-truth mask and the surface normal from top to bottom. Our methodology can significantly reduce the workload of manual annotation and generate a large amount of high-quality real-world data. path by reflection and refraction, resulting in noisy depth maps. Therefore, many depth-based algorithms are incapable to handle transparent objects such as plastic bottles and glass containers which can be found everywhere in our daily life.</p><p>The geometry estimation of transparent objects remains a challenging task in the computer vision field, though progress has been made by many researchers. Ba et al. <ref type="bibr" target="#b1">[2]</ref> utilized a special polarization camera to leverage polarization cues for shape estimation and reached satisfactory results, while Li et al. <ref type="bibr" target="#b16">[17]</ref> proposed a two-stage physical-based network to reconstruct the shape of the transparent objects using multi-view images and material prior. However, both methods require specialized hardware, which is not a general setting for robotic manipulation. A more common setting is a robot arm with an RGB-D camera, which is the setting that we mainly focus on.</p><p>Under this circumstance, Sajjan et al. <ref type="bibr" target="#b30">[31]</ref> adapt the depth completion pipeline <ref type="bibr" target="#b39">[40]</ref> to scenes that contain transparent objects and then propose ClearGrasp, which predicts the surface normal and the transparent boundary, followed by the global optimization to solve the depth estimation. A synthetic dataset and a small real-world dataset are also proposed along with the method. Zhu et al. <ref type="bibr" target="#b40">[41]</ref> present an end-to-end framework for depth completion using the local implicit depth function, along with a synthetic Omniverse Object dataset. Both synthetic datasets provide images containing transparent objects and their ground-truth depth maps, but the lack of real depth maps degrades the performance of those methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41]</ref> in realworld applications inevitably.</p><p>To close the synthetic-to-real gap in the field of grasping concerning transparent objects, we propose TransCG, a large-scale real-world dataset for transparent object depth completion. A novel semi-automatic pipeline is proposed to accelerate the data collection and annotation process. In total, our dataset contains 57,715 RGB-D images of 51 transparent objects and around 200 opaque objects captured from different perspectives of 130 scenes under real-world settings. The 3D mesh model of the transparent objects are also provided in our dataset. The methodology for building our dataset is shown in <ref type="figure">Fig. 1</ref>.</p><p>Futhermore, we propose a robust, efficient and effective network Depth Filler Net (DFNet) for depth completion based on our dataset, which allows it to assist 6-DoF grasping methods on transparent object grasping. The quantitative and qualitative results show that our proposed DFNet (1) generalizes better across different scenes compared to existing methods; <ref type="bibr" target="#b1">(2)</ref> improves most of the metrics to a great extent; (3) yields the highest inference speed and consumes the least computation overhead. We also apply our network to real-world object grasping for novel transparent objects, and a promising performance is witnessed. The full dataset, source code and pretrained models are released at www.graspnet.net/transcg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Transparent Datasets</head><p>Due to the special optical properties of transparent objects which leads to undetermined and inaccurate results of optical sensors, the datasets concerning of transparent objects is usually difficult to build.</p><p>For tasks that do not need the depth information, e.g., transparent object classification, segmentation and pose estimation, there are large-scale datasets such as Trans10K-V2 <ref type="bibr" target="#b36">[37]</ref>, Stanford2D-3D <ref type="bibr" target="#b0">[1]</ref> and StereOBJ-1M <ref type="bibr" target="#b18">[19]</ref>. For datasets that needs accurate sensor information as ground-truth, the common solutions are synthetic datasets, such as ClearGrasp synthetic dataset <ref type="bibr" target="#b30">[31]</ref> and Omniverse object dataset <ref type="bibr" target="#b40">[41]</ref>. Built by tools like SuperCaustics <ref type="bibr" target="#b23">[24]</ref>, the greatest shortcoming to those datasets is that the raw information collected by sensors is unlikely to be obtained in simulation. Xu et al. <ref type="bibr" target="#b37">[38]</ref> propose Toronto Transparent Object Depth Dataset (TODD), another real-world depth completion dataset. However, the diversities of transparent objects, translucent objects and cameras in the dataset are limited. Liu et al. <ref type="bibr" target="#b19">[20]</ref> build a real-world keypoint estimation dataset for transparent objects, which consists of the ground-truth depth map generated by substituting the transparent object with the identical opaque objects. But the large amount of real-world samples are all captured alone under certain environments, which is rare in real-world applications. The dataset collection approach is also used in ClearGrasp real-world dataset <ref type="bibr" target="#b30">[31]</ref> which only has 286 samples since generating the missing information is both time-consuming and labor-consuming.</p><p>To overcome the difficulties of building datasets concerning transparent objects, we propose a novel pipeline for transparent dataset construction. Using the pipeline, we build TransCG, a complete large-scale real-world dataset for transparent object depth completion. Details will be introduced in Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth Completion and Estimation</head><p>Depth completion and estimation has been studied by many researchers for a long time, and can be categorized into three classes: estimating depth directly from an RGB image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>, estimating depth from an RGB image with sparse depth information <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22</ref>] and depth completion from an RGB image with inaccurate depth information <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>. Since depth information concerning reflective objects is usually noisy and inaccurate, transparent object depth completion falls into the last category, and it can be further classified into multi-view depth completion and singleview depth completion.</p><p>For multi-view depth completion, Ichnowski et al. <ref type="bibr" target="#b12">[13]</ref> use NeRF to recover depth for grasping transparent objects. Li et al. <ref type="bibr" target="#b16">[17]</ref> present a physical-based network, which uses multi-view images to recover high-quality 3D geometry of transparent objects.</p><p>For single-view depth completion, Zhang et al. <ref type="bibr" target="#b39">[40]</ref> propose a two-stage depth completion pipeline, which predicts surface normals and occlusion boundaries according to the RGB image, followed by the global optimization to complete the depth map. ClearGrasp <ref type="bibr" target="#b30">[31]</ref> makes a few critical modifications to the pipeline and adapts it in depth completion concerning transparent object, but its inference speed is unacceptable in real-time grasping scenarios. Tang et al. <ref type="bibr" target="#b33">[34]</ref> utilize selfattentive adversarial network to replace the global optimization in ClearGrasp. Zhu et al. <ref type="bibr" target="#b40">[41]</ref> propose a two-stage system consisting of local implicit depth function prediction and depth refinement. Though it outperforms previous works on speed and accuracy, its generalization ability is very limited according to our cross-domain tests in Sec. V-B, which makes it difficult to fit into real-world robotic manipulation settings. A concurrent work <ref type="bibr" target="#b37">[38]</ref> combines previous depth completion method <ref type="bibr" target="#b31">[32]</ref> with point cloud completion to improve the quality of the refined depth.</p><p>To achieve a better generalization and applicability in realworld environment, a tiny but robust model is needed, which requires a large amount of real-world data as support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 6-DoF Grasping</head><p>6-DoF grasping refers to those methods that predict the position and rotation of the gripper in 3D domain. Enabling the robots to grasp objects from various angles, it is the basis of robotic manipulation. Due to the versatility and effectiveness of 6-DoF grasping, it is currently the main stream in the grasping field.</p><p>GPD <ref type="bibr" target="#b26">[27]</ref> propose a two-stage 6-DoF grasping method, which estimates grasp candidates sampled under empirical constraints. PointNetGPD <ref type="bibr" target="#b17">[18]</ref> improves GPD by adapting PointNet <ref type="bibr" target="#b27">[28]</ref>   2 51 58K Note. Clear refers to ClearGrasp Dataset (Synthetic/Real-world), OOD refers to Omniverse Object Dataset, TOD refers to Transparent Object Dataset. "#Cam." denotes types of RGB-D cameras, "#Obj." stands for the number of target objects and "#Img." represents the amount of samples. Here cluttered scenes refer to scenes that consists of more than 3 transparent objects and several opaque objects. variational auto-encoder to sample grasp poses, and add refinement process after evaluation for better performance. Qin et al. <ref type="bibr" target="#b29">[30]</ref> regress the grasp pose directly from the partial-view point cloud, while Ni et al. <ref type="bibr" target="#b25">[26]</ref> regress the grasp pose from features extracted by PointNet++ <ref type="bibr" target="#b28">[29]</ref>. Fang et al. <ref type="bibr" target="#b6">[7]</ref> propose the GraspNet-1Billion dataset for general object grasping and an end-to-end grasp pose prediction network. Gou et al. <ref type="bibr" target="#b10">[11]</ref> incorporate RGB and depth information to improve the performance of 6-DoF grasping. Sundermeyer et al. <ref type="bibr" target="#b32">[33]</ref> reduce 6-DoF grasp poses to 4-DoF grasp representations on a known contact point to facilitate the learning problems and improve the grasping quality in cluttered scenes. Wang et al. <ref type="bibr" target="#b34">[35]</ref> propose a geometrically-based quality graspness to evaluate the graspable area in cluttered scenes.</p><p>All these methods rely heavily on depth image, which makes them unsuitable for transparent object grasping. Thus, utilizing color information to generate high-quality depth map and point cloud to aid the grasping deserves further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>As introduced before, the previous transparent datasets that need accurate ground-truth depth usually use simulation platforms to generate synthetic data, and real-world datasets are usually small in scale due to the workload of annotations. To overcome the difficulties, we propose a novel pipeline to build the dataset efficiently, which utilizes a robot to perform data collection after limited object-level manual annotations. In brief, we aim to collect a dataset that contains RGBD images using real-world sensors, along with detail annotations for the transparent objects include their depths, masks, 6D poses, normals, etc. To reduce the annotation effort, our methodology for building the dataset is to automatically localize the transparent objects during data collection. To achieve that, we resort to an optical tracker that can accurately localize a target's 6D pose in real-time from several IR markers attached to it. Besides, we manage to obtain the 3D model of each transparent object in the training set by wrapping them with opaque materials and scanning with a 3D scanner. With these two preliminaries, we can easily obtain the transparent objects' 6D pose during data collection. The geometry of transparent objects can be restored by leveraging the tracker results and their 3D models during data annotation. Our pipeline is able to build large-scale real-world dataset conveniently and reduce the human workload by a large extent.</p><p>Using the pipeline, we build our TransCG dataset, which contains 57,715 RGB-D images captured by two different cameras, along with the refined ground-truth depth images, transparent ground-truth mask and the surface normals, from 130 scenes under various background settings, within a week. We collect 51 common objects in daily life that can lead to inaccurate depth map, including transparent objects, translucent objects, reflective objects and objects with dense tiny holes. Apart from 65 simple isolated scenes that are similar to the scenes in the previous datasets, we also provide 65 challenging cluttered scenes that are closer to the real-world grasping environment, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. More details are presented in supplementary video. The comparisons between our dataset and existing datasets are summarised in Tab. I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. System Setup</head><p>To support fast and accurate data collection process, we build a transparent object tracking system, which is the only part that requires human efforts in our dataset building pipeline. The system setup process is illustrated in <ref type="figure">Fig. 3</ref>.</p><p>Given a transparent object, we firstly attach a fixer with IR markers to it, as shown in <ref type="figure">Fig. 3b</ref>. A PST optical tracker 1 is used to record the IR markers and tracks them afterwards. Although we can directly attach IR markers to the object, we found that using a flat fixer can make the tracking more robust. Next, we temporarily wrap the transparent object with some  <ref type="figure">Fig. 3</ref>. System setup process. Given a transparent object (a), we attach a fixer with IR markers to it (b) and record its pattern with an optical tracker (c), which can enable tracking afterwards. Then we scan the object (d) and get its 3D model (e). After that, we manually perform tracker-object annotation to get the transformation matrix from marker to object (f), where an GUI is developed for real-time evaluation (h). The whole annotation and tracking process is assisted by our 6D pose tracking system (g), which consists of a PST tracker, an Intel RealSense D435 camera and an Intel RealSense L515 camera.</p><p>opaque materials that can preserve the shape of objects, and obtain its 3D model with a Shining3D EinScan-SP scanner 2 .</p><p>With these two steps, we can obtain the transparent object's 6D pose during data collection, where the details is further explained.</p><p>The core of our data collection system consists of a PST optical tracker, an Intel RealSense D435 camera and an Intel RealSense L515 camera. The tracker outputs the 6D pose of the markers in real-time and the two cameras provide RGBD images with different quality. The 6D pose of the i-th object w.r.t the j-th camera T obj i camj can be calculated as follows:</p><formula xml:id="formula_0">T obj i camj = T tracker camj T markeri tracker T obj i markeri ,<label>(1)</label></formula><p>where T tracker camj denotes the transformation matrix of the tracker origin w.r.t the j-th camera, T markeri tracker denotes the 6D pose of the markers attached on the i-th object w.r.t the tracker, and T obj i markeri denotes the transformation matrix of the i-th object's origin w.r.t the markers.</p><p>To obtain T tracker camj , we perform tracker-camera calibration using a calibration board with both Aruco marker and IR markers. T markeri tracker is the output of the optical tracker, and T obj i markeri are annotated by human. We develop a GUI application for the annotation process and evaluate the results in real time by rendering the objects in the corresponding RGB image. On average, the overall human efforts to process an object is around 1 hour. With the transparent object tracking system we introduced, we can easily recover the ground-truth depth information of the transparent objects afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Collection</head><p>To collect a large amount of data automatically, we attach our tracking system to a robot arm that moves along a fixed trajectory containing 240 distinct viewpoints. Transparent objects are randomly selected and placed in the scene. To align with real-world setting, we augment the scene with various table covers and opaque objects. At each viewpoint, the tracking system will capture the RGBD images and the tracking results. 2 https://www.einscan.com/desktop-3d-scanners/einscan-sp/ Before data collection, we perform camera extrinsic calibration for the 240 viewpoints, which can assist to recover the 6D pose of objects in case of tracker failure. ArUco markers are used during camera calibration to obtain the pose of the j-th camera T camj k w.r.t the marker coordinate in the k-th viewpoint for all k. Specifically, the 6D pose of the i-th object w.r.t the j-th camera in the perspective k can be obtained from the corresponding 6D pose in perspective k 0 and the camera poses in viewpoints k, k 0 , where k 0 is the successfully-tracked viewpoint:</p><formula xml:id="formula_1">T obj i camj k = T camj k T camj k0 ?1 T obj i camj k0 .</formula><p>(</p><p>Thus, we can recover the 6D pose of the objects that are not tracked in some viewpoints using the tracking results in the first viewpoint, which are always successfully detected.</p><p>After collecting the raw data, we use the collected 6D poses to render the ground-truth depth maps, the transparent masks and ground-truth surface normals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dataset Postprocessing</head><p>1) Data Verification: The blurry samples and improperly exposed samples are automatically detected and removed from the dataset using tools like Laplacian operators and histograms provided in OpenCV library <ref type="bibr" target="#b2">[3]</ref>. Manual validations are conducted by rendering the object mesh to the scene using the 6D pose and see if they can overlap with the original object. Finally, we generate a metadata containing all valid viewpoints for each scene.</p><p>2) Dataset Split: We randomly select 12 objects from different categories and regard all scenes that contain these objects as the testing set, while the other scenes are used as the training set. There are totally 34,191 training samples and 23,524 testing samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion</head><p>To ensure that the markers have minimal influence to the object, we do not directly place them on the surface of transparent objects. Instead they are placed on the surface  <ref type="figure">Fig. 4</ref>. The architecture of our proposed end-to-end depth completion network DFNet. Our network utilizes a U-Net architecture with CDCD blocks, CDC blocks and CDCU blocks. These blocks are mainly composed of dense blocks <ref type="bibr" target="#b11">[12]</ref>, with DUC <ref type="bibr" target="#b35">[36]</ref> replacing deconvolution layer in up-sampling of CDCU block. All convolution layers except the last one are followed by batch normalizations <ref type="bibr" target="#b13">[14]</ref> and ReLU activations, and have 3 ? 3 kernels.</p><p>of a fixer that is attached to the object. Therefore, we can make sure that the transparent part of the object is retained to the greatest extent. From the cross-dataset experiment conducted in <ref type="table" target="#tab_1">Table III and Table IV</ref>, we can see that methods trained on our dataset generalize well to other testing datasets. These experiments demonstrate that the markers have minimal impact on the training of network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHOD A. Overview</head><p>In this section, we detail our method for depth completion and grasping. For depth completion, we propose an end-toend network which is illustrated in <ref type="figure">Fig. 4</ref>. Given an RGB image C ? R H?W ?3 and an inaccurate partial depth image D ? R H?W , our network completes the depth information and predicts the full depth mapD ? R H?W , where H ? W is the size of an image. The details of the network will be introduced in Sec. IV-B. In Sec. IV-C, we demonstrate how our depth completion network can be applied to the grasping task and serve as a grasping baseline method. we apply our depth completion to a grasp pose detection network <ref type="bibr" target="#b6">[7]</ref> which takes point cloud as inputs and outputs the grasp poses. We demonstrate that our network can generate high quality depth for transparent objects that can enable depth-based grasping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth Completion</head><p>Inspired by previous literature <ref type="bibr" target="#b3">[4]</ref> about depth estimation from sparse sensing, we propose our end-to-end depth completion network Depth Filler Net (DFNet) that predicts full depth map according to RGB information and inaccurate partial depth. Adapting a U-Net architecture with depth of four layers, our network takes dense blocks <ref type="bibr" target="#b11">[12]</ref> as backbones and formulates Conv-Dense-Conv-Downsample (CDCD) blocks, Conv-Dense-Conv (CDC) blocks and Conv-Dense-Conv-Upsample (CDCU) blocks. Empirical statistics about depth estimations and completions show that original depth information is critical throughout the networks. Hence we provide the original depth information as an input to every CDCD, CDC and CDCU block. Skip paths are added to retain information in high resolutions. Inspired by AlphaPose <ref type="bibr" target="#b7">[8]</ref>, we adapt dense up-sampling convolution (DUC) <ref type="bibr" target="#b35">[36]</ref> instead of ordinary deconvolution layers in CDCU blocks.</p><p>Our network is trained using the following loss function:</p><formula xml:id="formula_3">L = L d + ?L s ,<label>(3)</label></formula><p>where ? is the weight parameter, L d penalizes depth inaccuracy and L s is the cosine distance of surface normals computed from predicted depth map and ground-truth depth map <ref type="bibr" target="#b40">[41]</ref>, which penalizes unsmoothness. Formally,</p><formula xml:id="formula_4">L d = D ? D * 2 , L s = 1 ? cos D h ?D w , D * h ? D * w ,<label>(4)</label></formula><p>whereD and D * denotes the predicted depth and the groundtruth depth, D w and D h are gradient vectors along width-axis and height-axis of depth map D respectively. For both losses, we regard depths out of range [0.3, 1.5] as invalid pixels and remove them from losses to reduce the impact of outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Object Grasping</head><p>Given an RGB image along with a depth image collected by an RGB-D camera, we first scale the images to an appropriate size and feed into our depth completion model DFNet, which outputs the refined depth in the same resolution as the input. Then, the refined depth is scaled back to the original size, which can be used to construct the scene point cloud using camera intrinsics. After that, the scene point cloud is sent to GraspNet-baseline <ref type="bibr" target="#b6">[7]</ref> as the input to the end-to-end grasp pose detection network. Other depth based grasping methods are also applicable. Finally, the grasp pose detection network outputs the grasp candidates, and the grasp will be executed by a parallel-jaw robot.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Depth Completion Experiments</head><p>We compare our method with several representative approaches on our TransCG dataset. ClearGrasp <ref type="bibr" target="#b30">[31]</ref> is the first algorithm which leverages deep learning with synthetic training data to estimate depth information concerning transparent objects, and LIDF-Refine <ref type="bibr" target="#b40">[41]</ref> utilizes local implicit depth function to solve transparent object depth completion task. We also evaluate the concurrent work TranspareNet <ref type="bibr" target="#b37">[38]</ref> in experiments. All baselines are trained in our TransCG dataset using their released source codes and optimal hyperparameters for fair comparisons.</p><p>For our model, the hidden channels in the network is set to 64. In every dense block, the layers L and the feature channels of each layer k are set to 5, 12 respectively, as suggested in literature <ref type="bibr" target="#b3">[4]</ref>. We use AdamW optimizer <ref type="bibr" target="#b20">[21]</ref> with initial learning rate of 10 ?3 and multi-step learning rate scheduler which decays the learning rate by 5 after 5, 15, 25, 35 epochs. We train the model for 40 epochs with the batch size of 32. Several data augmentation approaches such as random flipping, rotation, noise adding and chromatic transformations in HLS color space are conducted during training. Concerning loss, we set ? = 0.001.</p><p>For all methods, we scale the images to 320 ? 240 during training and testing. We use 4 NVIDIA GeForce RTX 3090 GPUs for training and one for testing, and the average time to train an epoch is approximately 20 minutes.</p><p>The following common metrics of depth completion for transparent objects are used in comparisons. All metrics are calculated on the transparent areas according to transparent masks unless specified.</p><p>? RMSE: the rooted mean squared error between depth estimates and ground-truth depths. ? REL: the mean absolute relative difference. ? MAE: the mean absolute error between depth estimates and ground-truth depths. ? Threshold ?: the percentage of pixels with predicted depths satisfying max(d/d * , d * /d) &lt; ?, where d, d * are corresponding pixels of D, D * , and ? is set to 1.05, 1.10 and 1.25 following <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41]</ref>. The quantitative results are reported in Tab. II, and the qualitative results are visualized in <ref type="figure" target="#fig_2">Fig. 5</ref>. Our method outperforms ClearGrasp <ref type="bibr" target="#b30">[31]</ref> and LIDF-Refine <ref type="bibr" target="#b40">[41]</ref> on both metrics and quality, and performs better than the concurrent work TranspareNet <ref type="bibr" target="#b37">[38]</ref> on some metrics. Qualitatively, it completes the inaccurate region of depth maps more completely <ref type="figure" target="#fig_2">(Fig.  5</ref>) than <ref type="bibr" target="#b37">[38]</ref>. It is also worth noticing that though LIDF-Refine <ref type="bibr" target="#b40">[41]</ref> performs well on masked metrics, it may introduce shadows on areas without transparent objects, which may result from the outliers of the original depth map. Moreover, our method has the smallest size, fastest inference time and lowest GPU memory occupation, which allows it to perform depth completion of high quality under limited resources (Tab. II).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross-Domain Experiments</head><p>Cross-domain experiments are performed to verify the robustness of our proposed depth completion method and the generalization ability of our proposed TransCG dataset.</p><p>For cross-domain experiments of different methods, two experiments are conducted, namely (1) test the performance on our TransCG dataset after training on ClearGrasp synthetic  dataset and Omniverse object dataset; (2) test the performance on ClearGrasp real-world dataset after training on our Tran-sCG dataset. Results shown in Tab. III reveal that our method is the most robust one among all methods. Also, it is worth noticing that although LIDF-Refine <ref type="bibr" target="#b40">[41]</ref> reaches satisfactory results when training domain is similar to the testing domain, the cross-domain testing decreases its performance a lot since its local implicit depth function is environment-dependent. On the contrary, our method is less sensitive to domain changes and is able to achieve satisfactory results under different environment settings.</p><p>For cross-domain experiements of different datasets, we select our method as the depth completion model to test the generality of the training dataset on a third-party dataset TOD <ref type="bibr" target="#b19">[20]</ref> and ClearGrasp real-world dataset. Results shown in Tab. IV demonstrate that our real-world dataset is more universal compared to the previous synthetic datasets, even though ClearGrasp real-world dataset has a similar environment as ClearGrasp synthetic dataset which is used for training. The performance differences also reflect the shortcomings of synthetic datasets compared to real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Real Robot Experiments</head><p>To verify the performance of our method in real-world settings, we conduct real robot object grasping experiments. The object grasping pipeline incorporated with our method is introduced in Sec. IV-C. The experiments are conducted on a UR-5 robot with an Intel RealSense D435 camera and a Robotiq two-finger gripper, as shown in <ref type="figure" target="#fig_3">Fig. 6</ref>.</p><p>We randomly select 8 transparent objects to perform realrobot experiments, 6 of which are completely novel, and the rest 2 objects are the same objects without fixers and markers from our testing set. For every experiment, we randomly put the objects and repeat grasping until an object fails for 3 times. The success rate is defined as #objects #attempts , and the completion rate is defined as #successfully-grasped objects #objects . <ref type="table" target="#tab_6">Table V</ref> reports the experiment results, which shows the effectiveness and feasibility of our method. More details are presented in supplementary video. VI. CONCLUSION In this paper, we propose TransCG, the first large-scale real-world dataset for transparent object depth completion and grasping, built by our novel data collecting pipeline. Our dataset fills the synthetic-to-real gap in the transparent depth completion area and is more general compared to previous synthetic datasets in real-world environments. Moreover, we propose an end-to-end depth completion network DFNet, which is more efficient and robust compared to previous methods according to experiments on various datasets. Real robot experiments of grasping also demonstrate that our method is applicable in real-world settings with novel objects. The compatibility of our method with depth-based manipulation methods allows it to become a default pre-processing step for all downstream tasks concerning transparent objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Cluttered scenes in non-reflective background Isolated scenes in non-reflective backgroundIsolated scenes in reflective backgroundCluttered scenes in reflective background Different types of scenes in our TransCG dataset. The sub-figures on the top left are the refined ground-truth depth images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>!"#$%"?'()$!* = !"#$%"?*%"!+$% ? *%"!+$%?#"%+$% ? #"%+$%?'()$!*</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Visualizations of the refined depth maps and error maps of different methods on several testing scenes of TransCG dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Real robot experiments on a cluttered scene that consists of daily transparent objects. A: Intel RealSense RGB-D camera. B: Robotiq two-finger gripper. C: UR-5 Robot. D: Daily transparent objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISONS</head><label>I</label><figDesc>OF TRANSPARENT DEPTH COMPLETION DATASETS Collection #Cam. #Obj. #Img. RGB Raw depth Refined depth Single Isolated Cluttered</figDesc><table><row><cell cols="2">Type Auto-Syn Dataset Dataset Completeness Scene Type Clear-Syn [31] OOD [41]</cell><cell>--</cell><cell>9 9</cell><cell>50K 60K</cell></row><row><cell></cell><cell>Clear-Real [31]</cell><cell>2</cell><cell>10</cell><cell>286</cell></row><row><cell>Real</cell><cell>TOD [20] TODD [38]</cell><cell>1 1</cell><cell>15 6</cell><cell>48K 15K</cell></row><row><cell></cell><cell>TransCG (ours)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II EXPERIMENTRefine RGB GT Depth Depth Depth Error Map Error Map RGB GT Depth</head><label>II</label><figDesc>RESULTS ON TRANSCG DATASET REL ? MAE ? ? 1.05 ? ? 1.10 ? ? 1.25 ? ? means lower is better, and ? means higher is better. GPU Memory occupation and inference time are measured with an NVIDIA GeForce RTX 3090 GPU.</figDesc><table><row><cell cols="2">Methods RMSE ? ClearGrasp [31] 0.054</cell><cell cols="2">Metrics 0.083 0.037 50.48</cell><cell>68.68</cell><cell>95.28</cell><cell>GPU Memory Occupation 2.1 GB</cell><cell>Inference Time 2.2813s</cell><cell>Model Size 934 MB</cell></row><row><cell>LIDF-Refine [41]</cell><cell>0.019</cell><cell>0.034 0.015</cell><cell>78.22</cell><cell>94.26</cell><cell>99.80</cell><cell>6.2 GB</cell><cell>0.0182s</cell><cell>251 MB</cell></row><row><cell cols="2">TranspareNet [38] 0.026</cell><cell>0.023 0.013</cell><cell>88.45</cell><cell>96.25</cell><cell>99.42</cell><cell>1.9 GB</cell><cell>0.0354s</cell><cell>336 MB</cell></row><row><cell>DFNet (ours)</cell><cell>0.018</cell><cell>0.027 0.012</cell><cell>83.76</cell><cell>95.67</cell><cell>99.71</cell><cell>1.6 GB</cell><cell>0.0166s</cell><cell>5.2 MB</cell></row><row><cell>Note. Scene 21</cell><cell></cell><cell>Raw</cell><cell cols="2">ClearGrasp</cell><cell cols="2">LIDF-</cell><cell>TranspareNet</cell><cell>DFNet (ours)</cell></row><row><cell>Scene 56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF CROSS-DOMAIN EXPERIMENTS OF METHODS Training/ Methods Metrics Testing RMSE ? REL ? MAE ? ? 1.05 ? ? 1.10 ? ? 1.25 ?</figDesc><table><row><cell></cell><cell>[31]</cell><cell>0.061 0.108 0.049 33.59 54.73 92.48</cell></row><row><cell>Clear+OOD/</cell><cell>[41]</cell><cell>0.146 0.262 0.115 13.70 26.39 57.95</cell></row><row><cell>TransCG</cell><cell>[38]</cell><cell>0.071 0.060 0.036 62.99 82.92 95.93</cell></row><row><cell></cell><cell>Ours</cell><cell>0.039 0.067 0.030 56.68 74.61 98.01</cell></row><row><cell></cell><cell>[31]</cell><cell>0.085 0.095 0.052 47.26 70.76 92.54</cell></row><row><cell>TransCG/</cell><cell>[41]</cell><cell>0.152 0.225 0.139 9.86 20.63 46.02</cell></row><row><cell>Clear-Real</cell><cell>[38]</cell><cell>0.045 0.071 0.040 33.43 70.14 99.40</cell></row><row><cell></cell><cell>Ours</cell><cell>0.041 0.054 0.031 62.74 83.31 97.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF CROSS-DOMAIN EXPERIMENTS OF DATASETS RMSE ? REL ? MAE ? ? 1.05 ? ? 1.10 ? ? 1.<ref type="bibr" target="#b24">25</ref> ? For TOD dataset, global metrics is used since masks are not provided. It denotes that the evaluation is conducted on the full depth map instead of the masked area.</figDesc><table><row><cell cols="2">Training (Global) Metrics  Clear+OOD Testing 0.077 0.043 0.037 83.44 89.46 95.53 TOD TransCG 0.044 0.013 0.011 97.54 98.30 98.82</cell></row><row><cell>Clear+OOD Clear-Real TransCG</cell><cell>0.040 0.058 0.032 55.08 81.23 98.75 0.041 0.054 0.031 62.74 83.32 97.33</cell></row><row><cell></cell><cell></cell></row></table><note>** :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V RESULTS</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">OF REAL ROBOT EXPERIMENTS</cell></row><row><cell></cell><cell cols="4"># Objects # Attempts Success Rate Completion Rate</cell></row><row><cell>Exper. 1</cell><cell>7</cell><cell>11</cell><cell>63.6%</cell><cell>100.0%</cell></row><row><cell>Exper. 2</cell><cell>7</cell><cell>8</cell><cell>87.5%</cell><cell>100.0%</cell></row><row><cell>Exper. 3</cell><cell>7</cell><cell>9</cell><cell>77.8%</cell><cell>100.0%</cell></row><row><cell>Exper. 4</cell><cell>8</cell><cell>8</cell><cell>100.0%</cell><cell>100.0%</cell></row><row><cell>Exper. 5</cell><cell>8</cell><cell>10</cell><cell>80.0%</cell><cell>100.0%</cell></row><row><cell>Total</cell><cell>37</cell><cell>46</cell><cell>80.4%</cell><cell>100.0%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.ps-tech.com/products-pst-base/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<idno>CoRR abs/1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep shape from polarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="554" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The openCV library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Dr. Dobb&apos;s Journal: Software Tools for the Professional Programmer</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="120" to="123" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating depth from rgb and sparse sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="167" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graspnet-1billion: A large-scale benchmark for general object grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition. 2020</meeting>
		<imprint>
			<biblScope unit="page" from="11444" to="11453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth Completion via Inductive Fusion of Planar LIDAR and Monocular Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2020</title>
		<imprint>
			<biblScope unit="page" from="10843" to="10848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rgb matters: Learning 7-dof grasp poses on monocular rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2021</title>
		<imprint>
			<biblScope unit="page" from="13459" to="13466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ichnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Annual Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="582" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Depth Completion using Plane-Residual Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeong-Uk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13916" to="13925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Through the looking glass: neural 3D reconstruction of transparent shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ying</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1262" to="1271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointnetgpd: Detecting grasp configurations from point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhuo</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3629" to="3635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stereobj-1m: Large-scale stereo image dataset for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Iwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="10870" to="10879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Keypose: Multi-view 3d labeling and keypoint estimation for transparent objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11602" to="11610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in Adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4796" to="4803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning ambidextrous robot grasping policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Mahler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SuperCaustics: Realtime, open-source simulation of transparent objects for deep learning applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolando</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">6-dof graspnet: Variational grasp generation for object manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Eppner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet++ grasping: Learning an endto-end spatial grasp generation algorithm from sparse point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyuan</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2020</title>
		<imprint>
			<biblScope unit="page" from="3619" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grasp pose detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pas</forename><surname>Andreas Ten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1455" to="1473" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles R Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Charles Ruizhongtai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">S4g: Amodal single-view single-shot se (3) grasp detection in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhe</forename><surname>Qin</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<biblScope unit="page" from="53" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clear grasp: 3d shape estimation of transparent objects for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreeyak</forename><surname>Sajjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2020</title>
		<imprint>
			<biblScope unit="page" from="3634" to="3642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decoder modulation for indoor depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Senushkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2020</title>
		<imprint>
			<biblScope unit="page" from="2181" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DepthGrasp: Depth Completion of Transparent Objects Using Self-Attentive Adversarial Network with Spectral Residual for Grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graspness Discovery in Clutters for Fast and Accurate Grasp Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="15964" to="15973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">IEEE winter conference on applications of computer vision (WACV)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
	<note>Understanding convolution for semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segmenting Transparent Objects in the Wild with Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-08-27" />
			<biblScope unit="page" from="1194" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Seeing Glass: Joint Point-Cloud and Depth Completion for Transparent Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoping</forename><surname>Xu</surname></persName>
		</author>
		<idno>PMLR. 2022</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<biblScope unit="page" from="827" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">RGB-D Local Implicit Function for Depth Completion of Transparent Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="4649" to="4658" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
