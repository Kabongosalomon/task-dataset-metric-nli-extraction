<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Label Attention Model for ICD Coding from Clinical Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Vu</surname></persName>
							<email>thanh.vu@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Australian e-Health Research Centre</orgName>
								<orgName type="institution" key="instit2">CSIRO</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Dat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">VinAI Research</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Nguyen</surname></persName>
							<email>anthony.nguyen@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Australian e-Health Research Centre</orgName>
								<orgName type="institution" key="instit2">CSIRO</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Label Attention Model for ICD Coding from Clinical Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ICD coding is a process of assigning the International Classification of Disease diagnosis codes to clinical/medical notes documented by health professionals (e.g. clinicians). This process requires significant human resources, and thus is costly and prone to error. To handle the problem, machine learning has been utilized for automatic ICD coding. Previous state-of-the-art models were based on convolutional neural networks, using a single/several fixed window sizes. However, the lengths and interdependence between text fragments related to ICD codes in clinical text vary significantly, leading to the difficulty of deciding what the best window sizes are. In this paper, we propose a new label attention model for automatic ICD coding, which can handle both the various lengths and the interdependence of the ICD code related text fragments. Furthermore, as the majority of ICD codes are not frequently used, leading to the extremely imbalanced data issue, we additionally propose a hierarchical joint learning mechanism extending our label attention model to handle the issue, using the hierarchical relationships among the codes. Our label attention model achieves new state-of-the-art results on three benchmark MIMIC datasets, and the joint learning mechanism helps improve the performances for infrequent codes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>International Classification of Diseases (ICD) is the global health care classification system consisting of metadata codes. 1 ICD coding is the process of assigning codes representing diagnoses and procedures performed during a patient visit using the patient's visit data, such as the clinical/medical notes documented by health professionals. ICD codes can be used for both clinical research and healthcare purposes, such as for epidemiological studies and billing of services <ref type="bibr" target="#b4">[O'malley et al., 2005;</ref><ref type="bibr" target="#b3">Nguyen et al., 2018]</ref>.</p><p>Manual ICD coding performed by clinical coders relies on manual inspections and experience-based judgment. The effort required for coding is thus labor and time intensive and prone to human errors <ref type="bibr" target="#b4">[O'malley et al., 2005;</ref><ref type="bibr" target="#b3">Nguyen et al., 2018]</ref>. As a result, machine learning has been utilized to help automate the ICD coding process. This includes both conventional machine learning <ref type="bibr" target="#b5">[Perotte et al., 2013;</ref><ref type="bibr" target="#b3">Koopman et al., 2015]</ref> and deep learning <ref type="bibr">[Karimi et al., 2017;</ref><ref type="bibr" target="#b5">Prakash et al., 2017;</ref><ref type="bibr" target="#b0">Baumel et al., 2018;</ref><ref type="bibr" target="#b3">Mullenbach et al., 2018;</ref><ref type="bibr" target="#b5">Wang et al., 2018;</ref><ref type="bibr">Song et al., 2019;</ref><ref type="bibr" target="#b5">Xie et al., 2019;</ref><ref type="bibr">Li and Yu, 2020]</ref>. Automatic ICD coding is challenging due to the large number of available codes, e.g. ?17,000 in ICD-9-CM and ?140,000 in ICD-10-CM/PCS, 2 and the problem of highly long tailed codes, in which some codes are frequently used but the majority may only have a few instances due to the rareness of diseases <ref type="bibr">[Song et al., 2019;</ref><ref type="bibr" target="#b5">Xie et al., 2019]</ref>.</p><p>Previous state-of-the-art (SOTA) models on the benchmark MIMIC datasets <ref type="bibr" target="#b3">[Lee et al., 2011;</ref><ref type="bibr" target="#b0">Johnson et al., 2016]</ref> were based on convolutional neural networks (CNNs) with single or several fixed window sizes <ref type="bibr" target="#b3">[Mullenbach et al., 2018;</ref><ref type="bibr" target="#b5">Xie et al., 2019;</ref><ref type="bibr">Li and Yu, 2020]</ref>. However, the lengths and interdependence of text fragments in clinical documentation related to ICD codes can vary significantly. For example, to identify the ICD code "V10.46: Personal history of malignant neoplasm of prostate" from the clinical text ". . . past medical history asthma/copd, htn, . . . prostate cancer. . . ", we need to highlight both the "past medical history" and "prostate cancer" fragments which are far from each other in the text. Although densely connected <ref type="bibr">CNN [Xie et al., 2019]</ref> and multifilter based <ref type="bibr">CNN [Li and Yu, 2020]</ref> could handle the different sizes of a single text fragment, selecting optimal window sizes of the CNN-based models for interdependent fragments with different lengths is challenging.</p><p>Our contributions. As the first contribution, we propose a label attention model for ICD coding which can handle the various lengths as well as the interdependence between text fragments related to ICD codes. In our model, a bidirectional Long-Short Term Memory (BiLSTM) encoder is utilized to capture contextual information across input words in a clinical note. A new label attention mechanism is proposed by extending the structured self-attention mechanism <ref type="bibr" target="#b3">[Lin et al., 2017]</ref> to learn label-specific vectors that represent the important clinical text fragments relating to certain labels. Each label-specific vector is used to build a binary classifier for a given label. As the second contribution, we additionally propose a hierarchical joint learning mechanism that extends our label attention model to handle the highly imbalanced data problem, using the hierarchical structure of the ICD codes. As our final contribution, we extensively evaluate our models on three standard benchmark MIMIC datasets <ref type="bibr" target="#b3">[Lee et al., 2011;</ref><ref type="bibr" target="#b0">Johnson et al., 2016]</ref>, which are widely used in automatic ICD coding research <ref type="bibr" target="#b5">[Perotte et al., 2013;</ref><ref type="bibr" target="#b5">Prakash et al., 2017;</ref><ref type="bibr" target="#b3">Mullenbach et al., 2018;</ref><ref type="bibr" target="#b5">Xie et al., 2019;</ref><ref type="bibr">Li and Yu, 2020]</ref>. Experimental results show that our model obtains the new SOTA performance results across evaluation metrics. In addition, our joint learning mechanism helps improve the performances for infrequent codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automatic ICD coding has been an active research topic in the healthcare domain for more than two decades <ref type="bibr" target="#b3">[Larkey and Croft, 1996;</ref><ref type="bibr" target="#b0">de Lima et al., 1998</ref>]. Many conventional machine learning and deep learning approaches have been explored to automatically assign ICD codes on clinical text data, in which the coding problem is formulated as a multi-label classification problem <ref type="bibr" target="#b5">[Perotte et al., 2013;</ref><ref type="bibr" target="#b3">Koopman et al., 2015;</ref><ref type="bibr">Karimi et al., 2017;</ref><ref type="bibr" target="#b5">Shi et al., 2017;</ref><ref type="bibr" target="#b3">Mullenbach et al., 2018;</ref><ref type="bibr" target="#b5">Xie et al., 2019;</ref><ref type="bibr">Li and Yu, 2020]</ref>.</p><p>Larkey and Croft <ref type="bibr">[1996]</ref> proposed an ensemble approach combining three feature-based classifiers (i.e., K nearest neighbors, relevance feedback, and Bayesian independence) to assign ICD-9 codes to inpatient discharge summaries. They found that combining the classifiers performed much better than individual ones. de <ref type="bibr" target="#b0">Lima et al. [1998]</ref> utilized the cosine similarity between the medical discharge summary and the ICD code description to build the classifier which assigns codes with the highest similarities to the summary. They also proposed a hierarchical model by utilizing the hierarchical relationships among the codes. Similarly, Perotte et al.</p><p>[2013] explored support vector machine (SVM) to build flat and hierarchical ICD code classifiers and applied to discharge summaries from the MIMIC-II dataset <ref type="bibr" target="#b3">[Lee et al., 2011]</ref>. Apart from discharge summaries, Koopman et al. <ref type="bibr">[2015]</ref> proposed a hierarchical model of employing SVM to assign cancer-related ICD codes to death certificates. <ref type="bibr">Karimi et al. [2017]</ref> utilized classification methods for ICD coding from radiology reports.</p><p>Deep learning models have been proposed to handle the task recently. Shi et al.</p><p>[2017] employed character-level LSTM to learn the representations of specific subsections from discharge summaries and the code description. They then applied an attention mechanism to address the mismatch between the subsections and corresponding codes. <ref type="bibr" target="#b5">Wang et al. [2018]</ref> proposed a joint embedding model, in which the labels and words are embedded into the same vector space and the cosine similarity between them is used to predict the labels. Mullenbach et al. <ref type="bibr">[2018]</ref> proposed a convolutional attention model for ICD coding from clinical text (e.g. discharge summaries). The model is the combination of a single filter CNN and label-dependent attention. <ref type="bibr" target="#b5">Xie et al. [2019]</ref> improved the convolutional attention model <ref type="bibr" target="#b3">[Mullenbach et al., 2018]</ref> by using densely connected CNN and multi-scale feature attention. Graph convolutional neural network <ref type="bibr" target="#b3">[Kipf and Welling, 2017]</ref> was employed as the model regularization to capture the hierarchical relationships among the codes. Li and Yu [2020] later proposed a multi-filter residual CNN combining a multi-filter convolutional layer and a residual convolutional layer to improve the convolutional attention model <ref type="bibr" target="#b3">[Mullenbach et al., 2018]</ref>. See Section 4.4 of baseline models for additional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we first describe our new label attention model (namely, LAAT) for ICD coding from clinical text. As most of ICD codes do not frequently occur in clinical text data <ref type="bibr" target="#b3">[Koopman et al., 2015;</ref><ref type="bibr" target="#b5">Xie et al., 2019]</ref>, 3 we additionally propose a hierarchical joint learning mechanism to improve the performance of predicting less-frequent ICD codes. We treat this ICD coding task as a multi-label classification problem <ref type="bibr" target="#b3">[McCallum, 1999]</ref>. Following Mullenbach et al. <ref type="bibr">[2018]</ref>, our objective is to train |L| binary classifiers (here, L is the ICD code set), in which each classifier is to determine the value of y j ? {0, 1}, the j th label in L given an input text. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the architecture of our proposed label attention model. Overall, the model consists of four layers. The first layer is an embedding layer in which pretrained word embeddings are employed to produce embedding vectors of tokens in the input clinical text. The second layer is a bidirectional Long Short-Term Memory (LSTM) network producing latent feature representations of all the input tokens. Given these latent representations, the third layer is an attention one producing label-specific weight vectors each representing the whole input text. The last layer consists of label-specific binary classifiers on top of the corresponding label-specific vectors. Each classifier uses a single feed-forward network (FFNN) to predict whether a certain ICD code is assigned to the input text or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our Label Attention Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Layer</head><p>Assume that a clinical document D consists of n word tokens w 1 , w 2 , ..., w i , ..., w n . We represent each i th token w i in D by a pre-trained word embedding e wi having the same embedding size of d e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bidirectional LSTM Layer</head><p>We use a BiLSTM architecture to capture contextual information across input words in D. In particular, we use the BiL-STM to learn latent feature vectors representing input words from a sequence e w1:wn of vectors e w1 , e w2 , ..., e wn . We compute the hidden states of the LSTMs corresponding to the i th word (i ? {1, . . . , n}) as:</p><formula xml:id="formula_0">? ? h i = ? ??? ? LSTM(e w1:wi ) (1) ? ? h i = ? ??? ? LSTM(e wi:wn )<label>(2)</label></formula><p>where ? ??? ? LSTM and ? ??? ? LSTM denote forward and backward LSTMs, respectively. Two vectors ? ? h i and ? ? h i are then concatenated to formulate the final latent vector h i :</p><formula xml:id="formula_1">h i = ? ? h i ? ? ? h i (3)</formula><p>The dimensionality of the LSTM hidden states is set to u, resulting in the size of the latent vectors h i at 2u. All the hidden state vectors of words in D are concatenated to formulate</p><formula xml:id="formula_2">a matrix H = [h 1 , h 2 , ..., h n ] ? R 2u?n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Layer</head><p>As the clinical documents have different lengths and each document has multi-labels, our goal is to transform H into label-specific vectors. We achieve that goal by proposing a label attention mechanism. Our label attention mechanism takes H as the input and output |L| label-specific vectors representing the input document D. First, we compute the labelspecific weight vectors as:</p><formula xml:id="formula_3">Z = tanh(WH) (4) A = softmax(UZ)<label>(5)</label></formula><p>Here, W is a matrix ? R da?2u , in which d a is a hyperparameter to be tuned with the model, resulting in a matrix Z ? R da?n . The matrix Z is used to multiply with a matrix U ? R |L|?da to compute the label-specific weight matrix A ? R |L|?n , in which each i th row of A refers to as a weight vector regarding the i th label in L. softmax is applied at the row level to ensure that the summation of weights in each row is equal to 1. After that, the attention weight matrix A is then multiplied with the hidden state matrix H to produce the label-specific vectors representing the input document D as:</p><formula xml:id="formula_4">V = HA (6) Each i th column v i of the matrix V ? R 2u?|L| is a repre- sentation of D regarding the i th label in L.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Layer</head><p>For each label-specific representation v i , we pass it as input to a corresponding single-layer feed-forward network (FFNN) with a one-node output layer followed by a sigmoid activation function to produce the probability of the i th label given the document. Here, the probability is then used to predict the binary output ? {0, 1} using a predefined threshold, such as 0.5. The training objective is to minimize the binary crossentropy loss between the predicted label y and the target y as:  <ref type="figure">Figure 2</ref>: The architecture of our hierarchical joint learning model JointLAAT has two levels: The first level is to predict the normalized codes composing of the first three characters of raw ICD codes. The second level utilizes the prediction produced from the first level to predict the raw ICD codes.</p><formula xml:id="formula_5">Loss(D, y, ?) = |L| j=1 y j log y j + (1 ? y j ) log(1 ? y j ) (7)</formula><p>Where ? denotes all the trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Our attention layer can be viewed as an extension of the structured self-attention mechanism proposed by Lin et al. <ref type="bibr">[2017]</ref> for the multi-label classification task. In particular, different from Lin et al. <ref type="bibr">[2017]</ref>, the number of attention hops is set to the number of labels; and we then use the document embedding from each hop separately to build a binary classifier for a certain label. Note that Lin et al.</p><p>[2017] create a single final text embedding aggregated from all the attention hops to make the classification prediction. The approach of using a single aggregated text embedding is suitable for single-label classification problems, such as sentiment analysis <ref type="bibr" target="#b3">[Lin et al., 2017]</ref>, but not suitable for multi-label text classification tasks, such as ICD coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Joint Learning Mechanism</head><p>A challenge of the ICD coding task is that most of the ICD codes are not frequently used leading to an extremely unbalanced set of codes <ref type="bibr">[Song et al., 2019;</ref><ref type="bibr" target="#b5">Xie et al., 2019]</ref>. As there are hierarchical relationships between ICD codes, in which codes starting with the same first three characters belong to the same higher-order category, we can utilize the hierarchical structure among the codes to help the model work better for infrequent codes. For example, "Nonpyogenic meningitis" (322.0), "Eosinophilic meningitis" (322.1), "Chronic meningitis" (322.2), "Meningitis, unspecified" (322.9) belong to a category of "Meningitis of unspecified cause" (322).</p><p>To this end, we propose a hierarchical joint learning model (namely JointLAAT) based on our label attention model, as detailed in <ref type="figure">Figure 2</ref>. For each input document D, the model firstly produces the prediction for the first level of the ICD codes' first three characters (i.e. normalized codes). The predicted output of the first level "normalization" is embedded into a vector s D ? R p with the projection size p. The vector s D is then concatenated with each label-specific vector v i2 of the second level of the "raw" ICD codes before being fed into the feed-forward network to produce the final prediction. The model is trained by minimizing the sum of the binary cross-entropy losses of the "normalization" and "raw" levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>This section details the methodology to evaluate the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We follow recent SOTA work on ICD coding from clinical text <ref type="bibr" target="#b3">[Mullenbach et al., 2018;</ref><ref type="bibr" target="#b5">Xie et al., 2019;</ref><ref type="bibr">Li and Yu, 2020]</ref>  <ref type="bibr" target="#b3">[Mullenbach et al., 2018;</ref><ref type="bibr" target="#b5">Xie et al., 2019;</ref><ref type="bibr">Li and Yu, 2020]</ref>, we focus on the discharge summaries, which condense all the information during a patient stay into a single document. Each admission was tagged manually by coders with a set of ICD-9 codes describing diagnoses and procedures during the patient stay. In this dataset, there were 52,722 discharge summaries and 8,929 unique codes in total. We conduct the experiments following the previous work <ref type="bibr" target="#b3">[Mullenbach et al., 2018]</ref>. For the first experiment of using the full set of codes, the data was split using patient ID so that no patient is appearing in both training and validation/test sets. In particular, there are 47,719 discharge summaries for training, 1,631 for validation and 3,372 for testing. For the second experiment of using the 50 most frequent codes, the resulting subset of 11,317 discharge summaries was obtained, in which there are 8,067 discharge summaries for training, 1,574 for validation and 1,730 for testing. We denote the datasets used in the two settings as MIMIC-III-full and MIMIC-III-50, respectively. MIMIC-II. We also conduct experiments on the MIMIC-II dataset, namely MIMIC-II-full. Following the previous work <ref type="bibr" target="#b5">[Perotte et al., 2013;</ref><ref type="bibr" target="#b3">Mullenbach et al., 2018;</ref><ref type="bibr">Li and Yu, 2020]</ref>, 20,533 and 2,282 clinical notes were used for training and testing, respectively (with a total of 5,031 unique codes). From the set of 20,533 clinical notes, we further use 1,141 notes for validation, resulting in only 19,392 notes for training our model. Preprocessing. Following the previous work <ref type="bibr" target="#b3">[Mullenbach et al., 2018;</ref><ref type="bibr" target="#b5">Xie et al., 2019;</ref><ref type="bibr">Li and Yu, 2020]</ref>, we tokenize the text and lowercase all the tokens. We remove tokens containing no alphabetic characters such as numbers, punctuations. For a fair comparison, similar to the previous work, on the preprocessed text from the discharge summaries in the MIMIC-III-full dataset, we pre-train word embeddings with the size d e = 100 using CBOW Word2Vec method <ref type="bibr" target="#b3">[Mikolov et al., 2013]</ref>. We then utilize the pretrained word embeddings for all experiments on the three MIMIC datasets. As shown in Li and Yu <ref type="bibr">[2020]</ref>, there were no significant performance differences when truncating the text to a maximum length ranging from 2,500 to 6,500. We, therefore, truncate all the text to the maximum length of 4,000 as in Xie et al. <ref type="bibr">[2019]</ref> for the fairness and reducing the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>To make a complete comparison with the previous work on ICD coding, we report the results of our proposed model on a variety of metrics, including macro-and micro-averaged F1 and AUC (area under the ROC curve), precision at k (P@k ? {5, 8, 15}). As detailed in Manning et al. <ref type="bibr">[2008]</ref>, "microaveraged" pools per-pair of (text, code) decisions, and then computes an effectiveness measure on the pooled data, while "macro-averaged" computes a simple average over all labels. P@k is the precision of the top-k predicted labels with the highest predictive probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation and Hyper-parameter Tuning</head><p>Implementation. We implement our LAAT and Joint-LAAT using PyTorch <ref type="bibr" target="#b4">[Paszke et al., 2019]</ref>. We train the models with AdamW [Loshchilov and Hutter, 2019], and set its learning rate to the default value of 0.001. <ref type="bibr">4</ref> The batch size and number of epochs are set to 8 and 50, respectively. We use a learning rate scheduler to automatically reduce the learning rate by 10% if there is no improvement in every 5 epochs. We also implement an early stopping mechanism, in which the training is stopped if there is no improvement of the micro-averaged F1 score on the validation set in 6 continuous epochs. For both LAAT and JointLAAT, we apply a dropout mechanism with the dropout probability of 0.3. Before each epoch, we shuffle the training data to avoid the influence of the data order in learning the models. We choose the models with the highest micro-averaged F1 score over the validation sets to apply to the test sets. Note that we ran our models 10 times with the same hyper-parameters using different random seeds and report the scores averaged over the 10 runs. Hyper-parameter tuning. For LAAT, we perform a grid search over the LSTM hidden size u ? {128, 256, 384, 512} and the projection size d a ? {128, 256, 384, 512}, resulting in the optimal values u at 512 and d a at 512 on the MIMIC-III-full dataset, and the optimal values u at 256 and d a at 256 on both the MIMIC-III-50 and MIMIC-II-full datasets. For JointLAAT, we employ the optimal hyper-parameters (d a and u) from LAAT and fix the projection size p at 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baselines</head><p>Our LAAT and JointLAAT are compared against the following recent SOTA baselines, including both conventional machine learning and deep learning models: LR. Logistic Regression was explored for ICD coding on the MIMIC datasets by building binary one-versus-rest classifiers with unigram bag-of-word features for all labels appearing in the training data <ref type="bibr" target="#b3">[Mullenbach et al., 2018]</ref>. SVM. <ref type="bibr" target="#b5">Perotte et al. [2013]</ref> utilized the hierarchical nature of ICD codes to build hierarchical classifiers using Support Vector Machine (SVM). Experiments on the MIMIC-II-full dataset showed that hierarchical SVM performed better than the flat SVM which treats the ICD codes independently. Xie et al. <ref type="bibr">[2019]</ref> applied the hierarchical SVM for ICD coding on the MIMIC-III-full dataset using 10,000 unigram features with the tf-idf weighting scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN. The one-dimensional Convolutional Neural</head><p>Network <ref type="bibr" target="#b1">[Kim, 2014]</ref>   <ref type="bibr">[2017]</ref>, which combines the memory network <ref type="bibr">[Sukhbaatar et al., 2015]</ref> with iterative condensed memory representations. This model produced competitive ICD coding results on the MIMIC-III-50 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-LSTM-Att. The</head><p>Character-aware LSTM-based Attention model was proposed by Shi et al. <ref type="bibr">[2017]</ref> for ICD coding. In the model, LSTM-based language models were utilized to generate the representations of clinical notes and ICD codes, and an attention method was proposed to address the mismatch between notes and codes. The model was employed to predict the ICD codes for the medical notes in the MIMIC-III-50 dataset.</p><p>HA-GRU. The Hierarchical Attention Gated Recurrent Unit (HA-GRU) <ref type="bibr" target="#b6">[Yang et al., 2016]</ref> was utilized by Baumel et al. <ref type="bibr">[2018]</ref> for ICD coding on the MIMIC-II dataset.</p><p>LEAM. The Label Embedding Attentive Model was proposed by Wang et al. <ref type="bibr">[2018]</ref> for text classification, where the labels and words were embedded in the same latent space, and the text representation was built using the text-label compatibility, resulting in competitive results on MIMIC-III-50.</p><p>CAML. The Convolutional Attention network for Multi-Label classification (CAML) was proposed by Mullenbach et al. <ref type="bibr">[2018]</ref>. The model achieved high performances on the MIMIC datasets. It contains a single layer CNN <ref type="bibr" target="#b1">[Kim, 2014]</ref> and an attention layer to generate label-dependent representation for each label (i.e., ICD code). <ref type="bibr">et al., 2018]</ref> is an extension of the CAML model, incorporating the text description of each code to regularize the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DR-CAML. Description Regularized CAML [Mullenbach</head><p>MSATT-KG. The Multi-Scale Feature Attention and Structured Knowledge Graph Propagation approach was proposed by Xie et al. <ref type="bibr">[2019]</ref> achieving the SOTA ICD coding results on the MIMIC-III-full and MIMIC-III-50 datasets. The model contains a densely connected convolutional neural network which can produce variable n-gram features and a multi-scale feature attention to adaptively select multi-scale features. In the model, the graph convolutional neural network <ref type="bibr" target="#b3">[Kipf and Welling, 2017]</ref> is also employed to capture the hierarchical relationships among medical codes.</p><p>MultiResCNN. The Multi-Filter Residual Convolutional Neural Network was proposed by Li and Yu [2020] for ICD coding achieving the SOTA results on the MIMIC-IIfull dataset and in-line SOTA results on the MIMIC-III-full dataset. The model contains a multi-filter convolutional layer to capture various text patterns with different lengths and a residual convolutional layer to enlarge the receptive field. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIMIC-III-full</head><p>On the MIMIC-III-full dataset, <ref type="table">Table 1</ref> shows the results of the evaluation across all quantitative metrics. Specifically, using an attention mechanism, CAML <ref type="bibr" target="#b3">[Mullenbach et al., 2018]</ref> produced better performance than both conventional machine learning models (i.e., LR and SVM) and deep learning models (i.e., CNN, BiGRU). Addressing the fixed window size problem of CAML <ref type="bibr" target="#b3">[Mullenbach et al., 2018]</ref>, <ref type="bibr">MASATT-KG [Xie et al., 2019]</ref> and <ref type="bibr">MultiResCNN [Li and Yu, 2020]</ref> achieved better results than CAML with improvements in micro-F1 by 1.4% and 1.3%, respectively. Our label attention model LAAT produces higher results in the macro-AUC, macro-F1, micro-F1, P@8 and P@15 metrics, compared to <ref type="bibr">MASATT-KG [Xie et al., 2019]</ref> and <ref type="bibr">Mul-tiResCNN [Li and Yu, 2020]</ref>, while achieving a slightly lower micro-AUC than that of MSATT-KG. In particular, LAAT improves the macro-AUC by 0.9%, macro-F1 by 0.9%, micro-F1 by 2.2%, P@8 by 0.4% and P@15 by 0.7%. LAAT also produces an impressive P@5 of 81.3%, indicating that on average at least 4 out of the top 5 predicted codes are correct. Regarding JointLAAT where we utilized the hierarchical structures of ICD codes to improve the prediction of infrequent codes, <ref type="table">Table 1</ref> also shows that JointLAAT produces better macro-AUC score and significantly higher macro-F1 score than LAAT with the improvement of 0.8% (p &lt; 0.01, using the Approximate Randomization test <ref type="bibr" target="#b0">[Chinchor, 1992]</ref> which is a nonparametric significance test suitable for NLP tasks <ref type="bibr">[Dror et al., 2018]</ref>). Due to the macro-metrics' emphasis on rare-label performance <ref type="bibr" target="#b3">[Manning et al., 2008]</ref>, this indicates that JointLAAT does better than LAAT for the infrequent codes (the P@k scores of JointLAAT are slightly lower than those of LAAT but the differences are not significant). <ref type="table" target="#tab_5">Table 2</ref> shows results on the MIMIC-III-50 dataset. LAAT outperforms all the baseline models across all the metrics. In particular, compared to the previous SOTA model MSATT-   KG <ref type="bibr" target="#b5">[Xie et al., 2019]</ref>, LAAT produces notable improvements of 1.1%, 1.0%, 2.8%, 3.1% and 3.1% in macro-AUC, micro-AUC, macro-F1, micro-F1 and P@5, respectively. From Table 2 , we also find that there is no significant difference between LAAT and JointLAAT regarding the obtained scores. The possible reason is that there is no infrequent codes in this dataset, which results in only 8 out of 40 normalized codes (i.e., three character codes) at the first "normalization" level that are linked to more than one raw ICD codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIMIC-III-50</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIMIC-II-full</head><p>On the MIMIC-II-full dataset, <ref type="table" target="#tab_6">Table 3</ref> shows that LAAT substantially outperforms all the baseline models. Specifically, the micro-F1 is 12.5% higher than HA-GRU [Baumel et al., 2018] which uses another attention mechanism and GRU for the ICD coding task. LAAT differs from HA-GRU in that our attention mechanism is label-specific. Compared to the previous SOTA model <ref type="bibr">MultiResCNN [Li and Yu, 2020]</ref>, LAAT improves the macro-AUC, micro-AUC, macro-F1, micro-F1 and P@8 by 1.8%, 0.5%, 0.7%, 2.2% and 0.6%, respectively. Similar to the results on the MIMIC-III-full dataset <ref type="table">(Table  1)</ref>, <ref type="table" target="#tab_6">Table 3</ref> shows that JointLAAT does better on infrequent codes than LAAT on the MIMIC-II-full dataset with the improvement of 0.9% on the macro-F1 (p &lt; 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>As discussed in Section 3.1, our label attention mechanism extends the self-attention mechanism proposed by Lin et  <ref type="table">Table 4</ref>: Ablation results on the MIMIC-III-full validation set. LAATCAML: A LAAT variant using the label attention mechanism proposed in CAML instead of our proposed label attention mechanism. CAMLLAAT: We modify CAML to use our label attention mechanism instead of the original one in CAML. LAATGRU: A LAAT variant using BiGRU instead of BiLSTM to learn latent feature vectors representing input words. All the score differences between LAAT and others are significant (p &lt; 0.01).</p><p>al.</p><p>[2017] for a multi-label classification task. <ref type="bibr">MASATT-KG [Xie et al., 2019]</ref> and <ref type="bibr">MultiResCNN [Li and Yu, 2020]</ref> used another per-label attention mechanism proposed in CAML by <ref type="bibr" target="#b3">Mullenbach et al. [2018]</ref>, in which the weight vector regarding each label was produced directly using the output of a CNN-based network.</p><p>To better understand the model influences, we performed an ablation study on the validation set of the MIMIC-III-full dataset. In particular, for the first setting, namely LAAT CAML , we couple the label attention mechanism proposed by Mullenbach et al. <ref type="bibr">[2018]</ref> with our BiLSTM encoder. Results of LAAT and LAAT CAML in <ref type="table">Table 4</ref> show that our label attention mechanism does better than the label attention mechanism proposed in CAML by <ref type="bibr" target="#b3">Mullenbach et al. [2018]</ref>.</p><p>For the second setting, namely CAML LAAT , we employ our attention mechanism on the output of the CNN network used in CAML. Results of LAAT and CAML LAAT show that employing BiLSTM helps produce better scores than employing CNN under the same attention mechanism.</p><p>We further investigate a variant of LAAT, namely LAAT GRU , using a BiGRU encoder instead of a BiLSTM encoder. <ref type="table">Table 4</ref> shows that using BiLSTM helps obtain higher performance than using BiGRU. The reason might be that LSTM with the separate memory cells can theoretically remember longer-term dependencies than GRU, thus LSTM is more suitable for ICD coding from long clinical text, e.g. the discharge summaries which are typically long. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we have presented a label attention model for ICD coding from clinical text. We also extend our model with a hierarchical joint learning architecture to handle the infrequent ICD codes. Experimental results on three standard benchmark MIMIC datasets show that our label attention model obtains new state-of-the-art performance with substantial improvements across various evaluation metrics over competitive baselines. The hierarchical joint learning architecture also helps significantly improve the performances for infrequent codes, resulting in higher macro-averaged metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of our label attention model which contains an embbedding layer, a Bidirectional LSTM layer, a label attention layer and an output layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>First level for predicting normalized codes Second level for predicting raw codes</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">sigmoid</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>v 11</cell><cell>FFNN 11</cell><cell>y 11</cell><cell>sigmoid</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>FFNN</cell><cell></cell></row><row><cell>Word embeddings</cell><cell>BiLSTM</cell><cell>Attention Attention (2 nd level) (1 st level)</cell><cell>v L1 v 12</cell><cell>FFNN L1</cell><cell>y L1</cell><cell>FFNN 12</cell><cell>y 12</cell></row><row><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell>...</cell><cell>...</cell></row><row><cell></cell><cell></cell><cell></cell><cell>v L2</cell><cell></cell><cell></cell><cell>FFNN L2</cell><cell>y L2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">sigmoid</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>: using benchmark Medical Information Mart for Intensive Care (MIMIC) datasets MIMIC-III [Johnson et al., 2016] and MIMIC-II [Lee et al., 2011]. MIMIC-III. Following previous work</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>was employed by Mullenbach et al. [2018] for ICD coding on the MIMIC datasets. BiGRU. The bidirectional Gated Recurrent Unit [Cho et al., 2014] was utilized by Mullenbach et al. [2018] for ICD coding on the MIMIC datasets. C-MemNN. The Condensed Memory Neural Network was proposed by Prakash et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results on the MIMIC-III-50 test set.</figDesc><table><row><cell>Model</cell><cell cols="6">AUC Macro Micro Macro Micro P@5 P@8 P@15 F1 P@k</cell></row><row><cell>LR</cell><cell cols="3">69.0 93.4 2.5</cell><cell>31.4 -</cell><cell cols="2">42.5 -</cell></row><row><cell>SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.3 -</cell><cell>-</cell><cell>-</cell></row><row><cell>HA-GRU</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.6 -</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN</cell><cell cols="3">74.2 94.1 3.0</cell><cell>33.2 -</cell><cell cols="2">38.8 -</cell></row><row><cell>BiGRU</cell><cell cols="3">78.0 95.4 2.4</cell><cell>35.9 -</cell><cell cols="2">42.0 -</cell></row><row><cell>CAML</cell><cell cols="3">82.0 96.6 4.8</cell><cell>44.2 -</cell><cell cols="2">52.3 -</cell></row><row><cell>DR-CAML</cell><cell cols="3">82.6 96.6 4.9</cell><cell>45.7 -</cell><cell cols="2">51.5 -</cell></row><row><cell cols="4">MultiResCNN 85.0 96.8 5.2</cell><cell>46.4 -</cell><cell cols="2">54.4 -</cell></row><row><cell>LAAT</cell><cell cols="3">86.8 97.3 5.9</cell><cell cols="3">48.6 64.9 55.0 39.7</cell></row><row><cell>JointLAAT</cell><cell cols="3">87.1 97.2 6.8</cell><cell></cell><cell></cell><cell></cell></row></table><note>* 49.1* 65.2 55.1 39.6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on the MIMIC-II-full test set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.who.int/classifications/icd/factsheet/en/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.cdc.gov/nchs/icd/icd10cm pcs background.htm arXiv:2007.06351v1 [cs.CL] 13 Jul 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">5,411 (60%) of all the 8,929 ICD codes appear less than 10 times in the MIMIC-III dataset [Johnson et al., 2016].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In preliminary experiments, we find that though AdamW and Adam<ref type="bibr" target="#b2">[Kingma and Ba, 2015]</ref> produce similar performances, AdamW converges faster than Adam when training our models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>5 The number of word tokens per document in the MIMIC datasets is about 1,500 on average and can be greater than 6,500 <ref type="bibr" target="#b3">[Mullenbach et al., 2018;</ref><ref type="bibr" target="#b5">Xie et al., 2019;</ref> Li and Yu, 2020].</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. The hitchhiker&apos;s guide to testing statistical significance in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baumel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Workshop on Health Intelligence</title>
		<editor>Sarvnaz Karimi, Xiang Dai, Hamed Hassanzadeh, and Anthony Nguyen</editor>
		<meeting>the AAAI Workshop on Health Intelligence</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="328" to="332" />
		</imprint>
	</monogr>
	<note>Proceedings of BioNLP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim ; Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ICD Coding from Clinical Text Using Multi-Filter Residual Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling ; Koopman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI workshop on Text Learning</title>
		<editor>Li and Yu, 2020] Fei Li and Hong Yu</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
	<note>Proceedings of AMIA</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Py-Torch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>O&amp;apos;malley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>Measuring diagnoses: ICD code accuracy. Health services research</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sainbayar Sukhbaatar, arthur szlam, et al. End-To-End Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perotte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04075</idno>
		<idno>arXiv:1909.13154</idno>
	</analytic>
	<monogr>
		<title level="m">Congzheng Song, Shanghang Zhang, et al. Generalized Zero-shot ICD Coding</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="649" to="658" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of CIKM</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
