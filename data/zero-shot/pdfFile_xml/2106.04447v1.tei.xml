<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Orlanski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gittens</surname></persName>
							<email>gittea@rpi.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Answering a programming question using only its title is difficult as salient contextual information is omitted. Based on this observation, we present a corpus of over 40,000 StackOverflow question texts to be used in conjunction with their corresponding intents from the CoNaLa dataset . Using both the intent and question body, we use BART to establish a baseline BLEU score of 34.35 for this new task. We find further improvements of 2.8% by combining the mined CoNaLa data with the labeled data to achieve a 35.32 BLEU score. We evaluate prior stateof-the-art CoNaLa models with this additional data and find that our proposed method of using the body and mined data beats the BLEU score of the prior state-of-the-art by 71.96%. Finally, we perform ablations to demonstrate that BART is an unsupervised multimodal learner and examine its extractive behavior. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of semantic parsing is to translate a Natural Language(NL) utterance to its logical components. There is a large body of research on applying semantic parsing for source code generation in a multitude of domain specific languages such as lambda calculus and SQL <ref type="bibr" target="#b3">(Dahl et al., 1994;</ref><ref type="bibr" target="#b35">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b36">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b14">Ling et al., 2016;</ref><ref type="bibr" target="#b26">Xiao et al., 2016;</ref><ref type="bibr" target="#b17">Rabinovich et al., 2017;</ref><ref type="bibr" target="#b4">Dong and Lapata, 2018;</ref><ref type="bibr" target="#b7">Guo et al., 2019;</ref><ref type="bibr">Hwang et al., 2019;</ref><ref type="bibr" target="#b23">Tabassum et al., 2020</ref>). However, the task of translating an NL utterance to a general-purpose programming language has proven to be more challenging. A significant issue contributing to this is the difficulty in acquiring quality data due to the necessary domain knowledge needed in the annotation process.</p><p>Despite this, the past few years have seen a large number of datasets released for different text-to- <ref type="figure">Figure 1</ref>: Overview of our approach. From the combined annotated + mined set, we concatenate the intent and question body for inputs to BART <ref type="bibr" target="#b12">(Lewis et al., 2020)</ref> and use beam search for generation. code related tasks <ref type="bibr" target="#b14">(Ling et al., 2016;</ref><ref type="bibr" target="#b34">Yu et al., 2018;</ref><ref type="bibr" target="#b16">Lu et al., 2021)</ref>. Some datasets such as CodeSearchNet <ref type="bibr" target="#b9">(Husain et al., 2019)</ref> contain snippets from a multitude of different languages. Others focus on distinct tasks within a specific language, such as JuICe <ref type="bibr" target="#b0">(Agashe et al., 2019)</ref>, which contains executable Python programming assignments. Utilizing these corpora, prior works <ref type="bibr" target="#b21">(Suhr et al., 2018;</ref><ref type="bibr">Neubig, 2017, 2018;</ref><ref type="bibr" target="#b22">Sun et al., 2019;</ref><ref type="bibr" target="#b8">Hayati et al., 2018;</ref><ref type="bibr" target="#b33">Yin and Neubig, 2019;</ref> have found success with a large variety of model architectures. These methods, however, struggle with domain agnostic open-ended code generation in general-purpose languages. One idea to combat this is to utilize large pretrained language models.</p><p>Transformers <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref> have demonstrated that they can both be few-shot <ref type="bibr">(Brown et al., 2020)</ref> and unsupervised multitask <ref type="bibr" target="#b18">(Radford et al., 2019)</ref> learners. They have been successfully applied to programming language tasks. CodeBERT achieved strong performance on the CodeSearch-Net task through pretraining on bimodal NL comment and code pairs <ref type="bibr" target="#b6">(Feng et al., 2020)</ref>, while <ref type="bibr" target="#b22">Sun et al. (2019)</ref> used abstract syntax trees(AST) and transformers to achieve state of the art performance on the HearthStone benchmark <ref type="bibr" target="#b14">(Ling et al., 2016)</ref>. <ref type="bibr" target="#b19">Roziere et al. (2021)</ref> proposed the deobfuscation pretraining task to incorporate structural features of code into transformer models without the use of ASTs. More recently, <ref type="bibr" target="#b20">Shin et al. (2021)</ref> explored the capabilities of large pretrained language models to be few-shot semantic parsers.</p><p>Yet open-domain programming question answering on sites such as StackOverflow(SO) 2 has remained an elusive goal.  created an annotated dataset with the site in which the intent and answer snippet pairs were automatically mined from the question. They then had crowd workers rewrite the intents to reflect the corresponding code better. Currently, state-of-the-art was achieved by pretraining an LSTM model on resampled API and mined data . Subsequent work conducted an empirical study on the effectiveness of using a code generation model in an IDE plugin and find that developers largely had favorable opinions of their experience <ref type="bibr" target="#b28">(Xu et al., 2021)</ref>. An inherent issue with the approach of Xu et al. <ref type="formula" target="#formula_1">(2020)</ref>, more fundamentally the dataset and parameters of the task, is that the intent can only contain a limited amount of information. Arriving at this answer from the intent "add a new axis to array a" requires not only the disambiguation of data types for variable a, but also the use of multiple distinct library-specific concepts. Further, this must be accomplished while maintaining syntactically correct code and proper order of arguments. However, neither the original title nor the rewritten intent contains the necessary information to accomplish this task. Although the previous state-of-the-art-model by  uses abstract syntax trees (AST) to guarantee syntactically valid python code, it incorrectly generates a[(-1),:]=a. One potential remedy would be to increase the amount of training data, but as discussed previously, getting high-quality annotated code generation data is especially difficult.</p><p>Motivated by the limitations to the amount of information a given intent can contain and the substantial difficulty involved with gathering more labeled data, we utilize the multimodal text from the question bodies provided by the StackExchange API 3 . We take advantage of the strong performances of transformer models to beat the previous state-of-the-art by 3.06 BLEU. We ensure a fair comparison by training the models from prior works with the extra data to adequately evaluate our proposed method. When all models are trained with the extra data, using BART beats the previous state of the art by 15.12 BLEU.</p><p>Our main contributions are the following:</p><p>? Expanding upon the original CoNaLa dataset  to include the multimodal textual question bodies and thus the pertinent contextual information they contain such as inputs, outputs, and required libraries.</p><p>? Demonstrating that BART does not rely on a single modality, but rather achieves its best performance on our dataset when all modalities are included. This indicates at least a shallow understanding of both natural and programming language as well as how they are related in the context of SO questions.</p><p>? Conducting experiments revealing that BART's struggle to generate syntacically correct code is likely a result of its tendency to be extractive rather than generative in the task of text-to-code generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>As detailed in <ref type="figure">Figure 1</ref>, our overarching approach is to: (1) gather textual bodies from SO for both the annotated and mined examples in the CoNaLa corpus, (2) use the concatenated intents and question bodies as inputs for a large pretrained language model, and (3) use beam search to generate the answer code snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">StackOverflow Data</head><p>Every example e i ? E from the CoNaLa dataset  is comprised of an intent x i ? X that concisely summarizes what the poster wants and a snippet of Python code y i ? Y that represents an implementation of x i . Crowd sourcing was used to rewrite a selection of the mined intents to reflect the snippet better and to ensure that the snippet was indeed a correct answer. As discussed, these intents are limited in the amount of information they can contain. The intent "add a new axis to array a" from Figure 2 could refer to a wide variety of different Python objects. It could range from the default list to the Tensor object from PyTorch 4 . The full question, or either its tags or title, is typically enough for a human to disambiguate the correct library to use. But the annotated intent lacks this crucial information as it is rather difficult to design an annotation task for SO data. <ref type="bibr">5</ref> We address this problem directly by using the additional data found in the SO question. In <ref type="figure" target="#fig_0">Figure 2</ref> there are four direct mentions of the NumPy library: two in the question body and one each in both the tags and the title. Further, there is a direct mention of the ndarray data type from NumPy. It is, therefore, rather intuitive to include this additional data as input with the hope that it improves the answer generation performance. Although we did mention that both the tags and title provide salient information, the focus of this paper is only on using the noisy textual question bodies. Therefore, for every example e i the inputs now become the concatenation of x i and the body q x i ? Q from the original SO question. It is important to note that |Q| = |E| as a single question can have many examples while every question is, by definition, unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Modality Learning</head><p>Multiple modalities are present in the textual body of a given question. These can range from embedded images to messages from administrators (or upset users) stating that the question is a duplicate of some tangentially related post that does not have an answer. While these are useful to readers, we limit our focus to three modalities: code blocks, inline code, and NL. These modalities are marked in <ref type="figure" target="#fig_0">Figure 2</ref> with blue, green, and red, respectively. Ideally, we would prefer to leave in the HTML tags to serve as sentinel tokens, but, looking at <ref type="figure" target="#fig_0">Figure 2</ref>, one immediately finds that the poster forgot to mark _to_col as inline code. Therefore, we remove all HTML tags from the inputs, creating an unsupervised learning environ-ment. Therefore, we propose that a transformer model will learn each of the three modalities and learn to use the relationships between each. We use BART <ref type="bibr" target="#b12">(Lewis et al., 2020)</ref> because its pretraining focuses on denoising textual data and, to the best of our knowledge, has minimal exposure to code examples. We used HuggingFace's <ref type="bibr" target="#b25">(Wolf et al., 2020)</ref> BartForConditionalGeneration model which has a default BART encoder-decoder model with a linear layer and bias for outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Unlabeled Data</head><p>We followed Xu et al. (2020) by using large amounts of the mined but not annotated data. <ref type="bibr">Unlike Xu et al. (2020)</ref>, however, we do not use this data for pretraining. Instead, we combine this data with the annotated data in our main training and validation sets. By adding more questions to the training set, we directly increase the probability that the model encounters a larger and more representative distribution of libraries. Intuitively, this will reduce the variances between experiments as we have reduced the dependency on the specific examples used in the training and validation sets. This variance reduction is especially useful when working with a small dataset such as CoNaLa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>CoNaLa  6 is an open domain text to code generation task constructed from SO questions. It has 2,879 7 annotated NL-code pairs with more than 590K mined pairs from over 40,000 unique SO questions in the dataset. StackOverflow Data For every unique question in both the annotated and mined sets, we gather additional data from the StackExchange API. As discussed in subsection 2.1, we only use the question body as input. Therefore the task is to generate a valid answer snippet from both the intent and the textual body. Detailed statistics for this dataset are given in <ref type="table" target="#tab_1">Table 1 and Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methods</head><p>We removed 238 ( 10%) examples from the training set to form the validation set. We then followed Xu et al. <ref type="formula" target="#formula_1">(2020)</ref>   is valid. However, we only used 10,000 samples rather than the 100,000 Xu et al. <ref type="formula" target="#formula_1">(2020)</ref> used. From this, we remove 1000 for validation. 8 For all tests of our model with the mined data, we combine the two training and validation sets into one. Every experiment and test conducted in this work was conducting using Google's Colab Pro service. It afforded us the ability to use 512 input tokens with a batch size of 16. More importantly, we were able to use P100 and V100 graphics cards. Following that, we perform an ablation study using BART and the different components of our approach. Every ablation is run five separate times with different seeds and validation splits. For each test, the model with the lowest validation loss is used in the evaluation. Each test is run for ten epochs as we consistently observed overfitting after five to eight epochs.</p><p>Because we introduce new data at inference, we needed to ensure we fairly compare our methods with previous work. To this end, we run the prior works with the question bodies as inputs. However, for testing  with the question bodies, we limited the amount of mined data in pretraining to 10,000 instead of 100,000. This was done due to Google Colab's execution time limits, as it took upwards of four hours for each run of Xu et al.</p><p>(2020) with only 10,000 samples. 8 Some questions were deleted from StackOverflow in both the annotated and mined sets, so we could not use those.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Metrics</head><p>We measure the corpus level BLEU score of the generated code snippets with the same postprocessing methods and smoothing as . We evaluate our ablations by comparing the corpus BLEU score and unigram, bigram, and trigram precision. Finally, we calculate the percentage of test examples for which our model generated a syntactically valid Python snippet.</p><p>For the previous state-of-the-art, we also report the Oracle BLEU proposed by <ref type="bibr" target="#b33">Yin and Neubig (2019)</ref>. This is calculated by choosing the candidate snippet s i with the highest sentence level BLEU score out of n generated snippets. Formally, given the candidate list C = [c 1 , . . . , c n ] and ground truth y i ,</p><formula xml:id="formula_0">z = argmax c j ?C BLEU(c j , y i )<label>(1)</label></formula><p>Furthermore, we want to quantify how much our model relies on the body of the question or "cheats." To do this, we calculate the cheating for the generated snippet s i ? [s 1 , . . . , s N ] = S and ground truth y i ? [y 1 , . . . , y N ] = Y with respect to the input text b i ? [b 1 , . . . , b N ] = B. Given the function m(a, b) that calculates a textual similarity metric m, we define the cheating w.r.t. m as</p><formula xml:id="formula_1">C m (S) = i?[1;N ] (m(s i , b i ) ? m(y i , b i )) N<label>(2)</label></formula><p>If the model is heavily "cheating" from the input, then m(s i , b i ) m(y i , b i ), which leads to a large C m . The quantity C m is, by design, similar to a standard mean squared error. The largest difference is that the difference is not squared, to facilitate distinguishing between less and more similar.</p><p>For the metric function m, we use BLEU and ROUGE <ref type="bibr" target="#b13">(Lin, 2004)</ref>. For the former, we take the bigram (C BB ) and trigram (C BT ) precision from BLEU. For ROUGE, we use bigram ROUGE (ROUGE-2/C R2 ) and the longest common subsequence (ROUGE-L/C RL ). The intuition behind using these metrics is that there is a high probability that unigram precision is large. The answers to a question must address the contents of the said question, leading to shared tokens between inputs and outputs. However, the probability should massively drop when considering multiple grams. Therefore, the similarity between n-grams when n &gt; 1 should indicate the reliance on the inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation</head><p>We implemented our model with Python and Hug-gingFace's transformer library <ref type="bibr" target="#b25">(Wolf et al., 2020)</ref> </p><formula xml:id="formula_2">9 .</formula><p>We used a BART model with a linear layer and a separate bias for text generation. We utilized the smallest available BART model from FAIR, which was the Facebook/BART-base 10 . For training, we again rely on HuggingFace's trainer and their implementation of the learning rate scheduler. We used Adam <ref type="bibr" target="#b15">(Loshchilov and Hutter, 2017)</ref> as our optimizer with a learning rate of 5e?5 and a linear learning rate scheduler. We also used a warmup ratio of 0.05. Finally, for generation, we used beam search with four beams, early stopping, and a length penalty of 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We list the previous state-of-the-art BLEU scores for the CoNaLa dataset as well as the performance of our models in <ref type="table" target="#tab_3">Table 3</ref>. Using the intent and question bodies achieved a BLEU score of 34.35?1.01. This was further increased to 35.32?0.42 by including the mined data in the training and validation set.</p><p>To better understand our model, we perform ablation tests and report their results in <ref type="table" target="#tab_5">Table 4</ref>. When comparing our top performance with the previous top performance, regardless of the data used, our model beats the previous state of the art by 3.40 BLEU, a 10.54% increase. Notably, our model outperforms the previous SoTA by 14.78 BLEU, 9 https://github.com/huggingface/transformers 10 https://huggingface.co/facebook/bart-base a 71.96% increase when only comparing the experiments with the question body. Furthermore, BART with the mined data and question bodies beats their Oracle BLEU by 1.61 BLEU, translating to a 4.78% increase. However, it is important to note that Xu et al. (2020) outperforms our model by 1.71(5.30%) when we do not use the textual body. But they still both beat the baseline TranX by 25.72% and 7.98%, respectively. The use of the mined data further beat the reranker by 1.46%.</p><p>The 71.96% increase is likely because TranX models were never intended to perform well with very noisy data, as evidenced by the 36% dropoff in corpus BLEU when adding the body to . In choosing BART, we intentionally picked a transformer model designed for denoising <ref type="bibr" target="#b12">(Lewis et al., 2020)</ref>. Further testing is likely needed to determine if our approach is heavily dependent on the underlying transformer, but that is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Impact of adding the Question Body</head><p>Adding the body of the question objectively improved the performance of the model. The BLEU score increased 30.92% to 34.35 and, per <ref type="table" target="#tab_5">Table 4</ref>, there was an increase across unigram, bigram, and trigram precision. While they all do increase, the amount is far from constant. The unigram precision only saw a 3.61% increase, whereas bigram and trigram precision increased by 12.77% and 22.90%, respectively. This indicates that while the model selected slightly more correct tokens, it greatly improved its ordering of said tokens.</p><p>Similar improvements, albeit smaller in value, also occurred when including the mined data without the question bodies. However, there was a sharp drop in the standard deviations for the three precision metrics. In contrast, adding the question body resulted in a steep increase in variance. This is most probably a result of the "shrinking" of the dataset that occurred when we added the bodies.</p><p>In <ref type="table">Table 1</ref> we report that every split of the dataset has fewer unique questions than it does examples. Also reported is that the number of tokens in the body is, on average, significantly greater than that of the intents. The effective dataset size is now much smaller, while the number of unique answer snippets stayed the same. The result is that the model now performs better on the difficult test set, at the cost of being more reliant on the training and validation split. Using both the bodies and mined  data does mitigate this "shrinking" effect, as shown by the lower standard deviations than those when only using the body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Is BART Reliant on a Single Modality</head><p>As discussed in subsection 2.2, we focus on three modalities in the textual bodies: code blocks, inline code, and natural language. We put forth the idea that a large pretrained language model such as BART learns each modality in an unsupervised manner. We designed four distinct ablations to test if this was the case. Each was run both with and without the mined data totaling eight ablations. We report the full BLEU scores from these in <ref type="table" target="#tab_5">Table 4</ref>. Further, we calculate the performance with respect to baselines in <ref type="table" target="#tab_6">Table 5</ref>. Notably, there was no modality whose removal resulted in a BLEU score worse than when the question body was not used in the input. There was also not a modality whose removal improved performance. From our ablations, it is clear that the most important modality in the question bodies is the code regardless of if it is inline or in a block. But, using only code is still 2.25% worse than when all three modalities are included with mined. This indicates that the NL surrounding acts not only as additional context, but likely further both direct and indirect indicators of salient code for the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Removing Code Improves Syntax</head><p>In <ref type="table" target="#tab_5">Table 4</ref> we report the percent of generated snippets that are syntactically valid-adding only the mined data results in a 9% increase. When using the question bodies, the addition of the mined data also increases the percent of valid snippets generated by 7.88%. While it is an improvement, it is still a 3.76% drop from when the body was excluded. Further, removing the code from the bodies resulted in the highest percentages of 92.00% and 84.92% with and without the mined data. We then performed a finer analysis using a single seed and the same training and validation data across all ablations and reported the results in Appendix A. Across all ablations, the majority of errors are caused by mismatches of parentheses. In reality, a large percentage of general syntax errors are likely caused by this. However, syntax errors prevent the extraction of the AST for further investigation of these errors. We also report in <ref type="table">Table 9</ref> the percentage of valid snippets generated when the print function is present. One of the more commonly occurring incompatibilities between Python 2 and 3 is that print now requires parentheses. Considering that the questions in the CoNaLa dataset are from March 2017 or earlier  and that support for Python 2.x only ended in January 2020 11 , we hypothesize that these deprecated calls are a large cause of the errors. When both the body and snippet have print, the inclusion of the question body led to the percent of valid snippets dropping by 21.06 with and 21.05 without the mined data with respect to their baselines. While there are only 19 such questions in the test set, this is a significant drop. The likely cause is that the autoregressive decoder of BART struggles to remember to close the parentheses when wrapping the snippet with a print statement. One solution would be to run the 2to3 12 translator on all of the code. However, the possibilities for code blocks to contain code and other modalities such as error messages and console executions present significant hurdles   as 2to3 does not support these. Therefore we leave that to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cheating</head><p>In subsection 3.3 we define the "cheating" equation to measure if the generated snippet is more similar to the question body than the ground truth is. The ideal model would maximize the BLEU score while minimizing the |C m |. We run multiple ablations on a single seed and calculate the "cheating" as defined by Equation 2 and present these results in <ref type="table" target="#tab_8">Table 6</ref>. Suffice to say that serious violations of academic integrity have occurred. As expected, the baseline is less similar to the question bodies than the ground truth is. When the body was used as input, C BT increased by 20.28 points, while C RL rose by 3.16 points, representing a 293.49% and 159.60% increase over their respective baselines. Including the mined data resulted in increases of 18.59 (308.13%) and 0.77(265.52%) when compared to using only the intents. Both indicate that the model's generated output has significantly more  Intent: concatenate items of list 'l' with a space ' ' print(' '.join(map(str, l))) x list(map(tuple,[])) y [item for item in L if " in item] z print(' '.join(str(x) for x in L)) Intent: concatenate elements of list 'b' by a colon ":" """:""".join(str(x) for x in b) We select three examples that demonstrate the benefits of our approach while also highlighting the issues in both the use of the question body and SO corpora in general and report them in <ref type="table" target="#tab_9">Table 7</ref>. In the first example, we can see that both x and y have learned how to use einsums, but neither is correct. z in this case produces an answer that returns the correct value. It is highly probable that BART understood from the poster's explicit mention that P.dot(T).transpose(1, 0, 2) gives the desired result and thus extracts it. However, this example has two critical issues: the poster's intent is to find a "cleaner" way to multiply a matrix with a tensor, and scipy.tensordot is deprecated. The latter is to be expected, considering the answer is from 2010. But it does indicate that a better evaluation based on inputs and outputs is likely needed.</p><formula xml:id="formula_3">C BB C BT C R2 C RL Baseline -6.</formula><formula xml:id="formula_4">x [' '.join(x) for x in b] y b = [int(i) for i in b] z print(':'.join(map(str, b))</formula><p>The next two examples are quite similar but are from two separate questions. x likely mistakes the core intent to be type conversion due to the inclusion of the words "items" and "with." y also suffers from the inclusion of these tokens but believes the problem involves filtering. In the final example, x recognizes that it must convert the items in b to str, but does not return a joined string. y recognizes that, again, the answer involves type conversion but predicts the incorrect type.</p><p>Similar to the first example, z produces answers for both the second and third examples that functionally return the correct results. However, running z's solution for the third example would result in a syntax error due to the missing ")." On further inspection of the question bodies, it becomes apparent that the probable reason why one snippet is syntactically valid while the other is not is the presence of a Python 2 print. The model recognizes that a suitable answer can be found in the question but must be converted to python 3. As discussed in subsection 4.3, these print statements are prone to cause syntactical issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We expand the CoNaLa dataset by adding the textual question bodies from the StackExchange API and achieve state-of-the-art performance with a simple BART model. Further, we demonstrate that, for this task, BART performs best when code blocks, inline code, and NL are all present. We then examine the impact of the question body on syntax errors and BART's cheating through multimodal understanding. Finally, we examine examples that highlight the issues with both StackOverflow data and code evaluation in general. Future work should focus on extracting desired inputs and outputs for a given intent. Further, additional efforts put into creating corpora of executable code are likely to improve not only generation but evaluation. Both will also protect datasets from deprecated functions and abandoned libraries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Example StackOverflow question with labeled elements. The corresponding rewritten intent for this question is "add a new axis to array a." Consider the question from Figure 2 in which a valid python snippet could be a[:, (np.newaxis)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>in taking the top mined samples based on their given probability that the NL-Code pair Split |E| |E|/|Q| x Intent Tokens y Snippet Tokens y Body Tokens z Mined 593837 40522 14.65?7.01 11.41?4.22 28.70?42.81 371.24?483.67 Table 1: Statistics for the CoNaLa dataset with data from the StackOverflow API. |E| is # of examples. |Q| number of questions. Values are reported as ? ? ? unless the column header has * . x Mean # of examples for a Question. Number of tokens in the body regardless of modalitiy. ? 12 of the 10K questions were removed because there was an issue with them.</figDesc><table><row><cell>Train 2376</cell><cell>1708</cell><cell>1.39?1.02</cell><cell cols="2">16.45?7.51</cell><cell>17.23?8.58</cell><cell>221.90?202.65</cell></row><row><cell>Test 498</cell><cell>364</cell><cell>1.37?0.88</cell><cell cols="2">15.98?6.62</cell><cell>18.47?12.90</cell><cell>208.04?164.74</cell></row><row><cell>Mined-10K 9988 ?</cell><cell>7181</cell><cell>1.39?0.80</cell><cell cols="2">11.29?3.94</cell><cell>16.58?9.27</cell><cell>297.53?367.09</cell></row><row><cell cols="4">Split Have Answer  *  Has Code Inline x</cell><cell>Blocks x</cell><cell cols="2">Code Tokens x NL Tokens x</cell></row><row><cell>Train 87.88%</cell><cell></cell><cell>85.95%</cell><cell cols="3">1.21?2.09 1.42?1.26 95.54?157.52</cell><cell>124.60?92.02</cell></row><row><cell>Test 87.09%</cell><cell></cell><cell>87.91%</cell><cell cols="3">1.08?1.87 1.50?1.26 88.21?116.01</cell><cell>118.52?79.51</cell></row><row><cell>Mined-10K 86.16%</cell><cell></cell><cell>84.00%</cell><cell cols="4">1.30?2.36 1.46?1.34 133.20?278.20 164.54?207.08</cell></row><row><cell>Mined 81.92%</cell><cell></cell><cell>81.83%</cell><cell cols="4">1.50?2.86 1.47?1.44 172.57?372.32 197.98?257.71</cell></row></table><note>* |Q|*y Per example.z</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detailed statistics for the StackOverflow questions. Mined-10K represents the top 10,000 samples selected from the Mined data based on their probability that they are a valid NL-Code pair.</figDesc><table /><note>* Percent of questions that have an accepted answer.x Per question body.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results compared to previous papers both with and without the use of the question body at inference. We do not calculate the Oracle BLEU for either of our models as our corpus BLEU already surpasses their Oracle BLEU. EK=Using External Knowledge. RR=Using Reranking. x Using only the rewritten intent, if available else normal intent, as input.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Baseline 26.24?0.31 67.53?0.46 44.10?0.60 29.80?0.69 84.08?1.27 +Mined 30.55?0.38 67.81?0.23 45.55?0.27 31.69?0.37 93.08?1.28 Body 34.35?1.01 69.97?0.89 49.74?0.99 36.62?0.97 81.44?2.25 -NL 34.06?0.48 68.29?0.48 47.91?0.45 35.33?0.40 81.92?0.75</figDesc><table><row><cell cols="2">Input Unigram  -Code BLEU 27.67?0.40 68.29?0.53 44.93?0.57 30.12?0.69 84.92?1.00</cell></row><row><cell>-Blocks</cell><cell>29.53?0.47 68.14?0.26 45.69?0.10 31.36?0.15 80.84?1.37</cell></row><row><cell>-Inline</cell><cell>33.57?0.94 70.50?0.27 49.56?0.40 36.54?0.46 82.16?1.53</cell></row><row><cell cols="2">Body+Mined 35.32?0.42 67.62?0.76 47.69?0.82 35.00?0.87 89.32?1.49</cell></row><row><cell>-NL</cell><cell>34.53?0.88 66.24?0.90 46.11?1.15 33.54?1.02 90.08?0.48</cell></row><row><cell>-Code</cell><cell>31.39?0.75 67.00?0.75 45.65?0.97 31.60?0.88 92.00?1.31</cell></row><row><cell>-Blocks</cell><cell>32.14?0.14 66.96?1.03 45.32?0.97 31.49?0.74 89.24?1.30</cell></row><row><cell>-Inline</cell><cell>35.06?0.49 67.04?1.54 46.99?1.29 34.31?1.04 89.20?0.42</cell></row></table><note>* Bigram* Trigram* Validx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation Experiments all with BART ran on 5 different random initializations. All tests have rewritten intent as input in addition to the input described in the Input column. The bolded ablation indicates our best performance while red text represents the worst performance.</figDesc><table><row><cell>Percent of generated snippets that are</cell></row></table><note>* Precisions.x</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Change in BLEU score for each ablation versus their respective baseline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Cheating Measurements calculated by Equation 2 using a single run but same seed and environment. C BB and C BT are the cheating w.r.t. BLEU Bigram and Trigram Precision. C R2 and C RL are the cheating w.r.t. ROUGE-2 and ROUGE-L. shared multigram subsequences with the question body than the ground truth does. In the ablations where code was removed from the body, C BT increased by only 0.98 and 1.86 with and without the mined data. This represents a percent of increase of only 16.25% and 26.92% over their respective baselines. However, in the case where all NL was removed, C BT increased by 17.35(287.73%) and 19.18(277.57%) points with respect to their baselines. The fact that these increases are lower than that when all modalities are included provides further evidence that BART is an unsupervised mul-timodal learner and understands the relationships between each modality. The NL likely provides both explicit and implicit hints about the importance of certain code spans.</figDesc><table><row><cell>4.5 Examples</cell></row><row><cell>Intent: multiply a matrix 'p' with a 3d tensor 't' in scipy</cell></row><row><cell>scipy.tensordot(P, T, axes=[1,</cell></row><row><cell>1]).swapaxes(0, 1)</cell></row><row><cell>x np.einsum('...j,...j-&gt;...', P, T)</cell></row><row><cell>y np.einsum('ij-&gt;ij-&gt;ik-&gt;j-&gt;ik', p)</cell></row><row><cell>x P.dot(T).transpose(1, 0, 2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Example intents and generated snippets. Screenshots of the questions are located in Appendix B and each intent links to the question. Red text indicates that it is incorrect while blue text marks correct tokens in the wrong place. ground truth. xEK+RR no body. yMined. zBody+Mined.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/gabeorlanski/stackoverflowencourages-cheating</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://stackoverflow.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://api.stackexchange.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://pytorch.org/ 5 We direct readers to for a full discussion of these challenges.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://conala-corpus.github.io/ 7 Actual Number is lower due to errors in the dataset preventing the usage of some examples.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://www.python.org/doc/sunset-python-2/ 12 https://docs.python.org/3/library/2to3.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">JuICe: A large scale distantly supervised dataset for open domain context-based code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajas</forename><surname>Agashe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1546</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5436" to="5446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<pubPlace>Amanda</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litwin</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>and Dario Amodei. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Expanding the scope of the ATIS task: The ATIS-3 corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><forename type="middle">A</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Hunicke-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Proceedings of a Workshop held at Plainsboro</title>
		<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-03-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coarse-to-fine decoding for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1068</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="731" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating code with the help of retrieved template functions and stack overflow answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changran</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Breslav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Code-BERT: A pre-trained model for programming and natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.139</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1536" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards complex text-to-SQL in crossdomain database with intermediate representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zecheng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1444</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4524" to="4535" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Retrieval-based neural code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Shirley Anugrah Hayati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravalika</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Avvaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Tomasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1111</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="925" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Codesearchnet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiferet</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonseok</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyeong</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Minjoon Seo. 2019. A comprehensive exploration on wikisql with table-aware word contextualization</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mapping language to code in programmatic context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1192</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1643" to="1652" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1057</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="599" to="609" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Codexglue: A machine learning benchmark dataset for code understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrosio</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">B</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno>abs/2102.04664</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1139" to="1149" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dobf: A deobfuscation pre-training objective for programming languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Baptiste Roziere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07492</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constrained language models yield few-shot semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emmanouil Antonios Platanios</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to map context-dependent sentences to executable formal queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1203</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2238" to="2249" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Treegen: A tree-based transformer architecture for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yican</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1911.09983</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Code and named entity recognition in StackOverflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeniya</forename><surname>Tabassum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounica</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.443</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4913" to="4926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence-based structured prediction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Incorporating external knowledge through pre-training for natural language to code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.538</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6045" to="6052" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11149</idno>
		<title level="m">-ide code generation from natural language: Promise and challenges</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Staqc: A systematically mined questioncode dataset from stack overflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1145/3178876.3186081</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference, WWW &apos;18</title>
		<meeting>the 2018 World Wide Web Conference, WWW &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1693" to="1703" />
		</imprint>
	</monogr>
	<note>Republic and Canton of Geneva. Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to mine aligned code and natural language pairs from stack overflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3196398.3196408</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="476" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TRANX: A transition-based neural abstract syntax parser for semantic parsing and code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reranking for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1447</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4553" to="4559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-SQL task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingning</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanelle</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1425</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3911" to="3921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
	<note>AAAI&apos;96</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, UAI&apos;05</title>
		<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence, UAI&apos;05<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A Error Categories Error Count General Invalid Syntax Paranthesis Matching Other Matching Baseline 61</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">No Print Has Print in Snippet Has Print in Body Has Print in Both Baseline</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
	<note>Percentages of syntax errors for ablations in a single run</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Percentage of valid snippets based on the presence of print. B Full Questions for Examples (a) Full Stack Overflow Question for Example 1 in Table 7</title>
		<ptr target="https://stackoverflow.com/questions/4490961/numpy-multiplying-a-matrix-with-a-3d-tensor-suggestion" />
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Full Stack Overflow Question for Example 2 in Table 7</title>
		<ptr target="https://stackoverflow.com/questions/13550423/python-printing-without-commas" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Full Stack Overflow Question for Example 3 in Table 7</title>
		<ptr target="https://stackoverflow.com/questions/13954222/how-to-join-mixed-list-array-with-integers-in-it-in-python" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
