<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Spatio-Temporal Random Fields for Efficient Video Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
							<email>siddhartha.chandra@inria.frcoupriec@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">INRIA GALEN</orgName>
								<orgName type="institution">Ecole CentraleSup?lec Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
							<email>iasonask@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Spatio-Temporal Random Fields for Efficient Video Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we introduce a time-and memory-efficient method for structured prediction that couples neuron decisions across both space at time. We show that we are able to perform exact and efficient inference on a denselyconnected spatio-temporal graph by capitalizing on recent advances on deep Gaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a) efficient, (b) has a unique global minimum, and (c) can be trained end-to-end alongside contemporary deep networks for video understanding. We experiment with multiple connectivity patterns in the temporal domain, and present empirical improvements over strong baselines on the tasks of both semantic and instance segmentation of videos. Our implementation is based on the Caffe2 framework and will be available at https://github.com/ siddharthachandra/gcrf-v3.0.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video understanding remains largely unsolved despite significant improvements in image understanding over the past few years. The accuracy of current image classification and semantic segmentation models is not yet matched in action recognition and video segmentation, to some extent due to the lack of large-scale benchmarks, but also due to the complexity introduced by the time variable. Combined with the increase in memory and computation demands, video understanding poses additional challenges that call for novel methods.</p><p>Our objective in this work is to couple the decisions taken by a neural network in time, in a manner that allows information to flow across frames and thereby result in decisions that are consistent both spatially and temporally. Towards this goal we pursue a structured prediction approach, where the structure of the output space is exploited in order to train classifiers of higher accuracy. For this we introduce VideoGCRF, an extension into video segmentation of the Deep Gaussian Random Field (DGRF) technique recently T T Inputs U S S U CRF inference Outputs FCN <ref type="figure" target="#fig_0">Figure 1</ref>: Overview of our VideoGCRF approach: we jointly segment multiple images by passing them firstly through a fully convolutional network to obtain per-pixel class scores ('unary' terms U), alongside with spatial (S) and temporal (T) embeddings. We couple predictions at different spatial and temporal positions in terms of the inner product of their respective embeddings, shown here as arrows pointing to a graph edge. The final prediction is obtained by solving a linear system; this can eliminate spurious responses, e.g. on the left pavement, by diffusing the per-pixel node scores over the whole spatio-temporal graph. The CRF and CNN architecture is jointly trained end-toend, while CRF inference is exact and particularly efficient.</p><p>proposed for single-frame structured prediction in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>We show that our algorithm can be used for a variety of video segmentation tasks: semantic segmentation (CamVid dataset), instance tracking (DAVIS dataset), and a combination of instance segmentation with Mask-RCNN-style object detection, customized in particular for the person class (DAVIS Person dataset).</p><p>Our work inherits all favorable properties of the DGRF method: in particular, our method has the advantage of delivering (a) exact inference results through the solution of a linear system, rather than relying on approximate mean-field inference, as <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, (b) allowing for exact computation of the gradient during back-propagation, thereby alleviating the need for the memory-demanding back-propagation-through-time used in <ref type="bibr" target="#b41">[42]</ref> (c) making it possible to use non-parametric terms for the pairwise term, rather than confining ourselves to pairwise terms of a predetermined form, as <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, and (d) facilitating inference on both densely-and sparsely-connected graphs, as well as facilitating blends of both graph topologies.</p><p>Within the literature on spatio-temporal structured prediction, the work that is closest in spirit to ours is the work of <ref type="bibr" target="#b25">[26]</ref> on Feature Space Optimization. Even though our works share several conceptual similarities, our method is entirely different at the technical level. In our case spatiotemporal inference is implemented as a structured, 'lateral connection' layer that is trained jointly with the feedforward CNNs, while the method of <ref type="bibr" target="#b25">[26]</ref> is applied at a post-processing stage to refine a classifier's results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Previous work</head><p>Structured prediction is commonly used by semantic segmentation algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> to capture spatial constraints within an image frame. These approaches may be extended naively to videos, by making predictions individually for each frame. However, in doing so, we ignore the temporal context, thereby ignoring the tendency of consecutive video frames to be similar to each other. To address this shortcoming, a number of deep learning methods employ some kind of structured prediction strategy to ensure temporal coherence in the predictions. Initial attempts to capture spatio-temporal context involved designing deep learning architectures <ref type="bibr" target="#b21">[22]</ref> that implicitly learn interactions between consecutive image frames. A number of subsequent approaches used Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> to capture interdependencies between the image frames. Other approaches have exploited optical flow computed from state of the art approaches <ref type="bibr" target="#b16">[17]</ref> as additional input to the network <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. Finally, <ref type="bibr" target="#b25">[26]</ref> explicitly capture temporal constraints via pairwise terms over probabilistic graphical models, but operate post-hoc, i.e. are not trained jointly with the underlying network.</p><p>In this work, we focus on three problems, namely (i) semantic and (ii) instance video segmentation as well as (iii) semantic instance tracking. Semantic instance tracking refers to the problem where we are given the ground truth for the first frame of a video, and the goal is to predict these instance masks on the subsequent video frames. The first set of approaches to address this task start with a deep network pretrained for image classification on large datasets such as Imagenet or COCO, and finetune it on the first frame of the video with labeled ground truth <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>, optionally leveraging a variety of data augmentation regimes <ref type="bibr" target="#b23">[24]</ref> to increase robustness to scale/pose variation and occlusion/truncation in the subsequent frames of the video. The second set of approaches poses this problem as a warping problem <ref type="bibr" target="#b30">[31]</ref>, where the goal is to warp the segmentation of the first frame using the images and optical flow as additional inputs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>A number of approaches have attempted to exploit tem-poral information to improve over static image segmentation approaches for video segmentation. Clockwork convnets <ref type="bibr" target="#b33">[34]</ref> were introduced to exploit the persistence of features across time and schedule the processing of some layers at different update rates according to their semantic stability. Similar feature flow propagation ideas were employed in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43]</ref>. In <ref type="bibr" target="#b28">[29]</ref> segmentations are warped using the flow and spatial transformer networks. Rather than using optical flow, the prediction of future segmentations <ref type="bibr" target="#b20">[21]</ref> may also temporally smooth results obtained frame-by-frame. Finally, the state-of-the-art on this task <ref type="bibr" target="#b14">[15]</ref> improves over PSPnet <ref type="bibr" target="#b40">[41]</ref> by warping the feature maps of a static segmentation CNN to emulate a video segmentation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">VideoGCRF</head><p>In this work we introduce VideoGCRF, extending the Deep Gaussian CRF approach introduced in [6, 7] to operate efficiently for video segmentation. Introducing a CRF allows us to couple the decisions between sets of variables that should be influencing each other; spatial connections were already explored in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and can be understood as propagating information from distinctive image positions (e.g. the face of a person) to more ambiguous regions (e.g. the person's clothes). In this work we also introduce temporal connections to integrate information over time, allowing us for instance to correctly segment frames where the object is not clearly visible by propagating information from different time frames.</p><p>We consider that the input to our system is a video V = {I 1 , I 2 , . . . , I V } containing V frames. We denote our network's prediction as x v , v = 1, . . . , V , where at any frame the prediction x i ? R P L provides a real-valued vector of scores for the L classes for each of the P image patches; for brevity, we denote by N = P ? L the number of prediction variables. The L scores corresponding to a patch can be understood as inputs to a softmax function that yields the label posteriors.</p><p>The Gaussian-CRF (or, G-CRF) model defines a joint posterior distribution through a Gaussian multivariate density for a video as:</p><formula xml:id="formula_0">p(x|V) ? exp(? 1 2 x A V x + B V x),</formula><p>where B V , A V denote the 'unary' and 'pairwise' terms respectively, with</p><formula xml:id="formula_1">B V ? R N V and A V ? R N V ?N V .</formula><p>In the rest of this work we assume that A, B depend on the input video and we omit the conditioning on V for convenience.</p><p>What is particular about the G-CRF is that, assuming the matrix of pairwise terms A is positive-definite, the Maximum-A-Posterior (MAP) inference merely amounts to solving the system of linear equations Ax = B. In fact, as in <ref type="bibr" target="#b5">[6]</ref>, we can drop the probabilistic formulation and treat the G-CRF as a structured prediction module that is part Predictions Input Images</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G-CRF Inference</head><p>Linear System Spatial Unary Temporal Spatial Unary Temporal <ref type="figure">Figure 2</ref>: VideoGCRF schematic for 2 video frames. Our network takes in 2 input images, and delivers the per frame unaries b 1 , b 2 , spatial embeddings A 1 , A 2 , and temporal embeddings T 1 , T 2 in the feed-forward mode. Our VideoGCRF module collects these and solves the inference problem in Eq. 2 to recover predictions x 1 , x 2 . During backward pass, the gradients of the predictions are delivered to the VideoGCRF model. It uses these to compute the gradients for the unary terms as well as the spatio-temporal embeddings and back-propagates them through the network.</p><p>of a deep network. In the forward pass, the unary and the pairwise terms B and A, delivered by a feed-forward CNN described in Sec. 2.1 are fed to the G-CRF module which performs inference to recover the prediction x by solving a system of linear equations given by</p><formula xml:id="formula_2">(A + ?I)x = B,<label>(1)</label></formula><p>where ? is a small positive constant added to the diagonal entries of A to make it positive definite.</p><p>For the single-frame case (V = 1) the iterative conjugate gradient <ref type="bibr" target="#b34">[35]</ref> algorithm was used to rapidly solve the resulting system for both sparse <ref type="bibr" target="#b5">[6]</ref> and fully connected <ref type="bibr" target="#b6">[7]</ref> graphs; in particular the speed of the resulting inference is in the order of 30ms on the GPU, almost two orders of magnitude faster than the implementation of DenseCRF <ref type="bibr" target="#b24">[25]</ref>, while at the same time giving more accurate results.</p><p>Our first contribution in this work consists in designing the structure of the matrix A V so that the resulting system solution remains manageable as the number of frames increases. Once we describe how we structure A V , we then will turn to learning our network in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Spatio-temporal connections</head><p>In order to capture the spatio-temporal context, we are interested in capturing two kinds of pairwise interactions: (a) pairwise terms between patches in the same frame and (b) pairwise terms between patches in different frames.</p><p>Denoting the spatial pairwise terms at frame v by A v and the temporal pairwise terms between frames u, v as T u,v we can rewrite Eq. 1 as follows:</p><formula xml:id="formula_3">? ? ? ? ? A 1 + ?I T 1,2 ? ? ? T 1,V T 2,1 A 2 + ?I ? ? ? T 2,V . . . T V,1 T V,2 ? ? ? A V + ?I ? ? ? ? ? ? ? ? ? ? x 1 x 2 . . . x V ? ? ? ? ? = ? ? ? ? ? b 1 b 2 . . . b V ? ? ? ? ? ,<label>(2)</label></formula><p>where we group the variables by frames. Solving this system allows us to couple predictions x v across all video frames v ? {1, . . . , V }, positions, p and labels l. If furthermore</p><formula xml:id="formula_4">A v = A T v , ?v and T u,v = T T v,u , ?u</formula><p>, v then the resulting system is positive definite for any positive ?.</p><p>We now describe how the pairwise terms A v , T u,v are constructed through our CNN, and then discuss acceleration of the linear system in Eq. 2 by exploiting its structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Connections:</head><p>We define the spatial pairwise terms in terms of inner products of pixel-wise embeddings, as in <ref type="bibr" target="#b6">[7]</ref>. At frame v we couple the scores for a pair of patches p i , p j taking the labels l m , l n respectively as follows:</p><formula xml:id="formula_5">A v,pi,pj (l m , l n ) = A lm v,pi , A ln v,pj ,<label>(3)</label></formula><p>where i, j ? {1, . . . , P } and m, n ? {1, . . . , L}, v ? {1, . . . , V }, and A ln v,pj ? R D is the embedding associated to point p j . In Eq. 3 the A ln v,pj terms are image-dependent and delivered by a fully-convolutional "embedding" branch that feeds from the same CNN backbone architecture, and is denoted by A v in <ref type="figure">Fig. 2</ref>.</p><p>The implication of this form is that we can afford inference with a fully-connected graph. In particular the rank of the block matrix A v = A v A v , equals the embedding dimension D, which means that both the memory-and time-complexity of solving the linear system drops from O(N 2 ) to O(N D), which can be several orders of magnitude smaller. Thus, A v ? R N ?D Temporal Connections: Turning to the temporal pairwise terms, we couple patches p i , p j coming from different frames u, v taking the labels l m , l n respectively as</p><formula xml:id="formula_6">T u,v,pi,pj (l m , l n ) = T lm u,pi , T ln v,pj ,<label>(4)</label></formula><p>where u, v ? {1, . . . , V }. The respective embedding terms are delivered by a branch of the network that is separate, temporal embedding network denoted by T v in <ref type="figure">Fig. 2</ref>.</p><p>In short, both the spatial pairwise and the temporal pairwise terms are composed as Gram matrices of spatial and temporal embeddings as</p><formula xml:id="formula_7">A v = A v A v , and T u,v = T u T v .</formula><p>We visualize our spatio-temporal pairwise terms in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VideoGCRF in Deep Learning:</head><p>Our proposed spatiotemporal Gaussian CRF (VideoGCRF) can be viewed as generic deep learning modules for spatio-temporal structured prediction, and as such can be plugged in at any stage of a deep learning pipeline: either as the last layer, i.e. classifier, as in our semantic segmentation experiments (Sec. 3.3), or even in the low-level feature learning stage, as in our instance segmentation experiments (Sec. 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Efficient Conjugate-Gradient Implementation</head><p>We now describe an efficient implementation of the conjugate gradient method <ref type="bibr" target="#b34">[35]</ref>, described in Algorithm 1 that is customized for our VideoGCRFs. </p><formula xml:id="formula_8">? k := r T k r k p T k Ap k 6:</formula><p>x k+1 := x k + ? k p k 7:</p><formula xml:id="formula_9">r k+1 := r k ? ? k Ap k 8:</formula><p>if r k+1 is sufficiently small, then exit loop 9:</p><formula xml:id="formula_10">? k := r T k+1 r k+1 r T k r k 10: p k+1 := r k+1 + ? k p k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>k := k + 1 <ref type="bibr">12:</ref> end repeat <ref type="bibr" target="#b12">13</ref>:</p><formula xml:id="formula_11">x = x k+1</formula><p>The computational complexity of the conjugate gradient algorithm is determined by the computation of the matrixvector product q = Ap, corresponding to line :7 of Algorithm 1 (we drop the subscript k for convenience).</p><p>We now discuss how to efficiently compute q in a manner that is customized for this work. In our case, the matrix-vector product q = Ap is expressed in terms of the spatial (A) and temporal (T ) embeddings as follows:</p><formula xml:id="formula_12">? ? ? ? ? q 1 q 2 . . . q V ? ? ? ? ? = ? ? ? ? ? A T 1 A 1 + ?I T T 1 T 2 ? ? ? T T 1 T V T T 2 T 1 A T 2 A 2 + ?I ? ? ? T T 2 T V . . . T T V T 1 T T V T 2 ? ? ? A T V A V + ?I ? ? ? ? ? ? ? ? ? ? p 1 p 2 . . . p V ? ? ? ? ?</formula><p>(5) From Eq. 5, we can express q i as follows:</p><formula xml:id="formula_13">q i = A T i A i p i + ?p i + j =i T T i T j p j .<label>(6)</label></formula><p>One optimization that we exploit in computing q i efficiently is that we do not 'explicitly' compute the matrixmatrix products</p><formula xml:id="formula_14">A T i A i or T T i T j . We note that A T i A i p i can be decomposed into two matrix-vector products as A T i (A i p i )</formula><p>, where the expression in the brackets is evaluated first and yields a vector, which can then be multiplied with the matrix outside the brackets. This simplification alleviates the need to keep N ? N terms in memory, and is computationally cheaper.</p><p>Further, from Eq. 6, we note that computation of q i requires the matrix-vector product T j p j ?j = i. A black-box implementation would therefore involve redundant computations, which we eliminate by rewriting Eq. 6 as:</p><formula xml:id="formula_15">q i = A T i A i p i + ?p i + T T i ? ? ( j T j p j ) ? T i p i ? ? . (7)</formula><p>This rephrasing allows us to precompute and cache j T j p j , thereby eliminating redundant calculations. While so far we have assumed dense connections between the image frames, if we have sparse temporal connections (Sec. 3.1), i.e. each frame is connected to a subset of neighbouring frames in the temporal domain, the linear system matrix A is sparse, and q i is written as</p><formula xml:id="formula_16">q i = A T i A i p i + ?p i + j?N (i) T T i T j p j ,<label>(8)</label></formula><p>where N (i) denotes the temporal neighbourhood of frame i.</p><p>For very sparse connections caching may not be necessary because these involve little or no redundant computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Backward Pass</head><p>Since we rely on the Gaussian CRF we can get the backpropagation equation for the gradient of the loss with respect to the unary terms, b v , and the spatial/temporal embedding terms A v , T v in closed form. Thanks to this we do not have to perform back-propagation in time which was needed e.g. in <ref type="bibr" target="#b41">[42]</ref> for DenseCRF inference. Following <ref type="bibr" target="#b6">[7]</ref>, the gradients of the unary terms ?L ?bv are obtained from the solution of the following system:</p><formula xml:id="formula_17">? ? ? ? ? A 1 + ?I T 1,2 ? ? ? T 1,V T 2,1 A 2 + ?I ? ? ? T 2,V . . . T V,1 T V,2 ? ? ? A V + ?I ? ? ? ? ? ? ? ? ? ? ?L ?b1 ?L ?b2 . . . ?L ?b V ? ? ? ? ? = ? ? ? ? ? ?L ?x1 ?L ?x2 . . . ?L ?x V ? ? ? ? ?<label>(9)</label></formula><p>Once these are computed, the gradients of the spatial embeddings can be computed as follows:</p><formula xml:id="formula_18">?L ?A v = ? ?L ?b v ? x v I ? A v + A v ? I Q D,N<label>(10)</label></formula><p>while the gradients of the temporal embeddings are given by the following form:</p><formula xml:id="formula_19">?L ?T v = ? u ?L ?b u ? x v I ? T u + T u ? I Q D,N<label>(11)</label></formula><p>where Q D,N is a permutation matrix, as in <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Implementation and Inference Time</head><p>Our implementation is GPU based and exploits fast CUDA-BLAS linear algebra routines. It is implemented as a module in the Caffe2 library. For spatial and temporal embeddings of size 128, 12 classes (Sec. 3.3), a 321 ? 321 input image, and network stride of 8, our 2, 3, 4 frame inferences take 0.032s, 0.045s and 0.061s on average respectively. Without the caching procedure described in Sec. 2.2, the 4 frame inference takes 0.080s on average. This is orders of magnitude faster than the DenseCRF method <ref type="bibr" target="#b24">[25]</ref> which takes 0.2s on average for spatial CRF for a single input frame. These timing statistics were estimated on a GTX-1080 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>Experimental Setup. We describe the basic setup followed for our experiments. As in <ref type="bibr" target="#b6">[7]</ref>, we use a 3?phase training strategy for our methods. We first train the unary network without the spatio-temporal embeddings. We next train the subnetwork delivering the spatio-temporal embeddings with the softmax cross-entropy loss to enforce the following objectives: A p1,p2 (l 1 , l 2 ) &lt; A p1,p2 (l 1 = l 1 , l 2 = l 2 ), and T u,v,p1,p2 (l 1 , l 2 ) &lt; T u,v,p1,p2 (l 1 = l 1 , l 2 = l 2 ), where l 1 , l 2 are the ground truth labels for pixels p 1 , p 2 . Finally, we combine the unary and pairwise networks, and train them together in end-to-end fashion. Unless otherwise stated, we use stochastic gradient descent to train our networks with a momentum of 0.9 and a weight decay of 5e ?4 .</p><p>For segmentation experiments, we use a base-learning rate of 2.5e ?3 for training the unaries, 2.5e ?4 for training the spatial affinities temporal affinities FCN: Frame-by-frame segmentation VideoGCRF: Spatio-temporal segmentation <ref type="figure">Figure 3</ref>: Visualization of instance segmentation through VideoGCRF: In row 1 we focus on a single point of the CRF graph, shown as a cross, and show as a heatmap its spatial (inter-frame) and temporal (intra-frame) affinities to all other graph nodes. These correspond to a single column of the linear system in Eq. 2. In row 2 we show the predictions that would be obtained by frame-by-frame segmentation, relying exclusively on the FCN's unary terms, while in row 3 we show the results obtained after solving the VideoGCRF inference problem. We observe that in frame-by-frame segmentation a second camel is incorrectly detected due to its similar appearance properties. However, VideoGCRF inference exploits temporal context and focuses solely on the correct object. embeddings, and 1e ?4 for finetuning the unary and embeddings together, using a polynomial-decay with power of 0.9. For the instance segmentation network, we use a single stage training for the unary and pairwise streams: we train the network for 16K iterations, with a base learning rate of 0.01 which is reduced to 0.001 after 12K iterations. The weight decay is 1e ?4 . For our instance tracking experiments, we use unaries from <ref type="bibr" target="#b37">[38]</ref> and do not refine them, rather use them as an input to our network. We employ horizontal flipping and scaling by factors between 0.5 and 1.5 during training/testing for all methods, except in the case of instance segmentation experiments (Sec. 3.1).</p><p>Datasets. We use the three datasets for our experiments: DAVIS. The DAVIS dataset <ref type="bibr" target="#b31">[32]</ref> consists of 30 training and 20 validation videos containing 2079 and 1376 frames respectively. Each video comes with manually annotated segmentation masks for foreground object instances.</p><p>DAVIS-Person. While the DAVIS dataset <ref type="bibr" target="#b32">[33]</ref> provides </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G-CRF</head><p>Masks RoI-Pool <ref type="figure">Figure 5</ref>: Spatio-temporal structured prediction in Mask-RCNN. Here we use CRFs in the feature learning stage before the ROI-Pooling (and not as the final classifier). This helps learn mid-level features which are better aware of the spatio-temporal context. densely annotated frames for instance segmentation, it lacks object category labels. For category prediction tasks such as semantic and instance segmentation, we create a subset of the DAVIS dataset containing videos from the category person. By means of visual inspection, we select 35 and 18 video sequences from the training and validation sets respectively containing 2463 training and 1182 validation images, each containing at least one person. Since the DAVIS dataset comes with only the foreground instances labeled, we manually annotate the image regions containing unannotated person instances with the do-not-care label. These image regions do not participate in the training or the evaluation. We call this the DAVIS-person dataset.</p><p>CamVid. The CamVid dataset <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, is a dataset containing videos of driving scenarios for urban scene understanding. It comes with 701 images annotated with pixellevel category labels at 1 fps. Although the original dataset comes with 32 class-labels, as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20]</ref>, we predict 11 semantic classes and use the train-val-test split of 367, 101 and 233 frames respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ablation Study on Semantic and Instance Segmentation Tasks</head><p>In these experiments, we use the DAVIS Person dataset described in Sec. 3. The aim here is to explore the various design choices available to us when designing networks for spatio-temporal structured prediction for semantic segmentation, and proposal-based instance segmentation tasks.</p><p>Semantic Segmentation Experiments. Our first set of experiments studies the effect of varying the sizes of the spatial and temporal embeddings, the degree of the temporal connections, and multi-scale temporal connections for VideoGCRF. For these set of experiments, our baseline network, or base-net is a single resolution ResNet-101 network, with altered network strides as in <ref type="bibr" target="#b8">[9]</ref> to produce a spatial down-sampling factor of 8. The evaluation metric used is the mean pixel Intersection over Union (IoU).</p><p>In <ref type="table">Table 1</ref> we study the effect of varying the sizes of the spatial and temporal embeddings for 2?frame inference. Our best results are achieved at spatio-temporal embeddings of size 128. The improvement over the base-net is 4.2%. In subsequent experiments we fix the size of our embeddings to 128. We next study the effect of varying the size of the temporal context and temporal neighbourhoods. The temporal context is defined as the number of video frames V which are considered simultaneously in one linear system (Eq. 2). The temporal context V is limited by the GPU RAM: for a ResNet-101 network, an input image of size 321 ? 321, embeddings of size 128, we can currently fit V = 7 frames on 12 GB of GPU RAM. Since V is smaller than the number of frames in the video, we divide the video into overlapping sets of V frames, and average the predictions for the common frames.</p><p>The temporal neighbourhood for a frame <ref type="figure" target="#fig_1">(Fig. 4)</ref> is defined as the number of frames it is directly connected to via pairwise connections. A fully connected neighbourhood (fc?) is one in which there are pairwise terms between every pair of frames available in the temporal context. We experiment with 2?, 4?, multiscale 6 ms ? and fc? connections. The 6 ms ? neighbourhood connects a frame to neighbours at distances of 2 0 , 2 1 and 2 2 (or 1, 2, 4) frames on either side. <ref type="table" target="#tab_1">Table 2</ref> reports our results for different combinations of temporal neighbourhood and context. It can be seen that dense connections improve performance for smaller temporal contexts, but for a temporal context of 7 frames, an increase in the complexity of temporal connections leads to a moderate decrease in performance. This could be a consequence of the long-range interactions having the same weight as short-range interactions. In the future we intend to mitigate this issue by complementing our embeddings with the temporal distance between frames.</p><p>Instance Segmentation Experiments. We now demonstrate the utility of our VideoGCRF method for the task of proposal-based instance segmentation. Our hypothesis is that coupling predictions across frames is advantageous for instance segmentation methods. We actually show that the performance of the instance segmentation methods improves as we increase the temporal context via VideoGCRF, and obtain our best results with fully-connected temporal neighbourhoods. Our baseline for this task is the Mask-  RCNN framework of <ref type="bibr" target="#b15">[16]</ref> using the ResNet-50 network as the convolutional body. The Mask-RCNN framework uses precomputed bounding box proposals for this task. It computes convolutional features on the input image using the convolutional body network, crops out the features corresponding to image regions in the proposed bounding boxes via Region-Of-Interest (RoI) pooling, and then has 3 head networks to predict (i) class scores and bounding box regression parameters, (ii) keypoint locations, and (iii) instance masks. Structured prediction coupling the predictions of all the proposals over all the video frames is a computationally challenging task, since typically we have 100 ? 1000s of proposals per image, and it is not obvious which proposals from one frame should influence which proposals in the other frame. To circumvent this issue, we use our VideoGCRF before the RoI pooling stage as shown in <ref type="figure">Fig. 5</ref>. Instead of coupling final predictions, we thereby couple mid-level features over the video frames, thereby improving the features which are ultimately used to make predictions.</p><p>For evaluation, we use the standard COCO performance metrics: AP 50 , AP 75 , and AP (averaged over IoU thresholds), evaluated using mask IoU. <ref type="table" target="#tab_2">Table 3</ref> reports our instance segmentation results. We note that the performance of the Mask-RCNN framework increases consistently as we increase the temporal context for predictions. Qualitative results are available in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>AP 50 AP 75 AP ResNet50-baseline 0.610 0.305 0.321 spatial CRF <ref type="bibr" target="#b6">[7]</ref> 0.618 0.310 0.329 2-frame VideoGCRF 0.619 0.310 0.331 3-frame VideoGCRF 0.631 0.321 0.330 4-frame VideoGCRF 0.647 0.336 0.349  <ref type="bibr" target="#b30">[31]</ref> 79.7 OSVOS <ref type="bibr" target="#b4">[5]</ref> 79.8 Online Adaptation <ref type="bibr" target="#b37">[38]</ref> 85.6 Online Adaptation + Spatial CRF <ref type="bibr" target="#b6">[7]</ref> 85.9 Online Adaptation + 2-Frame VideoGCRF 86.3 Online Adaptation + 3-Frame VideoGCRF 86.5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instance Tracking</head><p>We use the DAVIS dataset described in Sec. 3. Instance tracking involves predicting foreground segmentation masks for each video frame given the foreground segmentation for the first video frame. We demonstrate that incorporating temporal context helps improve performance in instance tracking methods. To this end we extend the online adaptation approach of <ref type="bibr" target="#b37">[38]</ref> which is the state-of-the-art approach on the DAVIS benchmark with our VideoGCRF. We use their publicly available software based on the Ten-sorFlow library to generate the unary terms for each of the frames in the video, and keep them fixed. We use a ResNet-50 network to generate spatio-temporal embeddings and use these alongside the unaries computed from <ref type="bibr" target="#b37">[38]</ref>. The results are reported in <ref type="table" target="#tab_3">table Table 4</ref>. We compare performance of VideoGCRF against that of just the unaries from <ref type="bibr" target="#b37">[38]</ref>, and also with spatial CRFs from <ref type="bibr" target="#b6">[7]</ref>. The evaluation criterion is the mean pixel-IoU. It can be seen that temporal context improves performance. We hypothesize that re-implementing the software from <ref type="bibr" target="#b37">[38]</ref> in Caffe2 and back-propagating on the unary branch of the network would yield further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantic Segmentation on CamVid Dataset</head><p>We now employ our VideoGCRF for the task of semantic video segmentation on the CamVid dataset. Our base network here is our own implementation of ResNet-101 with pyramid spatial pooling as in <ref type="bibr" target="#b40">[41]</ref>. Additionally, we pretrain our networks on the Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref>, and report results both with and without pretraining on Cityscapes. We report improvements over the baseline networks in both settings. Without pretraining, we see an improvement of 1.3%    <ref type="figure">Figure 6</ref>: Qualitative results on the CamVid dataset. We note that the temporal context from neighbouring frames helps improve the prediction of the truck on the right in the first video, and helps distinguish between the road and the pavement in the second video, overall giving us smoother predictions in both cases.</p><p>over the base-net, and with pretraining we see an improvement of 1.9%. The qualitative results are shown in <ref type="figure">Fig. 6</ref>. We notice that VideoGCRF benefits from temporal context, yielding smoother predictions across video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this work, we propose VideoGCRF, an end-to-end trainable Gaussian CRF for efficient spatio-temporal structured prediction. We empirically show performance improvements on several benchmarks thanks to an increase of the temporal context. This additional functionality comes at negligible computational overhead owing to efficient implementation and the strategies to eliminate redundant computations. In future work we want to incorporate optical flow techniques in our framework as they provide a natural means to capture temporal correspondence. Further, we also intend to use temporal distance between frames as an additional term in the expression of the pairwise interactions alongside dot-products of our embeddings. We would also like to use VideoGCRF for dense regression tasks such as depth estimation. Finally, we believe that our method for spatio-temporal structured prediction can prove useful in the unsupervised and semi-supervised setting. <ref type="figure">Figure 7</ref>: Instance Segmentation results on the DAVIS Person Dataset. We observe that prediction based on unary terms alone leads to missing instances and some false predictions. These errors are corrected by VideoGCRFs, which smooth the predictions by taking into account the temporal context.</p><p>As described in the manuscript, to capture the spatiotemporal context, we propose two kinds of pairwise interactions: (a) pairwise terms between patches in the same frame (spatial pairwise terms), and (b) pairwise terms between patches in different frames (temporal pairwise terms).</p><p>Denoting the spatial pairwise terms at frame v by A v and the temporal pairwise terms between frames u, v as T u,v , our inference equation is written as</p><formula xml:id="formula_20">? ? ? ? ? A 1 + ?I T 1,2 ? ? ? T 1,V T 2,1 A 2 + ?I ? ? ? T 2,V . . . T V,1 T V,2 ? ? ? A V + ?I ? ? ? ? ? ? ? ? ? ? x 1 x 2 . . . x V ? ? ? ? ? = ? ? ? ? ? b 1 b 2 . . . b V ? ? ? ? ? ,<label>(12)</label></formula><p>where we group the variables by frames. Solving this system allows us to couple predictions x v across all video frames v ? {1, . . . , V }, positions, p and labels l. If furthermore A v = A v , ?v and T u,v = T v,u , ?u, v then the resulting system is positive definite for any positive ?.</p><p>As in the manuscript, at frame v we couple the scores for a pair of patches p i , p j taking the labels l m , l n respectively as follows:</p><formula xml:id="formula_21">A v,pi,pj (l m , l n ) = A lm v,pi , A ln v,pj ,<label>(13)</label></formula><p>where i, j ? {1, . . . , P } and m, n ? {1, . . . , L}, v ? {1, . . . , V }, and A ln v,pj ? R D is the embedding associated to point p j .</p><p>Thus, A v ? R N ?D , where N = P ? L. Further, to design the temporal pairwise terms, we couple patches p i , p j coming from different frames u, v taking the labels l m , l n respectively as</p><formula xml:id="formula_22">T u,v,pi,pj (l m , l n ) = T lm u,pi , T ln v,pj ,<label>(14)</label></formula><p>where u, v ? {1, . . . , V }. In short, both the spatial pairwise and the temporal pairwise terms are composed as Gram matrices of spatial and temporal embeddings as</p><formula xml:id="formula_23">A v = A v A v , and T u,v = T u T v .</formula><p>Using the definitions from Eq. 13 and Eq. 14, we can rewrite the inference equation as</p><formula xml:id="formula_24">? ? ? ? ? A T 1 A 1 + ?I T T 1 T 2 ? ? ? T T 1 T V T T 2 T 1 A T 2 A 2 + ?I ? ? ? T T 2 T V . . . T T V T 1 T T V T 2 ? ? ? A T V A V + ?I ? ? ? ? ? ? ? ? ? ? x 1 x 2 . . . x V ? ? ? ? ? = ? ? ? ? ? b 1 b 2 . . . b V ? ? ? ? ?<label>(15)</label></formula><p>From Eq. 15, we can express b v as follows:</p><formula xml:id="formula_25">b v = A T v A v x v + ?x v + u =v T T v T u x u ,<label>(16)</label></formula><p>which can be compactly written as</p><formula xml:id="formula_26">b v = A v x v + ?x v + u =v T v,u x u .<label>(17)</label></formula><p>We will use Eq. 17 to derive gradient expressions for ?Av ?L and ?Tv ?L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gradients of the Unary Terms</head><p>As in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, the gradients of the unary terms ?bv ?L are obtained from the solution of the following system of linear equations:</p><formula xml:id="formula_27">? ? ? ? ? A T 1 A 1 + ?I T T 1 T 2 ? ? ? T T 1 T V T T 2 T 1 A T 2 A 2 + ?I ? ? ? T T 2 T V . . . T T V T 1 T T V T 2 ? ? ? A T V A V + ?I ? ? ? ? ? ? ? ? ? ? ?L ?b1 ?L ?b2 . . . ?L ?bV ? ? ? ? ? = ? ? ? ? ? ?L ?x1 ?L ?x2 . . . ?L ?xV ? ? ? ? ? ,<label>(18)</label></formula><p>where L is the network loss. Once we have ?L ?bv , we use it to compute the gradients of the spatio-temporal embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gradients of the Spatial Embeddings</head><p>We begin with the observation that computing ?Av ?L requires us to first derive the expression for ?Av ?L . To this end, we ignore terms from Eq. 17 that do not depend on b v or A v and write it as b v = A v x v + c. We now use the result from <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> that when</p><formula xml:id="formula_28">A v x v = b v ,</formula><p>the gradients of A v are expressed as</p><formula xml:id="formula_29">?L ?A v = ? ?L ?b v ? x v ,<label>(19)</label></formula><p>where ? denotes the Kronecker product operator. To compute ?Av ?L , we use the chain rule of differentiation as follows:</p><formula xml:id="formula_30">?L ?A v = ?L ?A v ?A v ?A v = ?L ?A v ? ?A v A T v A v ,<label>(20)</label></formula><formula xml:id="formula_31">where A v = A T v A v , by definition.</formula><p>We know the expression for ?L ?Av from Eq. 19, but to obtain the expression for ? ?Av A T v A v we define a permutation matrix Q m,n of size mn ? mn (as in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>) as follows: </p><p>where vec(M ) is the vectorization operator that vectorizes a matrix M by stacking its columns. Thus, the operator Q m,n is a permutation matrix, composed of 0s and 1s, and has a single 1 in each row and column. When premultiplied with another matrix, Q m,n rearranges the ordering of rows of that matrix, while when postmultiplied with another matrix, Q m,n rearranges its columns. Using this matrix, we can form the following expression <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_33">? ?A v A T v A v = I ? A T v + A T v ? I Q D,N ,<label>(22)</label></formula><p>where I is the N ? N identity matrix. Substituting Eq. 19 and Eq. 22 into Eq. 20, we obtain:</p><formula xml:id="formula_34">?L ?A v = ? ?L ?b v ? x v I ? A v + A v ? I Q D,N .<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gradients of Temporal Embeddings</head><p>As in the last section, from Eq. 17, we ignore any terms that do not depend on b v or T v,u and write it as b v = c + u =v T v,u x u . Using the strategies in the previous section and the sum rule of differentiation, the gradients of the temporal embeddings are given by the following form:</p><formula xml:id="formula_35">?L ?T v = ? u ?L ?b u ? x v I ? T u + T u ? I Q D,N<label>(24)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Conjugate Gradient Algorithm 1: procedure CONJUGATEGRADIENT 2: Input: A, B, x 0 Output: x | Ax = B 3: r 0 := B ? Ax 0 ; p 0 := r 0 ; k := 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Temporal neighbourhoods in our ablation study: boxes denote video frames and the arcs connecting them are pairwise connections. The frame in red has all neighbours present in the temporal context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) image (b) base-net (c) spatial G-CRF (d) st-G-CRF (e) GT (a) image (b) base-net (c) spatial G-CRF (d) st-G-CRF (e) GT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Q</head><label></label><figDesc>m,n vec(M ) = vec(M T ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study: mean IoU on the DAVIS-person dataset. Here we study the effect of varying the size of the temporal context and neighbourhood.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Instance Segmentation using ResNet-50 Mask R-CNN on the Davis Person Dataset</figDesc><table><row><cell>Method</cell><cell>mean IoU</cell></row><row><cell>Mask Track</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Instance Tracking on the Davis val Dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results on CamVid dataset. We compare our results with some of the previously published methods, as well as our own implementation of the ResNet-101 network which serves as our base network.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Gradient Expressions for Spatio-Temporal G-CRF Parameters</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequence segmentation using joint RNN and structured prediction models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cibelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2422" to="2426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
	</analytic>
	<monogr>
		<title level="m">ArXiV CoRR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep Gaussian CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dense and low-rank Gaussian CRFs using deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Deep Structured Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scharwachter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Notes on matrix calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Fackler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic video CNNs through representation warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05384</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1175" to="1183" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Video scene parsing with predictive feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1612.00119</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoderdecoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.02680</idno>
	</analytic>
	<monogr>
		<title level="m">ArXiV CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature space optimization for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3168" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Video object segmentation with re-identification. CVPR workshops -The 2017 DAVIS Challenge on Video Object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semantic video segmentation by gated recurrent flow propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno>abs/1612.08871</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1505.04366</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Clockwork convnets for video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1608.03609</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An introduction to the conjugate gradient method without the agonizing pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Shewchuk</surname></persName>
		</author>
		<ptr target="https://www.cs.cmu.edu/?quake-papers/painless-conjugate-gradient.pdf" />
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gaussian conditional random field network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellapa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reseg: A recurrent neural network-based model for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context-aware CNNs for person head detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2893" to="2901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/1612.01105</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno>abs/1611.07715</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
