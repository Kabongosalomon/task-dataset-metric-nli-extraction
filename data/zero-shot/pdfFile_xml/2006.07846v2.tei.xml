<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GLOBAL ATTENTION IMPROVES GRAPH NETWORKS GENERALIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Puny</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GLOBAL ATTENTION IMPROVES GRAPH NETWORKS GENERALIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper advocates incorporating a Low-Rank Global Attention (LRGA) module, a computation and memory efficient variant of the dot-product attention (Vaswani et al., 2017), to Graph Neural Networks (GNNs) for improving their generalization power. To theoretically quantify the generalization properties granted by adding the LRGA module to GNNs, we focus on a specific family of expressive GNNs and show that augmenting it with LRGA provides algorithmic alignment to a powerful graph isomorphism test, namely the 2-Folklore Weisfeiler-Lehman (2-FWL) algorithm. In more detail we: (i) consider the recent Random Graph Neural Network (RGNN) (Sato et al., 2020) framework and prove that it is universal in probability; (ii) show that RGNN augmented with LRGA aligns with 2-FWL update step via polynomial kernels; and (iii) bound the sample complexity of the kernel's feature map when learned with a randomly initialized two-layer MLP. From a practical point of view, augmenting existing GNN layers with LRGA produces state of the art results in current GNN benchmarks. Lastly, we observe that augmenting various GNN architectures with LRGA often closes the performance gap between different models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In many domains, data can be represented as a graph, where entities interact, have meaningful relations and a global structure. The need to be able to infer and gain a better understanding of such data rises in many instances such as social networks, citations and collaborations, chemoinformatics, epidemiology etc. In recent years, along with the major evolution of artificial neural networks, graph learning has also gained a new powerful tool -graph neural networks (GNNs). Since first originated <ref type="bibr" target="#b14">(Gori et al., 2005;</ref><ref type="bibr" target="#b37">Scarselli et al., 2009</ref>) as recurrent algorithms, GNNs have become a central interest and the main tool in graph learning.</p><p>Perhaps the most commonly used family of GNNs are message-passing neural networks <ref type="bibr" target="#b12">(Gilmer et al., 2017)</ref>, built by aggregating messages from local neighborhoods at each layer. Since information is only kept at the vertices and propagated via the edges, these models' complexity scales linearly with |V | + |E|, where |V | and |E| are the number of vertices and edges in the graph, respectively. In a recent analysis of the expressive power of such models, <ref type="bibr">(Xu et al., 2019a;</ref><ref type="bibr" target="#b32">Morris et al., 2018)</ref> have shown that message-passing neural networks are at most as powerful as the first Weisfeiler-Lehman (WL) test, also known as vertex coloring. The k-WL tests, are a hierarchy of increasing power and complexity algorithms aimed at solving graph isomorphism. This bound on the expressive power of GNNs led to the design of new architectures <ref type="bibr" target="#b32">(Morris et al., 2018;</ref><ref type="bibr" target="#b30">Maron et al., 2019a)</ref> mimicking higher orders of the k-WL family, resulting in more powerful, yet complex, models that scale super-linearly in |V | + |E|, hindering their usage for larger graphs.</p><p>Although expressive power bounds on GNNs exist, empirically in many datasets, GNNs are able to fit the train data well. This indicates that the expressive power of these models might not be the main roadblock to a successful generalization. Therefore, we focus our efforts in this paper on strengthening GNNs from a generalization point of view. Towards improving the generalization of GNNs we propose the Low-Rank Global Attention (LRGA) module which can be augmented to any GNN. Standard dot-product global attention modules <ref type="bibr">(Vaswani et al., 2017)</ref> apply |V | ? |V | attention matrix to node data with O(|V | 3 ) computational complexity making them impractical for large graphs. To overcome this barrier, we define a ?-rank attention matrix, where ? is a parameter, that requires O(?|V |) memory and can be applied in O(? 2 |V |) computational complexity.</p><p>To theoretically justify LRGA we focus on a GNN model family possessing maximal expressiveness (i.e., universal) but vary in the generalization properties of the family members. <ref type="bibr" target="#b33">(Murphy et al., 2019;</ref><ref type="bibr" target="#b27">Loukas, 2019;</ref><ref type="bibr" target="#b7">Dasoulas et al., 2019;</ref><ref type="bibr" target="#b28">Loukas, 2020)</ref> showed that adding node identifiers to GNNs improves their expressiveness, often making them universal. In this work, we prove that even adding random features to the network's input, as suggested in <ref type="bibr" target="#b36">(Sato et al., 2020)</ref>, a framework we call Random Graph Neural Network (RGNN), GNN models are universal in probability.</p><p>The improved generalization properties of LRGA-augmented GNN models is then showcased for the RGNN framework, where we show that augmenting it with LRGA algorithmically aligns with the 2-folklore WL (FWL) algorithm; 2-FWL is a strictly more powerful graph isomorphism algorithm than vertex coloring (which bounds message passing GNNs). To do so, we adopt the notion of algorithmic alignment introduced in <ref type="bibr">(Xu et al., 2019b)</ref>, stating that a neural network aligns with some algorithm if it can simulate it with simple modules, resulting in provable improved generalization. We opt to use monimials in the role of simple modules and prove the alignment using polynomial kernels. Lastly, we bound the sample complexity of the model when learning the 2-FWL update rule. Although our bound is exponential in the graph size, it nevertheless implies that RGNN augmented with LRGA can provably learn the 2-FWL step, when training each module independently with two-layer MLP. We evaluate our model on a set of benchmark datasets including tasks of graph classification and regression, node labeling and link prediction from <ref type="bibr" target="#b9">(Dwivedi et al., 2020;</ref><ref type="bibr" target="#b19">Hu et al., 2020)</ref>. LRGA improves state of the art performance in most datasets, often with a significant margin. We further perform ablation study in the random features framework to support our theoretical propositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Attention mechanisms. The first work to use an attention mechanism in deep learning was <ref type="bibr" target="#b3">(Bahdanau et al., 2015)</ref> in the context of natural language processing. Ever since, attention has proven to be a powerful module, even becoming the only component in the transformer architecture <ref type="bibr">(Vaswani et al., 2017)</ref>. Intuitively, attention provides an adaptive importance metric for interactions between pairs of elements, e.g., words in a sentence, pixels in an image or nodes in a graph. A natural drawback of classical attention models is the quadratic complexity generated by computing scores among pairs. Methods to reduce the computation complexity were introduced by <ref type="bibr" target="#b25">(Lee et al., 2018b)</ref> which introduced the set-transformer and addressed the problem by inducing point methods used in sparse Gaussian processes. Linearized versions of attention were suggested by <ref type="bibr">(Shen et al., 2020)</ref> factorizing the attention matrix and normalizing separate components and <ref type="bibr" target="#b21">(Katharopoulos et al., 2020)</ref> suggesting using fixed feature maps for linearizing the attention module. Our suggested linearized attention differs from those as we do not use fixed feature functions and use different normalization.</p><p>Attention in graph neural networks. In the field of graph learning, most attention works <ref type="bibr" target="#b26">(Li et al., 2016;</ref><ref type="bibr">Veli?kovi? et al., 2018;</ref><ref type="bibr" target="#b1">Abu-El-Haija et al., 2018;</ref><ref type="bibr" target="#b6">Bresson &amp; Laurent, 2017;</ref><ref type="bibr" target="#b24">Lee et al., 2018a)</ref> restrict learning the attention scores to the local neighborhoods of the nodes in the graph. Motivated by the fact that local aggregations cannot capture long range relations which may be important when node homophily does not hold, global aggregation in graphs using node embeddings have been suggested by <ref type="bibr">(You et al., 2019;</ref><ref type="bibr" target="#b34">Pei et al., 2020)</ref>. In a way, LRGA combines both approaches, allowing global weighted aggregations via the simple structure of global attention.</p><p>Generalization in graph neural networks. Although being a pillar stone of modern machine learning, the generalization capabilities of NN are still not very well understood, e.g., see <ref type="bibr" target="#b4">(Bartlett et al., 2017;</ref><ref type="bibr" target="#b13">Golowich et al., 2019)</ref>. Due to the irregular structure of graph data and the weight sharing nature of GNN, investigating their generalizing capabilities poses an even greater challenge. Despite the nonstandard setting, few works were able to construct generalization bounds for GNN via VC dimension <ref type="bibr">(Scarselli et al., 2018)</ref>, uniform stability (Verma &amp; Zhang, 2019), Rademacher Complexity <ref type="bibr" target="#b11">(Garg et al., 2020)</ref> and Neural Tangent Kernel .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES AND NOTATIONS</head><p>We denote a graph by G = (V, E, X) where V is the vertex set of size |V | = n, E is the edge set, and adjacency A. X = (x 1 , . . . , x n ) T represents the input vertex features. A vertex v i ? V carries an input feature vector x i ? R d0 ; in turn, X l ? R n?d l represents the output of the l th layer of a neural network.</p><p>A common form of evaluating GNNs is by their ability to distinguish different graphs, described by graph isomorphism which is an equivalence relation between graphs. The isomorphism type tensor of a graph G is a tensor Y ? R n 2 ?diso which holds the isomorphism types of all pairs (i, j) ? [n]?[n]. Given a pair (i, j), which represents either an edge or a node of graph G, Y i,j summarizes all the information this pair carries in graph G. More precisely put, isomorphism type is an equivalence relation defined by: (i, j) and (i , j ) have the same isomorphism type iff the following conditions hold:</p><formula xml:id="formula_0">(i) i = j ?? i = j ; (ii) x i = x i and x j = x j ; and (iii) (i, j) ? E ?? (i , j ) ? E.</formula><p>One way to build an isomorphism type tensor for graph G is Y = [I, 1 ? X, X ? 1, A] , where brackets denote concatentation in the feature dimension, I is the identity matrix, (1 ? X) i,j,: = x j , and similarly (with a slight abuse of notation) (X ? 1) i,j,: = x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LOW-RANK GLOBAL ATTENTION (LRGA)</head><p>We propose the Low-Rank Global Attention (LRGA) module that can augment any graph neural network layer, denoted here generically as GNN, in the following way:</p><formula xml:id="formula_1">X l+1 ? X l , LRGA(X l ), GNN(X l )</formula><p>(1) where the brackets denote concatenation along the feature dimension. The LRGA module is defined for an input feature matrix X ? R n?din via</p><formula xml:id="formula_2">LRGA(X) = 1 ?(X) m 1 (X) m 2 (X) T m 3 (X) , m 4 (X)<label>(2)</label></formula><p>where m 1 , m 2 , m 3 , m 4 : R n?din ? R n?? are MLPs operating on the feature dimension, that is m(X) = [m(x 1 ), . . . , m(x n )] T , and ? ? N 0 is a parameter representing the rank of the attention module. Lastly, ? is a normalization factor:</p><formula xml:id="formula_3">?(X) = 1 n 1 T m 1 (X) m 2 (X) T 1 ,<label>(3)</label></formula><p>where 1 = (1, 1, . . . , 1) T ? R n . The matrix ?(X) ?1 m 1 (X)m 2 (X) T can be thought of as a ?-rank attention matrix that acts globally on the graph's node features.</p><p>Computational complexity. Standard attention models <ref type="bibr">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b29">Luong et al., 2015)</ref> require explicitly computing the attention score between all possible pairs in the set, meaning that its memory requirement and computational cost scales as O(n 2 ). This makes global-attention seem impractical for large sets, or large graphs in our case. We address the global attention computational challenge by working with bounded rank (i.e., ?) attention matrices, and avoid the need to construct the attention matrix in memory by replacing the standard entry-wise normalization (softmax or tanh) with a the global normalization ?. In turn, the memory requirement of LRGA is O(n?), and using low rank matrix-vector multiplications LRGA allows applying global attention in O(n? 2 ) computation cost.</p><p>Permutation Equivariance. A common demand from GNN architectures is to respect the graph representation symmetries, namely the ordering of nodes <ref type="bibr" target="#b31">(Maron et al., 2019b)</ref>. As shown in <ref type="bibr" target="#b25">(Lee et al., 2018b)</ref> the set attention module is permutation equivariant. The same matrix product structure of the LRGA makes this module also permutation equivariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THEORETICAL ANALYSIS</head><p>In this section we establish the theoretical underpinning for LRGA. Since we want to analyse the generalization power added by LRGA, we focus on a family of GNNs with unbounded expressive power in probability (RGNN). Under this model we show the benefit of augmenting GNNs with LRGA in terms of improved generalization via the notion of algorithmic alignment with a powerful graph isomorphism testing algorithm (2-FWL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RANDOM GRAPH NEURAL NETWORKS</head><p>We analyse LRGA under the framework of Random Graph Neural Networks (RGNNs): Definition 1 (Random Graph Neural Network). Let D be a probability distribution of zero mean and variance c, and G = (V, E, X) a graph. RGNN is a GNN variant with random input features sampled at every forward pass i.e., the input to the network is</p><formula xml:id="formula_4">[X, R] where R are i.i.d. samples R ? R n?d ? D.</formula><p>RGNN, suggested by <ref type="bibr" target="#b36">Sato et al. (2020)</ref>, has related variants <ref type="bibr" target="#b28">(Loukas, 2020;</ref><ref type="bibr" target="#b27">2019;</ref><ref type="bibr" target="#b33">Murphy et al., 2019</ref>) that use node identifiers or distinctive features, which can be viewed as constant random features, in order to break symmetry between isomorphic nodes. Such models are proven to be universal but lose their inherent equivariance due to arbitrary prescription of node identifiers. We choose to work in the seemingly more limited setting of RGNN, which allows the network to distinguish between different nodes but does not overfit specific identifiers. Our main claims regarding this framework is that RGNN is both universal in probability and equivariant in expectation. Proposition 1 (Universal). RGNN can approximate an arbitrary continuous graph function given random features sampled from a bounded distribution D.</p><p>Here approximation is in a probabilitic sense:</p><formula xml:id="formula_5">Let ? ? R n?d0 ? R n 2 be a compact set of graphs, [X, A] ? ?, where A ? R n 2</formula><p>is the adjacency matrix. Then, given a continuous graph function f defined over ? and arbitrary ?, ? &gt; 0, there exist network parameters and d so that</p><formula xml:id="formula_6">P (|GNN([X, R]) ? f ([X, A])| &lt; ?) &gt; 1 ? ?, for all graphs [X, A] ? ?.</formula><p>Proposition 1 holds for GNN variants with a global attribute block such as <ref type="bibr" target="#b5">(Battaglia et al., 2018)</ref>. The proof is based on the idea that random features allow the GNN to transfer the graph's connectivity information to the node features. Once all graph information is encapsulated at the nodes, we exploit the universality of set functions <ref type="bibr">(Zaheer et al., 2017)</ref> to get universality. The full proof is in Appendix A. To the best of our knowledge this is the first result proving universality under the random feature assumption. Proposition 2 (Equivariant in expectation). RGNN is permutation equivariant in expectation.</p><p>Changing the random features at each forward pass allows RGNN to preserve equivariance in expectation. Indeed, equivariance of GNN implies that GNN(P ? [X, R]) = P ? GNN([X, R]), for any permutation matrix P and input [X, R]. Taking the expectation of both sides w.r.t. R ? D, noting that P R ? R and using linearity of expectation we get equivariance in expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RGNN AUGMENTED WITH LRGA ALIGNS WITH 2-FWL</head><p>In this section we will formulate our main theoretical result, Theorem 1, stating that augmenting RGNN with LRGA algorithmically aligns with a powerful graph isomorphism testing algorithm called 2-Folklore Weisfeiler-Lehman (2-FWL) <ref type="bibr" target="#b16">(Grohe &amp; Otto, 2015;</ref><ref type="bibr" target="#b15">Grohe, 2017)</ref>. We will first introduce the notion of algorithmic alignment and the 2-FWL algorithm, then formulate our main theorem, and continue in the next section with a proof.</p><p>Algorithmic alignment. The notion of algorithmic alignment was introduced in Xu et al. (2019b) and shown (both theoretically and practically) to lead to improved generalization. Intuitively, a neural network N is said to be aligned with an algorithm A if N can simulate A by a composition of modules, and each module is "simple", or learnable, i.e., have bounded (hopefully low) sample complexity. Our definition of algorithmic alignment is a slightly stricter version: Definition 2 (Monomial Algorithmic Alignment). A neural network N aligns with algorithm A if N can simulate A by learning only monomial functions, i.e., f (</p><formula xml:id="formula_7">x) = x ? , where x ? R d , ? ? N d , and x ? = x ?1 1 ? ? ? ? ? x ? d d .</formula><p>To motivate this choice of monomials as "simple" functions we note that <ref type="bibr" target="#b2">(Arora et al., 2019;</ref><ref type="bibr">Xu et al., 2019b)</ref> show a sample complexity bound for even-power polynomials learned by (two-layer) MLPs and we extend it to general monomials in the following proposition proved in Appendix E: Proposition 3. Let a two layer MLP trained with gradient descent be denoted as the learning algorithm A . The monomial g(x) = x ? , x ? R d , of degree n, |?| ? n, is PAC learnable with A with a sample complexity bound:</p><formula xml:id="formula_8">C A (g, , ?) = O C n,d + log(1/?) 2 ,</formula><p>C n,d = n 2 + 1 (n+1)/2 c n,d , ? &gt; 0 is the error parameter and ? ? (0, 1) the failure probability.</p><p>The asymptotic behaviour of c n,d is out of the scope of this paper. Therefore, a monomial algorithmic alignment of N to A means (under the assumptions and sequential training method of Theorem 3.6 in Xu et al. <ref type="formula" target="#formula_2">(2019b)</ref>) that A is learnable by N .</p><p>2-Folklore Weisfeiler-Lehman (2-FWL) Algorithm. 2-FWL is part of the k-WL hierarchy of polynomial-time (approximate) graph isomorphism iterative algorithms that recolor k-tuples of vertices at each step according to neighborhoods aggregation. Upon reaching a stable coloring, the algorithm terminates and if the histograms of colors of two graphs are not the same then the graphs are deemed not isomorphic. The 2-FWL algorithm is equivalent to 3-WL, strictly stronger than vertex coloring (2-WL) which bounds the expressive power of GNNs.</p><p>(1,j)</p><formula xml:id="formula_9">(i,1) (i,2) (i,3) (i,4) (4,j) (3,j) (2,j) (i,1) (1,j) (i,2) (2,j) (i,3) (i,4) (4,j) (3,j) [ ] A A A A B B B B C C C D D E E In more detail, let Y 0 ? R n 2 ?diso represent the isomorphism types of a given graph G = (V, E, X), that is Y 0 i,j ? R diso represents the isomorphism type of the pair (i, j). The 2-FWL algorithm is initialized with Y 0 . Let Y l ? R n 2 ?d l</formula><p>denote the coloring tensor after the l th update step. An update step in the algorithm aggregates information from the multiset of neighborhood colors for each pair. We represent the multiset of neighborhood colors of the tuple (i, j) with a matrix Z l (i,j) ? R n?2d l . That is, any permutation of the rows of Z l (i,j) represent the same multiset. The rows of Z l (i,j) , which represent the elements in the multiset, are</p><formula xml:id="formula_10">z k = [Y l i,k , Y l k,j ] ? R 2d l , k ? [n]</formula><p>. See the inset for an illustration. The 2-FWL update step of a pair (i, j) from Y l to Y l+1 concatenates the previous pair's color and an encoding of the multiset of neighborhoods colors:</p><formula xml:id="formula_11">Y l+1 i,j = Y l i,j , ENC Z l (i,j)<label>(4)</label></formula><p>where ENC : R n?2d l ? R denc is a multiset injective map invariant to the row-order of its input.</p><p>Main result. Consider the 2-FWL update rule in equation 4 and let Y l+1 ? R n 2 denote (arbitrary) single feature dimension pealed off Y l+1 ? R n 2 ?d l+1 ; we call Y l+1 a single-head of the update rule. Then, Theorem 1. LRGA augmented RGNN algorithmically aligns with a single head 2-FWL update step.</p><p>A corollary of this theorem is: Corollary 1. Multi-head LRGA augmented RGNN algorithmically aligns with 2-FWL.</p><p>Multi-head LRGA is a module of the form [X l , LRGA 1 (X l ), . . . , LRGA k (X l ), GNN(X l )], which is an equivalent to multi-head self-attention. In practice, we found single-head LRGA to be on par performance-wise with multi-head LRGA and therefore we focus on the single-head version in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">PROOF OF THEOREM 1</head><p>To prove Theorem 1 we need to show RGNN augmented with LRGA can simulate one head of the 2-FWL update step using only monomials as learnable functions. We achieve that by the following steps: (i) introduce the notion of node factorization to encode n ? n tensor data as node features; (ii) show that RGNN can approximate node factorization of the graph's isomorphism type tensor with a single GNN layer using learnable monomial functions; (iii) show that 2-FWL update step can be formulated using matrix multiplication of monomial functions; and (iv) show LRGA can approximate a single head 2-FWL update step using learnable monomials.</p><p>Part (i). We start with the definition of node feature factorization:</p><formula xml:id="formula_12">Definition 3 (Node factorization). Let Y ? R n 2 ?d be a tensor. X ? R n?D is called node factorization of Y if there exists a block structure X = X 1 , . . . , X k so that Y = X s1 (X t1 ) T , . . . , X s d (X t d ) T , where (s 1 , t 1 ), . . . , (s d , t d ) ? [k] ? [k] are index pairs. Note that for all i, j ? [n] we have Y i,j = x s1 i , x t1 j , . . . , x s d i , x t d j ? R d . Lets illustrate the definition with an example. Let A ? {0, 1}</formula><p>n?n be the adjacency matrix of some graph G, and for simplicity assume that there are no node features. Then, the isomorphism type tensor of G is Y 0 = [I, A] ? R n 2 ?2 . One possible way of node factoring Y 0 is using the SVD decomposition of the adjacency matrix A. Note that node factorization is not unique. </p><formula xml:id="formula_13">Y l+1 = Y, Y ? Y ? (?, ?) ? N 2d 0 , |?| + |?| ? n<label>(5)</label></formula><p>where for notational simplicity we denote Y = Y l and d = d l . By Y ? we mean that we apply the multi-power ? to the feature dimension, i.e., (Y ? ) i,j = Y ? i,j . Therefore, computing the multisets encoding amounts to calculating monomials Y ? , Y ? and their matrix multiplications Y ? Y ? .</p><p>Part (iv).</p><p>Proposition 5. The node factorization of each head of Y l+1 , the result of 2-FWL update step, can be approximated via LRGA module applied to node factorization of Y = Y l . The MLPs in the LRGA approximation need to learn only monomial functions.</p><p>Proof. Let X = [X 1 , . . . , X k ] ? R n?D be a node factorization of Y = Y l . The 2-FWL update step requires computation of polynomials of the form Y ? as shown in equation 5. Using the node factorization of Y,</p><formula xml:id="formula_14">Y i,j = x s1 i , x t1 j , . . . , x s d i , x t d j ? R d , we can write: Y ? i,j = d l=1 x s l i , x t l j ? l = d l=1 ? ? l (x s l i ), ? ? l (x t l j ) = d l=1 ? ? l (x s i ), ? ? l (x t j ) = ? ? (x s i ), ? ? (x t j )<label>(6)</label></formula><p>where the second equality is using the feature maps ? ? l of the (homogeneous) polynomial kernels <ref type="bibr">(Vapnik, 1998</ref>), x 1 , x 2 ? l ; the third equality is reformulating the feature maps ? ? l on the vectors</p><formula xml:id="formula_15">x s i = [x s1 i , . . . , x s d i ]</formula><p>, and x t i = x t1 i , . . . , x t d i ; and the last equality is due to the closure of kernels to multiplication. We denote the final feature map by ? ? .</p><formula xml:id="formula_16">Now, let ? ? (x i ) = ? ? (x s i ) and ? ? (x i ) = ? ? (x t i ) then we have: Y ? = ? ? (X)? ? (X) T ,</formula><p>where ? ? (X) is applying ? ? to every row of X. Therefore, arbitrary head of Y l+1 , i.e., of the form Y ? Y ? , can be written directly as a function of X using the feature maps ? ? , ? ? , ? ? , ? ? :</p><formula xml:id="formula_17">Y ? Y ? = ? ? (X)? ? (X) T ? ? (X)? ? (X) T .<label>(7)</label></formula><p>A node factorization of the head Y ? Y ? is therefore ? ? (X)? ? (X) T ? ? (X), ? ? (X) . Recalling the structure of the LRGA module introduced in equation 2: LRGA(X) = ?(X) ?1 m 1 (X) m 2 (X) T m 3 (X) , m 4 (X) , to implement the 2-FWL head the MLPs m 1 , m 2 , m 3 , m 4 need to learn the polynomial feature maps formulated in equation 7: m 1 ? ? ? , m 2 ? ? ? , m 3 ? ? ? , and m 4 ? ? ? . Every coordinate of these feature maps is a monomial (proof of this fact in Appendix C). Lastly, note that 2-FWL tensors Y l are insensitive to global scaling and therefore the normalization ? has no theoretical influence (it is assumed non-zero).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We evaluated our method on various tasks including graph regression, graph classification, node classification and link prediction. The datasets we used are from two benchmarks: (i) benchmarking GNNs <ref type="bibr" target="#b9">(Dwivedi et al., 2020)</ref>; and (ii) Open Graph Benchmark (OGB) <ref type="bibr" target="#b19">(Hu et al., 2020)</ref>. Each benchmark has its own evaluation protocol designed for a fair comparison among different models. These protocols define consistent splits of the data to train/val/test sets, set a budget on the size of the models (OGB), define a stopping criterion for reporting test results and require training with several different initializations to measure the stability of the results. We followed these protocols. Random Features Evaluation. In addition, we also conducted a set of experiments with the random feature framework. In this experiment we focused on the PATTERN node classification dataset from <ref type="bibr" target="#b9">(Dwivedi et al., 2020)</ref> and evaluated a variety of models under the RGNN framework. Rank Ablation Study. In this experiment we examined the relation between the rank parameter ?, which can limit the expressiveness of the attention module, and the network performance. Results are presented in Appendix F. Implementation details of LRGA. We implemented the LRGA module according to the description in Section 4 (equations 2, 3) using the pytorch framework and the DGL  and Pytorch geometric <ref type="bibr" target="#b10">(Fey &amp; Lenssen, 2019)</ref>   Datasets. This benchmark contains 6 main datasets (full description in appendix G.1) : (i) ZINC, graph regression task of molecular dataset evaluated with MAE metric; (ii) MNIST and CIFAR10, the image classification problem converted to graph classification using a super-pixel representation <ref type="bibr" target="#b23">(Knyazev et al., 2019)</ref>; (iii) CLUSTER and PATTERN, node classification tasks which aim to classify embedded node structures <ref type="bibr" target="#b0">(Abbe, 2017)</ref>; (iv) TSP, a link prediction variation of the Traveling Salesman Problem (Joshi et al., 2019) on 2D Euclidean graph. Evaluation protocol. All models were evaluated with two different sets of parameter budgets and restrictions. The first set restricted to have roughly 100K parameters and 4 layers, while the second set of experiments has a budget of roughly 500K parameters and up to 16 layers. The learning rate and its decay are set according to a predetermined scheduler using the validation loss. The stopping criterion is set to when the learning rate reaches a specified threshold. All results are averaged over a set of predetermined fixed seeds and standard deviation is reported as well.</p><p>Results. <ref type="table" target="#tab_1">Table 1</ref> summarizes the results of training and evaluating our model according to the evaluation protocol; We observe that LRGA improves GNN performance, often by a large margin, across all models and datasets, besides GCN on ZINC and GatedGCN in TSP, supporting our claim for improved generalization. We further note that SOTA in all datasets except TSP is achieved with LRGA augmented GNNs. In some datasets, such as CLUSTER and PATTERN, LRGA reaches top and roughly equivalent performance for all models it augmented, which emphasizes the empirical contribution of LRGA independently of the GNN variant.</p><p>6.2 LINK PREDICTION DATASETS FROM THE OGB BENCHMARK (HU ET AL., 2020) Datasets. We further evaluate LRGA on semi-supervised learning tasks including graphs with hundreds of thousands of nodes, from the OGB benchmark: (i) ogblppa, a graph of proteins and biological connections as edges ;(ii) ogbl-collab, an authors collaborations graph; (iii) ogbl-ddi drug interaction network. The evaluation metric for all of the tasks is Hits@K; more details in appendix G.2. Evaluation protocol. All models have a hidden layer of size 256 and the number of layers is 3 in ogbl-ppa and ogbl-collab and 2 in ogbl-ddi. Test results are reported by the best validation epoch averaged over 10 random seeds.</p><p>Results. <ref type="table" target="#tab_2">Table 2</ref> summarizes the results on the link prediction tasks. It should be noted that the first three rows correspond to node embedding methods where the rest are GNNs. Augmenting GCN with LRGA achieves SOTA results on those datasets, while still using order of magnitude less parameters than the node embedding runner-up method. In this experiment we wanted to validate the theoretical analysis presented at section 5. The dataset for this evaluation is the PATTERN dataset, which is originally equipped with random features, but in contrast to the RGNN framework those features are sampled only once at the dataset creation stage. We evaluated the different models according to the RGNN framework, i.e., resample the features with every forward pass. The features were sampled from a zero mean Gaussian distribution with variance 1 d , where d is the input feature dimension. The evaluation protocol is the same as the one used in section 6.1 and we followed the 500K budget. As seen from table 3, using alternating random features improves performance for all the models. GIN and GraphSage do not appear in the main table but according to <ref type="bibr" target="#b9">(Dwivedi et al., 2020)</ref> achieves 85.39% and 50.49% respectively. The LRGA augmented RGNN models maintain their superiority (even presenting a small improvement compared to <ref type="table" target="#tab_1">Table 1</ref>) and serve as an empirical validation to our main theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">RANDOM FEATURES ABLATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this work, we set ourself in a path for improving the generalization power of GNNs. To do so, we introduced the LRGA module, a global self attention module, which is a variant of the dot-product self-attention with linear complexity. In order to theoretically evaluate the contribution of LRGA we analyzed our model under the RGNN framework, which is proved to be universal in probability. Under this framework we were able to show that RGNN augmented with LRGA can align with the powerful 2-FWL isomorphism test by learning simple monomial functions, which have a known sample complexity bound. Under certain conditions the latter provides concrete generalization guarantees for RGNN augmented with LRGA. Empirically, we demonstrated augmenting GNN models with LRGA improves their performance significantly, often achieving SOTA performance. A PROOF OF PROPOSITION 1</p><p>Proof. We will now prove the universality in probability over the distribution D of RGNNs. Let ? ? R n?d0 ? R n?n be a compact set of graphs, [X, A] ? ?, where X are the node features and A is the adjacency matrix and we assume that n is fixed. Consider f , a continuous graph function. f is permutation invariant where the permutation acts on all n dimensions, namely, f ([P X, P AP T ]) = f ([X, A]) for all permutation matrices P ? R n?n . RGNN is defined as</p><formula xml:id="formula_18">RGNN(X) = GNN([X, R]) where R ? R n?d are i.i.d. samples from D.</formula><p>To prove universality in probability we need to show that RGNN can approximate f to an arbitrary precision ? with high probability 1 ? ?:</p><formula xml:id="formula_19">??, ? &gt; 0 ??, d s.t. P (|RGNN(X) ? f ([X, A])| &lt; ?) &gt; 1 ? ?</formula><p>where ? are the RGNN network parameters and d is the dimension of the random features of RGNN.</p><p>In fact, a simple RGNN composed of single message passing layer and a global attribute block, a DeepSets network <ref type="bibr">(Zaheer et al., 2017)</ref>, suffices. The message passing layer first transfers the graph structural information to the node features by creating a factorized representation of A. This means that all the graph information is now stored in a set. Then, using the universality of DeepSets network for invariant set functions we can approximate f to an arbitrary precision.</p><p>Let us denote the output of the message passing layer of RGNN by h 1 . The structural information of the graph can be transferred to the node features using the message passing layer by choosing parameters such that h 1 = [X, R, AR]. h 1 is then fed to the DeepSets network, so we have RGNN(X) = DeepSets([X, R, AR]).</p><p>Observing the approximation error:</p><formula xml:id="formula_20">|RGNN(X) ? f ([X, A])| = |DeepSets([X, R, AR]) ? f ([X, A])| = = |DeepSets([X, R, AR]) ? f ([X, 1 d ARR T ]) + f ([X, 1 d ARR T ]) ? f ([X, A])| ? ? |DeepSets([X, R, AR]) ? f ([X, 1 d ARR T ])| + |f ([X, 1 d ARR T ]) ? f ([X, A])|</formula><p>We can now bound the two terms in the last inequality above. Since f is defined on the compact set ? we first make sure that 1 d ARR T remains bounded (we assume f can be extended continuously to this domain). Since we assume D is bounded (given x ? D, |x| &lt; M/2), we get:</p><formula xml:id="formula_21">1 d ARR T F ? 1 d A F RR T ? 1 d A F d M 2 4 n</formula><p>For the second term we can achieve a bound in probability. Since f is a continuous function on a compact set, by the Heine-Cantor theorem, it is uniformly continuous, meaning that</p><formula xml:id="formula_22">?? &gt; 0 ? ? s.t ? Q, S ? ? d ? (Q, S) &lt; ? ? d R (f (Q), f (S)) &lt; ? Setting ? = ?/2 we can now choose d such that with probability 1 ? ? we have d ? ([X, 1 d ARR T ], [X, A]) &lt; ?. Let d ? be the euclidean metric, then, d ? ( 1 d ARR T , A) ? A F ? 1 d RR T ? I F .</formula><p>Since we assume a graph of fixed size n, A F ? n and we are left with bounding 1 d RR T ? I F in probability. Using Hoeffding's inequality we will be able to find d satisfying the conditions. A single entry in R has mean 0 and variance c, for simplicity we set c = 1. An entry in RR T is of the form (RR T ) ij = d l=1 R il R jl . Note that all elements of the sum are statistically independent and bounded. Using Hoeffding's inequality:</p><formula xml:id="formula_23">P 1 d d l=1 R il R jl ? E 1 d d l=1 R il R jl ? t ? 2 exp ? 2dt 2 M 4<label>(8)</label></formula><p>For i = j: E 1 d d l=1 R il R jl = 0 and for i = j:</p><formula xml:id="formula_24">E 1 d d l=1 R il R jl = 1.</formula><p>Using union bound over all entries of 1 d RR T :</p><formula xml:id="formula_25">P ? ? i,j?[n] 1 d (RR T ) ij ? I ij ? t ? ? ? 2n 2 exp ? 2dt 2 M 4 Setting t = ?/n 2 A F and requiring 2n 2 exp ? 2dt 2 M 4 &lt; ? we get d = M n 4 A 2 F ? 2 log 2n 2 ?</formula><p>where M accumulates all constant factors. Lastly, A F is bounded by n, so the d we should take is d = M n 6 ? 2 log 2n 2 ? . Finally, we have that for large enough d, 1 d RR T ? I F is arbitrarily small with a high probability.</p><p>For the first term, we note that f ([X, 1 d ARR T ]) = F ([X, R, AR]) is a continuous invariant set function over a bounded domain. Therefore the first term can be bounded by invoking the universal approximation theorem of invariant set functions <ref type="bibr">(Zaheer et al., 2017)</ref>, i.e., exist a set of parameters and model size such that the approximation error is less than ?/2. This concludes the proof. We found that exists a set of network parameters and d such that the approximation error is arbitrarily small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MULTISET ENCODING</head><p>As shown in <ref type="bibr" target="#b30">Maron et al. (2019a)</ref> the multiset encoding function, ENC, can be defined using the collection of Power-sum Multi-symmetric Polynomials (PMPs). That is, given a multiset Z = (z 1 , . . . , z n ) T ? R n?2d the encoding is defined by</p><formula xml:id="formula_26">ENC(Z) = n k=1 z ? k ? ? N 2d 0 , |?| ? n ,</formula><p>where ? = (? 1 , . . . , ? 2d ), and z ? = z ?1 1 ? ? ? z ? 2d 2d . Let us focus on computing a single output coordinate ? of the ENC function applied to a particular multiset Z <ref type="bibr">(i,j)</ref> . This can be efficiently computed using matrix multiplication <ref type="bibr" target="#b30">Maron et al. (2019a)</ref>:</p><formula xml:id="formula_27">Let ? = (?, ?) ? N 2d 0 , where ?, ? ? N d 0 . Then, ENC ? (Z (i,j) ) = n k=1 z ? k = n k=1 Y ? i,k Y ? k,j = (Y ? Y ? ) i,j .</formula><p>By Y ? we mean that we apply the multi-power ? to the feature dimension, i.e., (Y ? ) i,j = Y ? i,j . This implies that computing the multisets encoding amounts to calculating monomials Y ? , Y ? and their matrix multiplications Y ? Y ? . Thus the 2-FWL update rule, equation 4, can be written in the following matrix form, where for notational simplicity we denote Y = Y l :</p><formula xml:id="formula_28">Y l+1 = Y, Y ? Y ? (?, ?) ? N 2d 0 , |?| + |?| ? n C 2-FWL VIA POLYNOMIAL KERNELS</formula><p>In this section, we give a full characterization of feature maps, ? ? , of the final polynomial kernel we use to formulate the 2-FWL algorithm. A key tool for the derivation of the final feature map is the multinomial theorem, which we state here in a slightly different form to fit our setting.</p><p>Multinomial theorem. Let us define a set of m variables x 1 y 1 , . . . , x m y m composed of products of corresponding x and y's. Then,</p><formula xml:id="formula_29">(x 1 y 1 + ? ? ? + x m y m ) n = |?|=n n ? m i=1 (x i y i ) ?i</formula><p>where ? ? N m 0 , and the notation n ? = n! ?1!??????m! . The sum is over all possible ? which sum to n, in total n+m?1 m?1 elements.</p><p>Recall that we wish to compute Y ? i,j as in equation 6 in the paper:</p><formula xml:id="formula_30">Y ? i,j = d l=1 x s l i , x t l j ? l = d l=1 ? ? l (x s l i ), ? ? l (x t l j ) = d l=1 ? ? l (x s i ), ? ? l (x t j ) = ? ? (x s i ), ? ? (x t j )</formula><p>We will now follow the equalities in equation 6 to derive the final feature map. The second equality is using the feature maps ? ? k of the (homogeneous) polynomial kernels <ref type="bibr">(Vapnik, 1998</ref>), x 1 , x 2 ? k , which can be derived from the multinomial theorem.</p><p>Suppose the dimensions of X s l , X t l are n ? D l where d l=1 2D l = D. Then, ? ? l consists of monomials of degree ? l of the form</p><formula xml:id="formula_31">? ? l (x) ? = ? l ? D l i=1 x ?i i = ? l ? x ? , |?| = ? l .</formula><p>In total the size of the feaure map ? ? l is ? l +D l ?1</p><formula xml:id="formula_32">D l ?1 . i = [x s1 i , . . . , x s k i ] ? R D/2 , and x t i = x t1 i , . . . , x t k i ? R D/2 .</formula><p>The last equality is due to the closure of kernels to multiplication. The final feature map, which is the product kernel, is composed of all possible products of elements of the feature maps, i.e.,</p><formula xml:id="formula_33">? ? (x) = d l=1 ? l ? l x ? l l |? j | = ? j , ?j ? [d] , where x = [x 1 , x 2 , . . . , x k ] ? R D/2 , and x l ? R D l for all l ? [d].</formula><p>The size of the final feature map is d l=1</p><formula xml:id="formula_34">? l +D l ?1 D l ?1 ? N where N = n+D D .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXTENSION OF PROPOSITION 4</head><p>In this section we would like to extend the proof of proposition 4 to the case where the graph is equipped with prior node features X ? R n?d0 , s.t the network's input is [X, R]. As mentioned in Section 3 the isomorphism type of a graph equipped with node features is Y = [I, 1?X, X ?1, A].</p><p>Following this description we claim that the node factorization representation of the graph will be of the form R = [1, X, R, AR], where 1 = (1, 1, . . . , 1) T ? R n . To build the isomorphosm tensor we can use the sequence of outer products X 1 1 T , . . . , X d 1 T , 1X T 1 , . . . , 1X T d , where X l ? R n is the l-th column of X. This sequence could be represented using the two first components of R . The last two components, R and AR allow to approximate in probability A and I as shown in Appendix A, which complete the isomorphism tensor construction and conclude that [1, X, R, AR] is a node factorization representation. Lastly, we have to show that we can construct this structure using RGNN, and actually we are left to explain how to add the 1 vector to the representation. This could be done using a global attribute block as used to proof Proposition 1. E SAMPLE COMPLEXITY BOUND OF MONOMIALS Corollary 6.2 in <ref type="bibr" target="#b2">(Arora et al., 2019)</ref> provides a bound on the sample complexity, denoted C A (g, , ?), of a polynomial g : R D ? R of the form</p><formula xml:id="formula_35">g(x) = j a j ? j , x pj ,<label>(9)</label></formula><p>where p j ? {1, 2, 4, 6, 8, . . .}, a j ? R, ? j ? R D ; , ? are the relevant PAC learning constants, and A represents an over-parameterized, randomly initialized two-layer MLP trained with gradient descent.</p><formula xml:id="formula_36">C A (g, , ?) = O j p j |a j | ? j pj 2 + log(1/?) 2</formula><p>It is not immediately clear, however, how to use this theorem to learn an arbitrary monomial x ? since g has the above particular form. Nevertheless we show how it can be generalized to this case.</p><p>Let B = ? ? N D 0 | |?| ? n , and note that there are N = n+D D elements in B. We assume some fixed ordering in B is prescribed. Define the sample matrix (multivariate Vandemonde) V ? R N ?N by V ?,? = ? ? . Lemma 2.8 in <ref type="bibr">(Wendland, 2004)</ref> implies that V is non-singular. Let c n,D = V ?1 ? (i.e., the induced ? matrix norm); note that c n,D is dependant only upon n, D. Lemma 1. Fix D, n ? N, and let ? ? B be arbitrary. Then, there exist coefficients a ? R N , a 1 ? c n,D , so that x ? = ??B a ? ( ?, x + 1) n , for all x ? R D .</p><p>Proof. Using the multinomial theorem we have: ( ?, x + 1) n = ??B d ? ? ? x ? , where d ? are positive multinomial coefficients. This equation defines a linear relation between the monomial basis x ? and ( ?, x + 1) n , for ? ? B. The matrix of this system is V multiplied by a positive diagonal matrix with d ? on its diagonal. By inverting this matrix and solving this system for x ? the lemma is proved.</p><p>We can use this Lemma in the following way: Assume n is even or otherwise consider 2 n/2 . Further assume that the MLP m : R D+1 ? R is two-layer, over-parameterized of the form m(x, 1) (i.e., we assume there is a constant 1 plugged in an extra D + 1 coordinate). We consider training m with random initialization and gradient descent using data (x, x ? ) ? R D ? R where x is sampled i.i.d. from some distribution D over R D .</p><p>Let g : R D+1 ? R defined as g(x, x D+1 ) = ??B a ? ( ?, x + x D+1 ) n , where a ? R N is as promised by Lemma 1. Then, the learning setup described above is equivalent to training the MLP m(x, x D+1 ) using data of the form ((x, 1), g(x, 1) = x ? ), where (x, 1) is sampled i.i.d. from a distribution D over R D+1 concentrated on the hyperplane x D+1 = 1. Now using the Corollary 6.2 from <ref type="bibr" target="#b2">(Arora et al., 2019)</ref> in our case where g : R D+1 ? R is defined as g(x, x D+1 ) = ??B a ? ( ?, x + x D+1 ) n where B = ? ? N D 0 | |?| ? n and by Lemma 1 there exist a such that g(x, 1) = x ? . The sample complexity bound expression by Corollary 6.2 is therefore: ? n ? n 2 + 1 n/2 ??B |a ? | ? n 2 + 1 (n+1)/2 c n,D</p><formula xml:id="formula_37">C A (g, , ?) = O ? ? ? ??B n |a ? | ?</formula><p>The first inequality is due to ? 2 ? ? 1 , the second is by Lemma 1 and uniting n into the main term. From the above, the bound follows. We investigated the affects of the attention's rank ? on the performance of GNNs augmented with LRGA on the CLUSTER dataset. The dataset contains graphs of 40 to 190 nodes (117 nodes in average). Our experimental setting included fixing the GNN's hidden dimensions size and changing ?. <ref type="figure" target="#fig_2">Figure 1</ref> shows that accuracy increases with the rank values until it reaches a plateau around ? ? 30 (?/n = 0.25 where n is the average graph size), a fact that could be attributed to saturating the expressiveness of the LRGA module. Moreover, the maximal accuracy is achieved at a value that corresponds to the maximal graph size in the dataset, smaller than what the theory predicts as a function of the graph size n. This rank value is enough to compute any attention function on this graph collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G IMPLEMENTATION DETAILS</head><p>In this section we describe the datasets on which we performed our evaluation. In addition, we specify the hyperparameters for the experiments section in the paper. The rest of the model configurations are determined directly by the evaluation protocols defined by the benchmarks. It is worth noting that most of our experiments ran on a single Tesla V-100 GPU, if not stated otherwise. We performed our parameter search only on ? and d (except for CIFAR10 and MNIST were we searched over different dropout values), since the rest of the parameters were dictated by the evaluation protocol. The models sizes were restricted by the allowed parameter budget.</p><p>Initial node features are obtained by combining word embeddings of papers by that author (128-dimensional vector). Additionally, each collaboration is described by the year of collaboration and the number of collaborations in that year as a weight. The train/validation/test split sizes are 1.1M/60K/46K. Similarly to the previous dataset, the evaluation metric is Hits@K.</p><p>(iii) ogbl-ddi -an undirected unwighted graph which represent drug-drug interaction. Each Node represents FDA approved or experimental drug. The edges represent interactions between drugs and represent the joint effect of taking both drugs together. The learning task is to predict new drug to drug interactions. The train/validation/test split sizes are 1M/150K/150K. The evaluation here is also Hits@K.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>libraries. Each LRGA module contains 4 MLPs m 1 , m 2 , m 3 , m 4 . Each m i : R d ? R ? is a single layer MLP (linear with ReLU activation). The implementation of a layer is according to equation 2, where in practice we added another single layer MLP, m 5 : R d+2?+d GN N ? R d , for the purpose of reducing the feature dimension size. In the OGB benchmark dataset we did not use the skip connections (better performance), and as advised in (Wang et al., 2019), we used batch and graph normalization at each layer. Baselines. We compare performance with the following state of the art baselines: GCN (Kipf &amp; Welling, 2016),GraphSAGE (Hamilton et al., 2017),GIN (Xu et al., 2019a),GAT (Veli?kovi? et al.,  2018), GatedGCN<ref type="bibr" target="#b6">(Bresson &amp; Laurent, 2017)</ref>, Node2Vec<ref type="bibr" target="#b17">(Grover &amp; Leskovec, 2016)</ref> , DeepWalk<ref type="bibr" target="#b35">(Perozzi et al., 2014)</ref> and MATRIX FACTORIZATION<ref type="bibr" target="#b19">(Hu et al., 2020)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Let us bound the first term in the numerator of the sample complexity expression: ??B n |a ? | ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FFigure 1 :</head><label>1</label><figDesc>Ablation study on CLUSTER dataset. The X-axis represent the ratio between the rank parameter ? and the average graph size n = 117. The Y-axis represent the network's accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Part (ii). Proposition 4. RGNN with skip connection can approximate node factorization of the isomorphism type tensor Y 0 .Proof. We will prove the case of graph G = (V, E), i.e., with no vertex features; the general case can be found in Appendix D. Let R ? R n?d be a random node features matrix sampled i.i.d. fromD. A single layer of standard message passing can represent GNN(R) = d ?0.5 [AR, R], which requires learning only first degree (linear) monomials in the GNN's learnable parts. Furthermore, GNN(R) is an approximate node factorization of Y 0 , since d ?1 [RR T , ARR T ] ? [I, A] = Y 0 , where the approximation error d ?1 RR T ? I can be bounded using the result in Appendix A. Part (iii). As shown in (Maron et al., 2019a) the encoding function ENC from the 2-FWL update rule (see equation 4) can be expressed as follows (derivation can be found in Appendix B):</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance on the benchmarking GNN datasets. In bold: better performance between LRGA augmented and vanilla models; note the parameter (#) budget. Blue represents best performance with the 100K budget and red with the 500K budget. 90K 83.09 ? 0.73 91K 68.44 ? 0.16 92K 0.448 ? 0.009 91K 97.63 ? 0.11 91K 65.80 ? 0.43 97K 0.702 ? 0.001 GAT 109K 75.82 ? 1.82 110K 57.73 ? 0.32 102K 0.475 ? 0.007 110K 95.53 ? 0.20 110K 64.22 ? 0.45 96K 0.671 ? 0.002 LRGA + GAT 90K 82.54 ? 0.71 91K 69.05 ? 0.05 92K 0.421 ? 0.020 90K 97.47 ? 0.16 90K 68.00 ? 0.13 97K 0.680 ? 0.003 GatedGCN 104K 84.48 ? 0.12 104K 60.40 ? 0.41 105K 0.375 ? 0.003 104K 97.34 ? 0.14 104K 67.31 ? 0.31 97K 0.808 ? 0.003 LRGA + GatedGCN 93K 85.09 ? 0.11 93K 69.28 ? 0.16 94K 0.355 ? 0.010 93K 98.20 ? 0.03 93K 70.65 ? 0.18 97K 0.807 ? 0.001 GCN 500K 71.89 ? 0.33 501K 68.49 ? 0.97 505K 0.367 ? 0.011 504K 91.39 ? 0.25 504K 54.84 ? 0.44 --LRGA + GCN 400K 84.55 ? 0.57 400K 76.01 ? 0.67 501K 0.377 ? 0.009 463K 98.34 ? 0.06 463K 68.27 ? 0.46 --533K 85.82 ? 0.42 267K 76.16 ? 0.34 536K 0.360 ? 0.004 476K 98.41 ? 0.08 476K 71.57 ? 0.26 --GatedGCN 502K 85.56 ? 0.01 502K 73.84 ? 0.32 504K 0.282 ? 0.015 500K 98.24 ? 0.04 500K 71.33 ? 0.39 --LRGA + GatedGCN 486K 85.81 ? 0.31 438K 76.39 ? 0.13 446K 0.249 ? 0.011 486K 98.47 ? 0.16 487K 73.48 ? 0.29 -</figDesc><table><row><cell>Model</cell><cell>#</cell><cell>PATTERN Acc ? std</cell><cell>#</cell><cell>CLUSTER Acc ? std</cell><cell>#</cell><cell>ZINC MAE ? std</cell><cell>#</cell><cell>MNIST Acc ? std</cell><cell>#</cell><cell>CIFAR10 Acc ? std</cell><cell>#</cell><cell>TSP F1 ? std</cell></row><row><cell>GCN</cell><cell cols="12">100K 63.88 ? 0.07 101K 53.44 ? 2.02 103K 0.459 ? 0.006 101K 90.70 ? 0.21 101K 55.71 ? 0.38 95K 0.630 ? 0.001</cell></row><row><cell>LRGA + GCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GAT</cell><cell cols="11">526K 78.27 ? 0.18 528K 70.58 ? 0.44 531K 0.384 ? 0.007 441K 96.50 ? 0.18 442K 66.11 ? 0.98 -</cell><cell>-</cell></row><row><cell>LRGA + GAT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>- 6.1 BENCHMARKING GRAPH NEURAL NETWORKS (DWIVEDI ET AL., 2020)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance on the link prediction tasks from the OGB benchmark</figDesc><table><row><cell>Model</cell><cell cols="2">ogbl-ppa # Param Hits@100?std # Param Hits@50?std # Param Hits@20?std ogbl-collab ogbl-ddi</cell></row><row><cell>Node2vec</cell><cell>7.3M 0.223 ? 0.008</cell><cell>30M 0.489 ? 0.005 645K 0.233 ? 0.021</cell></row><row><cell>DeepWalk</cell><cell>150M 0.289 ? 0.015</cell><cell>61M 0.504 ? 0.003 11M 0.264 ? 0.061</cell></row><row><cell>MF</cell><cell>147M 0.323 ? 0.009</cell><cell>60M 0.389 ? 0.003 1.2M 0.137 ? 0.047</cell></row><row><cell>GraphSage</cell><cell cols="2">424K 0.165 ? 0.024 460K 0.481 ? 0.008 1.4M 0.539 ? 0.047</cell></row><row><cell>GCN</cell><cell cols="2">278K 0.187 ? 0.013 296K 0.447 ? 0.011 1.2M 0.370 ? 0.050</cell></row><row><cell cols="2">LRGA + GCN 814K 0.342 ? 0.016</cell><cell>1M 0.522 ? 0.007 1.5M 0.623 ? 0.091</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Random Features Evaluation</figDesc><table><row><cell>Model</cell><cell>PATTERN Acc ? std</cell></row><row><cell>GCN</cell><cell>74.891 ? 0.713</cell></row><row><cell>LRGA + GCN</cell><cell>84.118 ? 1.216</cell></row><row><cell>GAT</cell><cell>81.796 ? 0.661</cell></row><row><cell>LRGA + GAT</cell><cell>85.905 ? 0.109</cell></row><row><cell>GraphSage</cell><cell>85.039 ? 0.068</cell></row><row><cell cols="2">LRGA + GraphSage 85.229 ? 0.331</cell></row><row><cell>GatedGCN</cell><cell>85.848 ? 0.065</cell></row><row><cell cols="2">LRGA + GatedGCN 85.944 ? 0.664</cell></row><row><cell>GIN</cell><cell>85.760 ? 0.001</cell></row><row><cell>LRGA + GIN</cell><cell>86.765 ? 0.065</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Franco Scarselli, Ah Tsoi, and Markus Hagenbuchner. The vapnik-chervonenkis dimension of graph and recursive neural networks. Neural Networks, 108, 09 2018. doi: 10.1016/j.neunet.2018.08. 010. Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities, 2020. Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998. Rex Ying, and Jure Leskovec. Position-aware graph neural networks. CoRR, abs/1906.04817, 2019. URL http://arxiv.org/abs/1906.04817.</figDesc><table><row><cell>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</cell></row><row><cell>?ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information</cell></row><row><cell>processing systems, pp. 5998-6008, 2017.</cell></row><row><cell>Petar Veli?kovi?, Arantxa Casanova, Pietro Li?, Guillem Cucurull, Adriana Romero, and Yoshua</cell></row><row><cell>Bengio. Graph attention networks. In 6th International Conference on Learning Representations,</cell></row><row><cell>ICLR 2018, 2018. ISBN 1710.10903v3.</cell></row><row><cell>Saurabh Verma and Zhi-Li Zhang. Stability and generalization of graph convolutional neural net-</cell></row><row><cell>works, 2019.</cell></row><row><cell>Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou,</cell></row><row><cell>Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang</cell></row><row><cell>Li, Alexander J Smola, and Zheng Zhang. Deep graph library: Towards efficient and scalable</cell></row><row><cell>deep learning on graphs. ICLR Workshop on Representation Learning on Graphs and Manifolds,</cell></row><row><cell>2019. URL https://arxiv.org/abs/1909.01315.</cell></row><row><cell>Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka.</cell></row><row><cell>What Can Neural Networks reason About? Technical report, 2019b.</cell></row><row><cell>Jiaxuan You, Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,</cell></row><row><cell>and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp.</cell></row><row><cell>3391-3401, 2017.</cell></row></table><note>Holger Wendland. Scattered data approximation, volume 17. Cambridge university press, 2004. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural Networks? In International Conference on Learning Representations, 2019a. URL https: //openreview.net/forum?id=ryGs6iA5Km.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Preprint.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Datasets. This benchmark contains 6 main datasets :</p><p>(i) ZINC, a molecular graphs dataset with a graph regression task where each node represents an atom and each edge represents a bond. The regression target is a property known as the constrained solubility (with mean absolute error as evaluation metric). Additionally, the node features represent the atom's type (28 types) and the edge features represents the type of connection (4 types). The result reported for GCN used d = 60 for the 100K budget and d = 90 (network's depth is L = 12) for the 500K budget. For the GAT network we used d = 60 (4 attention heads of dimension 15) for the 100K budget and d = 120 (4 attention heads of dimension 30) with L = 8 for the 500K budget. For the GatedGCN network we used d = 45 for the 100K budget and d = 60 with L = 12 for the 500K budget. All the models used ? = 30. (ii) MNIST and CIFAR10, the known image classification problem is converted to a graph classification task using Super-pixel representation <ref type="bibr" target="#b23">(Knyazev et al., 2019)</ref>, which represents small regions of homogeneous intensity as nodes. The edges in the graph are obtained by applying k-nearest neighbor algorithm on the nodes coordinates. Node features are a concatenation of the Super-pixel intensity (RGB for CIFAR10 and greyscale for MNIST) and its image coordinate. Edges features are the k-nearest distances. The result reported for GCN used d = 60 for the 100K budget and d = 110 with L = 8 for the 500K budget. For the GAT network we used d = 60 (4 attention heads of dimension 15) for the 100K budget and d = 122 (4 attention heads of dimension 28) with L = 8 for the 500K budget. For the GatedGCN network we used d = 45 for the 100K budget and d = 80 with L = 8 for the 500K budget. All the models used ? = 30.</p><p>(iii) CLUSTER and PATTERN, node classification tasks which aim to identify embedded node structures in stochastic block model graphs <ref type="bibr" target="#b0">(Abbe, 2017)</ref>. The goal of the task is to assign each node to the stochastic block it was originated from, while the structure of the graph is governed by two probabilities that define the inner-structure and cross-structure edges. A single representative from each block is assigned with an initial feature that indicates its block while the rest of the nodes have no features (CLUSTER), while in the PATTERN dataset nodes are assigned with a random value as input feature at the creation stage. The result reported for GCN used d = 60 for the 100K budget and d = 100 with L = 8 for the 500K budget (PATTERN, CLUSTER respectively). For the GAT network we used d = 60 (4 attention heads of dimension 15) for the 100K budget and d = 120, 60 (8 attention heads of dimension 15, 4 attention heads of dimension 15) with L = 8, 12 for the 500K budget (PATTERN, CLUSTER respectively). For the GatedGCN network we used d = 45 for the 100K budget and d = 80, 50 with L = 8, 12 for the 500K budget (PATTERN, CLUSTER respectively). All the models used ? = 30. (iv) TSP, a link prediction task that tries to tackle the NP-hard classical Traveling Salesman <ref type="bibr">Problem (Joshi et al., 2019)</ref>. Given a 2D Euclidean graph the goal is to choose the edges that participate in the minimal edge weight tour of the graph. The evaluation metric for the task is F1 score for the positive class. The result reported for GCN used d = 60 . For the GAT network we used d = 60 (4 attention heads of dimension 15). For the GatedGCN network we used d = 45. All the models used ? = 30.</p><p>G.2 LINK PREDICTION DATASETS FROM THE OGB BENCHMARK (HU ET AL., 2020)</p><p>Datasets. In order to provide a more complete evaluation of our model we also evaluate it on semi-supervised learning tasks of link prediction. We searched over the same hyperparameter range ? ? {25, 50, 100} , d ? {150, 256} and used ? = 50, d = 256 in all tasks. The three datasets were:</p><p>(i) ogbl-ppa, an undirected unweighted graph. Nodes represent types of proteins and the edges signify biological connections between proteins. The initial node feature is a 58-dimensional one-hot-vector that indicates the origin specie of the protein. The learning task is to predict new connections between nodes. The train/validation/test split sizes are 21M/6M/3M . The evaluation metric is called Hits@K <ref type="bibr" target="#b19">(Hu et al., 2020)</ref>. (ii) ogbl-collab, is a graph that represents a network of collaborations between authors. Every author in the network is represented by a node and each collaboration is assigned with an edge.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Community detection and stochastic block models: recent developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Watch your step: Learning node embeddings via graph attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9180" to="9190" />
		</imprint>
	</monogr>
	<note>volume 2018-Decem</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung</forename><forename type="middle">Hyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Residual Gated Graph Convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Coloring graph neural networks for node disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aladin</forename><surname>Virmaux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangcheng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab?s</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast Graph Representation Learning with PyTorch Geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Size-independent sample complexity of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2005.1555942</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<title level="m">Descriptive complexity, canonisation, and definable graph structure theory</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pebble games and linear equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Symbolic Logic</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="797" to="844" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">node2vec: Scalable feature learning for networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2005.00687" />
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An efficient graph convolutional network technique for the travelling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bresson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Nikolaos Pappas, and Fran?ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1609.02907" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Understanding attention in graph neural networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.02850" />
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attention Models in Graphs: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Boaz</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchul</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nesreen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunyee</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koh</surname></persName>
		</author>
		<ptr target="https://doi.org/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">How hard is graph isomorphism for graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://nlp.stanford.edu/projects/nmt" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8488-provably-powerful-graph-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2156" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02244</idno>
		<title level="m">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02541</idno>
		<title level="m">Relational Pooling for Graph Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2002.05287" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepwalk</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623732</idno>
		<ptr target="http://dx.doi.org/10.1145/2623330" />
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;14</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2008.2005605</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
