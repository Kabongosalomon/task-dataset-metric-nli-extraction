<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Collaborative Filtering *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-04-03">2017. April 3-7, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<email>xiangnanhe@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
							<email>liaolizi.llz@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
							<email>nieliqiang@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
							<email>hu@cse.tamu.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Collaborative Filtering *</title>
					</analytic>
					<monogr>
						<title level="m">2017 International World Wide Web Conference Committee (IW3C2)</title>
						<meeting> <address><addrLine>Perth, Australia</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2017-04-03">2017. April 3-7, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3038912.3052569</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaborative Filtering</term>
					<term>Neural Networks</term>
					<term>Deep Learning</term>
					<term>Matrix Factorization</term>
					<term>Implicit Feedback *</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -collaborative filtering -on the basis of implicit feedback.</p><p>Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items.</p><p>By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural networkbased Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the era of information explosion, recommender systems play a pivotal role in alleviating information overload, having been widely adopted by many online services, including E-commerce, online news and social media sites. The key to a personalized recommender system is in modelling users' preference on items based on their past interactions (e.g., ratings and clicks), known as collaborative filtering <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref>. Among the various collaborative filtering techniques, matrix factorization (MF) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref> is the most popular one, which projects users and items into a shared latent space, using a vector of latent features to represent a user or an item. Thereafter a user's interaction on an item is modelled as the inner product of their latent vectors.</p><p>Popularized by the Netflix Prize, MF has become the de facto approach to latent factor model-based recommendation. Much research effort has been devoted to enhancing MF, such as integrating it with neighbor-based models <ref type="bibr" target="#b20">[21]</ref>, combining it with topic models of item content <ref type="bibr" target="#b37">[38]</ref>, and extending it to factorization machines <ref type="bibr" target="#b25">[26]</ref> for a generic modelling of features. Despite the effectiveness of MF for collaborative filtering, it is well-known that its performance can be hindered by the simple choice of the interaction functioninner product. For example, for the task of rating prediction on explicit feedback, it is well known that the performance of the MF model can be improved by incorporating user and item bias terms into the interaction function 1 . While it seems to be just a trivial tweak for the inner product operator <ref type="bibr" target="#b13">[14]</ref>, it points to the positive effect of designing a better, dedicated interaction function for modelling the latent feature interactions between users and items. The inner product, which simply combines the multiplication of latent features linearly, may not be sufficient to capture the complex structure of user interaction data. This paper explores the use of deep neural networks for learning the interaction function from data, rather than a handcraft that has been done by many previous work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>. The neural network has been proven to be capable of approximating any continuous function <ref type="bibr" target="#b16">[17]</ref>, and more recently deep neural networks (DNNs) have been found to be effective in several domains, ranging from computer vision, speech recognition, to text processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b46">47]</ref>. However, there is relatively little work on employing DNNs for recommendation in contrast to the vast amount of literature on MF methods. Although some recent advances <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref> have applied DNNs to recommendation tasks and shown promising results, they mostly used DNNs to model auxiliary information, such as textual description of items, audio features of musics, and visual content of images. With regards to modelling the key collaborative filtering effect, they still resorted to MF, combining user and item latent features using an inner product.</p><p>This work addresses the aforementioned research problems by formalizing a neural network modelling approach for collaborative filtering. We focus on implicit feedback, which indirectly reflects users' preference through behaviours like watching videos, purchasing products and clicking items. Compared to explicit feedback (i.e., ratings and reviews), implicit feedback can be tracked automatically and is thus much easier to collect for content providers. However, it is more challenging to utilize, since user satisfaction is not observed and there is a natural scarcity of negative feedback. In this paper, we explore the central theme of how to utilize DNNs to model noisy implicit feedback signals.</p><p>The main contributions of this work are as follows.</p><p>1. We present a neural network architecture to model latent features of users and items and devise a general framework NCF for collaborative filtering based on neural networks.</p><p>2. We show that MF can be interpreted as a specialization of NCF and utilize a multi-layer perceptron to endow NCF modelling with a high level of non-linearities.</p><p>3. We perform extensive experiments on two real-world datasets to demonstrate the effectiveness of our NCF approaches and the promise of deep learning for collaborative filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>We first formalize the problem and discuss existing solutions for collaborative filtering with implicit feedback. We then shortly recapitulate the widely used MF model, highlighting its limitation caused by using an inner product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning from Implicit Data</head><p>Let M and N denote the number of users and items, respectively. We define the user-item interaction matrix Y ? R M ?N from users' implicit feedback as,</p><formula xml:id="formula_0">yui = 1, if interaction (user u, item i) is observed; 0, otherwise.<label>(1)</label></formula><p>Here a value of 1 for yui indicates that there is an interaction between user u and item i; however, it does not mean u actually likes i. Similarly, a value of 0 does not necessarily mean u does not like i, it can be that the user is not aware of the item. This poses challenges in learning from implicit data, since it provides only noisy signals about users' preference. While observed entries at least reflect users' interest on items, the unobserved entries can be just missing data and there is a natural scarcity of negative feedback. The recommendation problem with implicit feedback is formulated as the problem of estimating the scores of unobserved entries in Y, which are used for ranking the items. Model-based approaches assume that data can be generated (or described) by an underlying model. Formally, they can be abstracted as learning?ui = f (u, i|?), where?ui denotes  From data matrix (a), u4 is most similar to u1, followed by u3, and lastly u2. However in the latent space (b), placing p 4 closest to p 1 makes p 4 closer to p 2 than p 3 , incurring a large ranking loss.</p><p>the predicted score of interaction yui, ? denotes model parameters, and f denotes the function that maps model parameters to the predicted score (which we term as an interaction function).</p><p>To estimate parameters ?, existing approaches generally follow the machine learning paradigm that optimizes an objective function. Two types of objective functions are most commonly used in literature -pointwise loss <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref> and pairwise loss <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>. As a natural extension of abundant work on explicit feedback <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b45">46]</ref>, methods on pointwise learning usually follow a regression framework by minimizing the squared loss between?ui and its target value yui.</p><p>To handle the absence of negative data, they have either treated all unobserved entries as negative feedback, or sampled negative instances from unobserved entries <ref type="bibr" target="#b13">[14]</ref>. For pairwise learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44]</ref>, the idea is that observed entries should be ranked higher than the unobserved ones. As such, instead of minimizing the loss between?ui and yui, pairwise learning maximizes the margin between observed entry?ui and unobserved entry?uj.</p><p>Moving one step forward, our NCF framework parameterizes the interaction function f using neural networks to estimate?ui. As such, it naturally supports both pointwise and pairwise learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Matrix Factorization</head><p>MF associates each user and item with a real-valued vector of latent features. Let p u and q i denote the latent vector for user u and item i, respectively; MF estimates an interaction yui as the inner product of p u and q i :</p><formula xml:id="formula_1">yui = f (u, i|p u , q i ) = p T u q i = K k=1 p uk q ik ,<label>(2)</label></formula><p>where K denotes the dimension of the latent space. As we can see, MF models the two-way interaction of user and item latent factors, assuming each dimension of the latent space is independent of each other and linearly combining them with the same weight. As such, MF can be deemed as a linear model of latent factors. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates how the inner product function can limit the expressiveness of MF. There are two settings to be stated clearly beforehand to understand the example well. First, since MF maps users and items to the same latent space, the similarity between two users can also be measured with an inner product, or equivalently 2 , the cosine of the angle between their latent vectors. Second, without loss of As such, the geometric relations of p 1 , p 2 , and p 3 in the latent space can be plotted as in <ref type="figure" target="#fig_1">Figure 1b</ref>. Now, let us consider a new user u4, whose input is given as the dashed line in <ref type="figure" target="#fig_1">Figure 1a</ref>. We can have s41(0.6) &gt; s43(0.4) &gt; s42(0.2), meaning that u4 is most similar to u1, followed by u3, and lastly u2. However, if a MF model places p 4 closest to p 1 (the two options are shown in <ref type="figure" target="#fig_1">Figure 1b</ref> with dashed lines), it will result in p 4 closer to p 2 than p 3 , which unfortunately will incur a large ranking loss.</p><p>The above example shows the possible limitation of MF caused by the use of a simple and fixed inner product to estimate complex user-item interactions in the low-dimensional latent space. We note that one way to resolve the issue is to use a large number of latent factors K. However, it may adversely hurt the generalization of the model (e.g., overfitting the data), especially in sparse settings <ref type="bibr" target="#b25">[26]</ref>. In this work, we address the limitation by learning the interaction function using DNNs from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NEURAL COLLABORATIVE FILTERING</head><p>We first present the general NCF framework, elaborating how to learn NCF with a probabilistic model that emphasizes the binary property of implicit data. We then show that MF can be expressed and generalized under NCF. To explore DNNs for collaborative filtering, we then propose an instantiation of NCF, using a multi-layer perceptron (MLP) to learn the user-item interaction function. Lastly, we present a new neural matrix factorization model, which ensembles MF and MLP under the NCF framework; it unifies the strengths of linearity of MF and non-linearity of MLP for modelling the user-item latent structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Framework</head><p>To permit a full neural treatment of collaborative filtering, we adopt a multi-layer representation to model a user-item interaction yui as shown in <ref type="figure" target="#fig_2">Figure 2</ref>, where the output of one layer serves as the input of the next one. The bottom input layer consists of two feature vectors v U u and v I i that describe user u and item i, respectively; they can be customized to support a wide range of modelling of users and items, such <ref type="bibr" target="#b2">3</ref> Let Ru be the set of items that user u has interacted with, then the Jaccard similarity of users i and j is defined as sij =</p><formula xml:id="formula_2">|R i |?|R j | |R i |?|R j | .</formula><p>as context-aware <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b0">1]</ref>, content-based <ref type="bibr" target="#b2">[3]</ref>, and neighborbased <ref type="bibr" target="#b25">[26]</ref>. Since this work focuses on the pure collaborative filtering setting, we use only the identity of a user and an item as the input feature, transforming it to a binarized sparse vector with one-hot encoding. Note that with such a generic feature representation for inputs, our method can be easily adjusted to address the cold-start problem by using content features to represent users and items.</p><p>Above the input layer is the embedding layer; it is a fully connected layer that projects the sparse representation to a dense vector. The obtained user (item) embedding can be seen as the latent vector for user (item) in the context of latent factor model. The user embedding and item embedding are then fed into a multi-layer neural architecture, which we term as neural collaborative filtering layers, to map the latent vectors to prediction scores. Each layer of the neural CF layers can be customized to discover certain latent structures of user-item interactions. The dimension of the last hidden layer X determines the model's capability. The final output layer is the predicted score?ui, and training is performed by minimizing the pointwise loss between?ui and its target value yui. We note that another way to train the model is by performing pairwise learning, such as using the Bayesian Personalized Ranking <ref type="bibr" target="#b26">[27]</ref> and margin-based loss <ref type="bibr" target="#b32">[33]</ref>. As the focus of the paper is on the neural network modelling part, we leave the extension to pairwise learning of NCF as a future work.</p><p>We now formulate the NCF's predictive model a?</p><formula xml:id="formula_3">yui = f (P T v U u , Q T v I i |P, Q, ? f ),<label>(3)</label></formula><p>where P ? R M ?K and Q ? R N ?K , denoting the latent factor matrix for users and items, respectively; and ? f denotes the model parameters of the interaction function f . Since the function f is defined as a multi-layer neural network, it can be formulated as</p><formula xml:id="formula_4">f (P T v U u , Q T v I i ) = ?out(?X (...?2(?1(P T v U u , Q T v I i ))...)),<label>(4)</label></formula><p>where ?out and ?x respectively denote the mapping function for the output layer and x-th neural collaborative filtering (CF) layer, and there are X neural CF layers in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Learning NCF</head><p>To learn model parameters, existing pointwise methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39]</ref> largely perform a regression with squared loss:</p><formula xml:id="formula_5">Lsqr = (u,i)?Y?Y ? wui(yui ??ui) 2 ,<label>(5)</label></formula><p>where Y denotes the set of observed interactions in Y, and Y ? denotes the set of negative instances, which can be all (or sampled from) unobserved interactions; and wui is a hyperparameter denoting the weight of training instance (u, i).</p><p>While the squared loss can be explained by assuming that observations are generated from a Gaussian distribution <ref type="bibr" target="#b28">[29]</ref>, we point out that it may not tally well with implicit data. This is because for implicit data, the target value yui is a binarized 1 or 0 denoting whether u has interacted with i. In what follows, we present a probabilistic approach for learning the pointwise NCF that pays special attention to the binary property of implicit data.</p><p>Considering the one-class nature of implicit feedback, we can view the value of yui as a label -1 means item i is relevant to u, and 0 otherwise. The prediction score?ui then represents how likely i is relevant to u. To endow NCF with such a probabilistic explanation, we need to constrain the output?ui in the range of [0, 1], which can be easily achieved by using a probabilistic function (e.g., the Logistic or Probit function) as the activation function for the output layer ?out. With the above settings, we then define the likelihood function as</p><formula xml:id="formula_6">p(Y, Y ? |P, Q, ? f ) = (u,i)?Y? ui (u,j)?Y ? (1 ??uj).<label>(6)</label></formula><p>Taking the negative logarithm of the likelihood, we reach</p><formula xml:id="formula_7">L = ? (u,i)?Y log?ui ? (u,j)?Y ? log(1 ??uj) = ? (u,i)?Y?Y ? yui log?ui + (1 ? yui) log(1 ??ui).<label>(7)</label></formula><p>This is the objective function to minimize for the NCF methods, and its optimization can be done by performing stochastic gradient descent (SGD). Careful readers might have realized that it is the same as the binary cross-entropy loss, also known as log loss. By employing a probabilistic treatment for NCF, we address recommendation with implicit feedback as a binary classification problem. As the classificationaware log loss has rarely been investigated in recommendation literature, we explore it in this work and empirically show its effectiveness in Section 4.3. For the negative instances Y ? , we uniformly sample them from unobserved interactions in each iteration and control the sampling ratio w.r.t. the number of observed interactions. While a nonuniform sampling strategy (e.g., item popularity-biased <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref>) might further improve the performance, we leave the exploration as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalized Matrix Factorization (GMF)</head><p>We now show how MF can be interpreted as a special case of our NCF framework. As MF is the most popular model for recommendation and has been investigated extensively in literature, being able to recover it allows NCF to mimic a large family of factorization models <ref type="bibr" target="#b25">[26]</ref>.</p><p>Due to the one-hot encoding of user (item) ID of the input layer, the obtained embedding vector can be seen as the latent vector of user (item). Let the user latent vector p u be P T v U u and item latent vector q i be Q T v I i . We define the mapping function of the first neural CF layer as</p><formula xml:id="formula_8">?1(p u , q i ) = p u q i ,<label>(8)</label></formula><p>where denotes the element-wise product of vectors. We then project the vector to the output layer:</p><formula xml:id="formula_9">yui = aout(h T (p u q i )),<label>(9)</label></formula><p>where aout and h denote the activation function and edge weights of the output layer, respectively. Intuitively, if we use an identity function for aout and enforce h to be a uniform vector of 1, we can exactly recover the MF model. Under the NCF framework, MF can be easily generalized and extended. For example, if we allow h to be learnt from data without the uniform constraint, it will result in a variant of MF that allows varying importance of latent dimensions. And if we use a non-linear function for aout, it will generalize MF to a non-linear setting which might be more expressive than the linear MF model. In this work, we implement a generalized version of MF under NCF that uses the sigmoid function ?(x) = 1/(1 + e ?x ) as aout and learns h from data with the log loss (Section 3.1.1). We term it as GMF, short for Generalized Matrix Factorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Layer Perceptron (MLP)</head><p>Since NCF adopts two pathways to model users and items, it is intuitive to combine the features of two pathways by concatenating them. This design has been widely adopted in multimodal deep learning work <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b33">34]</ref>. However, simply a vector concatenation does not account for any interactions between user and item latent features, which is insufficient for modelling the collaborative filtering effect. To address this issue, we propose to add hidden layers on the concatenated vector, using a standard MLP to learn the interaction between user and item latent features. In this sense, we can endow the model a large level of flexibility and non-linearity to learn the interactions between p u and q i , rather than the way of GMF that uses only a fixed element-wise product on them. More precisely, the MLP model under our NCF framework is defined as</p><formula xml:id="formula_10">z1 = ?1(p u , q i ) = p u q i , ?2(z1) = a2(W T 2 z1 + b2), ...... ?L(zL?1) = aL(W T L zL?1 + bL), yui = ?(h T ?L(zL?1)),<label>(10)</label></formula><p>where Wx, bx, and ax denote the weight matrix, bias vector, and activation function for the x-th layer's perceptron, respectively. For activation functions of MLP layers, one can freely choose sigmoid, hyperbolic tangent (tanh), and Rectifier (ReLU), among others. We would like to analyze each function: 1) The sigmoid function restricts each neuron to be in (0,1), which may limit the model's performance; and it is known to suffer from saturation, where neurons stop learning when their output is near either 0 or 1. 2) Even though tanh is a better choice and has been widely adopted <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b43">44]</ref>, it only alleviates the issues of sigmoid to a certain extent, since it can be seen as a rescaled version of sigmoid (tanh(x/2) = 2?(x) ? 1). And 3) as such, we opt for ReLU, which is more biologically plausible and proven to be non-saturated <ref type="bibr" target="#b8">[9]</ref>; moreover, it encourages sparse activations, being well-suited for sparse data and making the model less likely to be overfitting. Our empirical results show that ReLU yields slightly better performance than tanh, which in turn is significantly better than sigmoid.</p><p>As for the design of network structure, a common solution is to follow a tower pattern, where the bottom layer is the widest and each successive layer has a smaller number of neurons (as in <ref type="figure" target="#fig_2">Figure 2</ref>). The premise is that by using a small number of hidden units for higher layers, they can learn more abstractive features of data <ref type="bibr" target="#b9">[10]</ref>. We empirically implement the tower structure, halving the layer size for each successive higher layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fusion of GMF and MLP</head><p>So far we have developed two instantiations of NCF -GMF that applies a linear kernel to model the latent feature interactions, and MLP that uses a non-linear kernel to learn the interaction function from data. The question then arises: how can we fuse GMF and MLP under the NCF framework,  <ref type="figure">Figure 3</ref>: Neural matrix factorization model so that they can mutually reinforce each other to better model the complex user-iterm interactions? A straightforward solution is to let GMF and MLP share the same embedding layer, and then combine the outputs of their interaction functions. This way shares a similar spirit with the well-known Neural Tensor Network (NTN) <ref type="bibr" target="#b32">[33]</ref>. Specifically, the model for combining GMF with a one-layer MLP can be formulated a?</p><formula xml:id="formula_11">yui = ?(h T a(p u q i + W p u q i + b)).<label>(11)</label></formula><p>However, sharing embeddings of GMF and MLP might limit the performance of the fused model. For example, it implies that GMF and MLP must use the same size of embeddings; for datasets where the optimal embedding size of the two models varies a lot, this solution may fail to obtain the optimal ensemble.</p><p>To provide more flexibility to the fused model, we allow GMF and MLP to learn separate embeddings, and combine the two models by concatenating their last hidden layer. <ref type="figure">Figure 3</ref> illustrates our proposal, the formulation of which is given as follows</p><formula xml:id="formula_12">? GM F = p G u q G i , ? M LP = aL(W T L (aL?1(...a2(W T 2 p M u q M i + b2)...)) + bL), yui = ?(h T ? GM F ? M LP ),<label>(12)</label></formula><p>where p G u and p M u denote the user embedding for GMF and MLP parts, respectively; and similar notations of q G i and q M i for item embeddings. As discussed before, we use ReLU as the activation function of MLP layers. This model combines the linearity of MF and non-linearity of DNNs for modelling user-item latent structures. We dub this model "NeuMF", short for Neural Matrix Factorization. The derivative of the model w.r.t. each model parameter can be calculated with standard back-propagation, which is omitted here due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Pre-training</head><p>Due to the non-convexity of the objective function of NeuMF, gradient-based optimization methods only find locally-optimal solutions. It is reported that the initialization plays an important role for the convergence and performance of deep learning models <ref type="bibr" target="#b6">[7]</ref>. Since NeuMF is an ensemble of GMF and MLP, we propose to initialize NeuMF using the pretrained models of GMF and MLP.</p><p>We first train GMF and MLP with random initializations until convergence. We then use their model parameters as the initialization for the corresponding parts of NeuMF's parameters. The only tweak is on the output layer, where we concatenate weights of the two models with</p><formula xml:id="formula_13">h ? ?h GM F (1 ? ?)h M LP ,<label>(13)</label></formula><p>where h GM F and h M LP denote the h vector of the pretrained GMF and MLP model, respectively; and ? is a hyper-parameter determining the trade-off between the two pre-trained models.</p><p>For training GMF and MLP from scratch, we adopt the Adaptive Moment Estimation (Adam) <ref type="bibr" target="#b19">[20]</ref>, which adapts the learning rate for each parameter by performing smaller updates for frequent and larger updates for infrequent parameters. The Adam method yields faster convergence for both models than the vanilla SGD and relieves the pain of tuning the learning rate. After feeding pre-trained parameters into NeuMF, we optimize it with the vanilla SGD, rather than Adam. This is because Adam needs to save momentum information for updating parameters properly. As we initialize NeuMF with pre-trained model parameters only and forgo saving the momentum information, it is unsuitable to further optimize NeuMF with momentum-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we conduct experiments with the aim of answering the following research questions:</p><p>RQ1 Do our proposed NCF methods outperform the stateof-the-art implicit collaborative filtering methods?</p><p>RQ2 How does our proposed optimization framework (log loss with negative sampling) work for the recommendation task?</p><p>RQ3 Are deeper layers of hidden units helpful for learning from user-item interaction data?</p><p>In what follows, we first present the experimental settings, followed by answering the above three research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets. We experimented with two publicly accessible datasets: MovieLens 4 and Pinterest 5 . The characteristics of the two datasets are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>1. MovieLens. This movie rating dataset has been widely used to evaluate collaborative filtering algorithms. We used the version containing one million ratings, where each user has at least 20 ratings. While it is an explicit feedback data, we have intentionally chosen it to investigate the performance of learning from the implicit signal <ref type="bibr" target="#b20">[21]</ref> of explicit feedback. To this end, we transformed it into implicit data, where each entry is marked as 0 or 1 indicating whether the user has rated the item.</p><p>2. Pinterest. This implicit feedback data is constructed by <ref type="bibr" target="#b7">[8]</ref> for evaluating content-based image recommendation. The original data is very large but highly sparse. For example, over 20% of users have only one pin, making it difficult to evaluate collaborative filtering algorithms. As such, we filtered the dataset in the same way as the MovieLens data that retained only users with at least 20 interactions (pins). This results in a subset of the data that contains 55, 187 users and 1, 500, 809 interactions. Each interaction denotes whether the user has pinned the image to her own board.</p><p>Evaluation Protocols. To evaluate the performance of item recommendation, we adopted the leave-one-out evaluation, which has been widely used in literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>For each user, we held-out her latest interaction as the test set and utilized the remaining data for training. Since it is too time-consuming to rank all items for every user during evaluation, we followed the common strategy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref> that randomly samples 100 items that are not interacted by the user, ranking the test item among the 100 items. The performance of a ranked list is judged by Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) <ref type="bibr" target="#b10">[11]</ref>. Without special mention, we truncated the ranked list at 10 for both metrics. As such, the HR intuitively measures whether the test item is present on the top-10 list, and the NDCG accounts for the position of the hit by assigning higher scores to hits at top ranks. We calculated both metrics for each test user and reported the average score.</p><p>Baselines. We compared our proposed NCF methods (GMF, MLP and NeuMF) with the following methods: -ItemPop. Items are ranked by their popularity judged by the number of interactions. This is a non-personalized method to benchmark the recommendation performance <ref type="bibr" target="#b26">[27]</ref>.</p><p>-ItemKNN <ref type="bibr" target="#b30">[31]</ref>. This is the standard item-based collaborative filtering method. We followed the setting of <ref type="bibr" target="#b18">[19]</ref> to adapt it for implicit data.</p><p>-BPR <ref type="bibr" target="#b26">[27]</ref>. This method optimizes the MF model of Equation 2 with a pairwise ranking loss, which is tailored to learn from implicit feedback. It is a highly competitive baseline for item recommendation. We used a fixed learning rate, varying it and reporting the best performance.</p><p>-eALS <ref type="bibr" target="#b13">[14]</ref>. This is a state-of-the-art MF method for item recommendation. It optimizes the squared loss of Equation 5, treating all unobserved interactions as negative instances and weighting them non-uniformly by the item popularity. Since eALS shows superior performance over the uniform-weighting method WMF <ref type="bibr" target="#b18">[19]</ref>, we do not further report WMF's performance.</p><p>As our proposed methods aim to model the relationship between users and items, we mainly compare with useritem models. We leave out the comparison with item-item models, such as SLIM <ref type="bibr" target="#b24">[25]</ref> and CDAE <ref type="bibr" target="#b43">[44]</ref>, because the performance difference may be caused by the user models for personalization (as they are item-item model).</p><p>Parameter Settings. We implemented our proposed methods based on Keras 6 . To determine hyper-parameters of NCF methods, we randomly sampled one interaction for each user as the validation data and tuned hyper-parameters on it. All NCF models are learnt by optimizing the log loss of Equation 7, where we sampled four negative instances per positive instance. For NCF models that are trained from scratch, we randomly initialized model parameters with a Gaussian distribution (with a mean of 0 and standard deviation of 0.01), optimizing the model with mini-batch Adam <ref type="bibr" target="#b19">[20]</ref>. We tested the batch size of <ref type="bibr">[128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024]</ref>, and the learning rate of [0.0001 ,0.0005, 0.001, 0.005]. Since the last hidden layer of NCF determines the model capability, we term it as predictive factors and evaluated the factors of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">64]</ref>. It is worth noting that large factors may cause overfitting and degrade the performance. Without special mention, we employed three hidden layers for MLP; for example, if the size of predictive factors is 8, then the architecture of the neural CF layers is 32 ? 16 ? 8, and the embedding size is 16. For the NeuMF with pre-training, ? was set to 0.5, allowing the pre-trained GMF and MLP to contribute equally to NeuMF's initialization. <ref type="figure">Figure 4</ref> shows the performance of HR@10 and NDCG@10 with respect to the number of predictive factors. For MF methods BPR and eALS, the number of predictive factors is equal to the number of latent factors. For ItemKNN, we tested different neighbor sizes and reported the best performance. Due to the weak performance of ItemPop, it is omitted in <ref type="figure">Figure 4</ref> to better highlight the performance difference of personalized methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison (RQ1)</head><p>First, we can see that NeuMF achieves the best performance on both datasets, significantly outperforming the stateof-the-art methods eALS and BPR by a large margin (on average, the relative improvement over eALS and BPR is 4.5% and 4.9%, respectively). For Pinterest, even with a small predictive factor of 8, NeuMF substantially outperforms that of eALS and BPR with a large factor of 64. This indicates the high expressiveness of NeuMF by fusing the linear MF and non-linear MLP models. Second, the other two NCF methods -GMF and MLP -also show quite strong performance. Between them, MLP slightly underperforms GMF. Note that MLP can be further improved by adding more hidden layers (see Section 4.4), and here we only show the performance of three layers. For small predictive factors, GMF outperforms eALS on both datasets; although GMF suffers from overfitting for large factors, its best performance obtained is better than (or on par with) that of eALS. Lastly, GMF shows consistent improvements over BPR, admitting the effectiveness of the classificationaware log loss for the recommendation task, since GMF and BPR learn the same MF model but with different objective functions. <ref type="figure">Figure 5</ref> shows the performance of Top-K recommended lists where the ranking position K ranges from 1 to 10. To make the figure more clear, we show the performance of NeuMF rather than all three NCF methods. As can be seen, NeuMF demonstrates consistent improvements over other methods across positions, and we further conducted one-sample paired t-tests, verifying that all improvements are statistically significant for p &lt; 0.01. For baseline methods, eALS outperforms BPR on MovieLens with about 5.1% relative improvement, while underperforms BPR on Pinterest in terms of NDCG. This is consistent with <ref type="bibr" target="#b13">[14]</ref>'s finding that BPR can be a strong performer for ranking performance owing to its pairwise ranking-aware learner. The neighborbased ItemKNN underperforms model-based methods. And ItemPop performs the worst, indicating the necessity of modeling users' personalized preferences, rather than just recommending popular items to users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Utility of Pre-training</head><p>To demonstrate the utility of pre-training for NeuMF, we compared the performance of two versions of NeuMFwith and without pre-training. For NeuMF without pretraining, we used the Adam to learn it with random initializations. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the NeuMF with pretraining achieves better performance in most cases; only for MovieLens with a small predictive factors of 8, the pretraining method performs slightly worse. The relative improvements of the NeuMF with pre-training are 2.2% and 1.1% for MovieLens and Pinterest, respectively. This result justifies the usefulness of our pre-training method for initializing NeuMF. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Log Loss with Negative Sampling (RQ2)</head><p>To deal with the one-class nature of implicit feedback, we cast recommendation as a binary classification task. By viewing NCF as a probabilistic model, we optimized it with the log loss. <ref type="figure">Figure 6</ref> shows the training loss (averaged over all instances) and recommendation performance of NCF methods of each iteration on MovieLens. Results on Pinterest show the same trend and thus they are omitted due to space limitation. First, we can see that with more iterations, the training loss of NCF models gradually decreases and the recommendation performance is improved. The most effective updates are occurred in the first 10 iterations, and more iterations may overfit a model (e.g., although the training loss of NeuMF keeps decreasing after 10 iterations, its recommendation performance actually degrades). Second, among the three NCF methods, NeuMF achieves the lowest training loss, followed by MLP, and then GMF. The recommendation performance also shows the same trend that NeuMF &gt; MLP &gt; GMF. The above findings provide empirical evidence for the rationality and effectiveness of optimizing the log loss for learning from implicit data.</p><p>An advantage of pointwise log loss over pairwise objective functions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref> is the flexible sampling ratio for negative instances. While pairwise objective functions can pair only one sampled negative instance with a positive instance, we can flexibly control the sampling ratio of a pointwise loss. To illustrate the impact of negative sampling for NCF methods, we show the performance of NCF methods w.r.t. different negative sampling ratios in <ref type="figure">Figure 7</ref>. It can be clearly seen that just one negative sample per positive instance is insufficient to achieve optimal performance, and sampling more negative instances is beneficial. Comparing GMF to BPR, we can see the performance of GMF with a sampling ratio of one is on par with BPR, while GMF significantly betters BPR with larger sampling ratios. This shows the advantage of pointwise log loss over the pairwise BPR loss. For both datasets, the optimal sampling ratio is around 3 to 6. On Pinterest, we find that when the sampling ratio is larger than 7, the performance of NCF methods starts to drop. It reveals that setting the sampling ratio too aggressively may adversely hurt the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Is Deep Learning Helpful? (RQ3)</head><p>As there is little work on learning user-item interaction function with neural networks, it is curious to see whether using a deep network structure is beneficial to the recommendation task. Towards this end, we further investigated MLP with different number of hidden layers. The results are summarized in <ref type="table" target="#tab_2">Table 3</ref> and 4. The MLP-3 indicates the MLP method with three hidden layers (besides the embedding layer), and similar notations for others. As we can see, even for models with the same capability, stacking more layers are beneficial to performance. This result is highly encouraging, indicating the effectiveness of using deep models for collaborative recommendation. We attribute the improvement to the high non-linearities brought by stacking more non-linear layers. To verify this, we further tried stacking linear layers, using an identity function as the activation function. The performance is much worse than using the ReLU unit.</p><p>For MLP-0 that has no hidden layers (i.e., the embedding layer is directly projected to predictions), the performance is very weak and is not better than the non-personalized Item-Pop. This verifies our argument in Section 3.3 that simply concatenating user and item latent vectors is insufficient for modelling their feature interactions, and thus the necessity of transforming it with hidden layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>While early literature on recommendation has largely focused on explicit feedback <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, recent attention is increasingly shifting towards implicit data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>. The collaborative filtering (CF) task with implicit feedback is usually formulated as an item recommendation problem, for which the aim is to recommend a short list of items to users. In contrast to rating prediction that has been widely solved by work on explicit feedback, addressing the item recommendation problem is more practical but challenging <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>. One key insight is to model the missing data, which are always ignored by the work on explicit feedback <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b47">48]</ref>. To tailor latent factor models for item recommendation with implicit feedback, early work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref> applies a uniform weighting where two strategies have been proposed -which either treated all missing data as negative instances <ref type="bibr" target="#b18">[19]</ref> or sampled negative instances from missing data <ref type="bibr" target="#b26">[27]</ref>. Recently, He et al. <ref type="bibr" target="#b13">[14]</ref> and Liang et al. <ref type="bibr" target="#b22">[23]</ref> proposed dedicated models to weight missing data, and Rendle et al. <ref type="bibr" target="#b0">[1]</ref> developed an implicit coordinate descent (iCD) solution for feature-based factorization models, achieving state-of-the-art performance for item recommendation. In the following, we discuss recommendation works that use neural networks. The early pioneer work by Salakhutdinov et al. <ref type="bibr" target="#b29">[30]</ref> proposed a two-layer Restricted Boltzmann Machines (RBMs) to model users' explicit ratings on items. The work was been later extended to model the ordinal nature of ratings <ref type="bibr" target="#b35">[36]</ref>. Recently, autoencoders have become a popular choice for building recommendation systems <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref>. The idea of user-based AutoRec <ref type="bibr" target="#b31">[32]</ref> is to learn hidden structures that can reconstruct a user's ratings given her historical ratings as inputs. In terms of user personalization, this approach shares a similar spirit as the item-item model <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref> that represents a user as her rated items. To avoid autoencoders learning an identity function and failing to generalize to unseen data, denoising autoencoders (DAEs) have been applied to learn from intentionally corrupted inputs <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>. More recently, Zheng et al. <ref type="bibr" target="#b47">[48]</ref> presented a neural autoregressive method for CF. While the previous effort has lent support to the effectiveness of neural networks for addressing CF, most of them focused on explicit ratings and modelled the observed data only. As a result, they can easily fail to learn users' preference from the positive-only implicit data.</p><p>Although some recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref> have explored deep learning models for recommendation based on implicit feedback, they primarily used DNNs for modelling auxiliary information, such as textual description of items <ref type="bibr" target="#b37">[38]</ref>, acoustic features of musics <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43]</ref>, cross-domain behaviors of users <ref type="bibr" target="#b5">[6]</ref>, and the rich information in knowledge bases <ref type="bibr" target="#b44">[45]</ref>. The features learnt by DNNs are then integrated with MF for CF. The work that is most relevant to our work is <ref type="bibr" target="#b43">[44]</ref>, which presents a collaborative denoising autoencoder (CDAE) for CF with implicit feedback. In contrast to the DAE-based CF <ref type="bibr" target="#b34">[35]</ref>, CDAE additionally plugs a user node to the input of autoencoders for reconstructing the user's ratings. As shown by the authors, CDAE is equivalent to the SVD++ model <ref type="bibr" target="#b20">[21]</ref> when the identity function is applied to activate the hidden layers of CDAE. This implies that although CDAE is a neural modelling approach for CF, it still applies a linear kernel (i.e., inner product) to model user-item interactions. This may partially explain why using deep layers for CDAE does not improve the performance (cf. Section 6 of <ref type="bibr" target="#b43">[44]</ref>). Distinct from CDAE, our NCF adopts a two-pathway architecture, modelling user-item interactions with a multilayer feedforward neural network. This allows NCF to learn an arbitrary function from the data, being more powerful and expressive than the fixed inner product function.</p><p>Along a similar line, learning the relations of two entities has been intensively studied in literature of knowledge graphs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>. Many relational machine learning methods have been devised <ref type="bibr" target="#b23">[24]</ref>. The one that is most similar to our proposal is the Neural Tensor Network (NTN) <ref type="bibr" target="#b32">[33]</ref>, which uses neural networks to learn the interaction of two entities and shows strong performance. Here we focus on a different problem setting of CF. While the idea of NeuMF that combines MF with MLP is partially inspired by NTN, our NeuMF is more flexible and generic than NTN, in terms of allowing MF and MLP learning different sets of embeddings.</p><p>More recently, Google publicized their Wide &amp; Deep learning approach for App recommendation <ref type="bibr" target="#b3">[4]</ref>. The deep component similarly uses a MLP on feature embeddings, which has been reported to have strong generalization ability. While their work has focused on incorporating various features of users and items, we target at exploring DNNs for pure collaborative filtering systems. We show that DNNs are a promising choice for modelling user-item interactions, which to our knowledge has not been investigated before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>In this work, we explored neural network architectures for collaborative filtering. We devised a general framework NCF and proposed three instantiations -GMF, MLP and NeuMF -that model user-item interactions in different ways. Our framework is simple and generic; it is not limited to the models presented in this paper, but is designed to serve as a guideline for developing deep learning methods for recommendation. This work complements the mainstream shallow models for collaborative filtering, opening up a new avenue of research possibilities for recommendation based on deep learning.</p><p>In future, we will study pairwise learners for NCF models and extend NCF to model auxiliary information, such as user reviews <ref type="bibr" target="#b10">[11]</ref>, knowledge bases <ref type="bibr" target="#b44">[45]</ref>, and temporal signals <ref type="bibr" target="#b0">[1]</ref>. While existing personalization models have primarily focused on individuals, it is interesting to develop models for groups of users, which help the decision-making for social groups <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref>. Moreover, we are particularly interested in building recommender systems for multi-media items, an interesting task but has received relatively less scrutiny in the recommendation community <ref type="bibr" target="#b2">[3]</ref>. Multi-media items, such as images and videos, contain much richer visual semantics <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref> that can reflect users' interest. To build a multi-media recommender system, we need to develop effective methods to learn from multi-view and multi-modal data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">40]</ref>. Another emerging direction is to explore the potential of recurrent neural networks and hashing methods <ref type="bibr" target="#b45">[46]</ref> for providing efficient online recommendation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>An example illustrates MF's limitation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>PFigure 2 :</head><label>2</label><figDesc>M?K = {p uk } Q N?K = {q ik } Neural collaborative filtering framework generality, we use the Jaccard coefficient 3 as the groundtruth similarity of two users that MF needs to recover. Let us first focus on the first three rows (users) in Figure 1a. It is easy to have s23(0.66) &gt; s12(0.5) &gt; s13(0.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Performance of HR@10 and NDCG@10 w.r.t. the number of predictive factors on the two datasets. Evaluation of Top-K item recommendation where K ranges from 1 to 10 on the two datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Training loss and recommendation performance of NCF methods w.r.t. the number of iterations on MovieLens (factors=8). Performance of NCF methods w.r.t. the number of negative samples per positive instance (fac-tors=16). The performance of BPR is also shown, which samples only one negative instance to pair with a positive instance for learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the evaluation datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Interaction# Item# User# Sparsity</cell></row><row><cell>MovieLens</cell><cell>1,000,209</cell><cell>3,706</cell><cell>6,040</cell><cell>95.53%</cell></row><row><cell>Pinterest</cell><cell>1,500,809</cell><cell>9,916</cell><cell>55,187</cell><cell>99.73%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of NeuMF with and without pre-training.</figDesc><table><row><cell></cell><cell cols="2">With Pre-training</cell><cell cols="2">Without Pre-training</cell></row><row><cell cols="5">Factors HR@10 NDCG@10 HR@10 NDCG@10</cell></row><row><cell></cell><cell></cell><cell>MovieLens</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>0.684</cell><cell>0.403</cell><cell>0.688</cell><cell>0.410</cell></row><row><cell>16</cell><cell>0.707</cell><cell>0.426</cell><cell>0.696</cell><cell>0.420</cell></row><row><cell>32</cell><cell>0.726</cell><cell>0.445</cell><cell>0.701</cell><cell>0.425</cell></row><row><cell>64</cell><cell>0.730</cell><cell>0.447</cell><cell>0.705</cell><cell>0.426</cell></row><row><cell></cell><cell></cell><cell>Pinterest</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>0.878</cell><cell>0.555</cell><cell>0.869</cell><cell>0.546</cell></row><row><cell>16</cell><cell>0.880</cell><cell>0.558</cell><cell>0.871</cell><cell>0.547</cell></row><row><cell>32</cell><cell>0.879</cell><cell>0.555</cell><cell>0.870</cell><cell>0.549</cell></row><row><cell>64</cell><cell>0.877</cell><cell>0.552</cell><cell>0.872</cell><cell>0.551</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>HR@10 of MLP with different layers.</figDesc><table><row><cell cols="6">Factors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4</cell></row><row><cell></cell><cell></cell><cell cols="2">MovieLens</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>0.452</cell><cell>0.628</cell><cell>0.655</cell><cell>0.671</cell><cell>0.678</cell></row><row><cell>16</cell><cell>0.454</cell><cell>0.663</cell><cell>0.674</cell><cell>0.684</cell><cell>0.690</cell></row><row><cell>32</cell><cell>0.453</cell><cell>0.682</cell><cell>0.687</cell><cell>0.692</cell><cell>0.699</cell></row><row><cell>64</cell><cell>0.453</cell><cell>0.687</cell><cell>0.696</cell><cell>0.702</cell><cell>0.707</cell></row><row><cell></cell><cell></cell><cell cols="2">Pinterest</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>0.275</cell><cell>0.848</cell><cell>0.855</cell><cell>0.859</cell><cell>0.862</cell></row><row><cell>16</cell><cell>0.274</cell><cell>0.855</cell><cell>0.861</cell><cell>0.865</cell><cell>0.867</cell></row><row><cell>32</cell><cell>0.273</cell><cell>0.861</cell><cell>0.863</cell><cell>0.868</cell><cell>0.867</cell></row><row><cell>64</cell><cell>0.274</cell><cell>0.864</cell><cell>0.867</cell><cell>0.869</cell><cell>0.873</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>NDCG@10 of MLP with different layers.</figDesc><table><row><cell cols="6">Factors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4</cell></row><row><cell></cell><cell></cell><cell cols="2">MovieLens</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>0.253</cell><cell>0.359</cell><cell>0.383</cell><cell>0.399</cell><cell>0.406</cell></row><row><cell>16</cell><cell>0.252</cell><cell>0.391</cell><cell>0.402</cell><cell>0.410</cell><cell>0.415</cell></row><row><cell>32</cell><cell>0.252</cell><cell>0.406</cell><cell>0.410</cell><cell>0.425</cell><cell>0.423</cell></row><row><cell>64</cell><cell>0.251</cell><cell>0.409</cell><cell>0.417</cell><cell>0.426</cell><cell>0.432</cell></row><row><cell></cell><cell></cell><cell cols="2">Pinterest</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>0.141</cell><cell>0.526</cell><cell>0.534</cell><cell>0.536</cell><cell>0.539</cell></row><row><cell>16</cell><cell>0.141</cell><cell>0.532</cell><cell>0.536</cell><cell>0.538</cell><cell>0.544</cell></row><row><cell>32</cell><cell>0.142</cell><cell>0.537</cell><cell>0.538</cell><cell>0.542</cell><cell>0.546</cell></row><row><cell>64</cell><cell>0.141</cell><cell>0.538</cell><cell>0.542</cell><cell>0.545</cell><cell>0.550</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://alex.smola.org/teaching/berkeley2012/slides/8_ Recommender.pdf</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Assuming latent vectors are of a unit length.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://grouplens.org/datasets/movielens/1m/ 5 https://sites.google.com/site/xueatalphabeta/ academic-projects</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/hexiangnan/neural_ collaborative_filtering</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors thank the anonymous reviewers for their valuable comments, which are beneficial to the authors' thoughts on recommendation systems and the revision of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A generic coordinate descent framework for learning from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context-aware image tweet modelling and recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1018" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07792</idno>
		<title level="m">Wide &amp; deep learning for recommender systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multi-view deep learning approach for cross domain user modeling in recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Elkahky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="278" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning image and user features for recommendation in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4274" to="4282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TriRank: Review-aware explainable recommendation by modeling aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1661" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting the popularity of web 2.0 items based on user comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comment-based multi-view clustering of web 2.0 items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="771" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast matrix factorization for online recommendation with implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding blooming human groups in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1980" to="1988" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning visual semantic relationships for efficient visual retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="152" to="161" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Your neighbors affect your ratings: On geographical neighborhood influence to rating prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: A multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep collaborative filtering via marginalized denoising auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling user exposure in recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="951" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Slim: Sparse linear methods for top-n recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast context-aware recommendations with factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Collaborative filtering with stacked denoising autoencoders and sparse inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Machine Learning for eCommerce</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ordinal boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Truyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<biblScope unit="page" from="1235" to="1244" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable semi-supervised learning by efficient anchor graph regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1864" to="1877" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal graph-based reranking for web image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4649" to="4661" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual classification by l1 hypergraph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2564" to="2574" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unifying virtual and physical worlds: Learning towards local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving content-based and hybrid music recommendation using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collaborative denoising auto-encoders for top-n recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<biblScope unit="page" from="353" to="362" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discrete collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Start from scratch: Towards automatically identifying, modeling, and naming visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A neural autoregressive approach to collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
