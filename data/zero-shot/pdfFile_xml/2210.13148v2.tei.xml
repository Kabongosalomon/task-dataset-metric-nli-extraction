<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAGFORMER: DIRECTED ACYCLIC GRAPH TRANSFORMER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ACT&amp;BDBC</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DAGFORMER: DIRECTED ACYCLIC GRAPH TRANSFORMER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many fields, such as natural language processing and computer vision, the Transformer architecture has become the standard. Recently, the Transformer architecture has also attracted a growing amount of interest in graph representation learning since it naturally overcomes some graph neural network (GNNs) restrictions. In this work, we focus on a special yet widely used class of graphs-DAGs. We propose the directed acyclic graph Transformer, DAGformer, a Transformer architecture that processes information according to the reachability relation defined by the partial order. DAGformer is simple and flexible, allowing it to be used with various transformer-based models. We show that our architecture achieves state-of-the-art performance on representative DAG datasets, outperforming all previous approaches. The source code is available at https://github.com/LUOyk1999/DAGformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Graph neural networks (GNNs) have shown to be an efficient representation learning framework for graph-structured data. A broad family of GNNs construct multilayer models in which each layer operates on the previous layer to generate new representations utilizing a message-passing mechanism <ref type="bibr" target="#b0">[1]</ref> to aggregate neighborhood-specific information. Among different graph types, directed acyclic graphs (DAGs) are of special interest to machine learning researchers since many machine learning models, such as abstract syntax trees and neural architectures. A directed graph is a DAG if and only if the edges define a partial ordering over the nodes. The partial order is an additional strong inductive bias that one would naturally like to integrate into the neural network. By the information flow from predecessors to successors through edges, DAGs could gather information like a recurrence structure. A number of neural networks that employ DAGs architecture have been proposed, such as Tree-LSTM <ref type="bibr" target="#b1">[2]</ref>, DAG-RNN <ref type="bibr" target="#b2">[3]</ref>, D-VAE <ref type="bibr" target="#b3">[4]</ref> and DAGNN <ref type="bibr" target="#b4">[5]</ref>. Specifically, DAGNN conducts information aggregation using graph attention.</p><p>Even though those message-passing mechanisms have been developed, critical limits have been found. These include the limited expressiveness of GNNs as well as known problems such as over-smoothing and oversquashing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Recently, graph Transformers alleviate these basic constraints of the sparse message passing mechanism by enabling nodes to attend to all other nodes in a graph (global attention). However, most existing approaches are designed for undirected graphs, and they perform generally in DAGs.</p><p>In this work, we try to address the question of how to encode DAGs information into a Transformer architecture. We propose DAGformer-directed acyclic graph Transformer-that produce a representation for a DAG. Our principal contribution is to introduce a self-attention mechanism that considers the DAG reachability relation and thus captures effective interaction between nodes. In particular, we have modified the global attention mechanism by only allowing nodes to attend to their ancestors and descendants in a DAG. Furthermore, we propose an efficient positional encoding that can be implemented in linear time for DAGs. We show that our architecture can be used for any transformer-based graph representation model. Using our novel architecture DAGformer, we obtain state-of-the-art results on OpenGraphBenchmark-CODE2 dataset <ref type="bibr" target="#b7">[8]</ref> and the NA dataset <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In this section, we present our notation and then review relevant prior work. We refer to a graph as G = (V, E, X), where V and E ? V ? V are respectively the node set and the edge set. Node features are stored in X ? R n?d with each row representing the feature vector of one node, where n is the number of nodes and d is the dimension of the features, while the attributes of node v is denoted by x v ? R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Message Passing Graph Neural Networks</head><p>Recently, message passing graph neural networks have become one of the leading approaches to learning graph representations. The GCN <ref type="bibr" target="#b8">[9]</ref>, which was based on executing convolutions on the graph, is an early seminal example. Gilmer et al. <ref type="bibr" target="#b0">[1]</ref> reformulated the early GNNs into a framework of message passing GNNs, which computes representations h v for all nodes v in every layer , and a final graph representation h G as</p><formula xml:id="formula_0">h l v = U h ?1 v , M h ?1 u | u ? N (v) ,<label>(1)</label></formula><formula xml:id="formula_1">h G = R h L v , v ? V ,<label>(2)</label></formula><p>where h 0 v = x v is the input feature of node v, N (v) denotes a neighborhood of v, L is the number of layers, M are message functions, U are update functions, and R are readout functions.</p><p>As for those GNNs designs for DAGs, D-VAE <ref type="bibr" target="#b3">[4]</ref> has been proposed as a general purpose autoencoder for DAGs. Another notable work related to DAGs is the DAGNN (Directed Acyclic Graph Neural Network) <ref type="bibr" target="#b4">[5]</ref> of Thost and Chen, which incorporates the partial ordering entailed by DAGs as a strong inductive bias towards representation learning. However, as mentioned above, they have issues with limited expressiveness, over-smoothing, and over-squashing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformers on Graphs</head><p>In light of the tremendous success of Transformers in natural language processing (NLP) <ref type="bibr" target="#b9">[10]</ref>, it is reasonable to investigate their applicability in the graph domain. Particularly, they are anticipated to ease the difficulties of oversmoothing and over-squashing in message passing GNNs. Graph Transformer <ref type="bibr" target="#b10">[11]</ref> provided an early illustration of how to generalize the Transformer architecture to graphs, using Laplacian eigenvectors as an positional encoding (PE). Developing upon this work, SAN <ref type="bibr" target="#b11">[12]</ref> also used the Laplacian's eigenvectors for the PE and computing attention on the full graph. The Transformer itself is composed of two main parts: a self-attention module and a feed-forward neural network. In the self-attention module, the input node features X are projected by three matrices W Q ? R d?d K , W K ? R d?d K and W V ? R d?d K to the corresponding representations query(Q), key(K) and value (V). We can compute the selfattention via</p><formula xml:id="formula_2">Q = XW Q , K = XW K , V = XW V ,<label>(3)</label></formula><formula xml:id="formula_3">Attention (X) = softmax QK T ? d K V.<label>(4)</label></formula><p>Further, GraphTrans <ref type="bibr" target="#b12">[13]</ref> proposed the first hybrid architecture, using a stack of GNNs layers before the Transformer layers. Mialon et al. <ref type="bibr" target="#b13">[14]</ref> proposed a relative PE <ref type="bibr" target="#b14">[15]</ref> by means of kernels on graphs to bias the self-attention calculation. Finally, SAT <ref type="bibr" target="#b15">[16]</ref> reformulated the self-attention mechanism as a kernel smoother and incorporate graphs structural information into the Transformer architecture. As noticed by SAT, the self-attention in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref> can be rewritten as a kernel smoother:</p><formula xml:id="formula_4">Attention (x v ) = u?V k (x v , x u ) w?V k (x v , x w ) f (x u ) , ?v ? V,<label>(5)</label></formula><p>where f (x) = W V x, and k is a non-symmetric exponential kernel:</p><formula xml:id="formula_5">k (x, x ) = exp xW Q , x W K ? d K ,<label>(6)</label></formula><p>where ?, ? is the dot product on R d . In our work, we implement DAGformer based on Transformer architecture because it is able to overcomes limitations of over-smoothing and over-squashing on GNNs. We show that DAGformer outperforms the baselines on DAGs tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD: DAGFORMER</head><p>A DAG is a directed graph with no directed cycles. For any DAG, one can define a unique strong partial order on the node set V , such that for all pairs of elements u, v ? V , u v if and only if there is a directed path from u to v. The reachability relation of a DAG can be formalized as the partial order. That means, u v, when u can reach v (or v is reachable from u). In this section, we describe the details of our architecture, DAGformer. We incorporate the mathematical properties of DAGs into the attention calculation in Transformer layers. In this way, DAGformer can efficiently leverage selfattention, in which each node is aware of the nodes that are truly relevant to it. Note that our architecture can be used with different transformer-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DAG Attention</head><p>A transformer can be seen as a GNN processing a fullyconnected graph <ref type="bibr" target="#b13">[14]</ref>. Using a single self-attention layer, it is able to gather interaction information between any pair of nodes. However, Such a mechanism may allow nodes to get some irrelevant information. We want to explore the meaningful information between nodes. As for DAGs, the reachability relation is a strong inductive bias. So we naturally could incorporate it into the self-attention layer. To materialize this idea, we formally define the set of reachable relations of node v as R v = {u|u v or v u}. And we aggregate information only from R v at every node v. For this, Equation <ref type="formula" target="#formula_4">(5)</ref> is modified to</p><formula xml:id="formula_6">DAGAttn (x v ) = u?Rv k (x v , x u ) w?Rv k (x v , x w ) f (x u ) , ?v ? V,<label>(7)</label></formula><p>We call this module DAG attention. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an illustration of DAG attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Incorporating Positional Encoding</head><p>Positional encodings (PE) are intended to convey the position in space of a specific node inside the graph. It refers to the process of concatenating or adding the positional representations of the graph to the input node features before the main Transformer model, such as the Laplacian positional encoding <ref type="bibr" target="#b10">[11]</ref> or random walk positional encoding <ref type="bibr" target="#b16">[17]</ref>. </p><p>where pos is the position, i is the dimension and level pos is the level of topological equivalence <ref type="bibr" target="#b17">[18]</ref>, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. In our architecture, we incorporate PE in DAG attention and modify Equation <ref type="bibr" target="#b5">(6)</ref> to</p><formula xml:id="formula_8">k (x, x ) = exp (x + P E x )W Q , (x + P E x )W K ? d K ,<label>(9)</label></formula><p>where W Q PE , W K PE ? R d?d K are the query and key projection matrices of DAGPE, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we evaluate our architecture versus several SOTA methods for graph representation learning, including GNNs and Transformers, on two DAGs prediction tasks. And DAGformer outperforms these benchmarks, demonstrating the framework's generality and efficacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Baselines</head><p>The OGBG-CODE2 dataset <ref type="bibr" target="#b7">[8]</ref> is comprised of abstract syntax trees (ASTs) derived from about 450k Python method definitions. It is a large dataset, which has 452741 graphs each with 125 nodes on average. The task is to predict the first 5 sub-tokens of the original function's name. We adopt the standard dataset splitting from the benchmark. We compare our method to the following GNNs: GCN <ref type="bibr" target="#b8">[9]</ref>, GIN <ref type="bibr" target="#b18">[19]</ref>, PNA <ref type="bibr" target="#b19">[20]</ref> and DAGNN <ref type="bibr" target="#b4">[5]</ref>, and the Transformer baselines: Transformer <ref type="bibr" target="#b9">[10]</ref>, GraphTrans <ref type="bibr" target="#b12">[13]</ref> and SAT <ref type="bibr" target="#b15">[16]</ref> (SOTA). All baseline performances are as reported on the OGB leaderboard. The NA dataset <ref type="bibr" target="#b3">[4]</ref> contains 19,020 neural architectures generated by the ENAS software. The task is to predict the architecture performance on CIFAR-10 under the weightsharing scheme. Since this is a regression task, the metrics are RMSE and Pearson's r. We compare to DAGNN <ref type="bibr" target="#b4">[5]</ref>, D-VAE <ref type="bibr" target="#b3">[4]</ref> and S-VAE <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">The OGBG-CODE2 dataset</head><p>For our proposed architecture, we implement DAGformer upon vanilla Transformer, GraphTrans and SAT. For fair comparisons, we use the same hyper-parameters (including training schedule, optimizer, batch size, etc.) as baseline models for all of our three versions. And we repeat the experiments 10 times using different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">The NA dataset</head><p>To measure performance with <ref type="bibr" target="#b3">[4]</ref>, we put the Transformer with 2 heads, 2 layers and 128 hidden dimension in front of DAGNN <ref type="bibr" target="#b4">[5]</ref>. Then we similarly train autoencoders and use sparse Gaussian process regression on the latent representation to predict the architecture performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Results</head><p>The evaluation results of our experiments are summarized in <ref type="table">Table 1 and Table 2</ref>. The experiment results successfully demonstrate that our models outperform the baselines across all datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We have introduced DAGformer, a Transformer-based model for a special yet widely used class of graphs-DAGs. Our architecture is simple and universal to be used with any transformer-based models as demonstrated by three models in our experiments. It incorporates the reachability relation implied by DAGs as a powerful inductive bias towards representation learning. In addition, we propose a positional encoding for DAGs that is efficient and can be implemented in linear time. We show that DAGformer outperforms the baselines on two datasets with the assistance of this inductive bias.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An illustration of DAG attention (with 2 heads) by node 1 on its ancestors and descendants. Independent attention computations are represented by arrows with different styles and colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For DAGs, we propose a simpler positional encoding called directed acyclic graph positional encodings (DAGPE) that can be computed in linear time. Formally, DAGPE are defined as: P E (pos,2i) = sin (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>An illustration of topological level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>OGBG-CODE2 dataset.All the baselines are collected from the OGB leaderboard.We denote it as DAG+Transformer if the backbone is Transformer. DAG+SAT outperforms the state-of-the-art SAT. Predictive performance of latent representations for dataset NA.</figDesc><table><row><cell>Model</cell><cell cols="2">Valid F1 score Test F1 score</cell></row><row><cell>GIN</cell><cell cols="2">0.1376?0.0016 0.1495?0.0023</cell></row><row><cell>GCN</cell><cell cols="2">0.1399?0.0017 0.1507?0.0018</cell></row><row><cell>GIN-Virtual</cell><cell cols="2">0.1439?0.0026 0.1581?0.0020</cell></row><row><cell>GCN-Virtual</cell><cell cols="2">0.1461?0.0013 0.1595?0.0018</cell></row><row><cell>PNA</cell><cell cols="2">0.1453?0.0025 0.1570?0.0032</cell></row><row><cell>DAGNN</cell><cell cols="2">0.1607?0.0040 0.1751?0.0049</cell></row><row><cell>Transformer</cell><cell cols="2">0.1546?0.0018 0.1670?0.0015</cell></row><row><cell cols="3">DAG+Transformer 0.1731?0.0014 0.1895?0.0014</cell></row><row><cell>GraphTrans</cell><cell cols="2">0.1661?0.0012 0.1830?0.0024</cell></row><row><cell cols="3">DAG+GraphTrans 0.1700?0.0021 0.1864?0.0018</cell></row><row><cell>SAT (SOTA)</cell><cell cols="2">0.1773?0.0023 0.1937?0.0028</cell></row><row><cell>DAG+SAT</cell><cell cols="2">0.1821?0.0013 0.1982?0.0010</cell></row><row><cell>Model</cell><cell>RMSE ?</cell><cell>Pearson's r ?</cell></row><row><cell>S-VAE</cell><cell>0.521?0.002</cell><cell>0.847?0.001</cell></row><row><cell>D-VAE</cell><cell>0.375?0.003</cell><cell>0.924?0.001</cell></row><row><cell>DAGNN</cell><cell>0.264?0.004</cell><cell>0.964?0.001</cell></row><row><cell>Transformer</cell><cell>0.266?0.004</cell><cell>0.963?0.001</cell></row><row><cell cols="2">DAG+Transformer 0.259?0.003</cell><cell>0.965?0.001</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved semantic representations from treestructured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dag-recurrent neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3620" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">D-vae: A variational autoencoder for directed acyclic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shali</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Directed acyclic graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21618" to="21629" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representing long-range context for graph neural networks with global attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13266" to="13279" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Graphit: Encoding graph structure in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margot</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05667</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure-aware transformer for graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2022</title>
		<imprint>
			<biblScope unit="page" from="3469" to="3489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4465" to="4478" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving graph neural network representations of logical formulae with subgraph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Cornelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06904</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13260" to="13271" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016. Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
