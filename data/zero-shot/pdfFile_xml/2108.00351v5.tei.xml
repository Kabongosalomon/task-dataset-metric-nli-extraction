<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic Occlusion-Aware Data and Neural Mesh Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaibing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renshu</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Toyoura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic Occlusion-Aware Data and Neural Mesh Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A key challenge in the task of human pose and shape estimation is occlusion, including self-occlusions, objecthuman occlusions, and inter-person occlusions. The lack of diverse and accurate pose and shape training data becomes a major bottleneck, especially for scenes with occlusions in the wild. In this paper, we focus on the estimation of human pose and shape in the case of inter-person occlusions, while also handling object-human occlusions and self-occlusion. We propose a novel framework that synthesizes occlusion-aware silhouette and 2D keypoints data and directly regress to the SMPL pose and shape parameters. A neural 3D mesh renderer is exploited to enable silhouette supervision on the fly, which contributes to great improvements in shape estimation. In addition, keypointsand-silhouette-driven training data in panoramic viewpoints are synthesized to compensate for the lack of viewpoint diversity in any existing dataset. Experimental results show that we are among the state-of-the-art on the 3DPW and 3DPW-Crowd datasets in terms of pose estimation accuracy. The proposed method evidently outperforms Mesh Transformer, 3DCrowdNet and ROMP in terms of shape estimation. Top performance is also achieved on SSP-3D in terms of shape prediction accuracy. Demo and code will be available at https://igame-lab.github.io/LASOR/.</p><p>Index Terms-3D human pose and shape estimation, occlusionaware, neural mesh renderer, silhouette, 2D keypoint.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. Examples of inter-people occlusions. It is very challenging to estimate the pose and shape for the target persons enclosed by the yellow rectangles. single RGB image. However, occlusion remains a major challenge for images in the wild, where self-occlusions, personobject occlusions and inter-person occlusions usually appear. It is difficult to achieve good performance when recovering the full 3D human body shape without explicitly taking occlusions into account. In this paper, we mainly focus on estimating the full 3D human pose and shape from a single RGB image under various occlusion cases.</p><p>By exploiting a parametric human body model SMPL <ref type="bibr" target="#b5">[6]</ref>, the task of human pose and shape estimation is usually converted into SMPL parameters estimation. SMPLify <ref type="bibr" target="#b6">[7]</ref> fits the SMPL parameters to obtain a mesh consistent with image evidence, i.e., 2D keypoints. Omran et al. <ref type="bibr" target="#b7">[8]</ref> regress the SMPL parameters with a Convolutional Neural Network. Kolotouros et al. <ref type="bibr" target="#b9">[9]</ref> directly regress the 3D locations of the mesh vertices using a Graph-CNN. With further development, some targeted questions have been raised. To estimate the shape more accurately, a method is proposed by synthesizing the input data with different shapes <ref type="bibr" target="#b10">[10]</ref>. For the objectoccluded case, a method that utilizes a partial UV map to represent an object-occluded human body is proposed in <ref type="bibr" target="#b11">[11]</ref> . Compared with the object-occluded cases, the person-occluded cases involve more complex scenes and yet are quite common in reality. <ref type="figure">Fig.1</ref> shows some of such tricky cases. There are two main challenges. The first challenge is the lack of training data. Existing datasets do not contain enough samples with occlusions. Moreover, since training data is traditionally collected by motion capture (MoCap) systems, and due to the limitations of existing MoCap systems, data collection of inter-person occlusions can hardly scale up. Another challenge is that severe inter-person occlusions would introduce challenging ambiguities into the network prediction, and thus confuse the full 3D human body shape estimation.</p><p>To tackle the obstacles, inspired by <ref type="bibr" target="#b10">[10]</ref>, we propose to make use of synthetic keypoints-and-silhouette-driven training data and explicitly design an occlusion-aware framework during training. More specifically, silhouettes that reflect various inter-person occlusions are synthesized, and 2D keypoints are masked out if considered occluded. Using keypoints and silhouettes as the intermediate representation, large amount of inter-person occlusion samples can be generated with a low cost. Moreover, supervision of the silhouettes is enforced during training using a neural 3D mesh renderer integrated with the framework. The loss is backpropagated to the SMPL pose and shape parameters, which can enhance the human pose and shape estimation performance. Besides that, in many existing datasets, the cameras are installed in fixed positions. To further facilitate the network training and empower it with better generalization ability, we propose a viewpoint augmentation approach to create keypoints-and-silhouette-driven training data from panoramic views on top of any existing dataset.</p><p>The overall flowchart of the proposed framework is shown in <ref type="figure" target="#fig_0">Fig.2</ref>. Module A synthesizes the training data on-the-fly with the view augmentation and inter-person occlusions. The data then passes through the encoder and iterative regressor. The rendering and comparison structure is used to supervise the network. During inference, we just need the keypoint detector <ref type="bibr" target="#b12">[12]</ref> and DensePose <ref type="bibr" target="#b13">[13]</ref> to obtain the keypoints-andsilhouette data as input.</p><p>The main contributions of this work are summarized as follows:</p><p>? We propose a robust occlusion-aware 3D pose and shape estimation framework that exploits synthetic keypointsand-silhouette-driven training data, which overcomes the data scarcity especially for person-occluded scenes in the wild. ? Neural 3D mesh renderer is integrated with the framework to provide silhouette supervision during training, which contributes to much more accurate shape estimation. ? A viewpoint augmentation strategy is introduced to synthesize infinite viewpoint variation, creating keypointsand-silhouette-driven training data from panoramic views on top of any existing dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>I N this section, we discuss recent 3D human pose and shape estimation approaches, as well as the related methods for the occlusion problem. Optimization-based methods. Optimization-based approaches attempt to fit a parametric body model, like SMPL <ref type="bibr" target="#b5">[6]</ref> or SCAPE <ref type="bibr" target="#b14">[14]</ref>, to 2D observations. SMPLify <ref type="bibr" target="#b6">[7]</ref> is the first method to automatically estimate 3D pose and shape from a single unconstrained image by fitting the SMPL to 2D keypoint detections. Other than 2D keypoint detections, different cues like body surface landmarks <ref type="bibr" target="#b15">[15]</ref>, silhouettes <ref type="bibr" target="#b15">[15]</ref> or body part segmentation <ref type="bibr" target="#b2">[3]</ref> are also used in the fitting procedure. Recent works try to fit a more expressive model <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref> that includes face, body, and hand details. Gu et al. <ref type="bibr" target="#b18">[18]</ref> adopt a hierarchical 3D human model that imposes angle and bone length constraints for multi-person pose estimation. These approaches produce reliable results with good 2D observations. However, they are sensitive to initialisation and rely heavily on the quality of 2D information. Regression-based methods. To this end, recent works rely almost exclusively on deep neural networks to regress the 3D human pose and shape parameters. Owing to the lack of training data with full 3D shape ground truth, 2D annotations including 2D keypoints, silhouettes, or parts segmentation are usually essential. This information can be used as intermediate representation <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, or as supervision <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[19]</ref>. Kolotouros et al. <ref type="bibr" target="#b9">[9]</ref> directly regress the 3D location of the mesh vertices from a single image instead of predicting model parameters using a Graph-CNN. Pose2mesh <ref type="bibr" target="#b21">[21]</ref> takes a 2D pose as input and avoids the domain gap between training and testing. I2L-MeshNet <ref type="bibr" target="#b22">[22]</ref> predicts the per-lixel (line+pixel) likelihood on 1D heatmaps for each mesh vertex coordinate instead of directly regressing the parameters. Fundamentally, regression-based methods are dependent on the sample diversity of the training data. However, since obtaining 3D training data is awfully expensive, most datasets only contain a limited number of human bodies. This is also the reason why the existing datasets are insufficient to support robust and accurate human body shape estimation in the wild. Methods for occlusion problems. Huang and Yang <ref type="bibr" target="#b23">[23]</ref> propose a method to recover 3D pose when a person is partially or severely occluded in the scene from monocular images. However, the occlusions are limited to simple rectangles. A grammar-based model is introduced by <ref type="bibr" target="#b24">[24]</ref> with explicit occlusion part templates. To avoid specific design for occlusion patterns. Ghiasi et al. <ref type="bibr" target="#b25">[25]</ref> present a method to model occlusion by learning deformable models with many local part mixture templates using large quantities of synthetically generated training data, which is aimed at explicitly learning the appearance and statistics of occlusion patterns. Zhang et al. <ref type="bibr" target="#b11">[11]</ref> present a method that utilizes a partial UV map to represent an object-occluded human body, and the full 3D human shape estimation is ultimately converted as an image inpainting problem. The above methods mainly deal with the object-human occlusion problems. Bin et al. <ref type="bibr" target="#b26">[26]</ref> present a method that augments images by pasting segmented body parts with various semantic granularity, which has certain generalization ability to shield the human body from each other. The occlusion problem is explored in <ref type="bibr" target="#b27">[27]</ref> by using a temporal gated convolution network. Choi et al. <ref type="bibr" target="#b28">[28]</ref> present 3DCrowdNet, a 2D human pose-guided 3D crowd pose and shape estimation system for in-the-wild scenes. Sun et al. <ref type="bibr" target="#b29">[29]</ref> develop a novel Collision-Aware Representation (CAR) method named ROMP to improve the performance under person-person occlusion. Jiang et al. <ref type="bibr" target="#b30">[30]</ref> propose a depth ordering-aware loss to generate a rendering that is consistent with the annotated instance segmentation, but they make use of very few datasets with 3D ground truth. Currently, occlusion is still a major challenge for 3D pose estimation in the wild. Synthetic training data. When the data is too scarce to solve the problem, synthetic data becomes an option. Several papers explore the use of synthetic data in training for human pose and shape estimation. To make up for the scarcity of in-thewild training data with diverse and accurate body shape labels, Sengupta et al. <ref type="bibr" target="#b10">[10]</ref> propose STRAPS, a system that utilizes proxy representations, such as silhouettes and 2D joints, as inputs to a shape and pose regression neural network, which is trained with synthetic training data. Some methods <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b26">[26]</ref> try to obtain novel samples either by adding virtual objects to existing datasets or by pasting segmented body parts with various semantic granularity. However, while inter-person occlusion is common in 3D human pose estimation scenarios, the inter-person occlusion is rarely synthesized explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TECHNICAL APPROACH</head><p>I N the following section, we first describe the Skinned Multi-Person Linear model (SMPL) briefly and the basic notations defined in this paper. More details about the data generation are then provided. After that, the regression network and the loss function are presented in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SMPL Model</head><p>SMPL is a skinned vertex-based model that accurately represents a wide variety of body shapes in natural human poses. SMPL model provides the function M(?, ?) that takes as input the pose parameters ? and the shape parameters ? and returns the body mesh v ? R N ?3 with N = 6890 vertices. The 3D joints are calculated by j 3d = J v, where J ? R K?N is a pre-trained linear regressor matrix, and K is the number of joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. View Augmentation</head><p>For the datasets collected in the laboratory, the camera is usually placed in a fixed position. Meanwhile, datasets collected outdoors with 3D ground truth are scarce and of insufficient variations as well. Models trained using these datasets may depend on the camera's perspective and have poor generalization ability. To this end, this paper adopts a viewpoint augmentation strategy to generate intermediate representations, i.e., keypoints and silhouettes, under different camera parameters. Using these intermediate representations can make full use of this feature and reduce the model's dependence on a specific camera perspective. In the implementation, since the first three values of the SMPL model pose parameters control the global orientation of the human body model, we alter these parameters to modify the current human body orientation relative to the camera. Modifying the global orientation of the human body model does not modify the 3D posture of the human body, but the projection on the 2D image is quite different. The keypoints and contour maps of the input model in this paper are all two-dimensional data. This greatly enriches the training data when the camera viewpoint in the training data is limited. The specific formula is given as follows:?</p><formula xml:id="formula_0">g = ? g + ? * N (?, ? 2 )<label>(1)</label></formula><p>where ? g is the global orientation in the ? parameter, and N is a normal distribution function with mean ? and standard derivation ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Synthetic Data Generation</head><p>As shown in <ref type="figure">Fig. 3</ref>, two pairs of SMPL parameters (? 1 , ? 1 ),(? 2 , ? 2 ) are sampled from any training dataset. The camera translation vector t 1 is generate randomly, while the camera intrinsic matrix K and rotation matrix R are fixed. <ref type="figure">Fig. 3</ref>. Synthesizing the silhouette. First, two pairs of SMPL parameters (? 1 , ? 1 ),(? 2 , ? 2 ) are sampled. After viewpoint and shape augmentation is performed, a neural mesh renderer generates the 2D silhouettes. Finally, the pair of silhouettes are used to synthesize the overlap of human body.</p><p>Then, we obtain t 2 based on t 1 by adding a shift. In order to get various shape data, we replace ? i with a new random vector ? , generated by sampling n shape parameters ? n ? N (?, ? 2 n ) <ref type="bibr" target="#b10">[10]</ref>. We use ? 1 and ? 2 by replacing the global orientation with the orientation? g obtained by viewpoint augmentation method instead of the original ? 1 and ? 2 . Then, the 3D vertices v 1 and v 2 corresponding to (? 1 , ? 1 ) and</p><formula xml:id="formula_1">(? 2 , ? 2 ) are rendered [19] into silhouette S 1 , S 2 ? [0, 1] H?W .</formula><p>The final silhouette is calculated by (2) as follows,</p><formula xml:id="formula_2">S = S 1 ? S 1 * S 2<label>(2)</label></formula><p>As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, inter-person occlusion samples are generated with different levels of human body overlap. For 2D joints, the 3D joints j 3d corresponding to (? 1 , ? 1 ) are projected into 2D joints j 2d ? R K?2 using prospective projection. j 2d is encoded into 2D Gaussian joint heatmaps, G ? R K?H?W . We obtain the final input X ? R (K+1)?H?W by concatenating S and G. Note that the noises in keypoints detection will introduce noises into the 2D joint heatmap encoding. To handle occluded joints explicitly, we randomly remove some body parts from the human body and add occluding boxes to the silhouette in S during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Regression Network</head><p>Our method is architecture-agnostic. We use the baseline network architecture as in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b31">[31]</ref>, which consists of a convolutional encoder for feature extraction and an iterative regressor that outputs predicted SMPL pose, shape and camera parameters (?,? and?) from the extracted feature. Similarly as in STRAPS <ref type="bibr" target="#b10">[10]</ref>, the SMPL default discontinuous Euler rotation vectors are replaced by a continuous 6-dimensional rotation representation of? proposed by <ref type="bibr" target="#b32">[32]</ref>.</p><p>The weak-perspective camera model is adopted in this paper, represented by? = [?,t] where? ? R means scale and t ? R 2 represents x ? y camera translation. These parameters allow us to generate the mesh corresponding to the regressed parameters,v = M(?,?), as well as the 3D joints? 3d = Jv and their 2D reprojection? 2d =?(? 3d ). Neural 3D Mesh Renderer. Differentiable rendering is a novel field which allows the gradients of 3D objects to be calculated and propagated through images. We follow the common self-supervision pipeline with differentiable rendering and don't need correspondence to images. A neural 3D mesh renderer <ref type="bibr" target="#b19">[19]</ref> is integrated into our framework to provide silhouette supervision during training. The loss function is defined as</p><formula xml:id="formula_3">L S = 1 H ? W ? ? ? S 2 2<label>(3)</label></formula><p>where S is the target silhouette image,? is silhouette rendered fromv and H, W are the height and width of the target silhouette S. The reconstruction of 3D shape is an ill-posed problem when a single view is used during training. Additional loss functions are added to provide strong supervision. The first loss term L v performs the supervision between predicted 3D verticesv and ground-truth 3D vertices v, which is,</p><formula xml:id="formula_4">L v = v ? v 2 2<label>(4)</label></formula><p>The second loss term L j3d performs the supervision between predicted 3D joints and ground-truth 3D joints, which is,</p><formula xml:id="formula_5">L j3d = ? 3d ? j 3d 2 2<label>(5)</label></formula><p>The L ? and L ? losses can provide strong 3d supervision.</p><formula xml:id="formula_6">L ? = ? ? ? 2 2 (6) L ? = ? ? ? 2 2 (7)</formula><p>In order to enforce image-model alignment, we add the loss for the projected 2D joints</p><formula xml:id="formula_7">L j 2d = 1 K ? K i=1 ? i ? 2d i ? j 2d i 2 2 (8)</formula><p>where the value of ? i indicates the visibility (1 if visible, 0 otherwise) for i-th 2D keypoints (K in total). The final loss function can be described as follows:</p><formula xml:id="formula_8">L = 1 ? 2 v L v + 1 ? 2 j 3d L j 3d + 1 ? 2 ? L ? + 1 ? 2 ? L ? + 1 ? 2 j 2d L j 2d + 1 ? 2 S L S + log (? v ? j 3d ? ? ? ? ? j 2d ? S )<label>(9)</label></formula><p>where losses are adaptively combined using homoscedastic uncertainty <ref type="bibr" target="#b33">[33]</ref>. In the next section, we will analyze the contribution of the silhouette loss L S in 3D shape estimation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Datasets</head><p>This section gives a description of the datasets used for training and evaluation. Following STRAPS <ref type="bibr" target="#b10">[10]</ref>, we train the network using synthetic data generated from 3DPW <ref type="bibr" target="#b34">[34]</ref>, AMASS <ref type="bibr" target="#b35">[35]</ref>, and UP-3D <ref type="bibr" target="#b15">[15]</ref>. We report evaluation results on 3DPW, 3DOH50K <ref type="bibr" target="#b11">[11]</ref> and SSP-3D <ref type="bibr" target="#b10">[10]</ref>.</p><p>AMASS is a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterizaition. Some of the SMPL parameters are selected to train our network.</p><p>UP-3D combines two LSP datasets and the single-person part of MPII-HumanPose dataset, which is used to train our network.</p><p>3DPW is captured via IMUs and contains both indoor and outdoor scenes. It provides abundant 2D/3D annotations, such as 2D pose, 3D pose, SMPL parameters, human 3D mesh, etc. It includes inter-person occlusion, object-human occlusion, and non-occluded/truncated cases. Being a rare in-the-wild 3D human pose and shape dataset, 3DPW is the most suitable for testing our work. 3DPW is used for training and evaluation with the same split as in <ref type="bibr" target="#b10">[10]</ref>.</p><p>3DPW-Crowd is a subset of the 3DPW validation set and the selected sequences are courtyard hug 00 and courtyard dancing 00. 3DPW-Crowd is deemed a suitable benchmark for evaluating performance for inter-person occlusion scenes in <ref type="bibr" target="#b36">[36]</ref>, as it has much higher bounding box IoU and CrowdIndex <ref type="bibr" target="#b37">[37]</ref>, which measures crowding level.</p><p>SSP-3D contains 311 in-the-wild images of 62 tightly clothed sports persons (selected from the Sport-1M video dataset) with a diverse range of body shapes, along with the corresponding pseudo-ground-truth SMPL shape and pose labels.</p><p>3DOH50K is the first 3D human pose dataset that introduces object-human occlusions on purpose when recording. It contains 1290 test images. This dataset is used for evaluation. Dataset Statistics. Our work mainly focuses on human pose and shape estimation with inter-person occlusions. We compare different datasets to give insights about the types of occlusion and the number of people in <ref type="table" target="#tab_0">Table I</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Network Architecture. For a fair comparison with previous methods, we use fine-tuned ResNet-18 <ref type="bibr" target="#b38">[38]</ref> as the default encoder and ResNet-50 as the additional encoder. The encoder produces a feature vector ? ? R 512 . The iterative regression network consists of two fully connected layers. During inference, we use darkpose <ref type="bibr" target="#b39">[39]</ref> to get 2D keypoints of 3DPW and use Mask-RCNN <ref type="bibr" target="#b12">[12]</ref> to get 2D keypoint predictions of other datasets. We use DensePose <ref type="bibr" target="#b13">[13]</ref> to get silhouette predictions. All implementations are done in PyTorch <ref type="bibr" target="#b40">[40]</ref>. Setting Details. The input silhouettes are cropped and resized to 256 ? 256, while keeping the aspect ratio of 1.2. COCO keypoint order is used and the number of keypoint is 17. The input tensor is 18 ? 256 ? 256. The loss weights are set to be</p><formula xml:id="formula_9">? v = 1.0, ? j 3d = 1.0, ? ? = 0.1, ? ? = 0.1, ? j 2d = 0.1, ? S = 0.1.</formula><p>We use the Adam optimizer <ref type="bibr" target="#b41">[41]</ref> with a batch size of 140 and an initial learning rate of 0.0001 to train our encoder and regressor. The mean values in iterative regression are the same as SPIN <ref type="bibr" target="#b1">[2]</ref>. The training step takes 10 days with 100 epochs on a single 3090 GPU.</p><p>Training Datasets. For a fair comparison with the previous method STRAPS <ref type="bibr" target="#b10">[10]</ref>, we use the training data provided by the STRAPS <ref type="bibr" target="#b10">[10]</ref>. The training data are collected from AMASS, 3DPW and UP-3D. In the training step, we use the groundtruth SMPL parameters and do not use any pair of image and 3D label. Evaluation Metrics. We adopt Procrustes-aligned <ref type="bibr" target="#b42">[42]</ref> mean per joint position error (MPJPE PA) for evaluating the 3D pose accuracy. To evaluate the 3D shape error, we employ Procrustes-aligned per-vertex error (PVE PA). Besides, to evaluate the shape estimation, we calculate the scale-corrected per-vertex euclidean error in a neutral pose (PVE-T-SC) and mean intersection-over-union (mIOU) on SSP-3D. In all the tables, ? means the larger the better, otherwise it is the lower the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons to the State-of-the-Art:</head><p>To demonstrate the effectiveness of our method, we perform quantitative evaluations on 3DPW, 3DOH50K and SSP-3D. In <ref type="table" target="#tab_0">Table II</ref>, the methods in the upper rows do not use any paired images and 3D labels. Separated by the horizontal line, the methods in the bottom rows use the paired ground-truth 3D labels and images during training. Our method does not directly use any real images in the training step.  <ref type="bibr" target="#b28">[28]</ref>, ROMP <ref type="bibr" target="#b29">[29]</ref> and STRAPS <ref type="bibr" target="#b10">[10]</ref>, our method gives more accurate pose and shape predictions for the person-occluded cases.</p><p>3DPW. Firstly, we evaluate the test dataset in 3DPW. As shown in <ref type="table" target="#tab_0">Table II</ref>, our method achieves a 12 percent improvement compared with <ref type="bibr" target="#b10">[10]</ref>. Although our method does not use paired images and 3D labels, it shows competitive performance with the state-of-the-art that requires training data comprised of paired images and 3D labels. Imposing less restrictions on the training data means our method can scale up easier without relying on paired images and labels. Our method also shows excellent shape estimation performance. Note that while Mesh Transformer <ref type="bibr" target="#b28">[28]</ref>, 3DCrowdNet <ref type="bibr" target="#b36">[36]</ref> and ROMP <ref type="bibr" target="#b29">[29]</ref> report higher MPJPE-PA accuracy on 3DPW, the proposed method clearly shows better performance on the silhouette for human with various shapes, as shown in <ref type="figure">Fig. 6</ref>. 3DPW-Crowd. We also evaluate the proposed method for 3DPW-Crowd. As shown in <ref type="table" target="#tab_0">Table II</ref>, our method achieves the best performance among the methods that do not need paired training data (top rows), including STRAPS <ref type="bibr" target="#b10">[10]</ref> and Pose2Mesh <ref type="bibr" target="#b21">[21]</ref>. SSP-3D. The SSP-3D dataset contains 62 subjects. The proposed method shows the top performance on shape estimation in terms of PVE-T-SC. 3DOH50K. In 3DOH50K dataset, the samples have severe occlusions. As shown in <ref type="table" target="#tab_0">Table II</ref>, better performance is achieved compared with [10] on handling object-human occlusions. <ref type="table" target="#tab_0">The  TABLE II  COMPARISONS WITH THE STATE-OF-THE-ART METHODS ON 3DPW, 3DPW-CROWD, 3DOH50K AND SSP-3D. NUMBER MARKED WITH * WERE  EVALUATED USING THE OPEN-SOURCED OFFICIAL CODE. NUMBER MARKED WITH -WERE PROVIDED BY [10], ALL OTHER NUMBERS ARE REPORTED BY  THE RESPECTIVE PAPERS. IN ORDER TO BE MORE CONCISE, THE BEST PERFORMANCE GIVEN BY METHODS WITHOUT USING THE PAIRED  GROUND-TRUTH 3D LABELS AND IMAGES DURING TRAINING ARE MARKED IN BOLD BLACK, AND OTHER METHODS ARE MARKED IN LIGHT GREEN.   Method  3DPW  3DPW-Crowd  3DOH50K  SSP-3D  MPJPE-PA  MPJPE-PA  MPJPE-PA PVE-T-SC</ref>   <ref type="bibr" target="#b29">[29]</ref> 54.9 -43.9 --I2L-MeshNet <ref type="bibr" target="#b22">[22]</ref> 57.7 73.5 ---3DCrowdNet <ref type="bibr" target="#b36">[36]</ref> 52.2 56.8 ---Mesh Transformer <ref type="bibr" target="#b28">[28]</ref> 47.9 ----OOH <ref type="bibr" target="#b11">[11]</ref> 72.2 -58.5 -other methods also use the Human3.6M dataset to train the model while ours does not. Therefore, it would not be fair to directly compare our results to theirs. Finally, to show the effectiveness of our method on handling inter-person occlusions, some examples are also shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. To compare the shape estimation of the human body more intuitively, some examples with diverse body shapes are demonstrated in <ref type="figure">Fig. 6</ref>. Silhouette Loss. To study the effect of the neural-meshrendering-enabled silhouette loss L S in (3), we compare two models trained (i) without the silhouette loss, and (ii) with the silhouette loss. The results are shown in <ref type="table" target="#tab_0">Table III</ref>. The qualitative comparison with the baseline and silhouette loss are shown in <ref type="figure" target="#fig_3">Fig. 7</ref>  Experiments with Ground Truth 2D Inputs. We also report the performance of the proposed method using ground-truth keypoints and silhouettes. The results are shown in <ref type="table" target="#tab_0">Table VI</ref>. Compared with noisy 2D keypoint detection and silhouette, the input of 2D ground-truth improves the accuracy by a large margin. This indicates that if the proposed framework is equipped with a better 2D keypoint detector and silhouette segmentation method, much better results will be obtained. <ref type="figure">Fig. 6</ref>. Qualitative comparison for different body shapes with the state-of-the-art methods 3DCrowdNet <ref type="bibr" target="#b36">[36]</ref>, Mesh Transformer <ref type="bibr" target="#b28">[28]</ref>, ROMP <ref type="bibr" target="#b29">[29]</ref> and STRAPS <ref type="bibr" target="#b10">[10]</ref>. The samples are from the SSP-3D dataset. The results of ROMP <ref type="bibr" target="#b29">[29]</ref>, STRAPS <ref type="bibr" target="#b10">[10]</ref>, and Mesh Transformer <ref type="bibr" target="#b28">[28]</ref> are generated from their corresponding official codes, and the results of 3DCrowdNet <ref type="bibr" target="#b36">[36]</ref> are provided by the author. Our method is able to accurately predict a diverse range of body shapes. We also evaluate the effects of using different percentages of data for inter-person occlusion synthesis. 40, 80 and 100 percentage of the data are processed respectively, and the corresponding results are compared in <ref type="table" target="#tab_0">Table VII</ref>. Using 100  percentage of training data to synthesize occlusions, the best accuracy is achieved on the 3DPW dataset. This reasons well since occlusions are extremely common in 3DPW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>In the 2D keypoint detector, each joint prediction is associated with a confidence score. When the predicted confidence score is above a threshold, the joint is considered detected correctly. The keypoints with confidence lower than the threshold are removed. We test the effects of adjusting the confidence threshold. The results on 3DPW are shown in <ref type="table" target="#tab_0">Table VIII</ref>. For samples with severe occlusions, by removing the poorly detected 2D keypoints properly, better results are achieved. The line chart about the MPJPE PA is shown in <ref type="figure">Fig.  9</ref>. Empirically, setting the threshold to be 0.4 yields the best performance. <ref type="figure">Fig. 9</ref>. MPJPE PA on 3DPW with different confidence thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>I N this paper, we propose a new framework that synthesizes occlusion-aware silhouette and 2D keypoints representation to overcome data scarcity when regressing the human pose and shape, which proves to work well on inter-person occlusions as well as other occlusion cases. Differentiable rendering is integrated to enable silhouette supervision on the fly, contributing to higher accuracy in the pose and shape estimation. A new method for synthesizing keypoints-andsilhouette-driven data in panoramic viewpoints is presented to increase viewpoint diversity on top of existing dataset. Thanks to these strategies, the experimental results demonstrate that the proposed framework is among the start-of-the-art methods on the 3DPW and 3DPW-Crowd datasets in terms of pose accuracy. Although it is unfair to compare with these methods using paired training data, the proposed method evidently outperforms Mesh Transformer <ref type="bibr" target="#b28">[28]</ref>, 3DCrowdNet <ref type="bibr" target="#b36">[36]</ref> and ROMP <ref type="bibr" target="#b29">[29]</ref> in terms of shape estimation. Top performance is also achieved on SSP-3D in terms of human shape estimation accuracy. The occlusion-aware synthetic data generation strategy, neural-mesh-renderer-enabled silhouette supervision and viewpoint augmentation strategy can be also applied to other approaches for 3D human pose and shape estimation to improve the overall performance of the community. Future Work. Our occlusion-aware method can be extended to the crowded people case. Video information can also be considered to obtain more consistent and accurate human pose and shape. Additionally, compared with other methods, limited training data is used for now. In the next step, more datasets like Human3.6M and COCO can be added in the training procedure to train the model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of our method. (A) is used to synthesize the training input. The input is then passed through the regressor network (B). We use rendering and compare the input with the rendered images in (C) to supervise the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of synthetic data generation with different levels of interperson occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative comparison with the state-of-the-art methods. Some samples with different levels of inter-person occlusions from 3DPW dataset are shown. Compared with Mesh Transformer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Ablation study of the silhouette loss enabled by neural 3D mesh rendering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Effects of view augmentation (VA) and inter-person occlusion augmentation (IPOA). The model with both VA and IPOA yields the best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF DIFFERENT EVALUATION DATASETS RELATED TO 3D POSE ESTIMATION. THE OBJECT-OCCLUDED INFORMATION IS PROVIDED BY OOH [11], WHERE THE + MARK(S) INDICATES THE LEVEL OF OCCLUSIONS.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Object-Occluded Inter-person Occlusion Num of People</cell></row><row><cell>3DPW</cell><cell>++</cell><cell>18</cell></row><row><cell>3DPW-Crowd</cell><cell>-</cell><cell>2</cell></row><row><cell>SSP-3D</cell><cell>-</cell><cell>62</cell></row><row><cell>3DOH50K</cell><cell>++++</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III EVALUATION</head><label>III</label><figDesc>ON THE SSP-3D DATASET.</figDesc><table><row><cell>Method</cell><cell cols="3">PVE-T-SC mIOU? convergence time (epochs)</cell></row><row><cell>W/O silhouette loss</cell><cell>17.2</cell><cell>0.67</cell><cell>250</cell></row><row><cell>W/ silhouette loss</cell><cell>14.5</cell><cell>0.67</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="5">ABLATION STUDY ON SILHOUETTE AS THE INTERMEDIATE</cell></row><row><cell></cell><cell cols="2">REPRESENTATION.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">3DPW PVE PA MPJPE PA PVE T SC mIOU? SSP</cell></row><row><cell>Keypoint</cell><cell>78.8</cell><cell>61.9</cell><cell>18.3</cell><cell>0.57</cell></row><row><cell>Keypoint+Silhouette</cell><cell>72.8</cell><cell>57.9</cell><cell>14.5</cell><cell>0.67</cell></row><row><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell></row><row><cell cols="5">ABLATION STUDY OF VA AND IPOA MODULES ON 3DPW. THE BASELINE</cell></row><row><cell cols="4">WITH * MEANS THAT RESNET-50 IS THE ENCODER.</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="2">3DPW PVE PA MPJPE PA</cell><cell></cell></row><row><cell>Baseline</cell><cell></cell><cell>77.8</cell><cell>62.1</cell><cell></cell></row><row><cell cols="2">Baseline+VA</cell><cell>75</cell><cell>59.4</cell><cell></cell></row><row><cell cols="2">Baseline+VA+IPOA</cell><cell>73.2</cell><cell>58.6</cell><cell></cell></row><row><cell cols="2">Baseline*+VA+IPOA</cell><cell>72.8</cell><cell>57.9</cell><cell></cell></row><row><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="5">EVALUATION ON THE 3DPW DATASET USING GROUND-TRUTH KEYPOINTS</cell></row><row><cell></cell><cell cols="2">AND SILHOUETTE.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="2">PVE PA MPJPE PA</cell><cell></cell></row><row><cell>Baseline</cell><cell></cell><cell>44.3</cell><cell>35.0</cell><cell></cell></row><row><cell cols="2">Baseline+VA</cell><cell>40.8</cell><cell>32.1</cell><cell></cell></row><row><cell cols="2">Baseline+VA+IPOA</cell><cell>38.1</cell><cell>30.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII EVALUATION</head><label>VII</label><figDesc>OF THE 3DPW DATASET. THE OPERATION OF SYNTHESIZING THE SILHOUETTE CAN GENERATE DIFFERENT LEVELS OF INTER-PERSON OCCLUSIONS. PERCENT MEANS THE PROBABILITY OF PERFORMING THE OPERATION.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VIII</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">ABLATION STUDY ON ADJUSTING THE CONFIDENCE THRESHOLD OF THE</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">KEYPOINT DETECTION FOR THE 3DPW DATASET.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Threshold</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PVE PA</cell><cell cols="4">73.9 73.7 73.5 73.3</cell><cell>73.2 73.3 73.9</cell></row><row><cell cols="3">Percentage PVE PA MPJPE PA</cell><cell cols="5">MPJPE PA 59.3 59.1 58.8 58.7</cell><cell>58.6</cell><cell>58.7 59.3</cell></row><row><cell>40%</cell><cell>76.9</cell><cell>62.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>80%</cell><cell>74.8</cell><cell>60.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>100%</cell><cell>73.2</cell><cell>58.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Kaibing Yang received the Bachelor's degree in Electronic Information Engineering in 2017 from Anhui Polytechnic University, P.R. China. He is currently a graduate student in the College of Computer Science and Technology, Hangzhou Dianzi University. His research interests are on geometric modeling and computer vision. Renshu Gu received the B.Sc. degree from Nanjing University in 2011, and the Master's and Ph.D. degrees from University of Washington in 2020, advised by Professor Jenq-neng Hwang. She is now with the College of Computer Science and Technology, Hangzhou Dianzi University, P.R. China. Her research interests include image/video analytics, computer vision and multimedia. Dr. Gu is a member of IEEE. Maoyu Wang graduated from Harbin Institute of Technology in 2016 with a bachelor's degree in Internet of Things Engineering. He is currently a graduate student in Hangzhou Dianzi University ITMO Joint Institute. His research interests include geometric modeling and 3D reconstruction. Masahiro Toyoura received the B.Sc. degree in Engineering, M.Sc. and Ph.D. degrees in Informatics from Kyoto University in 2003, 2005 and 2008 respectively. He is currently an Associate Professor at Department of Computer Science and Engineering, University of Yamanashi, Japan. His research interests are digital fabrication, computer and human vision. He is a member of IEEE and ACM. Gang Xu received the B.Sc. degree in Mathematics from Shandong University in 2003, Ph.D. degrees in Mathematics from Zhejiang University in 2008. He is currently a full Professor at College of Computer Science and Technology, Hangzhou Dianzi University, P.R. China. His research interests are geometric modeling and simulation, computer graphics and 3D visual computing. Dr. Xu is a distinguished member of China Computer Federation.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for providing constructive suggestions and comments, that contributed to highly improve this work. The authors would like to thank the authors of 3DCrowdNet, Dr. Hongsuk Choi et al., for providing the experimental results on the SSP-3D dataset in <ref type="figure">Fig. 6</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Indirect deep structured learning for 3d human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2015-10" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016, ser. Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>international conference on 3D vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="484" to="494" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthetic training for accurate 3d human pose and shape estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object-occluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8320" to="8329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-person hierarchical 3d pose estimation in natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4245" to="4257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7760" to="7770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating human pose from occluded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="48" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object detection with grammar models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="442" to="450" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2401" to="2408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial semantic data augmentation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="606" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring severe occlusion: Multi-person 3d pose estimation with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8243" to="8250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-toend recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AMASS: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="5442" to="5451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">3dcrowdnet: 2d human pose-guided3d crowd human pose and shape estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07300</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
