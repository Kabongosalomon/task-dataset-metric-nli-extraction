<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfa</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Rojas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijun</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>Guan</surname></persName>
						</author>
						<title level="a" type="main">A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatio-temporal information is key to resolve occlusion and depth ambiguity in 3D pose estimation. Previous methods have focused on either temporal contexts or localto-global architectures that embed fixed-length spatio-temporal information. To date, there have not been effective proposals to simultaneously and flexibly capture varying spatio-temporal sequences and effectively achieves real-time 3D pose estimation. In this work, we improve the learning of kinematic constraints in the human skeleton: posture, local kinematic connections, and symmetry by modeling local and global spatial information via attention mechanisms. To adapt to single-and multiframe estimation, the dilated temporal model is employed to process varying skeleton sequences. Also, importantly, we carefully design the interleaving of spatial semantics with temporal dependencies to achieve a synergistic effect. To this end, we propose a simple yet effective graph attention spatiotemporal convolutional network (GAST-Net) that comprises of interleaved temporal convolutional and graph attention blocks. Experiments on two challenging benchmark datasets (Human3.6M and HumanEva-I) and YouTube videos demonstrate that our approach effectively mitigates depth ambiguity and self-occlusion, generalizes to half upper body estimation, and achieves competitive performance on 2D-to-3D video pose estimation. Code, video, and supplementary information is available at: http://www.juanrojas.net/gast/ Index Terms-2D-to-3D human pose, video pose estimation, graph attention, spatio-temporal networks</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I. INTRODUCTION 3D human pose estimation from video is a very active research area that impacts domains like action recognition, virtual reality, and human-robot interaction. Previously, 3D pose estimation was computed using depth sensors, motion capture, or multi-view images in indoor environments. However, with recent advances in 2D human pose estimation through deep learning along with massive availability of inthe-wild data, there has been great progress in solving 3D pose estimation from monocular images <ref type="bibr" target="#b0">[1]</ref>. Recent works <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref> only use 2D human pose representations and achieve competitive performance and generalization, which avoid the influence of background noise and human external appearance. Additionally, compared to processing RGB images, 2D-to-3D methods leverage 2D keypoints (which use less computation) and enable longer-term frame estimations. In this paper, we focus on 2D-to-3D estimation. Estimating 3D poses from 2D keypoints remains an illposed problem due to: (i) depth ambiguity: caused by a many-to-one 3D-to-2D pose mapping as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (a); (ii) self-occlusions: caused under certain human poses as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (a-e)); and (iii) prediction errors: caused by inaccurate human pose models as shown in <ref type="figure" target="#fig_0">Fig. 1 (c,d)</ref>.</p><p>To mitigate depth ambiguity, some works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> introduce weakly supervised methods to regular the rationality of the generated 3D structure. For example, Drover et al. <ref type="bibr" target="#b4">[5]</ref> utilizes an adversarial framework to impose a prior on the 3D structure via random 2D-to-3D projections. Even so, selfocclusion still remains difficult to solve as well as jittery motions in estimated video. To address these problems, temporal modeling has used joint-coordinated vectors in sequenceto-sequence models to generate smoother motion <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. However, the vector representation of joint sequences lacks expressivity for spatial relations, which is critical to mitigate depth ambiguities and self-occlusions. To make full use of spatio-temporal information, Cai et al. <ref type="bibr" target="#b7">[8]</ref> designs a local-toglobal networks to construct spatial configurations and temporal consistencies, which takes 2D keypoints sequence as a spatio-temporal graph. But, The use of graph convolutional networks to encode temporal relationships cannot effectively model long-term dependencies. This network architecture is also limit to input fixed length.</p><p>Despite considerable progress in 2D-to-3D video-based methods, none of them integrate the following significant characteristics:</p><p>(i) Extract more informative contextual spatio-temporal information from the hierarchical structure of 2D keypoint sequences and posture semantics to resolve depth ambigui- ties, mitigate self-occlusions, and smoothen motion.</p><p>(ii) Flexible input frame length handling, noting that videos effectively exhibit varying length sequences.</p><p>(iii) Real-time estimation of 3D poses without redundant intermediate frame calculations. Real-time estimation facilitates the downstream combination of high-semantic visual tasks; such as combining skeleton-based action recognition with human-robot interaction in real-time.</p><p>These challenges inspire us to study richer spatio-temporal representations and flexibly interleave spatial and temporal information. To this end, we contribute an interleaved graphattention spatio-temporal network that better learns three aspects of human kinematic constraints via graph attention blocks and leverages dilated convolutions to model long-term temporal contexts. The graph attention block learns skeletal joint symmetries, local kinematic relations in distal joints, and global postural joint semantics. The dilated temporal convolutional networks (TCNs) can flexibly capture varying sequences and work with causal convolutions to achieve realtime pose estimation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref> (see <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>). For single-frame scenarios, dilated convolutions can be replaced with strided convolutions to quickly inference without need to retrain a new model <ref type="bibr" target="#b6">[7]</ref>. In our works, we understand spatial and temporal data to be heterogeneous. As such we treat them independently but interleave them in a synergistic manner allowing us to leverage the benefits of TCNs.</p><p>With regards to temporal modeling, we base our design on the dilated temporal convolutions of <ref type="bibr" target="#b6">[7]</ref>, but extend it to tackle three-dimensional spatio-temporal sequences. With regards to spatial modeling, local spatial features from local connections and symmetries are modeled via graph convolution networks (GCNs) and referred to as "Local Attention Graph's" in our system (see <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>). For global spatial features, we draw inspiration from <ref type="bibr" target="#b9">[10]</ref> and leverage graph attention networks <ref type="bibr" target="#b10">[11]</ref> to express posture semantics with data-driven learning. These blocks are referred to as "Global Attention Graph's" and depicted in <ref type="figure" target="#fig_1">Fig. 2</ref> (c). The graph attention block effectively express the hierarchical symmetrical structure of the human body and adaptively extracts global semantic information over time. Particularly, local-and global-spatial blocks are interleaved with temporal blocks to effectively extract and fuse spatio-temporal features of 2D keypoint sequences (see <ref type="figure" target="#fig_3">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2D-to-3D Pose Estimation</head><p>Since Martinez et al. <ref type="bibr" target="#b1">[2]</ref> proposed a simple and effective linear layer to lift 2D joint locations to 3D positions, many works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref> have sought to generate accurate 3D pose estimation from underlying 2D keypoints. Wandt et al. <ref type="bibr" target="#b3">[4]</ref>, proposed a semi-supervised approach to solve overfitting by projecting a generated 3D pose back to the 2D image and comparing it with the ground truth. Wang et al. <ref type="bibr" target="#b11">[12]</ref>, designed a novel stereo network with a geometric search scheme to generate a high quality 3D pose in the wild without the need of indoor 3D input. Even so, generating accurate 3D poses from a single image is an ill-posed problem. Recent works have exploited temporal information to obtain more robust and smooth 3D poses <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>. For instance, Hossain et al. <ref type="bibr" target="#b5">[6]</ref> proposed a 2-layered normalized LSTM network with residual connections that first encode 2D poses into a fixed feature vector and then decode it to a 3D pose. But, encoding the two-dimensional poses into a one-dimensional vector ignores the expression of the spatial configuration of 2D poses. Other recent works incorporate spatial configuration constraints and temporal information to estimate 3D poses <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Wang <ref type="bibr" target="#b13">[14]</ref> et al. designed a U-shaped graph convolutional network to aggregate long-range information through temporal pooling operations. Cai <ref type="bibr" target="#b7">[8]</ref> et al. exploited graph pooling and graph upsampling to process and consolidate features across scales. However, their local-to-global network architectures are limit to embed fixed-length spatio-temporal sequences.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatio-Temporal Graph</head><p>GCNs generalize convolutions to graph-structured data and are roughly classified into spectral-based and spatialbased categories <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Spatial-based GCNs are more relevant to our work. Our spatial network uses both GCNs proposed by <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b15">[16]</ref> to obtain local and global features of each joint. Due to the outstanding performance of GCNs in non-European data, there are many recent works also modeling skeleton sequence as spatio-temporal graphs to understand human tasks including skeleton-based action recognition <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref> and motion prediction <ref type="bibr" target="#b17">[18]</ref>. Our approach bears some similarity with adaptive graph convolutional blocks <ref type="bibr" target="#b9">[10]</ref> that extends the topology of the graph and combines common convolutional networks to integrate the spatio-temporal information. But our works has four distinct features: (i) instead of setting the local spatial configuration to three subsets based on gravity, our approach aims to model the symmetrical hierarchy of the human body as well as the kinematic joint constraints; (ii) our local and global adjacency matrices are applied to different graph convolutions to explicitly extract diverse spatial semantics; (iii) the dilated convolution is used to effectively model longterm temporal information; and (iv) as with the inception module, we exploit concatenation to better integrate the three-steam spatio-temporal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>Given a sequence of 2D pose predictions from videos, our goal is to output a sequence of 3D coordinates based on a root joint-the pelvis (See <ref type="figure" target="#fig_4">Fig. 4 (a)</ref>). In this section, we introduce our interleaved graph attention spatio-temporal network. The temporal component is designed from dilated TCNs to tackle long-term patterns (Sec. III-A). As for the spatial components, we have a local spatial attention network (Sec. III-B) to model the hierarchical and symmetrical structure of the human skeleton and a global spatial attention network (Sec. III-C) to adaptively extract global semantic information to better encode the human body's spatial characteristic. <ref type="figure" target="#fig_3">Fig. 3</ref> (a) depicts an instantiation of the proposed framework with a receptive field size of 27 frames, whilst <ref type="figure" target="#fig_3">Fig. 3</ref> (b) depicts the graph attention block which is composed of local and global spatial blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Temporal Convolutional Network</head><p>The original temporal dilated convolutional model <ref type="bibr" target="#b6">[7]</ref> consists of an input layer, an output layer and B temporal convolutional blocks that flexibly control the receptive field by setting the kernel size and the dilation factor of the convolution. Each block first performs a 1D convolution with kernel size k and dilation factor d = k B , followed by a convolution with kernel size 1. The main difference compared to <ref type="bibr" target="#b6">[7]</ref> is that we represent the input 2D pose sequence as a three-dimensional vector (T, N, C), where T is the number of receptive fields, N is the number of joints in each frame, and C is the number of coordinate dimensions (x, y). To save the spatial information across time steps, we replace the original 1D convolution with 2D convolutions for a kernel size of k ? 1. Simultaneously, each batch normalization is changed to 2D, and it is added at the beginning to normalize the input data. The dropout is only employed at the second convolution layer of blocks to improve generalization. <ref type="figure" target="#fig_3">Fig. 3</ref> shows an instantiation of GAST-Net for a receptive field size of 27 frames with B = 2 blocks. Note that according to the network characteristics of TCNs, our proposed model can train under varying numbers of long-sequence receptive fields as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Local Attention Graph</head><p>At any given frame in time, 2D keypoints represent the joints of the human skeleton. The skeleton is naturally represented by an undirected graph with joints as nodes and human-links as edges. We construct a skeleton graph of 2D keypoints for a given frame based on the SemGCN proposed by Zhao <ref type="bibr" target="#b0">[1]</ref>. We define the skeletal 2D poses as a graph G = (V, E), where V is the set of N nodes and E edges.</p><formula xml:id="formula_0">X = {x 1 , x 2 , . . . , x N | x i ? R 1?C } is the set of node</formula><p>features with C channels. The structure of the graph can be initialized by a first-order adjacency matrix A ? R N ?N that indicates the existing connections between joints and an identity matrix I indicating self-connections. A = (A + I) expresses the convolutional kernel in GCNs. According to the definition of SemGCN, given the node features of the l-th layer, the output features of the subsequent layer are obtained through the following convolution:</p><formula xml:id="formula_1">X (l+1) = ?(M A)X (l) W,<label>(1)</label></formula><p>where W ? R C l ?C l+1 is a learnable matrix used to transform output channels, M ? R N ?N is a learnable mask matrix, is an element-wise multiplication operation, and ? is a Softmax nonlinearity that normalizes the contribution of the features of a node to a corresponding neighboring node in a graph.</p><p>By introducing a set of mask matrices M c ? R N ?N for the channels of the output node features, Eqtn. 1 can be extended to:</p><formula xml:id="formula_2">X (l+1) = C l+1 || c=1 ?(M c A)X (l) w c ,<label>(2)</label></formula><p>where denotes a channel-wise concatenation and w c is the c-th row of matrix W . Eqtn. 2 jointly learns the unique semantics across neighboring nodes. However, and very notably, this first-order neighbor representation poorly models (i) the symmetrical structure of a torso-centered human body and (ii) kinematic constraints in the human body. Thus, we propose that structural knowledge pertaining to symmetrical functions in the human body is explicitly considered. Furthermore, another reason why first-order neighbor representations struggle to model human spatial relations is that joint constraints are confined to first-order neighboring joints. More precisely, distal joints like the wrist, ankle, and head, located at the end of the kinematic chain, only have one first-order neighboring joint. As such, their position in space are not effectively located due to the first-order neighborhood. Such joints are the largest single source of modeling errors <ref type="bibr" target="#b5">[6]</ref>. Nonetheless, we exploit the relationship across sub-segments of the kinematic chain; that is, the lower limbs (ankle-kneehip), upper limbs (wrist-elbow-shoulder), and the axial-body (head-neck-thorax) to mitigate location ambiguity.</p><p>Based on the aforementioned limitations, we design two novel convolution kernels: (i) a symmetric matrix A s (See <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>) that encodes the human skeleton symmetrical structure for joints that have a symmetrical counterpart (i.e. limb joints). (ii) an adjacency matrix A c (See <ref type="figure" target="#fig_4">Fig. 4 (c)</ref>) that explicitly encodes first-and second-order (kinematic) connections for distal joints (i.e. ankle-knee, ankle-hip). The rest of the nodes are only modeled through first-order connections.</p><p>Note that each of these two convolution kernels are applied to two distinct GCNs; where each GCNs is followed by batch normalization and rectified linear units as shown in the right dotted box of <ref type="figure" target="#fig_3">Fig. 3 (b)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Global Attention Graph</head><p>The relationship across disconnected joints, those that exist across sub-segments of the skeleton (e.g. wrist-ankle), play a key role in encoding global postural and constraint information (think running). As such, disconnected joint representation aid in addressing depth ambiguities and occlusions. To adaptively and effectively encode non-local relationships, we propose a global end-to-end GCN with a multi-head attention mechanism that extends the mechanism introduced in Eqtn. 2 from first-order relationships to global relationships. The global attention mechanism is first presented in Eqtn. 3 and then explained in further detail.</p><formula xml:id="formula_3">X (l+1) = K || k=1 (B k + C k ) X (l) W k<label>(3)</label></formula><p>where, K is the number of attention heads, B k ? R N ?N is an adaptive global adjacency matrix, C k ? R N ?N is a learnable global adjacency matrix, and W k ? R C l ?(C l /K) is a transformed matrix. In this work we set K = 4 parallel attention heads. Next, we discuss the redefined adjacency matrix B k and the global adjacency matrix C k in detail. B k expresses a data-dependent matrix which learns a unique graph for each node. We adopt the attention coefficient function proposed by <ref type="bibr" target="#b15">[16]</ref> to determine whether a connection exists between nodes and how strong the connection is. That is, given two node features x i and x j , we first apply two embedding functions ? and ? to downsample the features of each node from C l to C l /K channels. Since the number of channels for each node is reduced, the total computational cost for multi-attention is similar to that of single-headed attention with full channels. Then we concatenate the two embedded features, and compute their dot product with a weight vector w f to produce a scalar output. To facilitate coefficient comparisons across nodes, the scalar outputs are normalized by the softmax function. The operation is presented in Eqtn. 4:</p><formula xml:id="formula_4">? ij = e ?(w f ?[?(xi) ?(xj )]) N k=1 e ?(w f ?[?(xi) ?(x k )]) ,<label>(4)</label></formula><p>where ? and ? are convolutions with the kernel size 1;</p><p>[? ?] denotes concatenation, and ? denotes a LeakyReLU nonlinearity with negative input slope ? = 0.2. C k is a learnable adjacency matrix, inspired by <ref type="bibr" target="#b9">[10]</ref>, with an initialization value of zero. The value of C k is not limited to special node features like B k , which is updated during the training process. The elements of C k are arbitrary. They indicate the existence and strength of connections between two joints. It plays a similar role to the attention mechanism performed by M c in Eqtn. 2. However, note how in Eqtn. 2, M c is dot multiplied with A, and if any of the elements in A is 0, the product will always be 0 irrespective the value of M c . Thus, no new connections can be created in the original physical graph. From this perspective, C k is more flexible than M c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASETS</head><p>We evaluate our method on two publicly available datasets: Human3.6M <ref type="bibr" target="#b18">[19]</ref> and HumanEva-I <ref type="bibr" target="#b19">[20]</ref>. Human3.6M captures data through four synchronized cameras at 50 Hz and contains 3.6 million video frames with 11 professional subjects performing 15 daily activities (i.e. walking and sitting). Following previous methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, we employ subjects 1, 5, 6, 7, 8 for training and 9, 11 for testing. HumanEva-I, is a much smaller dataset and captures data through three camera views at 60 Hz. Following <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, the time-series data from three actions (walk, jog, box) is split between training and testing.</p><p>We use two common evaluation protocols in our experiments. Protocol #1 calculates the mean per joint positioning error (MPJPE) between the ground truth and the predicted 3D coordinates across all cameras and joints. Protocol #2 employs a rigid alignment (Procrustes analysis) with the ground truth before calculating the mean per joint positioning error (P-MPJPE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training &amp; Inference</head><p>GAST-Net was trained with receptive fields of sizes 9, 27, 81 and 243 to verify the effectiveness of our model architecture. To make the model lightweight, for networks with receptive fields of 9 and 27, we increase the number of output channels of the first dilated convolutional layer to 128, while the network with 81 and 243 receptive fields is set to 64 and 32 channels respectively. Note that our loss function only computes the MPJPE between the predicted 3D location and ground truth without using any tricks (i.e. motion constraint and pose regulation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Training &amp; Inference Strategy:</head><p>To train the proposed model, we use Pavllo's optimized training strategy for singleframe predictions instead of the layer-by-layer implementation <ref type="bibr" target="#b6">[7]</ref>. For single-frame scenarios, dilated convolutions are known to waste a large number of computations. To reduce the inefficiency, dilated convolutions are replaced with strided convolutions. At inference, we switch and consider the entire video sequence. We change from the optimized training strategy to the layer-by-layer implementation to make faster predictions.</p><p>2) Implementation Details: Note that the different datasets have different joint setups. In Human 3.6M we predict poses using a 17-joint skeleton and in HumanEva-I, we use 15-joints. Also, we noted that high frame rates lead to information redundancy that negatively affects the  We implement our method with the PyTorch framework and train end-to-end. For Human3.6M, we optimize with Amsgrad with a mini-batch size of b = 128, and train for 80 epochs. The learning rate starts at 0.001 and then applies a learning shrink factor ? = 0.95 in each epoch. The dropout rate p in each dropout layer is set to 0.05. For HumanEva-I, we use b = 32, ? = 0.98, p = 0.5, and train for 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies</head><p>In our ablation studies, as with those in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we will use 2D poses detected from Cascaded Pyramidal Networks (CPNs) <ref type="bibr" target="#b20">[21]</ref> in our 27 receptive fields model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Effects of Spatial Semantics:</head><p>We perform ablation studies to analyze the effect of spatial semantics in our networks architecture as shown in <ref type="table" target="#tab_1">Table I</ref>. As the baseline, we build a plain GAST-Net comprised of TCNs and firstorder SemGCNs to regress 2D keypoints to 3D poses. We then add different semantic GCNs one-by-one to conduct ablation studies. The semantics consist of: (i) local kinematic relations A c , (ii) symmetric relations A s , (ii) global adaptive matrix B k , and (iv) global learnable matrix C k . We see that as we consider additional local-to-global postural constraints, the performance improves steadily. The largest improvements come from local kinematic connections, symmetry, and the global adaptive matrix. These spatial constraints exactly express a hierarchical and symmetrical human structure and conveys global posture semantics, which better reconstructs III: Quantitative comparisons of MPJPE in millimeters between the estimated pose and the ground-truth (GT) on the Human3.6M under Protocol #1 and Protocol #2. T denotes the number of receptive fields, ( ?) indicates the use of pose refinement and spatio-temporal information. Best in bold, second best underlined.  valid 3D poses. These results support the significance of rich spatial semantics in 3D pose estimation.</p><p>2) Sensitivity Analysis of Spatial Semantics: Global graph matrices (B k and C k ) encompass both local and symmetric joint relations. We wish to explore the contributions of each of these configurations under the global analysis. To this end, we study the effect of removing local kinematic connections A c and symmetry A s separately on the Human3.6M as shown in <ref type="table" target="#tab_1">Table II</ref>. The study reveals that removing local connections and symmetry increase errors by 1.4mm and 1.2mm respectively. From this we conclude that explicitly embedding local connections and symmetrical prior knowledge is indispensable and complementary to global semantics in generating more accurate 3D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Comparison with State-of-the-Art:</head><p>With respect to the Human3.6M dataset, we train using CPNs <ref type="bibr" target="#b20">[21]</ref> to detect 2D poses for fair comparison as this is the most commonly used detector in the compared works. <ref type="table" target="#tab_1">Table III</ref> shows the performance of our 243 receptive field model compared to stateof-the-art (SOTA) results. Our method achieves competitive performance on human3.6M under both protocols. Note that Wang et al. <ref type="bibr" target="#b13">[14]</ref> not only exploited spatial and temporal information but also adopted pose refinement and motion loss to regular reconstructed 3D poses. In our work, we only model spatio-temporal information through a simple network and use a common MPJPE loss without using any bells and whistles. In addition, we also report the results when using ground truth 2D poses, which yield approximately 13.5mm improvements in MPJPE.</p><p>With respect to HumanEva-I dataset, which is comprised of videos with much smaller durations compared with those in Human3.6M, we choose to use fewer receptive fields-27-for evaluation. We compare our results with SOTA under Protocol #2. <ref type="table" target="#tab_1">Table IV</ref> shows we achieved the best results in each action except the S3 of "Walking" due to corrupted mocap data.</p><p>2) Comparison with Temporal Convolutional Networks: <ref type="figure" target="#fig_5">Fig. 5</ref> compares the number of parameters and the 3D pose estimation errors between TCNs <ref type="bibr" target="#b6">[7]</ref> and our various receptive field models. Like <ref type="bibr" target="#b6">[7]</ref>, we use CPNs as 2D pose detector for fair comparison. As can be seen on the left plot of <ref type="figure" target="#fig_5">Fig. 5</ref>, we obtain smaller estimation errors for all receptive field combinations on Human3.6M under both protocols. Additionally, our model with 27 receptive fields is also slightly better than the TCNs with 243 receptive fields, which shows that the use of spatial information significantly  contributes to reconstructing more accurate 3D poses. For the right side bar chart of <ref type="figure" target="#fig_5">Fig. 5</ref>, we see that our model uses 62.9%, 19.2%, 44.7%, and 58.2% fewer parameters compared to the TCNs' 9, 27, 81, and 243 receptive field models respectively. This shows that interleaving of our spatial and temporal mechanisms results in a more efficient network for video pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>1) Significance of Spatial Semantics: These studies do 3D pose estimation on YouTube videos using our 27 receptive field mode. The left of <ref type="figure">Fig. 6</ref> shows pose reconstructions with/without local kinematic connections A c in Yoga videos, while the right shows pose reconstructions with/without the global adaptive matrix B k in baseball videos. Pose errors are circled in red. Qualitative analysis for Yoga shows that when local kinematic connections are not considered there is considerable location ambiguity for distal joints. For the baseball videos, even when working with erroneous 2D input poses due to occlusions, the global posture semantics work effectively with temporal continuity to mitigate selfocclusion effects and yield accurate and smooth poses.  2) Visualization of Global Attention Matrix B k : To further understand the construction of global semantics in occluded joints, we visualize our model's global attention matrices B k on the baseball case. <ref type="figure" target="#fig_6">Fig. 7</ref> is the visualization of the sample represented by skeleton graphs, where the circle mass indicates the strength of the relationship between the current joint and the right wrist in matrices. Skeleton graphs of each layer contain the average result across the multihead attention of Eqtn. 3. From the relationship visualized by the three-layer skeleton graphs, we see that the global graph attention tends to establish strong connections with the spine, left hip and right hip-joints close to the root joint. We speculate that the position information of these joints is easier to predict and stabilizes the reconstruction of the valid 3D structure. Apart from the aforementioned relationships, the skeleton graph in the first layer focuses on self-connections. For the 2nd and 3rd layers, strong relationships are constructed with the joints along the left and right leg. We argue that higher layers convey higher-level posture semantics that contribute to modeling the non-local spatial configuration constraints.</p><p>3) Reconstruction of Special Case: We also consider situations in which cameras only capture a person's upper body (common across applications, e.g. human-robot interaction). We conducted experiments that reveal that GAST-Net yields reasonable 3D pose reconstructions as shown on the left of <ref type="figure" target="#fig_7">Fig. 8</ref>. Note that whilst GAST-Net is only trained on whole bodies, the model effectively reconstructs upper body test data it has not seen before. The right side of <ref type="figure" target="#fig_7">Fig 8 shows</ref> two failure cases caused by large 2D detection errors (ice skating) as well as significant occlusions over long-term periods of time (wall climbing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Testing the Speed of 3D Pose Estimation</head><p>We implemented our model with different inference modes and receptive fields to test the speed of 2D-to-3D video pose estimation. The speed time results are summarized in <ref type="table" target="#tab_7">Table V</ref>. These tests used an Intel CORE CPU@2.20GHz(6 cores) laptop and an NVIDIA GTX 1060 GPU. We use native Python without parallel optimization for inference. Since layer-by-layer inference enables parallel processing on the input frames, the estimation speed is faster compared to single-frame inference. To intuitively understand the estimation speed of single-frame inference from RGB videos, we adopt YOLOv3 (160?160) <ref type="bibr" target="#b24">[25]</ref> and SORT <ref type="bibr" target="#b25">[26]</ref> for human detection and tracking, HRNet (256?192) <ref type="bibr" target="#b26">[27]</ref> for 2D pose estimation, and GAST-Net (27 receptive fields) for 2D-to-3D pose reconstruction. Experiments show that our top-down video pose estimation achieves 11 fps for a single person with the same test environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we presented a real-time 3D pose estimation methodology that simultaneously and flexibly captures varying spatio-temporal sequences. The proposed graph attention blocks, effectively model the symmetrical hierarchy of 2D skeleton as well as global postural constraints, are synergistically combined with temporal dependencies to better mitigate depth ambiguity and resolve self-occlusion. Qualitative results show that our approach also generalizes to 3D pose estimation in the half upper body, which helps to close-range interactive applications (e.g., human-robot interaction).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Pose estimation reconstruction under depth ambiguities, self-occlusion, and biased 2D poses. Row 1: 2D pose estimation, where red is the prediction with errors. Row 2, 3: our reconstructions from two different perspectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Schematic overview of the GAST-Net framework. The input consists of consecutive 2D pose estimates from RGB images. The output is a sequence of reconstructed 3D poses from the corresponding 2D keypoints. GAST-Net synergistically interleaves 3 components: (a) a dilated temporal convolutional model (with 2D keypoint sequences as input (bottom) and 3D pose estimates as output (top)) with (b) a set of local attention mechanisms for visualized joints (i.e. the right-wrist) including local kinematic dependencies and symmetric relations, and (c) a global attention mechanism that informs about posture semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>(a) An instantiation of GAST-Net for 3D pose estimation. The GAST-Net consists of 2 Temporal Convolution Blocks and 3 Graph Attention Blocks. Given a 2D pose sequence, the output is a sample 1-frame prediction. Dimensions are enclosed in parenthesis: e.g. (27, 17, 2) denotes a receptive field of 27 frames, 17 joints, and 2 channels. (b) The graph attention block architecture. The left dotted box indicates the local graph attention layer. The right dotted box indicates the global graph attention layer. The layer outputs is concatenated followed by a 2D convolution layer before outputting the spatio-temporal features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>(a) A human skeleton graph with 17 joints; (b) A symmetrical matrix A s ; (c) An adjacency matrix A c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Left: Comparison with TCNs [7] in different receptive fields on Human3.6M under protocol #1 and #2. Right: Comparison with the parameters of model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Visualization of global attention matrix B k . The right wrist is set as the visualized joint. Three colored layers represent three different attention layers advancing from top to bottom. Circle mass indicates the relationship strength between the current and the visualized joint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Left: Examples of results from our model reconstructed valid 3D structure from a half body. Right: Two failure cases caused by big 2D detection error and long-term heavy occlusion respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Ablation study on different spatial semantics in our network architecture on Human3.6M under both protocols.</figDesc><table><row><cell>Method (T=27, CPN)</cell><cell cols="2">MPJPE (mm) P-MPJPE (mm)</cell></row><row><cell>Baseline</cell><cell>60.9</cell><cell>50.5</cell></row><row><cell>+ Local GCNs with Ac</cell><cell>55.4</cell><cell>44.0</cell></row><row><cell>+ Local GCNs with As</cell><cell>51.9</cell><cell>40.9</cell></row><row><cell>+ Global GCNs with B k</cell><cell>47.3</cell><cell>36.2</cell></row><row><cell>+ Global GCNs with C k</cell><cell>46.2</cell><cell>36.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">: Ablation study on the sensitivity of spatial</cell></row><row><cell cols="3">semantics in our model on Human3.6M under Protocol #</cell></row><row><cell>1.</cell><cell></cell><cell></cell></row><row><cell>Method (T=27, CPN)</cell><cell>MPJPE(mm)</cell><cell>?</cell></row><row><cell>Ours w/o Local GCNs with Ac</cell><cell>47.6</cell><cell>1.4</cell></row><row><cell>Ours w/o Local GCNs with As</cell><cell>47.4</cell><cell>1.2</cell></row><row><cell>Ours (GAST-Net)</cell><cell>46.2</cell><cell>-</cell></row></table><note>encoding of global semantics over time. For this reason, we decided to downsample the Human3.6M dataset from 50 FPS to 10 FPS. On the other hand, as the duration of videos in the HumanEva-I dataset is short, no downsampling is performed here. With regards to real-time estimation, long video durations are not suitable for fast estimation; as such we do not perform downsampling for the 243 receptive field model. Finally, we adopt horizontal flip augmentation at both training and testing time.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="10">: Comparison on HumanEva-I under protocol #2.</cell></row><row><cell cols="10">Best in bold, second best underlined. Note that the high error</cell></row><row><cell cols="8">on S3's "Walk" is due to corrupted mocap data.</cell><cell></cell></row><row><cell>Protocol #2</cell><cell></cell><cell>Walk</cell><cell></cell><cell></cell><cell>Jog</cell><cell></cell><cell></cell><cell>Box</cell></row><row><cell></cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell></row><row><cell>Martinez [2]</cell><cell cols="2">19.7 17.4</cell><cell>46.8</cell><cell cols="3">26.9 18.2 18.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pavlakos [24]</cell><cell cols="2">18.8 12.7</cell><cell>29.2</cell><cell cols="3">23.5 15.4 14.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Lee [13]</cell><cell cols="2">18.6 19.9</cell><cell>30.5</cell><cell cols="3">25.7 16.8 17.7</cell><cell cols="3">42.8 48.1 53.4</cell></row><row><cell>Pavllo [7]</cell><cell>13.9</cell><cell cols="2">10.2 46.6</cell><cell cols="2">20.9 13.1</cell><cell>13.8</cell><cell>23.8</cell><cell>33.7</cell><cell>32.0</cell></row><row><cell>Ours (T=27 CPN)</cell><cell>13.7</cell><cell>9.2</cell><cell>46.2</cell><cell cols="3">20.1 12.5 12.7</cell><cell cols="3">21.8 27.8 27.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Left: Examples of results from our model with/without local kinematic connections A c . Right: Examples of results from our model with/without global adaptive matrix B k . Wrong estimations are labeled in red circles.</figDesc><table><row><cell>Frame 10</cell><cell></cell><cell>Frame 20</cell><cell></cell><cell></cell><cell>Frame 40</cell><cell>Frame 50</cell><cell>Frame 78</cell><cell>Frame 80</cell><cell>Frame 82</cell><cell>Frame 85</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input</cell></row><row><cell>2D Keypoints</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2D Keypoints</cell></row><row><cell>Without ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Without</cell></row><row><cell>Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results</cell></row><row><cell>With ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>With</cell></row><row><cell>Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results</cell></row><row><cell cols="2">Right wrist Fig. 6: First layer Frame 78 Frame 80</cell><cell cols="2">Second layer Frame 82</cell><cell cols="2">Third layer Frame 85</cell><cell>Attention</cell><cell></cell></row><row><cell>Frame 78</cell><cell cols="2">Frame 80</cell><cell cols="2">Frame 82</cell><cell cols="2">Frame 85</cell><cell></cell></row><row><cell>First layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Second layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Third layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>Test time of estimation speed</figDesc><table><row><cell>GAST-Net</cell><cell cols="3">Layer-by-layer inference</cell><cell cols="3">Single-frame inference</cell></row><row><cell>Receptive fields</cell><cell>27</cell><cell>81</cell><cell>243</cell><cell>27</cell><cell>81</cell><cell>243</cell></row><row><cell>Frames per second</cell><cell cols="2">1270 1120</cell><cell>960</cell><cell>74</cell><cell>56</cell><cell>45</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Phuoc</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rayat Imtiaz Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalizing monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13985</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-18 AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic multiscale graph neural networks for 3d skeleton based human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Trajectory space factorization for deep videobased 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08289</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3464" to="3468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
