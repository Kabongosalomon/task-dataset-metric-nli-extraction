<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Human-Object Interaction via Fabricated Compositional Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
							<email>baosheng.yu@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
							<email>pengxiaojiang@sztu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen Technology University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Human-Object Interaction via Fabricated Compositional Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-Object Interaction (HOI) detection, inferring the relationships between human and objects from images/videos, is a fundamental task for high-level scene understanding. However, HOI detection usually suffers from the open long-tailed nature of interactions with objects, while human has extremely powerful compositional perception ability to cognize rare or unseen HOI samples. Inspired by this, we devise a novel HOI compositional learning framework, termed as Fabricated Compositional Learning (FCL), to address the problem of open long-tailed HOI detection. Specifically, we introduce an object fabricator to generate effective object representations, and then combine verbs and fabricated objects to compose new HOI samples. With the proposed object fabricator, we are able to generate large-scale HOI samples for rare and unseen categories to alleviate the open long-tailed issues in HOI detection. Extensive experiments on the most popular HOI detection dataset, HICO-DET, demonstrate the effectiveness of the proposed method for imbalanced HOI detection and significantly improve the state-of-the-art performance on rare and unseen HOI categories. Code is available at https://github.com/zhihou7/HOI-CL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-Object Interaction (HOI) detection, which aims to localize and infer relationships between human and objects in images/videos, human, verb, object , is an essential step towards deeper scene and action understanding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>. In real-world scenarios, long-tailed distributions are common for the data perceived by human vision system, e.g., actions/verbs and objects <ref type="bibr" target="#b38">[39]</ref>. The combinatorial nature of HOI further highlights the issues of longtailed distributions in HOI detection, while human can effi-  ciently learn to recognize seen and even unseen HOIs from limited samples. An intuitive example of open long-tailed HOI detection is shown in <ref type="figure" target="#fig_1">Figure 1</ref>, in which one can easily recognize the unseen action "ride bear", nevertheless it never even happened. However, existing HOI detection approaches usually focus on either the head <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b56">57]</ref>, the tail <ref type="bibr" target="#b61">[62]</ref> or unseen categories <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b42">43]</ref>, leaving the problem of open long-tailed HOI detection poorly investigated.</p><p>Open long-tailed HOI detection falls into the category of the long-tailed zero-shot learning problem, which is usually referred into several isolated problems, including long-tailed learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22]</ref>, few-shot learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b51">52]</ref>, zero-shot learning <ref type="bibr" target="#b32">[33]</ref>. To address the problem of imbalanced training data, existing methods mainly focus on three strategies: 1) re-sampling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>; 2) re-weighted loss functions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21]</ref>; and 3) knowledge transfer <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b10">11]</ref>. Specifically, re-sampling and re-weighted loss functions are usually designed for imbalance problem, while knowledge transfer is introduced to relieve all the long-tailed <ref type="bibr" target="#b57">[58]</ref>, few-shot <ref type="bibr" target="#b48">[49]</ref>, and zero-shot problem <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b10">11]</ref>. Recently, two popular knowledge transfer methods have received increasing attention from the com- munity, data generation <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29]</ref> (transferring head/base classes to tail/unseen classes) and visual-semantic embedding <ref type="bibr" target="#b10">[11]</ref> (transferring from language knowledge). Along the first way, we address the problem of open long-tailed HOI detection from the perspective of HOI generation.</p><p>Unlike the samples in typical long-tailed zero-shot learning for visual recognition, each HOI sample is composed of a verb and an object, and different HOIs may share the same verb or object (e.g., "ride bike" and "ride horse"). In cognitive science, human perceives concepts as the compositions of shareable components <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref> (e.g., verb and object in HOI), which indicates that human can conceive a new concept through a composition of existing components. Inspired by this, several zero-and few-shot HOI detection approaches have been proposed to enforce the factored primitive (verb and object) representation of the same primitive class to be similar among different HOIs, such as factorized model <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b2">3]</ref> and factor visual-language model <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b2">3]</ref>. However, regularizing factor representation, i.e. enforcing the same verb/object representation to be similar among different HOIs, is only sub-optimal for HOI detection. Recently, Hou et al. <ref type="bibr" target="#b23">[24]</ref> present to compose novel HOI samples via combining decomposed verbs and objects between pair-wise images and within image. Nevertheless, it still remains a great challenge to compose massive HOI samples in each minibatch from images due to limited number of HOIs in each image, especially when the distribution of objects/verbs is also long-tailed. We demonstrate the distribution of the number of objects in <ref type="figure">Figure 2</ref>.</p><p>The long-tailed distribution of objects/verbs makes it difficult to compose new HOIs from each mini-batch, significantly degrading the performance of compositional learning-based methods for rare and zero-shot HOI detection <ref type="bibr" target="#b23">[24]</ref>. Inspired by recent success of visual object representation generation <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b56">57]</ref>, we thus apply fabricated object representation, instead of fabricated verb representation, to compose more balanced HOIs. We referred to the proposed compositional learning framework with fabricated object representation as Fabricated Compositional Learning or FCL. Specifically, we first extract verb representations from input images, and then design a simple yet efficient object fabricator to generate object representation. Next, the generated visual object features are further combined with the verb features to compose new HOI samples. With the proposed object fabricator, we are able to generate balanced objects for each verb within the mini-batch of training data as well as compose massive balanced HOI training samples.</p><p>The main contributions of this paper can be summarized as follows: 1) proposing to compose HOI samples for Open Long-Tailed HOI detection; 2) designing an object fabricator to generate objects for HOI composition; 3) significantly outperforming recent state-of-the-art methods on HICO-DET dataset among rare and unseen categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>HOI Detection. HOI detection is essential for deeper scene and action understanding <ref type="bibr" target="#b6">[7]</ref>. Recent HOI detection approaches usually focus on representation learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b52">53]</ref>, zero/few-shot generalization <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref>, and One-Stage HOI detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b55">56]</ref>. Specifically, existing methods improve HOI representation learning by exploring the relationships among different features <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b50">51]</ref>, including pose information <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr">34]</ref>, context <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b54">55]</ref>, and human parts <ref type="bibr" target="#b65">[66]</ref>; Generalization methods for HOI detection mainly include visual-language model <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b61">62]</ref>, factorized model <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b2">3]</ref>, and HOI composition <ref type="bibr" target="#b23">[24]</ref>. Recently, Liao et al. <ref type="bibr" target="#b35">[36]</ref> and Wang et al. <ref type="bibr" target="#b55">[56]</ref> propose to detect the interaction point for HOI by heatmap-based localization <ref type="bibr" target="#b40">[41]</ref>. Wang et al. <ref type="bibr" target="#b53">[54]</ref> try to detect HOI with novel objects by leveraging human visual clues to localize interacting objects. However, existing HOI approaches usually fail to investigate the imbalance issue and zero-shot detection. Inspired by the factorized model <ref type="bibr" target="#b47">[48]</ref>, we propose to compose visual verb and fabricated objects to address the open long-tailed issue in HOI detection. Furthermore, according to whether detect the objects with a separated detector or not, existing HOI detection approaches can be divided into two categories: 1) one-stage <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b13">14]</ref> and two-stage <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b50">51]</ref>. Two-stage methods usually achieve better performance and our method falls into this category.</p><p>Compositional Learning. Irving Biederman illustrates that human representations of concepts are decomposable <ref type="bibr" target="#b3">[4]</ref>. Meanwhile, Lake et al. <ref type="bibr" target="#b31">[32]</ref> argue compositionality is one of the key blocks in a human-like learning system. Tokmakov et al. <ref type="bibr" target="#b49">[50]</ref> apply the compositional deep representation into few-shot learning. External knowledge graph and graph convolutional networks in <ref type="bibr" target="#b27">[28]</ref> are used to compose verb-object pairs for HOI recognition. Recently, Hou et al. <ref type="bibr" target="#b23">[24]</ref> propose a novel visual compositional learning framework to compose HOIs from image-pairs for HOI detection, failing to address the open and long-tailed issues. Therefore, we further compose verb and fake object representations for HOI detection.</p><p>Generalized Zero/Few-Shot Learning. Different from typical zero/few-shot learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b51">52]</ref>  <ref type="figure">Figure 3</ref>. An overview of the proposed multi-branch fabricated compositional learning framework for HOI detection. We first detect human and object with Faster-RCNN <ref type="bibr" target="#b44">[45]</ref> from the image. Next, with ROI-Pooling and residual CNN blocks, we extract human features, verb features and object features. Meanwhile, an object identity embedding, verb feature and noise are input into Fabricator to generate fake object feature. Then, these features are fed into the following branches: individual spatial HOI branch, HOI branch and fabricated compositional HOI branch. Finally, HOI representations from HOI branch and fabricated branch are optimized by a shared FC-Classifier, while HOI representations from spatial branch are classified by an individual FC-Classifier. In fabricated compositional HOI branch, verb features are combined with fabricated objects to construct fabricated HOIs. zero/few-shot learning <ref type="bibr" target="#b59">[60]</ref> is a more realistic variant, since the performance is evaluated on both seen and unseen classes <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b5">6]</ref>. The distribution of HOIs is naturally longtailed <ref type="bibr" target="#b6">[7]</ref>, i.e., most classes have a few training examples.</p><p>Moreover, the open long-tailed HOI detection aims to handle the long-tailed, low-shot and zero-shot issue in a unified way. The long-tailed data distribution <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref> is one of challenging problem in visual recognition. Currently, resampling <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>, specific loss <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21]</ref>, knowledge transfer <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b38">39]</ref>, and data generation <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b1">2]</ref> are major strategies for imbalanced learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. To make full use of the composition characteristic of HOI, we aim to compose HOI samples by visual feature generation to relieve the open long-tailed issue in HOI detection. Recent feature generation methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b60">61]</ref> mainly depend on Variational Autoencoder <ref type="bibr" target="#b29">[30]</ref> and Generative Adversarial Network <ref type="bibr" target="#b14">[15]</ref>, which usually suffer from the problem of model collapse <ref type="bibr" target="#b45">[46]</ref>. Wang et al. <ref type="bibr" target="#b56">[57]</ref> present a new method for low-shot learning that directly learns to hallucinate examples that are useful for classification. Similar to <ref type="bibr" target="#b56">[57]</ref>, we compose HOI samples with an object fabricator in an endto-end optimization without using the adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first describe the multi-branch compositional learning framework for HOI detection. We then introduce the proposed fabricated compositional learning for open long-tailed HOI detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-branch HOI Detection</head><p>HOI detection aims to find the interactions between human and different objects in a given image/video. Existing HOI detection methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b2">3]</ref> usually contain two separated stages: 1) human and object detection; and 2) interaction detection. Specifically, we first use a common object detector, e.g., Faster R-CNN <ref type="bibr" target="#b44">[45]</ref>, to localize the positions and extract the features for both human and objects. According to the union of human and object bounding boxes, we then extract the verb feature from the feature map of backbone networks via the ROI-Pooling operation. Similar to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref>, an additional stream for spatial pattern, i.e., spatial stream, is defined as the concatenation of human and object masks, i.e., the value in the human/object bounding box region is 1 and 0 elsewhere. As a result, we obtain several input streams from the first stage, i.e., human stream, object stream, verb stream, and spatial stream.</p><p>The input streams from the first stage then are used to construct different branches in the second stage: 1) the spatial HOI branch, which concatenates the spatial and the human streams to construct spatial HOI feature for HOI recognition; 2) the HOI branch, which concatenates the verb and the object streams; and 3) the fabricated compositional branch, which is based on a new stream, the fabricator stream, to generate fake object features for composing new HOIs. Specifically, the fabricated compositional branch generates novel HOIs by combining visual verb features and generated object features. The main multi-branch HOI detection framework is shown in <ref type="figure">Figure 3</ref>, and we leave the details of the fabricated compositional branch in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fabricated Compositional Learning</head><p>The motivation of compositional learning is to decompose a model/concept into several sub-models/concepts, in which each sub-model/concept focuses on a specific task, and then all responses are coordinated and aggregated to make the final prediction <ref type="bibr" target="#b3">[4]</ref>. Recent compositional learning method for HOI detection considers each HOI as the combination of a verb and an object to compose new HOIs from objects and verbs within the mini-batch of training samples <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref>. However, existing compositional learning methods fail to address the problem of long-tailed distribution on objects.</p><p>To address the open long-tailed issue, we propose to generate balanced objects for each decoupled visual verb as follows. Formally, we denote l v as the label of a verb x v , l o as the label of an object x o and y as the HOI label of x v , x o . Given another verb representationx v (sharing the same label l v with x v ), and another object representationx o (sharing the same label l o with x o ), regardless of the sources of the verb and object representations, an effective composition of verb and object should be</p><formula xml:id="formula_0">g hoi (x v ,x o ) ? g hoi (x v , x o ),<label>(1)</label></formula><p>where g hoi indicates the HOI classification network. By doing this, we can compose new verb-object pair x v ,x o , which have similar semantic type y to the real pair x v , x o , to relieve the scarcity of rare and unseen HOI categories. To generate effective verb-object pair x v ,x o , we regularize the verb representationx v and object representationx o such that same verbs/objects have similar feature representation. Similar to previous approaches, such as factor visuallanguage joint embedding <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b42">43]</ref> and factorized model <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b17">18]</ref>, whenx v is similar to x v andx o is similar to x o , we then have that Equation (1) can be generalized to HOI detection via the compositional branch. We refer to the proposed compositional learning framework with fabricated object representation as Fabricated Compositional Learning or FCL. We train the proposed method with composited HOI samples x v ,x o in an end-to-end manner, and  <ref type="figure">Figure 4</ref>. For a given visual verb feature and each j th (0 ? j &lt; No), we firstly select the j th object identity embedding. Then, we concatenate verb feature, object embedding and Gaussian noise to input to fabricator for generating a fake object feature. We can fabricate No objects for a verb feature. We finally remove nonexisting HOIs as described in Section 3.2.2.</p><p>the overall loss function are defined as follows:</p><formula xml:id="formula_1">L = ? 1 L hoi + ? 2 L CL + ? 3 L reg + L hoi sp ,<label>(2)</label></formula><p>where L reg aims to regularize verb and object features, L CL indicates a typical compositional learning loss function for the classification network g hoi with composite HOI samples x v ,x o as the input, L hoi sp is the loss for Spatial HOI Branch. ? 1 , ? 2 , ? 3 are the hyper-parameters to balance different loss functions. Specifically, object feature extracted from a pre-trained object detector backbone network (i.e. Faster-RCNN <ref type="bibr" target="#b44">[45]</ref>) are usually discriminative. Thus, we only regularize verb representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Object Generation</head><p>The HOI is composed of a verb and an object, in which the verb is usually a very abstract notation compared to the object, making it difficult to directly generate verb features. Recent visual feature generation methods have demonstrated the effectiveness of feature generation for visual object recognition <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b60">61]</ref>. Therefore, we devise an object fabricator to generate object feature representations for composing novel HOI samples. The overall framework of object generation is shown in <ref type="figure">Figure 4</ref>. Specifically, we maintain a pool of object identity embeddings, i.e., v id . We provide three kinds of embeddings in supplementary material. In each HOI, the pose of the object is usually influenced by the human who is interacting the object <ref type="bibr" target="#b64">[65]</ref>, and the person who is interacting with the object is firmly related to verb feature representation. Thus, for each extracted verb and the j th object (0 ? j &lt; N o and N o is the number of all different objects), we concatenate the j th object identity embedding v j id , the verb feature x v and a noise vector ? N (0, 1), as the input of the object fabricator, i.e.,</p><formula xml:id="formula_2">x o = f obj ({v j id , x v , }),<label>(3)</label></formula><p>wherex o is the fake object feature and f indicates the object fabricator network. Here, the noise is used to increase the diversity of generated objects. We then combine the fake object featurex o and the verb x v to compose a new HOI sample x v ,x o . Specifically, during training, both real HOIs and composite HOIs share the same HOI classification network g hoi .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Efficient HOI Composition</head><p>To compose new HOIs from verb and object representations, we need to remove some infeasible composite HOIs (e.g., "ride vase") as illustrated in <ref type="figure">Figure 4</ref>. To avoid frequently checking the pair (x v , x o ), we use an efficient HOI composition similar to <ref type="bibr" target="#b23">[24]</ref>. Specifically, the HOI label space is decoupled into verb and object spaces, i.e., the co-occurrence matrices</p><formula xml:id="formula_3">A v ? R Nv?C and A o ? R No?C , where N v , N o ,</formula><p>and C indicate the number of verbs, objects and HOI categories, respectively. Given an one-hot HOI label vector y ? R C , we then have the verb label vectors,</p><formula xml:id="formula_4">l v = yA T v ,<label>(4)</label></formula><p>where l v ? R Nv can be a multi-hot vector with multiple verbs, e.g., {hold, read}, book ). Similarly, combining the verb l v with all N o objects, we have the matrixl o ? R No?No as labels of all N o fake objects. Let l v ? R No?Nv denote the verb labels corresponding to fake object featuresl o , the new interaction label can then be evaluated as follows,?</p><formula xml:id="formula_5">= (l o A o ) &amp; (l v A v ),<label>(5)</label></formula><p>where &amp; indicates the logical operation "and". Finally, the logical operation automatically filters out the infeasible HOIs since the labels of those infeasible HOIs are all-zero vectors in the label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>Training. The verb feature contains the pose information of the object, making it difficult to jointly train the network with an object fabricator from scratch. Therefore, we introduce a step-wise training strategy for the long-tailed HOI detection. Firstly, we pre-train the network by L hoi , L hoi sp and L reg without the fabricator branch. Then, we fix the pre-trained model and train the randomly initialized object fabricator via the loss function for the fabricator branch L CL . Lastly, we jointly fine-tune all branches by L in an end-to-end manner. To avoid the bias to seen data in the first step, we optimize the network in one step for zero-shot HOI detection (See analysis in Section A).</p><p>Inference. The fabricated branch is only used in the training stage, i.e., we remove it during the inference stage. Similar to previous multi-branch methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b23">24]</ref>, for each human-object bounding box pair (b h , b o ), the final HOI prediction S c h,o for each category c ? 1, ..., C, can be evaluated as follows,</p><formula xml:id="formula_6">S c h,o = s h ? s o ? S c sp ? S c hoi ,<label>(6)</label></formula><p>where s h and s o indicate the object detection scores for the human and object, respectively. S c sp and S c hoi are the scores from the Spatial branch and the HOI branch, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce datasets and metrics, and then provide the details of the implementation of our method. Next, we present our experimental results compared with state-of-the-art approaches. Finally, we conduct ablation studies to validate the components in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>We adopt the largest HOI datasets HICO-DET <ref type="bibr" target="#b6">[7]</ref>, which contains 47,776 images including 38,118 images for training and 9,658 images for testing. All 600 HOI categories are constructed from 80 object categories and 117 verb categories. HICO-DET provides more than 150k annotated human-object pairs. In addition, V-COCO is another small HOI dataset with 29 categories <ref type="bibr" target="#b16">[17]</ref>. Considering that V-COCO mainly focuses to verb recognition and do not contain a severe long-tailed issue, we mainly evaluate the proposed method on HICO-DET. We also illustrate the result on visual relation detection <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b62">63]</ref>, which requires to detect the triplet (subject, predicate, object) in supplementary materials. We follow the evaluation settings in <ref type="bibr" target="#b6">[7]</ref>, i.e. a HOI prediction is a true positive if 1) both the human and object bounding boxes have IoUs larger than 0.5 with the reference ground truth bounding boxes; and 2) the HOI prediction is accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, our HOI detection model contains two separated stages: 1) we finetune the Faster R-CNN detector pre-trained on COCO <ref type="bibr" target="#b37">[38]</ref> using HICO-DET to detect the human and objects 1 ; 2) we use the proposed FCL model for HOI classification. Specifically, all branches are twolayer MLP sigmoid classifiers with 2048-d input and 1024d hidden units. Fabricator is a two-layer MLP. The L reg is a sigmoid classifier for verb representation. L CL , L hoi and L hoi sp are binary cross entropy losses. A v and A o are set according to HOI dataset, and we can also set them by prior knowledge to detect more types of unseen HOIs. Besides, to prevent the fabricated HOIs from dominating the model optimization process, we randomly sample fabricated HOIs in each mini-batch to keep that the number of fabricated HOIs is not more than three times the number of non-fabricated HOIs. We train our network for one million iterations by SGD optimizer on the HICO-DET dataset with an initial learning rate of 0.01, a weight decay of 0.0005, and a momentum of 0.9. We set ? 1 as 2.0, ? 2 as 0.5 and ? 3 as 0.3, while we set 1 for the coefficient of L hoi sp . The hyper-parameters are ablated in supplementary materials. We jointly fine-tune the model with the object fabricator for 500k iterations, and decay the initial learning rate 0.01 with a cosine annealing schedule. All our experiments on HICO-DET are conducted using TensorFlow [1] on a single Nvidia GeForce RTX 2080Ti GPU. We evaluate V-COCO based on PMFNet <ref type="bibr" target="#b52">[53]</ref> with two GPUs. We do not use auxiliary verb loss since there are only two kinds of objects on V-COCO. We set ? 1 as 1 and ? 2 as 0.25 on V-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to Recent State-of-the-Arts</head><p>Our method aims to relieve open long-tailed HOI detection. However current approaches usually focus on full categories, rare categories and unseen categories separately. In order to compare with state-of-the-art methods, we evaluate our method on long-tailed detection and generalized zeroshot detection separately. The HOI detection result is evaluated with mean average precision (mAP) (%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Effectiveness for Zero-Shot HOI Detection</head><p>There are different settings <ref type="bibr" target="#b2">[3]</ref> for zero-shot HOI detection: 1) unseen composition; and 2) unseen object. Specifically, for the unseen composition setting, it indicates that the training data contains all factors (i.e., verbs and objects) but misses the verb-object pairs; for the unseen object setting, it requires to detect unseen HOIs, in which the object do not appear in the training data. For unseen composition HOI detection, similar to <ref type="bibr" target="#b23">[24]</ref>, we select two groups of 120 unseen HOIs from tail preferentially (rare first) and from head preferentially (non-rare first) separately, which roughly compares the lowest and highest performances. As a result, we report our result in the following settings: Unseen (120 HOIs), Seen (480 HOIs), Full (600 HOIs) in the "Default" mode on HICO-DET dataset. For a better comparison, we implement the factorized model <ref type="bibr" target="#b47">[48]</ref> under our framework for unseen composition zero-shot HOI detection. For unseen object HOI detection, we use the same HOI categories for unseen data as <ref type="bibr" target="#b2">[3]</ref> (i.e. randomly selecting 12 objects from the 80 objects and picking all HOIs containing there objects as unseen HOIs). Then, we report our results in the setting: Unseen (100 HOIs), Seen (500 HOIs), Full (600 HOIs). To compare with the contemporary work <ref type="bibr" target="#b23">[24]</ref>, we use the same object detection result released by <ref type="bibr" target="#b23">[24]</ref>. Here, our baseline method is the model without object fabricator, i.e., the compositional branch.  <ref type="bibr" target="#b6">[7]</ref>. FCL DRG is FCL with object detector provided by <ref type="bibr" target="#b11">[12]</ref>. FCL + VCL means we fuse the result provided in <ref type="bibr" target="#b23">[24]</ref> with FCL. VCL DRG uses the released model of VCL. Unseen composition. <ref type="table" target="#tab_1">Table 1</ref> shows that FCL achieves large improvement on Unseen category by 4.22% and 5.19% than baseline, and by 3.10% and 2.44% compared to previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref> on the two selection strategies respectively. Meanwhile, the two selection strategies witness a consistent improvement with FCL on nearly all categories, which indicates that composing novel HOI samples contributes to overcome the scarcity of HOI samples. In rare first selection, FCL has a similr result to baseline and VCL <ref type="bibr" target="#b23">[24]</ref> on Seen category. But step-wise optimization can improve the result on Seen category and Full category (See <ref type="table">Table 6</ref>). In addition, the factorized model has a very poor performance in the head classes compared to our baseline. Noticeably, factorized model achieves better performance on Unseen category than baseline in non-rare first selection while has worse result on Unseen category in rare first selection. FCL witnesses a consistent improvement in differ- ent evaluation settings. In the remaining data, unseen HOIs of rare first zero-shot have more rare verbs (less than 10 instances) than that of non-rare first zero-shot. Unseen object. We further evaluate FCL in novel object zero-shot HOI detection, which requires to detect HOIs that is interacting with novel objects. <ref type="table" target="#tab_1">Table 1</ref> shows FCL effectively improves the baseline by 2.68% on Unseen Category, although there are no real objects of unseen HOIs in training set. This illustrates the ability of FCL for detecting unseen HOIs with novel objects. Here, the same as <ref type="bibr" target="#b2">[3]</ref>, we also use a generic detector to enable unseen object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Effectiveness for Long-Tailed HOI Detection</head><p>We compare FCL with recent state-of-the-art HOI detection approaches <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12]</ref> using fine-tuned object detector on HICO-DET to validate its effectiveness on longtailed HOI detection. For fair comparison, we use the same fine-tuned object detector provided by <ref type="bibr" target="#b23">[24]</ref>. For evaluation, we follow the settings in <ref type="bibr" target="#b6">[7]</ref>: Full (600 HOIs), Rare (138 HOIs), Non-Rare (462 HOIs) in "Default" and "Known Object" on HICO-DET.</p><p>In <ref type="table">Table 2</ref>, we find that the proposed method achieves new state-of-the-art performance, 24.68% and 26.80% mAP on "Default" and "Known Object". Meanwhile, we achieve a significant performance improvement of 2.82% over the contemporary best rare performance model <ref type="bibr" target="#b23">[24]</ref> under the same object detector, which indicates the effectiveness of the proposed compositional learning for the long-tailed HOI detection. Furthermore, with the same object detection result to <ref type="bibr" target="#b11">[12]</ref>, our results surprisingly increase to 29.12% on "Default" mode. Here, we merely change the detection result provided in <ref type="bibr" target="#b23">[24]</ref> to that provided in <ref type="bibr" target="#b11">[12]</ref> during inference. Particularly, we find our method is complementary to compose HOIs between images <ref type="bibr" target="#b23">[24]</ref>. By simply fusing the result provided by <ref type="bibr" target="#b23">[24]</ref> with FCL, we can further largely improve the results under different object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Effectiveness on V-COCO</head><p>We also evaluate FCL on V-COCO. Although the data on V-COCO is balanced, FCL still improves the baseline (reproduced PMFNet <ref type="bibr" target="#b52">[53]</ref>) in <ref type="table">Table 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>For a robust validation of the proposed method in rare categories and unseen categories simultaneously, we select 24 rare categories and 96 non-rare categories for zero-shot learning (remained 30,662 training instances). This result is roughly between non-rare first selection and rare first selection in <ref type="table" target="#tab_1">Table 1</ref>. See supplementary material for unseen type details and ablation study of long-tailed HOI detection based on <ref type="table">Table 2</ref>. We conduct ablation study on FCL, verb regularization loss, verb fabricator, step-wise optimization and the effect of object detector.</p><p>Fabricated Compositional Learning. In <ref type="table" target="#tab_3">Table 3</ref>, we find that the proposed compositional method with fabricator can steadily improve the performance and it is orthogonal to verb feature regularization (verb regularization loss).</p><p>Verb Feature Regularization. We use a simple auxiliary verb loss to regularize verb features. Although verb regularization loss can slightly improve the rare and unseen category performance (See row 1 and row 3 in <ref type="table" target="#tab_3">Table 3</ref>), FCL further achieves better performance. This indicates that regularizing factor features is suboptimal compared to the proposed method. Semantic verb regularization like <ref type="bibr" target="#b61">[62]</ref> has a similar result (See supplementary materials).</p><p>Verb and Noise for Fabricator. <ref type="table" target="#tab_4">Table 4</ref> demonstrates that performance drops without verb representation or noise. This shows verb representations can provide useful information for generating objects and noise efficiently improves the performance by increasing feature diversity. We meanwhile find the fabricator still effectively improves the baseline without verb or noise by comparing <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>, which indicates the efficiency of FCL.</p><p>Verb Fabricator. The result of fabricating verb features (from verb identity embedding, object features and noise) is even worse as in <ref type="table" target="#tab_4">Table 4</ref>. This verifies that it is difficult to directly generate useful verb or HOI samples due to the complexity and abstraction. Supplementary materials provide more visualized analysis of verb and object feature.  Step-wise Optimization. <ref type="table">Table 6</ref> illustrates that stepwise training has better performance in rare and non-rare categories while has worse performance in unseen categories. We think it might be because the model with the step-wise training has the bias to seen categories in the first step since there are no training data for unseen categories.</p><p>Object Detector. The quality of detected objects has important effect on two-stage HOI Detection methods <ref type="bibr" target="#b23">[24]</ref>. <ref type="table">Table 7</ref> shows that the improvement of FCL over baseline is higher with the fine-tuned detector on HOI data. COCO detector without finetuning on HICO-DET contains a large number of false positive and false negative boxes on HICO-DET due to domain shift, which is in fact less useful to evaluate the effectiveness of modeling human interactions for HOI detection. If the detected boxes during inference are false, the features extracted from the false boxes are also unreal and have large shift to the fabricated objects during training. This causes that fabricated objects are less useful for inferring HOIs during inference. Besides, GT boxes provide a strong object label prior for verb recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Qualitative Analysis</head><p>Illustration of improvement among categories. In <ref type="figure" target="#fig_4">Figure 5</ref>, we find that the rarer the category is, the more the proposed method can improve. The result illustrates the benefit of FCL for long-tailed issue in HOI Detection.</p><p>Visualized Analysis between fabricated and real object features. <ref type="figure" target="#fig_5">Figure 6</ref> presents that cosine similarity between fabricated and real object features gradually goes down to stability in step-wise training. This demonstrates the end-to-end optimization with shared HOI classifier helps fabricate efficient and similar objects during optimization process. More analysis of generated object representations by t-SNE is provided in Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduce a Fabricated Compostional Learning approach to compose samples for open long-tailed HOI Detection. Specifically, we design an object fabricator to fabricate object features, and then stitch the fake object features and real verb features to compose HOI samples. Meanwhile, we utilize an auxiliary verb regularization loss to regularize the verb feature for improving Human-Object Interaction generalization. Extensive experiments illustrate the efficiency of FCL on the largest HOI detection benchmarks, particularly for low-shot and zero-shot detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview of Appendixes</head><p>In this supplementary file, we provide additional details of the proposed method in Section B. Section C demonstrates more quantitative analysis (e.g. Object identity embedding, VRD, Semantic verb regularization, comparison of detector, additional ablation study and so on). In the last section, we illustrate the qualitative results (e.g. analysis of fabricated features). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Details of the Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Factorized model</head><p>We implement the factorized model under our framework. In details, we replace the HOI branch in <ref type="figure">Figure 3</ref> in the paper with verb and object stream. The two streams predict the verb and object respectively. During inference, we merge the score of verb and object to obtain HOI score as follows,</p><formula xml:id="formula_7">S hoi = (S o A o ) + (S v A v ),<label>(7)</label></formula><p>where A v (A o ) is the co-occurrence matrix between verbs (objects) and HOIs, S o is the score from object stream and S v is the score from verb stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. The Effect of Objects on HOI Detection</head><p>In the nature, different types of objects form a long-tail distribution. Then, all those actions that people perform on those objects are inevitably long-tailed. As a result, those HOIs that we observed are long-tailed. This motivates us to fabricate balanced objects for composing HOI samples with visual verbs. We have demonstrated the long-tailed distribution of objects in <ref type="figure">Figure 2</ref> in the paper and the effect of different object detector on HOI detection in <ref type="table">Table 7</ref> in paper. We further illustrate HOI detection has roughly similar performance to object detection among most object categories in <ref type="figure" target="#fig_9">Figure 8</ref>, which also illustrates the importance of object detector for HOI detection at the same time. Meanwhile, it is necessary to balance the the distribution of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. The Number of Primitives in two Zero-Shot Setting</head><p>We have count the number of unseen HOI primitives (i.e. verb and object) in the remaining data of two zero-shot setting. Unseen HOIs of rare first zero-shot has 40 verbs, 5 of which have less than 10 instances in the remaining data, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Fusion of HOI prediction and Generic Object Detector</head><p>In our experiment, we directly predict 600 HOI classes in HICO-DET. The predictions of HOI (verb-object pair) also contain object information. We think the object information in HOI prediction and the generic object detector might be complementary. Thus, we convert HOI scores S hoi to object scores and fuse it with s o as follow,</p><formula xml:id="formula_8">s o = ? 1 (S sp ? S hoi )A T o B + ? 2 s o ,<label>(8)</label></formula><p>Where ? 1 and ? 2 are 0.3 and 0.7 respectively, B ? R No and B i = C j=0 A oi,j . Then, we use the new object scor? s o in Equation <ref type="bibr" target="#b5">6</ref>. Meanwhile, we can also update the object category according to? o . <ref type="table" target="#tab_6">Table 8</ref> shows we can improve the result a bit under VCL detector which provides all scores for each object category. Noticeably, our baseline under VCL detector also uses this strategy and we do not use this in zero-shot settings. For the DRG object detector, we also do not use this strategy. To some extent, this slightly shows HOI prediction and object detection can be mutually promoted, and provides some insights for our future work although this strategy is not much useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Quantitative analysis C.1. Object Identity</head><p>In <ref type="table">Table 9</ref>, we compare three kinds of object identity. The object variables are identified after we fine-tune the fabricator in the first step. Meanwhile, in the end-to-end optimization, the object variables can maintain object semantic information. We find word embedding <ref type="bibr" target="#b41">[42]</ref> and object variables achieve similar performance ( 24.78% vs 24.68%), while the performance of one-hot representation is a bit worse. Particularly, the HOI model is initialized with a pretrained object detector model. Thus, one-step optimization can also optimize the Fabricator according to the   pre-trained backbone. In the main paper, the result of longtailed HOI detection is the model using word embedding as identity embedding. For simplicity, we use randomly initialized variables as object identity embedding for other model, i.e. randomly initialize identity embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Visual Relation Detection</head><p>We also present the efficiency of FCL in Predicate Detection on Visual Relation Detection <ref type="bibr" target="#b39">[40]</ref> in <ref type="table" target="#tab_1">Table 10</ref>. Here, we combine subject, predicate and fabricated object to generate novel relation samples <ref type="bibr" target="#b62">[63]</ref>. <ref type="table" target="#tab_1">Table 10</ref> illustrates an important improvement on zero-shot predicate detection compared to the state-of-the-art approach with FCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Semantic Verb Regularization</head><p>We also experiment with semantic verb regularization similar to <ref type="bibr" target="#b61">[62]</ref> with Graph Convolutional Network and verb word embeddings graph. In details, we use the cosine distance loss to regularize the visual verb representation to be similar to the corresponding word embedding. Here, similar to <ref type="bibr" target="#b61">[62]</ref>, we equally treat same category of verbs among <ref type="table">Table 9</ref>. Illustration of the effect of different object identity in the proposed fabricator on HICO-DET dataset <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Full  <ref type="table" target="#tab_1">Table 11</ref>. Illustration of semantic regularization modules based on the ablated setting in paper. FCL Means proposed Compostional Learning. S means semantic regularize loss. V means auxiliary verb loss (verb regularization loss in paper). mantic regularization and auxiliary verb loss, the improvement is limited. This means verb regularization loss in the paper and semantic verb regularization have similar effect on the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Object Feature Regularization</head><p>visual object feature regularization. Object features are usually more discriminative. Meanwhile, we initialize our backbone with the faster-rcnn pre-trained in COCO dataset, which largely helps us to obtain discriminative object features. Thus, it is unnecessary to use auxiliary object loss to regularize object features (See <ref type="table" target="#tab_1">Table 12</ref>). Meanwhile, we find the object features is more discriminative from the t-SNE graph in <ref type="figure" target="#fig_1">Figure 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. The Effect of Union Box on FCL</head><p>We extract verb representation from the union box of human and object. In <ref type="table" target="#tab_1">Table 13</ref>, we illustrate with human box verb, FCL still effectively improves the baseline. This shows the proposed method is orthogonal to the verb representation. Noticeably, although the union box contains the object, the HOI model mainly learns the verb representation via compositional learning, and largely ignores the identity information of the object. Thus, the object in the union box do not have much effect on Fabricator. By comparing human box and union box for verb representation in <ref type="table" target="#tab_1">Table 2 in paper and Table 13</ref>, we find verb representation from union box largely improves the performance since it provides more context information for verb representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6. Additional Object Detector Analysis</head><p>We notice there is a large gap between VCL <ref type="bibr" target="#b23">[24]</ref> detector and DRG <ref type="bibr" target="#b11">[12]</ref>. VCL provides the detection result (i.e. 30.79% mAP), while we do not know the detection re- sult of DRG detector. We do not achieve the similar object detection performance to DRG <ref type="bibr" target="#b11">[12]</ref> when we fine-tune Faster R-CNN on HICO-DET training set. However, we think we can compare the two detector by the recall of HOI detection as illustrated in <ref type="table" target="#tab_1">Table 16</ref>. Recall can also be used to compare the object detection performance between onestage HOI detection and two-stage HOI detection. <ref type="table" target="#tab_1">Table 16</ref> shows FCL DRG nearly achieves similar result to FCL GT on Recall. FCL GT still requires the network to discriminate which pair of human and object boxes has interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7. Verb Analysis</head><p>The same verb might has different meanings in different HOIs. However, the verb in HOI dataset (e.g. HICO-DET) mainly represents action. Thus, the verb in HOI dataset is usually not ambiguous. Meanwhile, the deep convolutional network (e.g. Resnet) is able to fit some ambiguous and even random data <ref type="bibr" target="#b63">[64]</ref>. Therefore, we can use factorized method <ref type="bibr" target="#b61">[62]</ref> for HOI detection and the ambiguous verbs do not affect the compositional learning on HICO-DET <ref type="bibr" target="#b23">[24]</ref>, even if there are still some ambiguous verbs (e.g. hold) who can be related to multiple objects. Besides, we further demonstrates the improvement of FCL among different categories of verbs in <ref type="figure" target="#fig_11">Figure 9</ref>. We find the ambiguity does not affect the performance of those verbs in fact. For example, although the verb "hold" is related to 61 kinds of objects in HICO-DET, the correpsonding HOIs of "hold" still achieves considerable improvement.</p><p>Inspired by that people interact similar objects in a similar manner. we also design an approach to select composite HOIs according to the similarity between different object of objects, i.e. we only keep those composite HOIs whose object is in the top K neighbors of the verb's original object. The original object of the verb is the visual object paired with the verb in the HOI annotation. This helps us to filter out those ambiguous composite HOIs. Specifically, we calculate the similarity between different classes of objects by its word embedding <ref type="bibr" target="#b41">[42]</ref>. Then we can obtain the top K neighbors for each class of objects. <ref type="table" target="#tab_1">Table 14</ref> shows with more similar objects, the performance steadily improves. Particularly, there are only one verb relating to more than 40 HOIs, and 4 verbs with more than 20 HOIs in HICO-DET. When K = 1, we only keep composite HOIs whose objects have the same label to the original object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8. Orthogonality to previous methods</head><p>Orthogonal to spatial pattern. <ref type="table">Table 20</ref> illustrates that the spatial pattern strategy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b52">53]</ref> largely improves the performance, and the proposed compositional learning is orthogonal to spatial pattern.</p><p>Orthogonal to re-weighting. In our baseline, we utilize the re-weighting strategy that is used in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b23">24]</ref> to compare directly with <ref type="bibr" target="#b23">[24]</ref>. We demonstrate FCL is orthogonal to re-weighting in <ref type="table" target="#tab_1">Table 17</ref>. Without the useful re-weighting strategy, FCL still achieves similar improvement than baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.9. Complementary Analysis of fabricator</head><p>In this section, we conduct analysis of fabricator on HOI detection without unseen data (the full long-tailed HOI detection). We witness the similar trend compared to the ablation study in the paper.</p><p>Verb and Noise for fabricating objects. <ref type="table">Table C</ref>.9 demonstrates the efficiency of verb and noise. Particularly, the performance in the full HOI detection drops larger than that in zero-shot study in the paper. We think it is because the improvement on unseen category is large, while there are no unseen category in the full HOI detection. Verb Fabricator. <ref type="table">Table C</ref>.9 illustrates if we fabricate verb features to augment HOI samples, the performance apparently decreases to 23.93% in long-tailed HOI detection. This again illustrates that the verb feature is more complex and it is difficult to generate efficient verb features to facilitate HOI detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.10. Additional Ablation Study</head><p>Step-wise optimization. We also provide the comparison between step-wise optimization and one-step optimization in unseen object HOI detection in <ref type="table" target="#tab_1">Table 15</ref>.</p><p>Hyper-Parameters. We follow the hyper-parameters in <ref type="bibr" target="#b23">[24]</ref> for ? 1 and ? 2 . For ? 3 , we provide the ablated experiment in <ref type="table" target="#tab_1">Table 21</ref> based on 0.5 because we think L reg is less important than L CL .</p><p>Fine-tune the network. In the step-wise optimization, we fine-tune the whole FCL network in the last step. For a fair comparison, we also fine-tune our baseline after we train our network. <ref type="table">Table 22</ref> shows fine-tuning the network improves effectively the baseline. This is the reason why our baseline is strong. It might be because the initial learning 0.01 in our optimization is high.   <ref type="figure" target="#fig_1">Figure 10</ref>. The illustration of real object representations, fabricated object representations and joint representations extracted from longtailed HOI detection model. We select top 10 frequent object classes from HICO-DET training data. For each classes, we randomly select 100 instances. Column 1 is real object representations, Column 2 is fabricated object representations and Column 3 is the joint representations. In Column 3, diamond point means fabricated object representations. Raw a is the base t-SNE figure. In raw b, we label different verbs with different edges (color) in Raw b. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Object Representations</head><p>We analyze the real object features and fabricated object features in detail in <ref type="figure" target="#fig_1">Figure 10</ref>  <ref type="figure" target="#fig_1">Figure 11</ref>. The illustration of real object representations, fabricated object representations and joint representations extracted from unseen object zero-shot model. Column 1 is real object representations, Column 2 is fabricated object representations and Column 3 is the joint representations. In Column 3, diamond point means fabricated object representations. Raw a is the base t-SNE figure. In raw b, we point out the unseen objects with red edge. In Raw c, we label different verbs with different edges (color).  10 frequent classes in HICO-DET. 1) In <ref type="figure" target="#fig_1">Figure 10</ref> (a) and <ref type="figure" target="#fig_1">Figure 11</ref> (a), we find the fake object features of the same class are close to each other, while the features from different classes are separable although they might share the same verb. 2) <ref type="figure" target="#fig_1">Figure 10</ref> (b) and <ref type="figure" target="#fig_1">Figure 11</ref> (c) show features of different verbs slightly cluster together within each object class. We can find there are outliers in some object classes because those outliers have different verbs.</p><p>3) for unseen object ZSL, <ref type="figure" target="#fig_1">Figure 11</ref> shows all fake object features of the same class are also closer to each other. Particularly, the unseen objects (red edge in row b) are also separable from others. 4) The Column 3 in <ref type="figure" target="#fig_1">Figure 10</ref> and <ref type="figure" target="#fig_1">Figure 11</ref> illustrate fake object features are still separable from its real objects of the same class. However, there are still some fabricated features are closer to it's corresponding real features (e.g. the dark blue class in <ref type="figure" target="#fig_1">Figure 10</ref> and the jade-green class in <ref type="figure" target="#fig_1">Figure 11</ref>). We think Column 3 in the two Figures also shows a future direction for fabricating objects, i.e. generate more realistic objects. <ref type="figure" target="#fig_1">Figure 12</ref> illustrates verb features are apparently more difficult to distinguish. The verb representation is abstract and complicated. By contrast, object representations extracted from modern object detector are more discriminative. By comparing <ref type="figure" target="#fig_1">Figure 12</ref> with the Figures in VCL <ref type="bibr" target="#b23">[24]</ref>, we can find the objects of FCL are more discriminative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Primitive Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Qualitative Comparison</head><p>In <ref type="figure" target="#fig_1">Figure 13</ref>, we compare our baseline with our proposed method. Apparently, our proposed method efficiently detects rare categories, while the corresponding baseline can not. In fact, all the HOIs detected by our method in <ref type="figure" target="#fig_1">Figure 13</ref> have less than five samples in training set which is much less than the rare setting (less than 10 samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Failure cases analysis</head><p>We provide some false positive results on Rare category in <ref type="figure" target="#fig_1">Figure 14</ref>. All failure cases can be separated into four groups: blurry image, wrong verb, wrong object, wrong match. If the image is blurry or has partial occlusion, it is hard to detection the interaction right. Besides, verb is usually hard to classify. Meanwhile, small objects also cause that the network detect object wrongly (e.g. the carrot in <ref type="figure" target="#fig_1">Figure 14</ref>). Lastly, even though the network can recognize action and object correctly, it also possibly mismatches the interaction. For example, in <ref type="figure" target="#fig_1">Figure 14</ref>, the women do not interact with the banana on the corner of the table.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Open long-tailed HOI detection addresses the problem of imbalanced learning and zero-shot learning in a unified way. We propose to compose new HOIs for open long-tailed HOI detection. Specifically, the blurred HOIs, e.g., "ride bear", are composite. See more examples in supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 arXivFigure 2 .</head><label>12</label><figDesc>Illustration of distribution of the number of object box in HICO-DET dataset. The categories are sorted by the number of instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of the improvement in those improved categories between FCL and baseline on HICO-DET dataset under default setting. The graph is sorted by the frequency of category samples and the horizontal axis is the number of training samples for each category. The result is reported in mAP (%). The details of category name are provided in supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>The changing trend of cosine similarity between fabricated object features and real object features during optimization in long-tailed HOI detection in step-wise training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>B. 1 .Figure 7</head><label>17</label><figDesc>More examples of Open Long-tailed HOI Detection provides more clear illustration of open longtailed HOI detection. Open long-tailed HOI detection aims to detect head, tail and unseen classes in one integrated way from long-tailed HOI examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Open long-tailed HOI detection addresses the problem of imbalanced learning and zero-shot learning in a unified way. We propose to compose new HOIs for open long-tailed HOI detection. Specifically, the blurred HOIs, e.g., "ride bear", are composite, while the black HOIs are real.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>sandwich carrot apple fork keyboard baseball_glove parking_meter fire_hydrant handbag remote potted_plant vase oven toilet orange bear bowl mouse broccoli hair_drier refrigerator sink zebra stop_sign microwave toaster Object Detection (AP) (%) HOI (mAP) (%) Illustration of Object detection result and HOI detection result in HICO-DET dataset. Blue is Object result. Yellow is HOI result. We average HOI detection AP according to the object categories for a direct comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>eat feed hug jump kiss pet clean load lick pick_up smell stand_on walk make watch board buy drive lie_on wear cook direct exit hop_on hose operate peel straddle adjust chase park pick read sip stand_under stir swing throw block catch check control cut_with drag drink_with dry fill fly grind groom herd launch paint pour pull push race run scratch set sign spin stick stop_at tie train type_on wield</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>The improvement among the classes of verbs on HICO-DET. The verbs are sorted by the number of HOIs that the particular verb is related. The clear figure is in the directory of Compressed package.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 . 16 Figure 13 .</head><label>121613</label><figDesc>The comparison between verb features and object features. Visual Comparison between FCL and our baseline. The two models use same detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Illustration of failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of zero-shot detection results of our proposed method. UC indicates unseen composition zero-shot HOI detection. UO indicates unseen object zero-shot HOI detection. For better illustration, we choose the mean UC result of<ref type="bibr" target="#b2">[3]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="4">Type Unseen Seen Full</cell></row><row><cell>Shen et al. [48]</cell><cell>UC</cell><cell>5.62</cell><cell>-</cell><cell>6.26</cell></row><row><cell>FG [3]</cell><cell>UC</cell><cell cols="3">11.31 12.74 12.45</cell></row><row><cell>VCL [24] (rare first)</cell><cell>UC</cell><cell cols="3">10.06 24.28 21.43</cell></row><row><cell>Baseline (rare first)</cell><cell>UC</cell><cell>8.94</cell><cell cols="2">24.18 21.13</cell></row><row><cell>Factorized (rare first)</cell><cell>UC</cell><cell>7.35</cell><cell cols="2">22.19 19.22</cell></row><row><cell>FCL (rare first)</cell><cell>UC</cell><cell cols="3">13.16 24.23 22.01</cell></row><row><cell cols="2">VCL [24] (non-rare first) UC</cell><cell cols="3">16.22 18.52 18.06</cell></row><row><cell>Baseline (non-rare first)</cell><cell>UC</cell><cell cols="3">13.47 19.22 18.07</cell></row><row><cell cols="2">Factorized (non-rare first) UC</cell><cell cols="3">15.72 16.95 16.71</cell></row><row><cell>FCL (non-rare first)</cell><cell>UC</cell><cell cols="3">18.66 19.55 19.37</cell></row><row><cell>FG [3]</cell><cell>UO</cell><cell cols="3">11.22 14.36 13.84</cell></row><row><cell>Baseline</cell><cell>UO</cell><cell cols="3">12.86 20.77 19.45</cell></row><row><cell>FCL</cell><cell>UO</cell><cell cols="3">15.54 20.74 19.87</cell></row></table><note>Table 2. Comparison to the state-of-the-art approaches on HICO- DET dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>DRG 28.33 20.69 30.62 30.59 22.40 33.04 Baseline DRG 28.12 21.07 30.23 30.13 22.30 32.47 FCL DRG 29.12 23.67 30.75 31.31 25.62 33.02 (FCL + VCL) DRG 30.11 24.46 31.80 32.17 26.00 34.02</figDesc><table><row><cell>Method</cell><cell cols="5">Default Full Rare NonRare Full Rare NonRare Known Object</cell></row><row><cell>FG [3]</cell><cell>21.96 16.43</cell><cell>23.62</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IP-Net [56]</cell><cell>19.56 12.79</cell><cell cols="4">21.58 22.05 15.77 23.92</cell></row><row><cell>PPDM [36]</cell><cell>21.73 13.78</cell><cell cols="4">24.10 24.58 16.65 26.84</cell></row><row><cell>VCL [24]</cell><cell>23.63 17.21</cell><cell cols="4">25.55 25.98 19.12 28.03</cell></row><row><cell>DRG [12]</cell><cell>24.53 19.47</cell><cell cols="4">26.04 27.98 23.11 29.43</cell></row><row><cell>Baseline</cell><cell>23.35 17.08</cell><cell cols="4">25.22 25.44 18.78 27.43</cell></row><row><cell>FCL</cell><cell>24.68 20.03</cell><cell cols="4">26.07 26.80 21.61 28.35</cell></row><row><cell>FCL + VCL</cell><cell>25.27 20.57</cell><cell cols="4">26.67 27.71 22.34 28.93</cell></row><row><cell>VCL [24] VCL [24] GT</cell><cell>43.09 32.56</cell><cell>46.24</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FCL GT</cell><cell>44.26 35.46</cell><cell>46.88</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>(FCL + VCL) GT</cell><cell>45.25 36.27</cell><cell>47.94</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Illustration of proposed modules under step-wise optimization. FCL means proposed Fabricated Compositional Learning. V indicates the verb regularization loss.</figDesc><table><row><cell cols="4">FCL V Full Rare NonRare Unseen</cell></row><row><cell>-</cell><cell>-18.12 15.99</cell><cell>20.65</cell><cell>12.41</cell></row><row><cell></cell><cell>-19.08 17.47</cell><cell>20.95</cell><cell>14.90</cell></row><row><cell>-</cell><cell>18.32 16.73</cell><cell>20.82</cell><cell>12.23</cell></row><row><cell></cell><cell>19.61 18.69</cell><cell>21.13</cell><cell>15.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of fabricator under step-wise optimization. FCL within image means we compose HOIs within image. + verb fabricator is we fabricate verb and object features.</figDesc><table><row><cell>Method</cell><cell cols="4">Full Rare NonRare Unseen</cell></row><row><cell>FCL</cell><cell cols="2">19.61 18.69</cell><cell>21.13</cell><cell>15.86</cell></row><row><cell>FCL w/o noise</cell><cell cols="2">19.45 17.69</cell><cell>21.22</cell><cell>15.74</cell></row><row><cell>FCL w/o verb</cell><cell cols="2">19.20 18.02</cell><cell>21.04</cell><cell>14.71</cell></row><row><cell cols="3">FCL + verb fabricator 19.47 16.93</cell><cell>21.43</cell><cell>15.89</cell></row><row><cell cols="5">Table 5. Illustration of Fabricated Compositional Learning on V-</cell></row><row><cell cols="2">COCO based on PMFNet [53]</cell><cell></cell><cell></cell></row><row><cell cols="2">Method</cell><cell>AP role</cell><cell></cell></row><row><cell cols="3">PMFNet [53] 52.0</cell><cell></cell></row><row><cell cols="2">Baseline</cell><cell>51.85</cell><cell></cell></row><row><cell>FCL</cell><cell></cell><cell>52.35</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .Table 7</head><label>67</label><figDesc>Comparison between step-wise optimization and one step optimization. ZS is the setting in our ablation study.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">Full Rare NonRare Unseen</cell></row><row><cell cols="2">one step (long-tailed)</cell><cell>24.03 18.42</cell><cell>25.70</cell><cell>-</cell></row><row><cell cols="3">step-wise (long-tailed) 24.68 20.03</cell><cell>26.07</cell><cell>-</cell></row><row><cell cols="2">one step (ZS)</cell><cell>19.69 18.22</cell><cell>20.82</cell><cell>17.64</cell></row><row><cell cols="2">step-wise (ZS)</cell><cell>19.61 18.69</cell><cell>21.13</cell><cell>15.86</cell></row><row><cell cols="3">one step (rare first ZS) 22.01 15.55</cell><cell>24.56</cell><cell>13.16</cell></row><row><cell cols="3">step-wise (rare first ZS) 22.45 17.19</cell><cell>25.34</cell><cell>12.12</cell></row><row><cell cols="3">one step (non-rare ZS) 19.37 15.39</cell><cell>20.56</cell><cell>18.66</cell></row><row><cell cols="3">step-wise (non-rare ZS) 19.11 17.12</cell><cell>21.02</cell><cell>15.97</cell></row><row><cell>Method</cell><cell>Detector</cell><cell cols="3">Full Rare NonRare Object mAP</cell></row><row><cell>Baseline</cell><cell>COCO</cell><cell>21.24 17.44</cell><cell>22.37</cell><cell>20.82</cell></row><row><cell>FCL</cell><cell>COCO</cell><cell>21.80 18.73</cell><cell>22.71</cell><cell>20.82</cell></row><row><cell cols="3">Baseline HICO-DET 23.94 17.48</cell><cell>25.87</cell><cell>30.79</cell></row><row><cell>FCL</cell><cell cols="2">HICO-DET 24.68 20.03</cell><cell>26.07</cell><cell>30.79</cell></row><row><cell>Baseline</cell><cell>GT</cell><cell>43.63 34.23</cell><cell>46.43</cell><cell>100.00</cell></row><row><cell>FCL</cell><cell>GT</cell><cell>44.26 35.46</cell><cell>46.88</cell><cell>100.00</cell></row></table><note>. Illustration of the effect of fine-tuned detectors on FCL. The COCO detector is trained on COCO dataset provided in [59]. We fine-tune the ResNet-101 Faster R-CNN detector based on De- tectron2 [59]. Here, the baseline is our model without fabricator. The last column is object detection result on HICO-DET test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Illustration of the effect of fusing HOI predicition to object score. This experiment is based on word-embedding object identity FCL model. Method Full Rare NonRare FCL V CL w/o Fusion 24.42 19.68 25.84 FCL V CL 24.68 20.03 26.07while Unseen HOIs of non-rare first zero-shot have only 30 verbs and all have more 10 instances. We think this partly explains why Factorized method has worse result on unseen category in rare first setting. When the primitives of unseen HOI are few in the training data. Factorized method possibly achieves worse result on unseen category.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 12 .</head><label>12</label><figDesc></figDesc><table><row><cell cols="2">Illustration of auxiliary object loss on HICO-DET</cell></row><row><cell cols="2">dataset[7] based object variables identity. Here, auxiliary object</cell></row><row><cell cols="2">loss aims to regularize visual objects</cell></row><row><cell>Method</cell><cell>Full Rare NonRare</cell></row><row><cell>w/o object loss</cell><cell>24.78 20.05 26.19</cell></row><row><cell cols="2">auxiliary object loss 24.54 19.93 25.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 13 .</head><label>13</label><figDesc>Illustration of the box for verb representation on HICO-DET dataset[7]. Method Full Rare NonRare baseline(human box) 22.91 16.66 24.77 FCL (human box) 23.83 18.62 25.39Table 14. The result while filtering out the composite HOIs according to the similarity between the fake objects and original objects. #Neighbors (K) means top K neighbors according to similarity. This experiment is based on ablated setting inTable 3in paper.</figDesc><table><row><cell cols="7">When the number of neighbors is 80, we do not filter out compos-</cell></row><row><cell cols="3">ite HOIs according to similarity.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>#Neighbors (K)</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>40</cell><cell>80</cell></row><row><cell>FCL (Full)</cell><cell cols="6">18.70 19.15 19.19 19.48 19.60 19.61</cell></row><row><cell cols="7">Table 15. Comparison between step-wise optimization and one</cell></row><row><cell cols="5">step optimization in unseen object HOI detection.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="5">Full Rare NonRare Unseen</cell><cell></cell></row><row><cell cols="5">one step 19.87 15.01 22.51</cell><cell>15.54</cell><cell></cell></row><row><cell cols="5">step-wise 20.13 16.71 22.82</cell><cell>13.85</cell><cell></cell></row><row><cell cols="7">Table 16. Illustration of recall of HOI under DRG detector, VCL</cell></row><row><cell cols="2">detector and GT boxes.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Detector Full (mAP) Recall (mRec)</cell><cell></cell></row><row><cell cols="2">FCL V CL</cell><cell>24.68</cell><cell></cell><cell>62.07</cell><cell></cell><cell></cell></row><row><cell cols="2">FCL DRG</cell><cell>29.12</cell><cell></cell><cell>82.81</cell><cell></cell><cell></cell></row><row><cell>FCL GT</cell><cell></cell><cell>44.26</cell><cell></cell><cell>86.08</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 17 .</head><label>17</label><figDesc>Illustration of FCL without re-weighting on long-tailed HOI detection. FCL Full Rare NonRare -20.79 13.19 23.06 21.20 15.48 22.90</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 18 .</head><label>18</label><figDesc>Illustration of proposed modules on long-tailed HOI detection. FCL Means proposed Fabricated Compostional Learning. V means verb regularization loss. FCL V Full Rare NonRare --23.35 17.08 25.22 -23.86 18.16 25.56 -23.94 17.48 25.87 24.78 20.05 26.19</figDesc><table><row><cell cols="4">Table 19. Ablation study of fabricator. Verb fabricator means we</cell></row><row><cell cols="3">fabricate verb features.</cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell cols="2">Full Rare NonRare</cell></row><row><cell></cell><cell>FCL</cell><cell>24.78 20.05 26.19</cell><cell></cell></row><row><cell></cell><cell cols="2">FCL w/o noise 24.22 19.23 25.72</cell><cell></cell></row><row><cell></cell><cell cols="2">FCL w/o verb 24.29 18.98 25.87</cell><cell></cell></row><row><cell></cell><cell cols="2">verb fabricator 23.93 17.10 25.97</cell><cell></cell></row><row><cell cols="4">FCL SP ZS Full Rare NonRare Unseen</cell></row><row><cell>-</cell><cell>-</cell><cell>-21.07 14.11 23.15</cell><cell>-</cell></row><row><cell></cell><cell>-</cell><cell>-21.68 16.92 23.11</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>-24.78 20.05 26.19</cell><cell>-</cell></row><row><cell>-</cell><cell>-</cell><cell>15.29 14.45 17.85</cell><cell>8.27</cell></row><row><cell></cell><cell>-</cell><cell>16.82 16.57 18.17</cell><cell>12.94</cell></row><row><cell></cell><cell></cell><cell>19.61 18.69 21.13</cell><cell>15.86</cell></row></table><note>Table 20. Illustration of spatial pattern. SP means we use spatial pattern. ZS means zero-shot setting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 21 .Table 22 .</head><label>2122</label><figDesc>Illustration of ablated study on ?3 in HICO-DET based on open long-tailed HOI detection (corresponding to Table 3 in paper). Ablation study of fine-tuning the network. Method Full Rare NonRare Baseline (w/o fine-tune) 22.83 16.32 24.77 Baseline 23.35 17.08 25.22</figDesc><table><row><cell>? 3</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell></row><row><cell cols="4">FCL 19.30 19.61 19.10</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the Faster R-CNN detector implemented in detectron2<ref type="bibr" target="#b58">[59]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th symposium on operating systems design and implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label-set operations networks for multi-label few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Alfassy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6548" to="6557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sai Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition-by-components: a theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irving</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zeroshot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced datasets II</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ican: Instancecentric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gaussian affinity for max-margin class imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parts of recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitman</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The class imbalance problem: A systematic study. Intelligent data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaju</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compositional learning for human object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keizo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized zero-shot learning via over-complete distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Keshari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Vatsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalized zero-shot learning via synthesized examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gundeep</forename><surname>Vinay Kumar Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4281" to="4289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tomer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by betweenclass attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transferable interactiveness prior for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning compositional representations for few-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6372" to="6381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Discovering human interactions with novel objects via zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11652" to="11661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep contextual attention for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5694" to="5702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2251" to="2265" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5542" to="5551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On exploring undetermined relationships for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Perceiving 3d human-object spatial arrangements from a single image in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Pepose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
