<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Panoptic Segmentation with a Joint Semantic and Instance Segmentation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gijs</forename><surname>Dubbelman</surname></persName>
							<email>g.dubbelman@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Daan de Geus</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Panagiotis Meletis</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Panoptic Segmentation with a Joint Semantic and Instance Segmentation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a single network method for panoptic segmentation. This method combines the predictions from a jointly trained semantic and instance segmentation network using heuristics. Joint training is the first step towards an end-to-end panoptic segmentation network and is faster and more memory efficient than training and predicting with two networks, as done in previous work. The architecture consists of a ResNet-50 feature extractor shared by the semantic segmentation and instance segmentation branch. For instance segmentation, a Mask R-CNN type of architecture is used, while the semantic segmentation branch is augmented with a Pyramid Pooling Module. Results for this method are submitted to the COCO and Mapillary Joint Recognition Challenge 2018. Our approach achieves a PQ score of 17.6 on the Mapillary Vistas validation set and 27.2 on the COCO test-dev set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A key task in computer vision is image recognition, for which the ultimate goal is to recognize all elements in an image. At a high level these elements can be divided into two categories: things and stuff <ref type="bibr" target="#b3">[4]</ref>. Things are usually countable objects, such as vehicles, persons and furniture items. On the other hand, stuff is the set of remanining elements, usually not countable, such as sky, road and water. Within image recognition, many tasks have been introduced to identify these elements in images. Instance segmentation and semantic segmentation are two of such tasks that have become very prominent, with state-of-the-art methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref> and <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>, respectively. The first task, instance segmentation, focuses on the detection and segmentation of things. If an object is detected, a pixel mask is predicted for this object, and the output of such a method is a set of pixel masks. By design, this method does not account for all elements in an image, as it does not consider stuff classes. The second task, semantic segmentation, does consider all elements, as the aim is to make a class prediction for each pixel in an image, for both things and stuff classes. However, the semantic segmentation output does not differentiate between different instances of things classes. As a result, both methods lack the ability to fully describe the contents of an image.</p><p>To fill this gap, the task of panoptic segmentation is introduced in <ref type="bibr" target="#b7">[8]</ref>. For this task, each pixel of an image must be assigned with a class label and an instance id. For things classes, the instance id is used to distinguiscoh between different objects. On the other hand, for the stuff classes, it is not necessary -and sometimes not even possible -to distinguish between different instances. Therefore, pixels in these classes always get the same instance id. In <ref type="bibr" target="#b7">[8]</ref>, a baseline method for this task is presented, for which they take the outputs of the best scoring instance segmentation and semantic segmentation networks, and combine them using basic heuristics to generate an output in the panoptic format.</p><p>Before the task of panoptic segmentation was formally introduced, there were already some publications that focused on exactly this task. In <ref type="bibr" target="#b12">[13]</ref>, depth layering and direction predictions are used to detect different instances of objects in a semantic segmentation map. In <ref type="bibr" target="#b0">[1]</ref>, a Dynamically Instantiated Network is used to combine the outputs from an external object detector and an internal semantic segmentation network to form a panoptic-like output.</p><p>In this report, we present a single network that makes both instance segmentation and semantic segmentation predictions, using a shared feature extractor. These predictions are combined to form panoptic segmentation outputs using heuristics, augmenting those in <ref type="bibr" target="#b7">[8]</ref>. The main contribution of our approach is the fact that we apply a single network to make panoptic segmentation predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We propose a Joint Semantic and Instance Segmentation Network (JSIS-Net) for panoptic segmentation. This method consists of two main sections: 1) a Convolutional Neural Network (CNN) that jointly predicts semantic segmentation and instance segmentation outputs (Section 2.1) and 2) heuristics that are used to merge these outputs to generate panoptic segmentation predictions (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Network architecture</head><p>We propose a CNN that jointly predicts semantic segmentation and instance segmentation outputs. The base of the network is a ResNet-50 feature extractor <ref type="bibr" target="#b6">[7]</ref>, which is shared by the semantic segmentation and instance segmentation branch. This is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The semantic segmentation branch first applies a Pyramid Pooling Module to the generated feature map, as presented in <ref type="bibr" target="#b13">[14]</ref>, and uses hybrid upsampling to reshape the predictions to the size of the input image <ref type="bibr" target="#b10">[11]</ref>. This hybrid upsampling first applies a deconvolution operation and then bilinearly resizes the predictions to the dimensions of the input image. The output of this branch is a pixel map where each entry corresponds to the predicted class label for that pixel in the input image.</p><p>The instance segmentation branch is based on Mask R-CNN <ref type="bibr" target="#b5">[6]</ref>. First, a Region Proposal Network (RPN) is used to generate region proposals for potential objects in the image. The features corresponding to these proposals are then extracted from the feature map and subjected to the final layers of ResNet-50. Finally, these features are used to make three different parallel predictions: a classification score, bounding box coordinates, and an instance mask. After applying non-maximum suppression, the output of this branch is a set of pixel clusters with class labels predicted to correspond to the location of different objects in the image. With post-processing, these pixel clusters are transformed to form per-object normalized instance masks with the dimensions of the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Loss balancing for joint learning</head><p>To enable joint learning for this network, a single loss function is formed. This means that the various loss functions from the different network branches have to be combined and balanced. This total loss, L tot has the following form:</p><formula xml:id="formula_0">L tot = ? 1 L rpn,obj + ? 2 L rpn,reg + ? 3 L det,cls + ? 4 L det,reg + ? 5 L mask + ? 6 L seg + ? 7 R.</formula><p>Here, L rpn,obj is the softmax cross-entropy objectness loss function for the RPN, L rpn,reg is the smooth L1 regression loss function for the RPN <ref type="bibr" target="#b4">[5]</ref>, L det,cls is the softmax cross-entropy classification loss function for object detection, L det,reg is the smooth L1 regression loss function for the object bounding boxes, L mask is the sigmoid crossentropy loss on the instance masks, and L seg is the sparse softmax cross-entropy segmentation loss on the segmentation outputs. Finally, R is the L2 regularization on the model parameters. The weights ? 1 ...? n are the n tuning parameters that are used to balance the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Merging heuristics</head><p>Our network outputs semantic segmentation and instance segmentation predictions. These outputs cannot be directly combined into the panoptic segmentation output format. For panoptic segmentation, two values have to be predicted for each pixel: a class label and an instance id. There are essentially two conflicts that need to be solved before being able to generate this output: overlapping instance masks and conflicting predictions for things classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Overlapping instance masks</head><p>Because the instance segmentation prediction is essentially based on an object detector and many overlapping region proposals, there is usually overlap between different predicted instance masks. This could be solved by applying non-maximum suppression to all overlapping instance masks, but this would remove many true predictions. Instead, we chose to leverage the per-instance probability maps to resolve conflicting sections. In the case that two or more instance masks predict that a certain pixel belongs to their object, the pixel is assigned to the instance mask with the highest probability at that specific pixel. These probabilities are predicted by the instance segmentation branch for each pixel in an instance mask. As a result of this heuristic, all output pixels are assigned to only one object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Conflicting predictions for things classes</head><p>Unlike the stuff classes, which are only considered in the semantic segmentation branch, the things classes are part of the prediction of both the semantic segmentation and the instance segmentation branch. As a result, there are inevitably things prediction conflicts between the two outputs. Because the semantic segmentation output does not distinguish between different instances of objects, the two outputs cannot directly be compared. For this reason, we opt for the merging heuristics used in <ref type="bibr" target="#b7">[8]</ref>: we remove all things classes from the semantic segmentation output, and replace them with the most probable stuff class according to the semantic segmentation prediction. This leads to a segmentation map consisting of only stuff class labels. Subsequently, the instance segmentation output is used to replace stuff predictions at pixels where things are predicted. Hence, the instance segmentation output is prioritized over the semantic segmentation output. As a second heuristic, any predicted stuff class with a total pixel count below 4096 is removed from the output as well, and replaced by the next best scoring stuff class above this threshold. This is done because it is very unlikely that a stuff class consists of such a limited number of pixels, if it is present in an image.</p><p>After resolving these conflicts, all detected objects get a unique id, and the network has the desired output: a class label and an instance id for all pixels in the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation</head><p>During training, the entire network is trained jointly, using a stochastic gradient descent optimizer with a momentum of 0.9. The initial learning rate is 0.075, and the learning rate is decreased twice with a factor of 2. The time steps at which this happens depend on the dataset that is trained on. The loss and regularization weights are provided in Table 2. The network is initialized using weights pre-trained on the ImageNet dataset <ref type="bibr" target="#b2">[3]</ref>, and it is always trained on a single Titan Xp GPU. All presented results are from a single model.  The network is trained and applied to two different datasets: Mapillary Vistas <ref type="bibr" target="#b11">[12]</ref> and COCO <ref type="bibr" target="#b8">[9]</ref>. For both datasets we use slightly different hyperparameters.</p><p>Mapillary Vistas. For Mapillary Vistas, the feature extractor has input dimensions of 512 x 1024 pixels, and we use a batch size of 2. All input images are resized to these dimensions before being fed to the network. The network is trained for 19 epochs, and the learning rate is decreased after 7 and 13 epochs. This dataset consists of three splits: training, validation and testing. The network is trained on the training set and the hyperparameters are tuned by evaluating on the validation set. In this report, the performance on the validation set is presented. The performance on the test set will be known when the results of the Mapillary Vistas Panoptic Segmentation Challenge are published.</p><p>COCO. Because the COCO images are much smaller than the Mapillary Vistas images, the input dimensions of the feature extractor are decreased to 400 x 400 pixels. Again, all input images are resized to these dimensions. In this case, a batch size of 5 is used. Because the amount of training images is much larger for COCO, the network is trained for 8 epochs, and the learning rate is decreased after 4 and 7 epochs. This dataset consists of four splits: training, validation, test-dev and test-challenge. The network is trained on the training set and the hyperparameters are tuned by evaluating on the validation set. In this report, the performance on both the validation and the test-dev set is presented. The performance on the test-challenge set will be known when the results of the COCO Panoptic Segmentation Challenge are published.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The results on the aformentioned datasets have been submitted to the COCO and Mapillary Joint Recognition Challenge 2018. At the time of publication, the results on the challenge test sets for both datasets have not yet been announced.</p><p>To enable proper evaluation of panoptic segmentation methods, a new metric is introduced as a main evaluation criterium for the challenge, called Panoptic Quality (PQ) <ref type="bibr" target="#b7">[8]</ref>. This metric is designed to assess both the segmentation and recognition performance of the different methods, and it can be divided into the Segmentation Quality (SQ) and the Recognition Quality (RQ the two datasets are shown in <ref type="table" target="#tab_1">Table 1</ref>. Examples of model outputs can be seen in <ref type="figure" target="#fig_0">Figures 1 and 3</ref>. Especially in <ref type="figure" target="#fig_0">Figure  1</ref>, it can clearly be seen that the predictions by both methods are combined to generate predictions for every pixel in the input image, while differentiating between different things.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Joint training vs. independent training</head><p>The major difference between our method and the baseline method proposed in <ref type="bibr" target="#b7">[8]</ref> is the fact that we jointly learn the semantic segmentation and instance segmentation branch, instead of using two independently trained models. To evaluate the effectiveness of this joint approach, we compare joint training on our network with independent training of the instance segmentation and semantic segmentation branches of our network. We use the same hyperparameters for all training runs, we train on Mapillary Vistas for 17 epochs, and we evaluate on the Mapillary Vistas validation set. To generate a panoptic output, we merge the results from the independently trained networks using the same heuristics used for the joint approach.</p><p>As evaluation criteria, we use not only the PQ, but also metrics to assess the segmentation and instance segmentation performance. For semantic segmentation, we use the commonly used mean Intersection over Union (mIoU), and for instance segmentation we use the mean Average Precision at an IoU threshold of 0.5 (mAP 0.5 ). The results are provided in <ref type="table" target="#tab_3">Table 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Detecting small objects</head><p>From the results in <ref type="table" target="#tab_1">Table 1</ref>, it is clear that there is a large gap in PQ between things and stuff classes on the Mapillary Vistas dataset. When looking at the network output qualitatively, it appears that the instance segmentation branch of the network is not able to detect small and oddly-shaped objects very well.</p><p>Since we suspect the cause for this problem to be a suboptimal performing RPN, we evaluate the performance of the RPN on the two different datasets. We do this by assessing the mean recall, which is the mean of the recall values of all images in a given image set. Recall is defined as</p><formula xml:id="formula_1">recall = |T P | |T P | + |F N | ,</formula><p>where |T P | is the number of true positives and |F N | is the number of false negatives. We define a true positive as a ground-truth object bounding box that has an IoU ? 0.5 with a region proposal generated by the RPN. A false negative is a ground-truth object bounding box thas does not meet this requirement. Essentially, the recall is the fraction of object bounding boxes in the ground-truth that is covered by the region proposals generated by the RPN. mean recall COCO val 0.827 Mapillary Vistas val 0.363 The RPN mean recall for the two different datasets is given in <ref type="table" target="#tab_4">Table 4</ref>. From this, it becomes clear that the mean recall for the Mapillary Vistas dataset is much lower than for COCO. On average, only 36.3% of the objects is covered by the region proposals. As a result, it is nearly impossible for the remaining part of the instance segmentation branch to detect the majority of objects in an image. This indicates that the RPN is currently a bottleneck in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a method that is able to generate panoptic segmentation predictions by merging outputs from a Joint Semantic and Instance Segmentation Network. As is clear from Section 4.1, this joint approach has the potential to outperform independently trained networks.</p><p>Although this method works, the performance is worse than the baseline presented in <ref type="bibr" target="#b7">[8]</ref>. For this report, we have only tested the architecture with one feature extractor, <ref type="figure">Figure 3</ref>: Panoptic segmentation predictions by the network. Images are from the COCO test-dev set (first two rows) and the Mapillary Vistas test set (last three rows).</p><p>instance segmentation method and semantic segmentation method. There is no reason, however, that this architecture would not work with other individual methods. Because of this, the performance of the method is greatly dependent on the performance and complexity of the individual submethods that are used, and the resources that are required to apply these methods. The hyperparameters that we used for training could also be sub-optimal. After submitting the results to the challenge, we were able to achieve better results for PQ, but the performance is still not as desired.</p><p>In future work, we want to further explore the potential benefits of joint learning for panoptic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Predictions by the network for an image from the Mapillary Vistas validation set. Top left: original image. Top right: panoptic segmentation. Bottom left: semantic segmentation. Bottom right: instance segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The JSIS-Net architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>? 1 ? 2 ? 3 ? 4 ? 5 ? 6 ? 7 1 .</head><label>1</label><figDesc>0 1.0 1.0 0.15 0.3 1.0 5.5e-5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>The loss and regularization weights.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>). The best overall results on Dataset PQ SQ RQ PQ Th SQ Th RQ Th PQ St SQ St RQ St Mapillary Vistas val 17.6 55.9 23.5 10.0 The best overall results on the COCO and Mapillary Vistas datasets. St = stuff classes; Th = things classes.</figDesc><table><row><cell>47.6</cell><cell>14.1</cell><cell>27.5 66.9 35.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The performance of the jointly trained model compared with independently trained models, evaluated on the Mapillary Vistas validation set.</figDesc><table><row><cell>From the results it is clear that the jointly trained net-</cell></row><row><cell>work outperforms the independently trained networks in all</cell></row><row><cell>evaluated metrics. It should be noted that all experiments</cell></row><row><cell>are conducted with the same learning rate and regulariza-</cell></row><row><cell>tion weight.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The RPN mean recall of the model on COCO and Mapillary Vistas.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pixelwise Instance Segmentation with a Dynamically Instantiated Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<title level="m">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Im-ageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding pictures of objects in large collections of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Fleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Object Representation in Computer Vision II</title>
		<editor>J. Ponce, A. Zisserman, and M. Hebert</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="335" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
	<note type="report_type">Panoptic Segmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="740" to="755" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01534</idno>
		<title level="m">Path Aggregation Network for Instance Segmentation</title>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Training of Convolutional Networks on Multiple Heterogeneous Datasets for Street Scene Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05675</idno>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.05096</idno>
		<title level="m">Pixel-Level Encoding and Depth Layering for Instance-Level Semantic Labeling</title>
		<imprint>
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
