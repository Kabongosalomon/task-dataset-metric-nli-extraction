<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D 2 Conv3D: Dynamic Dilated Convolutions for Object Segmentation in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schmidt</surname></persName>
							<email>christian.schmidt4@rwth-aachen.deathar</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">D 2 Conv3D: Dynamic Dilated Convolutions for Object Segmentation in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite receiving significant attention from the research community, the task of segmenting and tracking objects in monocular videos still has much room for improvement. Existing works have simultaneously justified the efficacy of dilated and deformable convolutions for various image-level segmentation tasks. This gives reason to believe that 3D extensions of such convolutions should also yield performance improvements for video-level segmentation tasks. However, this aspect has not yet been explored thoroughly in existing literature. In this paper, we propose Dynamic Dilated Convolutions (D 2 Conv3D): a novel type of convolution which draws inspiration from dilated and deformable convolutions and extends them to the 3D (spatio-temporal) domain. We experimentally show that D 2 Conv3D can be used to improve the performance of multiple 3D CNN architectures across multiple video segmentation related benchmarks by simply employing D 2 Conv3D as a drop-in replacement for standard convolutions. We further show that D 2 Conv3D out-performs trivial extensions of existing dilated and deformable convolutions to 3D. Lastly, we set a new state-ofthe-art on the DAVIS 2016 Unsupervised Video Object Segmentation benchmark. Code is made publicly available at https://github.com/Schmiddo/d2conv3d.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of segmenting objects from monocular video sequences has received significant attention from the research community in recent years, mainly because of useful applications in self-driven cars, autonomous robots, etc. Several existing approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b3">4]</ref> for this task follow a two-step paradigm where objects are first segmented in individual image frames, followed by a second temporal association step. Such methods leveraged the availability of accurate image-level instance segmentation networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref> and various cues for temporal association (e.g. attention, optical flow, Re-ID) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b32">33]</ref>. More recently however, methods have emerged <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28]</ref> which use 3D convolutions to jointly reason over spatial and temporal dimensions, resulting in improved performance for various video object segmentation related tasks.</p><p>In parallel to the aforementioned developments in the video domain, another research area in computer vision was focusing on improving the performance of image-level segmentation networks. To this end, one line of reasoning considers the limited receptive field of convolution operations as a drawback and aims to mitigate it. Even though a restricted receptive field is useful for weight sharing and imparting translation invariance, it is also a limitation for dense segmentation tasks where a wider view of the feature map can be beneficial. Chen et al. published a series of works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> that use atrous convolutions (also called dilated convolutions) for semantic segmentation in images. Dilated convolutions effectively add padded zeros between the values in the convolutional kernel, thus enlarging the receptive field without incurring computational overhead or increasing the parameter count. Chen et al. argued that the high degree of spatial downsampling usually applied in CNNs is detrimental for dense segmentation tasks. They instead maintained feature maps at a higher resolution and used dilated convolutions to capture a larger receptive field.</p><p>Another method for enhancing the receptive field of convolutions is the idea of deformable convolutions <ref type="bibr" target="#b12">[13]</ref> (DCNv1). Here, the convolutional kernel can be arbitrarily shaped depending on the input feature map, as opposed to being a regular grid as in standard or dilated convolutions. Practically, this is realized by using the input feature map to predict offsets (or deformations) to the sampling locations of the convolution operation. The underlying idea here is to enable the network to dynamically adapt the kernel based on the input image. Zhu et al. <ref type="bibr" target="#b52">[53]</ref> further extended this by adding a dynamic modulation parameter which scales the kernel weight value for each sampling location (DCNv2). By simply using deformable convolutions as a drop-in replacement for standard convolutions, it was shown that the performance of a variety of network architectures could be improved for object detection and segmentation.</p><p>Keeping these developments in mind gives rise to a question: Can 3D dilated/deformable convolutions retrace the success story of their 2D counter-parts and deliver improvements for video segmentation tasks? In this paper, we show that the answer is 'yes'. To this end, we propose a novel type of convolution called D 2 Conv3D (Dynamic Dilated 3D Convolutions), which combines elements from dilated and deformable convolutions by dynamically learning a multiplicative scaling factor for the sampling locations of a convolutional kernel. Additionally, we also a predict a modulation parameter which dynamically scales the kernel weights based on the input features. We show that D 2 Conv3D outperforms trivial extensions of dilated and deformable convolutions to 3D. <ref type="figure" target="#fig_0">Fig. 1</ref> provides an illustrative comparison between: (i) standard 3D convolutions, (ii) a 3D extension of the modulated deformable convolutions proposed by Zhu et al. <ref type="bibr" target="#b52">[53]</ref>, (iii) our proposed D 2 Conv3D.</p><p>In summary, our contributions are as follows:</p><p>? We propose a novel D 2 Conv3D operator which can be used as drop-in replacements for standard convolutions in 3D CNNs to improve their performance on video segmentation tasks. ? We experimentally justify the efficacy of D 2 Conv3D by applying it to two different 3D CNN based architectures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref> and evaluating them on five different benchmarks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32]</ref>. ? We set a new state-of-the-art on the DAVIS 2016 Unsupervised challenge <ref type="bibr" target="#b28">[29]</ref> by achieving a J &amp;F score of 86.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image-level Segmentation: Dense prediction tasks such as segmentation need to predict full resolution output maps and, at the same time, use multi-scale context for effective reasoning. Existing approaches for such tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b48">49]</ref> utilize dilated convolutions for this purpose, which dilate the convolutional kernel by a fixed factor to increase the receptive field, thus mitigating the need for downsampling the image features. Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b7">[8]</ref> goes a step further by using multiple dilation rates on the same feature map to capture a multi-scale feature representation, and has been successfully used for both instance and semantic segmentation tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Although dilated convolutions and ASPP help capture objects of different sizes, the convolutional kernels have fixed geometric structures since the dilation rates are constant. Several existing works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b52">53]</ref> attempt to adapt these kernels by learning offsets or other transformation parameters from the image features. Spatial Transformer Networks (STN) <ref type="bibr" target="#b19">[20]</ref> learn deformations of the sampling grid, for a regular convolution operation, from the input feature map and warp the sampling grid based on the learnt deformation parameters. Deformable Convolutional Network (DCNv1) <ref type="bibr" target="#b12">[13]</ref> on the other hand apply learned offsets to the sampling locations of a regular convolution, thereby enhancing its capability of capturing non-rigid transformations. DCNv1 can adapt to varying object sizes and scene geometry, and is shown to be effective for image-level tasks such as object detection and segmentation <ref type="bibr" target="#b12">[13]</ref>. Nevertheless, the sampling locations learned by DCNv1 often spread beyond the region of interest, which can lead to unnecessary feature influences. To overcome this issue, Zhu et al.</p><p>introduced DCNv2 <ref type="bibr" target="#b52">[53]</ref> where, in addition to the offsets, a dynamic modulation parameter is learned which scales the kernel weights. This parameter gives the convolution kernels additional freedom to adjust the influence of the sampled regions. A teacher network based on R-CNN <ref type="bibr" target="#b16">[17]</ref> is then used to train this modulation mechanism, where the teacher provides additional guidance to learn a more focused feature representation. D 2 Conv3D, similar to deformable convolutions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b52">53]</ref>, can be directly plugged-in to any existing architecture and improve its performance. However, unlike deformable convolutions, D 2 Conv3D works with 3D models and can be used effectively for segmentation tasks in videos. In addition, the modulation mechanism used in D 2 Conv3D does not need additional supervision from a teacher network as in DCNv2 <ref type="bibr" target="#b52">[53]</ref>.</p><p>Video Processing using 3D Convolutions: Videos can be interpreted as 3D data with the third dimension being time. To leverage temporal context effectively, video classification networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref> successfully use 3D-CNNs and show their superior performance. However, unlike segmentation tasks, these networks do not need large resolution feature maps, and hence the increase in computational overhead caused by 3D-CNNs is still manageable. Recent works in the field of Unsupervised Video Object Segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>, which target foreground-background segmentation, have also adapted 3D-CNNs to improve the segmentation performance. Hou et al. <ref type="bibr" target="#b18">[19]</ref> uses an encoderdecoder architecture based on a variant of 3D-CNNs called R2plus1D <ref type="bibr" target="#b35">[36]</ref>, and insert an ASPP after the last layer of the encoder. However, they adopt a relatively shallow network to compensate for the additional computation needed by ASPP, which in turn affects the final performance. Mahadevan et al. <ref type="bibr" target="#b27">[28]</ref> on the other hand employ a much deeper channel-separated 3D-CNN <ref type="bibr" target="#b34">[35]</ref> as backbone with much fewer parameters in combination with their novel 3D Global Convolutions and 3D Refinement modules in the decoder, and achieve state-of-the-art results. In this paper, we show that augmenting <ref type="bibr" target="#b27">[28]</ref> with D 2 Conv3D further improves the network performance even with significantly less training data.</p><p>Instance Segmentation in Videos: Multi-instance Segmentation in Videos has recently emerged as a popular field due to its applicability in autonomous driving and robotics. Some of the popular tasks in this domain are Video Object Segmentation (VOS) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>, Video Instance Segmentation (VIS) <ref type="bibr" target="#b44">[45]</ref>, and the more recent Occluded Video Instance Segmentation (OVIS) <ref type="bibr" target="#b31">[32]</ref>. Here the primary goal is to segment all object instances in a video and associate them over time. For VIS and OVIS, there is an additional task of classifying the predicted tracks into one of the predefined object categories. Multi Object Tracking and Segmentation (MOTS) <ref type="bibr" target="#b39">[40]</ref> is another similar task that focuses on autonomous driving scenes and requires segmenting and tracking cars and pedestrians in these scenarios.</p><p>Popular methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b26">27]</ref> that tackle the aforementioned tasks typically first generate image-level instance proposals, and then associate them either by using multiple cues such as optical flow and re-id, or by learning some kind of pixel affinity based on attention mechanism <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b26">27]</ref>. Most of these methods employ 2D dilated convolutions in the backbone, and process each frame separately, thereby not effectively making use of the larger temporal context. Bertasius et al. recently proposed MaskProp <ref type="bibr" target="#b3">[4]</ref>, which modifies Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> with a mask propagation branch to adapt it to videos The features from the middle frame of an input video clip are aligned with the remaining frames using learnt spatial offsets similar to <ref type="bibr" target="#b4">[5]</ref>. Unlike D 2 Conv3D, the mask propagation branch in MaskProp only operates on two frames, and it does not use learnt temporal dilation. STEm-Seg <ref type="bibr" target="#b0">[1]</ref> is another architecture that is relevant to our work. It processes an input video clip and generates spatio-temporal embeddings that can be directly clustered to obtain temporally consistent instance segmentation masks. STEm-Seg is a bottom-up approach, and uses a decoder comprising of entirely 3D convolutions for this purpose. In this paper, we show that by plugging in D 2 Conv3D to just the decoder of STEm-Seg further improves its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our proposed D 2 Conv3D predicts a dilation scaling factor and a modulation value for every pixel in the input feature map. Before explaining this in detail, we will first briefly recap the details of existing 2D deformable convolutions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b52">53]</ref> in Sec. 3.1:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deformable Convolutions in 2D</head><p>Let X ? R H?W denote the feature map for an image with resolution H ? W (we ignore the channel dimension for ease of notation). Let X(p) denote the value of X at coordinates p = (p y , p x ). Furthermore, let W denote the weights of a given convolutional kernel with K entries, and let S ? R K?2 denote the sampling region for the convolution. E.g. for a standard 3 ? 3 convolution, K = 9, and the sampling region S = {(?1, ?1), (?1, 0), ..., (1, 0), (1, 1)}.</p><p>In deformable convolutions <ref type="bibr" target="#b12">[13]</ref>, this sampling region is shifted by a set of offsets predicted for every point in X which we denote with ?S ? R H?W ?K?2 . Zhu et al. <ref type="bibr" target="#b52">[53]</ref> additionally also predict a set of modulation parameters M ? R H?W ?K . If we let Y denote the feature map obtained after applying the deformable convolution, then the value of Y at coordinates p 0 is calculated as follows:</p><formula xml:id="formula_0">Y(p 0 ) = pn?S M(p 0 , p n ) ? W(p n )? X(p 0 + p n +?S(p 0 , p n ))<label>(1)</label></formula><p>Here, M(p 0 , p n ) and ?S(p 0 , p n ) are used to denote the modulation value and sampling offset, respectively, predicted for point p 0 in X at sampling location p n in the kernel. Thus, deformable convolutions are able to dynamically attend to spatial locations outside of the fixed sampling region S which standard convolutions are bound to follow.</p><p>Note that Eq. 1 is a generalization of the standard convolutions operation: if ?S(p 0 , p n ) = 0 and M(p 0 , p n ) = 1, the deformable convolution reduces to a standard convolution. The modulation parameters in M are sigmoid activated and thus lie in the range [0, 1], but the offsets in ?S are unconstrained and may be fractional. Therefore, bilinear interpolation is applied to sample the input feature map <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b52">53]</ref>. Points outside the feature map are assumed to have a value of 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">D 2 Conv3D -Dynamic Dilated Convolutions</head><p>The fixed grid structure of convolutions imposes useful inductive bias for computer vision tasks due to the regular grid structure of rasterized images. However, convolutional filters cannot adapt to changes in the underlying geometry of a scene. By contrast, deformable convolutions can adapt to changes in scene geometry, but lose the inductive bias imposed by the fixed grid structure of a standard convolution kernel.</p><p>For video tasks, the size of the temporal dimension of an input spatio-temporal volume is usually orders of magnitude smaller than the spatial dimensions. This means that a trivial extension of DCN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b52">53]</ref> to the 3D (spatiotemporal) domain often results in sampling locations that lie outside the spatio-temporal volume of the feature map. D 2 Conv3D on the other hand, strikes a compromise between these two types of convolutions: it maintains the grid structure of a convolutional kernel, but allows the kernel to be dilated dynamically and independently along each of the three dimensions. Furthermore, compared to DCNv1 and DCNv2, sampling locations predicted by D 2 Conv3D are better aligned with the input feature. This is shown in <ref type="figure">Fig. 2</ref>, where it can be seen that D 2 Conv3D generates far less out of bounds sampling locations. We refer to the supplementary material for further analysis of out-of-bounds sampling behaviour.</p><p>Architecture. In the 3D spatio-temporal domain of temporal dimension T , features can be redefined as X ? R T ?H?W , the sampling region of a convolution as S ? R K?3 , and the point coordinates p = (p t , p y , p x ). E.g. a 3 ? 3 ? 3 convolution has K = 27 entries. Our proposed D 2 Conv3D is a novel type of convolution that keeps the grid structure of normal convolutions while applying learnt dilations, and can be applied to spatiotemporal features. In contrast to deformable convolutions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b52">53]</ref> that learn K additive offsets for the sampling region, we learn 3 multiplicative factors, one each for the (t, y, x) dimensions, and apply them to the coordinates in the sampling region S. D 2 Conv3D can thus be seen as dilated convolutions with dynamically learned dilation rates. We will henceforth use the term 'dilation map' to refer to the set of dilated rates predicted for X, i.e. D ? R T ?H?W ?3 .</p><p>To predict the dilation map, we input feature map X to a standard 3 ? 3 ? 3 convolution f d followed by an elu activation function <ref type="bibr" target="#b11">[12]</ref> and addition by 1:</p><formula xml:id="formula_1">D = 1 + elu(f d (X))<label>(2)</label></formula><p>This forces the values in D to lie in the range [0, ?). We found that simply applying a ReLU activation to f d (X) frequently results in zero gradients. By contrast, the formulation in Eq. 2 produces more well-behaved gradients during training, and also better results during inference (see <ref type="bibr">Sec. 4.4)</ref>.</p><p>Separately, a 3 ? 3 ? 3 convolution f m is applied to X followed by sigmoid activation to produce the modulation map M ? R T ?H?W ?K (c.f . <ref type="bibr" target="#b52">[53]</ref>). The value of the output feature map Y at point p 0 with D 2 Conv3D is then calculated as follows:</p><formula xml:id="formula_2">Y(p 0 ) = pn?S M(p 0 , p n ) ? W(p n )? X(p 0 + (p n ? D(p 0 )))<label>(3)</label></formula><p>Here, with some abuse of notation, we use p n ? D(p 0 ) to denote the multiplication of the sampling location coordinates p n with the tuple of 3 dilation rates in D at point p 0 . <ref type="figure">Fig. 3</ref> illustrates the architecture of a D 2 Conv3D block, which comprises the actual D 2 Conv3D layer, and also the two layers and activations required to produce M and D. Note that, similar to Eq. 1, D 2 Conv3D is also a generalization of the convolution operation -if the modulation parameter and dilation rates are unity, D 2 Conv3D reduces to a standard convolution. Moreover, it can also specialize to a 2D, 1D, or point-wise convolution by predicting one or more dilation rates as zero. It is therefore possible to use an D 2 Conv3D block as a drop-in replacement for 3D convolutions in existing pre-trained networks without modifying their behavior at the start of training. This can be done by simply copying the existing kernel weights to the D 2 Conv3D layer, and initializing the weights and bias parameters in f d and f m with zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Qualitative Analysis</head><p>The dilation rates and modulation values predicted by D 2 Conv3D when it is used as an intermediate layer in a video object segmentation network are illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref> and <ref type="figure" target="#fig_3">Fig. 5</ref>. The visualizations shows the mean dilation rates (second row) and mean modulation values (third row). It is evident that the network learns to use different dilation rates to distinguish the foreground object from the background. Dilation rates are highest inside the object, medium on the object borders, and zero in the background. The modulation values, on the other hand, are highest on the object boundaries. This indicates that the network mostly focuses on refining the edges of the segmentation mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on Video Saliency</head><p>To justify the efficacy of D 2 Conv3D, we apply it as a drop-in replacement for convolution layers in the 3D CNN architecture proposed by Mahadevan et al. <ref type="bibr" target="#b27">[28]</ref>. Subsequently, we conduct several ablations and also compare our results to existing state-of-the arts. The experimental evaluation is performed on the validation set of the DAVIS'16 Unsupervised Video Object Segmentation benchmark <ref type="bibr" target="#b28">[29]</ref>. The task here is to perform foreground segmentation of the 'salient' regions of the video. Here, saliency is subjectively defined as regions which undergo motion changes significant enough to capture the attention of the human eye. Note that even though the word 'unsupervised' occurs in the benchmark name, this is a fully supervised task where labeled training data is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Architecture</head><p>We build on the baseline network architecture proposed by Mahadevan et al. <ref type="bibr" target="#b27">[28]</ref> which has a compact encoderdecoder architecture composed entirely of 3D convolutions. It accepts an input video clip and outputs a foreground probability map for each pixel in the input clip. The encoder is an efficient channel-separated 3D variant of ResNet-152 <ref type="bibr" target="#b34">[35]</ref> from which feature maps are extracted at four different spatially downsampled scales (4x, 8x, 16x, 32x). The decoder is made up of three so-called Refinement modules which upsample the given input feature map and combine it with the encoder feature map at the corresponding scale. To process arbitrarily long videos, the input video is split into multiple 8-frame clips with an overlap of 3 frames between successive clips. The output probability maps for the overlapping frames are subsequently averaged to get the final pixel foreground probabilities. We refer the reader to <ref type="bibr" target="#b27">[28]</ref> for more details of the baseline.</p><p>Our Changes. We replace all convolution layers in the first two Refinement modules with D 2 Conv3D. These two modules are the ones which upsample the current feature map and combine it with the encoder features at the 16x and 8x spatially downsampled scales. We will henceforth refer to these modules as rf1 and rf2.</p><p>Moreover, we apply GroupNorm <ref type="bibr" target="#b43">[44]</ref> to the outputs of the convolution layers in order to improve the gradient flow to deeper network layers. It should be noted that D 2 Conv3D is slower than a standard convolution, so it is infeasible to apply it to high resolution feature maps (e.g. at the 4x spatial scale). We chose rf1 and rf2 as the replacement locations since this yields a good trade-off between performance and speed. We found that replacing layers in the encoder only provides a minor improvement that does not justify the increased computation/memory overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>We initialize the encoder with weights from a publicly available model which is trained for video action classification on Kinetics400 <ref type="bibr" target="#b22">[23]</ref> and Sports-1M <ref type="bibr" target="#b21">[22]</ref>. The weights of layers which predict offsets and modulation parameters are initialized to zero. All other weights in the decoder are initialized randomly. The network is trained for 20 epochs on 8-frame clips sampled randomly from the DAVIS'17 <ref type="bibr" target="#b30">[31]</ref> training set without any data augmentation. We use the Lovasz-Hinge loss <ref type="bibr" target="#b1">[2]</ref> and train the network using using the Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with a learning rate of 10 ?5 , which is reduced by a factor of 0.1 after 10 epochs. Additionally, we employ gradient clipping by limiting the L2-norm of the gradient to a maximum value of 10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations on Deformations</head><p>We report the results of our first set of ablations in Tab. 1. The most comparable baseline setting from <ref type="bibr" target="#b27">[28]</ref> which uses the same (pre-)training data as we do achieves a J &amp;F score of 82.2. For a fair comparison, we re-train this baseline with our training settings and added normalization layers. This 'Revised Baseline' achieves a J &amp;F score of 83.5.</p><p>ASPP. Here, we replace the convolution layers in rf1 and rf2 with ASPP blocks <ref type="bibr" target="#b7">[8]</ref>, which comprise multiple dilated convolutions in parallel, but with a fixed dilation rate. Doing so actually reduces the J &amp;F score from 83.5 to 82.6.</p><p>Deformable Convolutions. Next, we use 3D extensions of existing deformable convolutions in rf1 and rf2. Using DCNv1 <ref type="bibr" target="#b12">[13]</ref> improves the J &amp;F by 0.9% over the revised baseline (83.5 ? 84.4), while using DCNv2 <ref type="bibr" target="#b52">[53]</ref> provides a 1.3% improvement (83.5 ? 84.8). This clearly shows that dynamically adapting the convolution sampling locations is beneficial for video segmentation tasks.</p><formula xml:id="formula_3">D 2 Conv3D</formula><p>Variants. Finally, we replace the convolutions in rf1 and rf2 with variants of dynamic dilated convolutions. In (I), a single dilation rate is predicted for the two spatial dimensions, fixing the temporal dilation to 1. This improves the J &amp;F from 83.5 in the revised baseline to 84.2, but still lags behind DCNv1 and DCNv2. In (II) we allow different dilation rates for the two spatial dimensions, but the temporal dilation remains fixed. Doing so yields an insignificant improvement of 0.1% J &amp;F over variant (I).</p><p>In variant (III), we predict separate dilation rates for the 3 spatio-temporal dimensions, but do not predict modulation parameters. This variant achieves 85.0 J &amp;F which is 1.5% higher than the revised baseline (83.5), and 0.6% higher than DCNv1 variant (84.4) which also does not use modulation parameters. Finally, variant (IV) is the full D 2 Conv3D with modulation parameters (as explained in Sec. 3.2). This achieves a J &amp;F score of 85.5 which is 2% higher than the Activation None 1 + ReLU ReLU 1 + elu J &amp;F 83.8 83.9 85.1 85.5 <ref type="table">Table 2</ref>: Ablations on dilation map activation function.</p><p>revised baseline, and 0.7% higher than the second-best performing DCNv2 variant. The fact that variant (IV) achieves a 1.8% higher J &amp;F compared to variant (II) shows the effectiveness of the dynamic temporal dilation rate predicted by the network. In light of these results, we can see that for video segmentation tasks, D 2 Conv3D out-performs 3D extensions of existing deformable convolutions despite having fewer parameters (our dilation map D has 3 channels, whereas the offset maps for DCNv1 and DCNv2 have 81 channels for a 3 ? 3 ? 3 convolution). This justifies our earlier claim that D 2 Conv3D allow the network to adapt to geometric scene variations in the input features while retaining the useful inductive bias associated with the grid shaped structure of a convolutional kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablations on Dilation Activation Function</head><p>Tab. 2 examines the effect of using different activation functions to predict the dilation map D. Using no activation achieves a J &amp;F score of 83.8. Note that this setting allows the network to predict a negative dilation rate, which mirrors the convolution kernel along that dimension. '1 + ReLU' restricts the range of dilations to <ref type="bibr">[1, ?)</ref>. Looking at <ref type="figure" target="#fig_2">Fig. 4</ref>, we see that the network often specializes D 2 Conv3D to a point-wise convolution for background regions by predicting very small dilation rates (close to zero). Since '1 + ReLU' disallows such behavior, the J &amp;F reduces by 1.2% to 83.9 compared to the 85.1 achieved by just applying a ReLU. Finally, our chosen '1 + elu' activation (85.5 J &amp;F) out-performs ReLU (85.1 J &amp;F) by 0.4% due to improved gradient flow for small dilation rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State of the Art</head><p>Tab. 3 shows the results of our method (baseline <ref type="bibr" target="#b27">[28]</ref> with D 2 Conv3D) in comparison to existing state-of-the-art approaches on the validation set of the DAVIS'16 Unsupervised Video Object Segmentation. Our method achieves 85.5 J &amp;F using a clip overlap of 3 frames during inference. If this overlap is increased to 7 frames, the J &amp;F increases to 86.0 at the cost of slower run-time.</p><p>Our method out-performs the current state-of-the-art RT-Net <ref type="bibr" target="#b32">[33]</ref> (85.2 J &amp;F) by 0.8% even though RTNet uses CRF post-processing and optical flow as an external cue. In fact, almost all other works either train on significantly more data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>, or use additional performance improvement cues such as optical flow <ref type="bibr" target="#b32">[33]</ref>, CRF postprocessing <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref> or other heuristics <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b50">51]</ref>. By contrast, we use only the DAVIS'17 dataset for training and use no other </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>J &amp;F J -mean F-mean  <ref type="table">Table 3</ref>: Quantitative results on the DAVIS'16 unsupervised validation set. ? Optical flow ? CRF post-processing * Multi-scale inference * * large-scale pretraining ? heuristic post-processing external cues, augmentations or post-processing techniques. Also note that our F score (86.5) is significantly higher than the second-highest (84.7), indicating that D 2 Conv3D can produce highly accurate object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Multi-Instance Segmentation in Video</head><p>To show the generalization capability of D 2 Conv3D, we evaluate on four other benchmarks involving multi-instance segmentation in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Network Architecture</head><p>We extend the network architecture of STEm-Seg <ref type="bibr" target="#b0">[1]</ref>, which is a single-stage approach that segments multiple object instances by clustering per-pixel embeddings in a given input video clip. STEm-Seg also has an encoder-decoder architecture, but unlike <ref type="bibr" target="#b27">[28]</ref>, it only has 3D convolutions in the decoder. For our experiments, we replace the convolution layers in the two 'deepest' blocks of the decoder (which process the 32x and 16x down-sampled feature maps) with D 2 Conv3D. Furthermore, in order to reduce training time, we use a lighter ResNet-50 encoder backbone compared to the ResNet-101 used in the original paper <ref type="bibr" target="#b0">[1]</ref>. All other details, including the training schedule, are kept identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Benchmarks</head><p>We compare the performance of STEm-Seg enhanced with D 2 Conv3D on four popular, challenging benchmarks for multi-instance segmentation in videos. These are briefly described below: KITTI-MOTS. KITTI-MOTS <ref type="bibr" target="#b39">[40]</ref> is an extension of the popular KITTI dataset for Multi-Object Tracking (MOT) <ref type="bibr" target="#b15">[16]</ref> which requires pixel-precise object masks as opposed to bounding boxes, hence the name MOTS (Multi-Object Tracking and Segmentation). It contains 21 lengthy videos captured from a moving vehicle wherein the task is to segment and track all car and pedestrian object instances. Performance is primarily assessed using the 'sMOTSA' measure <ref type="bibr" target="#b39">[40]</ref>, which is an extension of the CLEAR MOT metrics <ref type="bibr" target="#b2">[3]</ref> to account for pixel-precise segmentation masks. DAVIS'19 Unsupervised. The DAVIS 2019 Unsupervised Video Object Segmentation benchmark <ref type="bibr" target="#b5">[6]</ref> requires all salient objects in the video to be segmented and tracked over time. The training and validation sets comprise 60 and 30 videos, respectively. Similar to the DAVIS'16 Unuspervised benchmark, the evaluation metrics here are the J and F scores, which are averaged into a single J &amp;F metric. For this benchmark however, the J &amp;F is computed separately for each object instance. YouTube-VIS. The YouTube Video Instance Segmentation dataset <ref type="bibr" target="#b44">[45]</ref> consists of 2,883 videos with a total of more than 130k object instances. Here, in addition to segmenting and tracking object instances over time, a class label (from one of 40 known classes) also has to be assigned to each predicted instance. The evaluation measure is mean Average Precision (mAP).    OVIS. Occluded Video Instance Segmentation <ref type="bibr" target="#b31">[32]</ref> comprises 5,233 videos with labeled masks for 25 known object classes. The dataset is similar to YouTube-VIS in that it also uses mean Average Precision (mAP) as the evaluation measure, but is more challenging since it comprises longer videos where objects undergo significant occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results</head><p>We compare the results of STEm-Seg with D 2 Conv3D against the original baseline <ref type="bibr" target="#b0">[1]</ref>, and also against the case where 3D extensions of DCNv1 <ref type="bibr" target="#b12">[13]</ref> and DCNv2 <ref type="bibr" target="#b52">[53]</ref> are used instead of D 2 Conv3D. On all four benchmarks, using D 2 Conv3D in the decoder improves the results over the baseline, whereas DCNv1 and DCNv2 perform inconsistently, sometimes even degrading performance.</p><p>On the MOTS task (Tab. 4), using D 2 Conv3D leads to a significant performance increase. For the car class, DCNv1 and DCNv2 improve the sMOTSA score over the baseline by 2.1 and 0.9, respectively, whereas D 2 Conv3D yields a more profound improvement of 3.1. For the pedestrian class, D 2 Conv3D improves the sMOTSA score over the baseline by 4.1 <ref type="bibr">(38.5 ? 42.6)</ref>, whereas DCNv1 actually reduces the sMOTSA by 3.1 <ref type="bibr">(38.5 ? 35.4)</ref> and DCNv2 yields only a minor 0.2 improvement.</p><p>On DAVIS'19 (Tab. 5), D 2 Conv3D improves the J &amp;F to 64.6, which is 1.2% higher than the baseline (63.4). The 3D extension of DCNv2 achieves similar performance, while DCNv1 performs slightly worse than the baseline. Both D 2 Conv3D and DCNv2 significantly improve the F measure, which indicates that modulating kernel weights improves the networks ability to accurately predict the contours of object instances.</p><p>On YoutubeVIS and OVIS (Tab. 6), D 2 Conv3D improves over the baseline by 1.7% and 0.9% mAP, respectively. This indicates that D 2 Conv3D improves segmentation quality for complex scenes with several occluded objects. DCNv1 improves result on YouTube-VIS by 1.1% mAP and achieves an even greater improvement on OVIS, where it boosts the mAP from 14.3 to 15.9, outperforming D 2 Conv3D by 0.7%. On the other hand, DCNv2 improves over the baseline on OVIS, but performs worse on Youtube-VIS. We note that our training schedule for OVIS may be sub-optimal since OVIS was not evaluated in the original paper <ref type="bibr" target="#b0">[1]</ref> and we simply used the same training setup and hyper-parameters as for YouTube-VIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we presented D 2 Conv3D, a new type of dynamic 3D convolution, and justified its efficacy by applying it to two different network architectures and five different video segmentation tasks. Furthermore, we showed that D 2 Conv3D out-performs 3D extensions of existing deformable convolutions because it experiences fewer out-ofbounds sampling locations and preserves the useful inductive bias associated with rectangular convolutional kernels. Future work could explore shifting the kernel center position, and additional types of kernel shape deformation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. Modulation Map Visualization</head><p>In <ref type="figure">Fig. S2</ref>, we visualize a full volume of predicted modulation values for each of the convolutional layers in the refinement modules when D 2 Conv3D is applied to <ref type="bibr" target="#b27">[28]</ref>. It is visible that every channel reacts to different parts of the foreground or background. Kernel points that potentially sample from neighbouring frames receive higher modulation values on the object boundaries or on the background. Kernel points that sample the current frame, however, have low modulation values in the background and larger modulation values on the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. Out-of-bounds Sampling Behaviour</head><p>As mentioned in Sec. 3 of the main paper, we perform a detailed comparison of the percentage of sampling locations that are sampled outside the input feature volume, per convolutional layer, in <ref type="figure" target="#fig_0">Fig. S1</ref>. It can be observed that D 2 Conv3D predicts fewer sampling locations beyond the input features than DCNv1 or DCNv2 in most of the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3. Runtime</head><p>Although deformable convolutions are not as heavily optimized as regular convolutions, the impact on the runtime is small because we use them only on low-resolution feature maps. Detailed runtimes can be found in Tab. S1   The methods that are grayed out do not use 3D convolutions and hence D 2 Conv3D cannot be plugged-in to them for a direct comparison. UnOVOST <ref type="bibr" target="#b53">[54]</ref> performs the best among all the methods with a J &amp;F score of 67.0%, but it uses multiple 2D networks along with heuristic-based postprocessing and hence D 2 Conv3D cannot be used here as a drop-in replacement to further push its performance. In fact, STEm-Seg <ref type="bibr" target="#b0">[1]</ref> is the only method that uses 3D convolutions to incorporate temporal context, and as seen in <ref type="table">Table.</ref> S2, D 2 Conv3D improves its performance from 63.4 to 64.6 J &amp;F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YouTube-VIS:</head><p>We provide an overview of current methods for video instance segmentation on YoutubeVIS <ref type="bibr" target="#b44">[45]</ref> in     <ref type="figure">Figure S2</ref>: Modulation values predicted during inference on the dance-twirl sequence in DAVIS'16. Recall that for a 3?3?3 convolution, the modulation map M ? R T ?H?W ?K has K = 27 channels for each pixel in the input feature map. Here we visualize these 27 channels by splitting them into a row of 3 image blocks, with each block having size 3 ? 3. Consider the row of image blocks for T = t 0 : here the image block under "Previous" corresponds to the modulation values predicted for those kernel weights which will be applied to the video features in the previous timestep (T = t 0 ? 1). Likewise, "Curent" and "Next" show the modulation values for the kernel weights which will be applied to the video features from the current (T = t 0 ) and next (T = t 0 + 1) timesteps, respectively. The modulation map M is shown here for a total of 4 time-steps (t 0 , ..., t 0 + 3); thus, there are four sets of image blocks along the vertical dimension.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of regular convolutions (left), modulated deformable convolutions [53] (middle), and D 2 Conv3D (right). Note that D 2 Conv3D predicts a distinct spatiotemporal dilation for every point in the volume. Different colors indicate different modulation values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Average percentage of sampling locations outside the input volume during inference on DAVIS'16. Architecture of the D 2 Conv3D block. f d and f m are the names of the convolution layers which produce D and M, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on DAVIS'16<ref type="bibr" target="#b28">[29]</ref>. Depicted are results from our model (top row), mean predicted dilation factors (middle row), and mean predicted modulation values. Lighter pixels denote higher values. Dilation factors are highest on the object, while modulation values are highest on the object boundary. Left sequence is dance-twirl, right sequence is horsejump-high.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on OVIS<ref type="bibr" target="#b31">[32]</ref>. Depicted are results from our model (top row), mean predicted dilation factors (middle row), and mean predicted modulation values. Left side: embedding decoder. Right side: semantic segmentation decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>D 2 Figure S1 :</head><label>2S1</label><figDesc>Conv3D: Dynamic Dilated Convolutions for Object Segmentation in Videos Supplementary Material Percentage of out-of-bounds sampling locations, per layer. Measured during inference on DAVIS'16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>VariantMod. J &amp;F FPS Mem. (GB)</figDesc><table><row><cell>Baseline [28]</cell><cell>-</cell><cell>82.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Revised Baseline</cell><cell>-</cell><cell>83.5</cell><cell>4.96</cell><cell>1.92</cell></row><row><cell>ASPP</cell><cell>-</cell><cell>82.6</cell><cell>4.50</cell><cell>1.96</cell></row><row><cell>DCNv1 [13]</cell><cell></cell><cell>84.4</cell><cell>4.41</cell><cell>1.95</cell></row><row><cell>DCNv2 [53]</cell><cell></cell><cell>84.8</cell><cell>4.36</cell><cell>1.96</cell></row><row><cell>(I) S (|D| = 1)</cell><cell></cell><cell>84.2</cell><cell>4.49</cell><cell>1.94</cell></row><row><cell>(II) S (|D| = 2)</cell><cell></cell><cell>84.3</cell><cell>4.46</cell><cell>1.94</cell></row><row><cell>(III) S+T (|D| = 3)</cell><cell></cell><cell>85.0</cell><cell>4.46</cell><cell>1.93</cell></row><row><cell>(IV) S+T (|D| = 3)</cell><cell></cell><cell>85.5</cell><cell>4.46</cell><cell>1.94</cell></row><row><cell cols="5">Table 1: Ablation on offset modality. Runtime and</cell></row><row><cell cols="5">memory were measured during inference on an NVIDIA</cell></row><row><cell cols="5">GTX1080ti. Mod: Modulation, S: spatial, S+T: spatio-</cell></row><row><cell>temporal</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance improvements on KITTI MOTS. Baseline is STEm-Seg [1] with a ResNet50 backbone.</figDesc><table><row><cell>Conv Type</cell><cell cols="3">J &amp;F J -mean F-mean</cell></row><row><cell>Baseline</cell><cell>63.4</cell><cell>60.3</cell><cell>66.5</cell></row><row><cell>DCNv1 [13]</cell><cell>63.2</cell><cell>59.7</cell><cell>66.6</cell></row><row><cell>DCNv2 [53]</cell><cell>64.6</cell><cell>61.0</cell><cell>68.2</cell></row><row><cell>D 2 Conv3D</cell><cell>64.6</cell><cell>60.8</cell><cell>68.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Dataset Conv Type</cell><cell cols="5">mAP AP50 AP75 AR1 AR10</cell></row><row><cell></cell><cell>Baseline</cell><cell>30.6</cell><cell>50.7</cell><cell>33.5</cell><cell>31.6</cell><cell>37.1</cell></row><row><cell>YVIS</cell><cell cols="2">DCNv1 [13] 31.7 DCNv2 [53] 29.4</cell><cell>50.8 48.1</cell><cell>34.0 31.8</cell><cell>31.9 30.4</cell><cell>37.8 36.1</cell></row><row><cell></cell><cell>D 2 Conv3D</cell><cell>32.3</cell><cell>51.3</cell><cell>34.7</cell><cell>32.2</cell><cell>38.1</cell></row><row><cell></cell><cell>Baseline</cell><cell>14.3</cell><cell>31.5</cell><cell>12.4</cell><cell>10.2</cell><cell>20.7</cell></row><row><cell>OVIS</cell><cell cols="2">DCNv1 [13] 15.9 DCNv2 [53] 14.9</cell><cell>34.0 31.6</cell><cell>13.2 13.8</cell><cell>10.8 10.5</cell><cell>22.4 21.5</cell></row><row><cell></cell><cell>D 2 Conv3D</cell><cell>15.2</cell><cell>33.8</cell><cell>13.7</cell><cell>10.6</cell><cell>22.2</cell></row></table><note>Performance improvements on DAVIS'19. Base- line is STEm-Seg [1] with a ResNet50 backbone.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Performance improvements on YoutubeVIS</cell></row><row><cell>(YVIS) and OVIS. Baseline is STEm-Seg [1] with a</cell></row><row><cell>ResNet50 backbone.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S1 :</head><label>S1</label><figDesc>Runtimes during inference on DAVIS'16. Measured on an Nvidia GTX-1080Ti.</figDesc><table><row><cell>[11]</cell><cell>59.9</cell><cell>-</cell><cell>-</cell></row><row><cell>UnOVOST  *  [54]</cell><cell>67.0</cell><cell>67.0</cell><cell>68.4</cell></row><row><cell>RVOS [38]</cell><cell>41.2</cell><cell>36.8</cell><cell>45.7</cell></row><row><cell>AGNN [41]</cell><cell>61.1</cell><cell>58.9</cell><cell>63.2</cell></row><row><cell>STEm-Seg [1]</cell><cell>63.4</cell><cell>60.3</cell><cell>66.5</cell></row><row><cell>STEm-Seg +D 2 Conv3D</cell><cell>64.6</cell><cell>60.8</cell><cell>68.5</cell></row></table><note>? Not including time for CRF post-processing.* runtime reported on an Nvidia RTX-2080Ti.DAVIS 2019 Unsupervised Method J &amp;F Mean J Mean F Mean KIS*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S2 :</head><label>S2</label><figDesc>Results on the validation set of DAVIS'19 unsupervised VOS. Table. S2 reports the results of the state-ofthe-art methods on DAVIS'19 unsupervised validation set.</figDesc><table><row><cell>S4. Comparison with State-of-the-art</cell></row><row><cell>DAVIS 2019:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S3 :</head><label>S3</label><figDesc>Performance comparison on the validation set of YoutubeVIS 2019<ref type="bibr" target="#b44">[45]</ref>. Baseline is STEm-Seg [1] with a ResNet50 backbone.</figDesc><table><row><cell>Method</cell><cell cols="5">mAP AP50 AP75 AR1 AR10</cell></row><row><cell>CSipMask [32]</cell><cell>14.3</cell><cell>29.9</cell><cell>12.5</cell><cell>9.6</cell><cell>19.3</cell></row><row><cell cols="2">CMaskTrack R-CNN [32] 15.4</cell><cell>33.9</cell><cell>13.1</cell><cell>9.3</cell><cell>20.0</cell></row><row><cell>CrossVIS [47]</cell><cell>18.1</cell><cell>35.5</cell><cell>16.9</cell><cell>-</cell><cell>-</cell></row><row><cell>STEm-Seg [1]</cell><cell>14.3</cell><cell>31.5</cell><cell>12.4</cell><cell>10.2</cell><cell>20.7</cell></row><row><cell>STEm-Seg + D 2 Conv3D</cell><cell>15.2</cell><cell>33.8</cell><cell>13.7</cell><cell>10.6</cell><cell>22.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S4 :</head><label>S4</label><figDesc>Performance comparison on the validation set of OVIS<ref type="bibr" target="#b31">[32]</ref>.Tab. S3. Again, methods in gray do not use 3D convolutions. The best performing method, MaskProp<ref type="bibr" target="#b3">[4]</ref>, achieves an impressive score of 46.6 mAP. It extends Mask R-CNN<ref type="bibr" target="#b17">[18]</ref> with a mask propagation branch branch; there are no 3D convolutions which we can replace with D 2 Conv3D in order to boost performance further. STEm-Seg<ref type="bibr" target="#b0">[1]</ref> is the only method relying on 3D convolutions. Replacing regular convolutions with D 2 Conv3D in the decoder increases performance from 30.6 mAP to 32.3 mAP. Despite a weaker ResNet50 backbone, STEm-Seg + D 2 Conv3D is still competitive to many current architectures. KITTI-MOTS: Recently, HOTA<ref type="bibr" target="#b25">[26]</ref> has been proposed as a metric for tracking and segmentation. We provide HOTA scores for our models in Tab. S5, and compare our performance with Track R-CNN<ref type="bibr" target="#b39">[40]</ref>. Our STEm-Seg baseline performs overall better than Track R-CNN; Track R-CNN provides a better detection accuracy (DetA in Tab. S5), while STEm-Seg achieves a better association accuracy. Both methods perform comparable in terms of localization accuracy.Table S5: HOTA score on the validation set of KITTI MOTS. Baseline is STEm-Seg [1] with a ResNet50 backbone.</figDesc><table><row><cell></cell><cell></cell><cell>Car</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pedestrian</cell><cell></cell></row><row><cell>Method</cell><cell cols="8">HOTA DetA AssA LocA HOTA DetA AssA LocA</cell></row><row><cell>Track R-CNN [40]</cell><cell>72.3</cell><cell>77.4</cell><cell>67.8</cell><cell>88.3</cell><cell>42.1</cell><cell>54.9</cell><cell>32.7</cell><cell>78.6</cell></row><row><cell>STEm-Seg [1]</cell><cell>73.1</cell><cell>68.6</cell><cell>78.2</cell><cell>88.7</cell><cell>47.9</cell><cell>48.8</cell><cell>47.2</cell><cell>79.6</cell></row><row><cell>STEm-Seg + DCNv1</cell><cell>73.3</cell><cell>70.4</cell><cell>76.7</cell><cell>88.8</cell><cell>45.5</cell><cell>46.6</cell><cell>44.8</cell><cell>78.5</cell></row><row><cell>STEm-Seg + DCNv2</cell><cell>72.7</cell><cell>70.0</cell><cell>75.9</cell><cell>88.7</cell><cell>47.7</cell><cell>47.8</cell><cell>48.1</cell><cell>78.9</cell></row><row><cell>STEm-Seg + D 2 Conv3D</cell><cell>74.1</cell><cell>70.5</cell><cell>78.2</cell><cell>89.4</cell><cell>50.1</cell><cell>50.3</cell><cell>50.3</cell><cell>80.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This project was funded, in parts, by ERC Consolidator Grant DeeVise (ERC-2017-COG-773161), EU project CROWDBOT (H2020-ICT-2017-779942). Computing resources for several experiments were granted by RWTH Aachen University under project 'thes0863'. We thank Paul Voigtlaender, Istv?n S?r?ndi, Jonas Schult and Alexey Nekrasov for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatio-temporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljo?a</forename><surname>O?ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The lov?sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIVP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<title level="m">The 2019 davis challenge on vos: Unsupervised multi-object segmentation. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Rao Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Key instance selection for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungeun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungil</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Compfeat: Comprehensive feature aggregation for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An efficient 3d CNN for action/object segmentation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<title level="m">The kinetics human action video dataset. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">See more, know more: Unsupervised video object segmentation with co-attention siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Hota: A higher order metric for evaluating multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Making a case for 3d convolutions for object segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljo?a</forename><surname>O?ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hennen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Pedro O Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Occluded video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01558</idno>
		<editor>Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and Song Bai</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Guoqiang Han, and Shengfeng He. Reciprocal transformations for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxi</forename><surname>Sucheng Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zero-shot video object segmentation via attentive graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Crossover learning for fast online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05970</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Anchor diffusion for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dmm-net: Differentiable maskmatching network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sanja Fidler, and Raquel Urtasun</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning discriminative feature with crf for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Matnet: Motion-attentive transition network for zero-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking for the 2019 Unsupervised DAVIS Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Zulfikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
