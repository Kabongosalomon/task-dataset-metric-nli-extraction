<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noisy Differentiable Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
							<email>chuxiangxiang@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<email>zhangbo11@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Noisy Differentiable Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>CHU ET AL.: NOISYDARTS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Simplicity is the ultimate sophistication. Differentiable Architecture Search (DARTS) has now become one of the mainstream paradigms of neural architecture search. However, it largely suffers from the well-known performance collapse issue due to the aggregation of skip connections. It is thought to have overly benefited from the residual structure which accelerates the information flow. To weaken this impact, we propose to inject unbiased random noise to impede the flow. We name this novel approach NoisyDARTS. In effect, a network optimizer should perceive this difficulty at each training step and refrain from overshooting, especially on skip connections. In the long run, since we add no bias to the gradient in terms of expectation, it is still likely to converge to the right solution area. We also prove that the injected noise plays a role in smoothing the loss landscape, which makes the optimization easier. Our method features extreme simplicity and acts as a new strong baseline. We perform extensive experiments across various search spaces, datasets, and tasks, where we robustly achieve state-of-the-art results. Our code is available 1 .</p><p>This work was done when both authors were with Xiaomi.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Differentiable architecture search <ref type="bibr" target="#b31">[32]</ref> suffers from a well-known performance collapse issue noted by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. Namely, while the over-parameterized model is well optimized, its inferred model tends to have an excessive number of skip connections, which dramatically degrades the searching performance. Quite an amount of previous research has focused on addressing this issue <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51]</ref>. Among them, Fair DARTS <ref type="bibr" target="#b6">[7]</ref> concludes that it is due to an unfair advantage in an exclusively competitive environment. Under this perspective, earlystopping methods like <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b50">51]</ref> or greedy pruning <ref type="bibr" target="#b24">[25]</ref> can be regarded as means to prevent such unfairness from overpowering. However, the one-shot network is generally not well converged if halted too early, which gives low confidence to derive the final model.</p><p>More precisely, most of the existing approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51]</ref> addressing the fatal collapse can be categorized within the following framework: first, characterize the outcome when the collapse occurs (e.g larger Hessian eigenvalue as in RobustDARTS <ref type="bibr" target="#b50">[51]</ref> or too many skip connections in a cell <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>), and then carefully design various criteria to avoid stepping into it. There are two main drawbacks of these methods. One is that the search results heavily rely on the validity of human-designed criteria, otherwise, inaccurate criteria may reject good models (see <ref type="bibr">Section 5)</ref>. The other is that these criteria only force the searching process to stay away from a bad solution. However, the goal of neural architecture search is not just to avoid bad solutions but to robustly find much better ones.</p><p>Our contributions can be summarized in the following, ? Other than designing various criteria, we demonstrate a simple but effective approach to address the performance collapse issue in DARTS. Specifically, we inject various types of independent noise into the candidate operations to make good ones robustly win. This approach also has an effect of smoothing loss landscape. ? We prove that the required characteristics of the injected noise should be unbiased and of moderate variance. Furthermore, it is the unbiasedness that matters, not a specific noise type. Surprisingly, our well-performing models are found with rather high Hessian eigenvalues, disproving the need for the single-point Hessian norm as an indicator of the collapse <ref type="bibr" target="#b50">[51]</ref>, since it can't describe the overall curvatures of its wider neighborhood. ? Extensive experiments performed across various search spaces (including the more difficult ones proposed in <ref type="bibr" target="#b50">[51]</ref> and datasets (15 benchmarks in total) show that our method can address the collapse effectively. Moreover, we robustly achieve state-ofthe-art results with 3? fewer search costs than RobustDARTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Differentiable architecture search DARTS <ref type="bibr" target="#b31">[32]</ref> has widely disseminated the paradigm of solving architecture search with gradient descent <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47]</ref>. It constructs an overparameterized supernet incorporating all the choice operations. Each discrete choice is assigned with a continuous architectural weight ? to denote its relative importance, and the outputs of all the paralleling choices are summed up using a softmax function ? . Through iterative optimization of supernet parameters and architectural ones, competitive operations are supposed to stand out with the highest ? (?) to be chosen to derive the final model. Though being efficient, it is known unstable to reproduce <ref type="bibr" target="#b49">[50]</ref>. Endeavors to improve the performance collapse in DARTS Several previous works have focused on addressing the collapse. For instance, P-DARTS <ref type="bibr" target="#b4">[5]</ref> point out that DARTS gradually leans towards skip connection operations since they ease the training. However, while being parameter-free, they are essentially weak to learn visual features which lead to degenerate performance. To resolve this, they proposed to drop out paths through skip connections with a decay rate. Still, the number of skip connections in normal cells varies, for which they impose a hard-coded constraint, limiting this number to be M. Later DARTS+ <ref type="bibr" target="#b27">[28]</ref> simply early stops when there are exactly two skip connections in a cell. RobustDARTS <ref type="bibr" target="#b50">[51]</ref> discovers degenerate models (where skip connections are usually dominant) correlate with increasingly large Hessian eigenvalues, for which they utilize an early stopping strategy while monitoring these values.</p><p>3 Noisy DARTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>We are motivated by two distinct and orthogonal aspects of DARTS: how to make the optimization easier and how to remove the unfair competition from candidate operations.</p><p>Smooth loss landscape helps stochastic gradient optimization (SGD) to find the solution path at early optimization stages. SGD can escape local minima to some extent <ref type="bibr" target="#b21">[22]</ref> but still have difficulty navigating chaotic loss landscapes <ref type="bibr" target="#b25">[26]</ref>. Combining it with a smoother loss function can relieve the pain for optimization which leads to better solutions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref>. Previously, RobustDARTS <ref type="bibr" target="#b50">[51]</ref> empirically finds that the collapse is highly related to the sharp curvature of the loss w.r.t ?, for which they use Hessian eigenvalues as an indicator of the collapse. However, this indirect indicator at a local minimum fails to characterize its relatively larger neighborhood, which we discuss in detail in Section 5. Therefore, we are driven to contrive a more direct and effective way to smooth the landscape.</p><p>Adding noise is a promising way to smooth the loss landscape. Random noises are used to boost adversarial generalization by <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>. A recent study by <ref type="bibr" target="#b44">[45]</ref> points out that a flatter adversarial loss landscape is closely related to better generalization. This leads us to first reformulate DARTS from the probabilistic distribution's perspective as follows,</p><formula xml:id="formula_0">? * = arg min ? L v (w, ?, z) = arg max ? E x,y?P val ;z?P(z) log P(y|x, w * , ?, z) s.t. w * = arg max w E x,y?P train ;z?P(z) log P(y|x, w, ?, z)<label>(1)</label></formula><p>where z ? ? (z). The random variable z is subject to the Dirac distribution and added to the intermediate features. For a multiplicative version, we can simply set z ? ? (z ? 1). We follow <ref type="bibr" target="#b31">[32]</ref> for the rest notations. To incorporate noise and smooth DARTS (Equation 1), we propose a direct approach by setting, z ? N(?, ? ).</p><p>We choose additive Gaussian noise for simplicity. Experiments on uniform noise are also provided in Section C.1 (supplementary). The remaining problem is where to inject the noise and how to calibrate ? and ? .</p><p>Unfairness of skip connections from fast convergence. Apart from the abovementioned perspective, we notice from prior work that skip connections are the primary subject to consider <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28]</ref>. While being summed with other operations, a skip connection builds up a residual structure as in <ref type="bibr" target="#b15">[16]</ref>. A similar form is also proposed in highway networks <ref type="bibr" target="#b39">[40]</ref>. Such a residual structure is generally helpful for training deep networks, as well as the supernet of DARTS. However, as skip connections excessively benefit from this advantage <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, it leads us to overestimate its relative importance, while others are under-evaluated. Therefore, it is appropriate to disturb the gradient flow (by injecting noise as a natural choice) right after the intermediate outputs of various candidate operations. In this way, we can regularize the gradient flow from different candidate operations and let them compete in a fair environment. We term this approach NFA, short for "Noise For All". Considering the unfair advantage is mainly from the skip connection, we can also choose to inject noises only after this operation. We call this approach OFS, short for "Only For Skip". This option is even simpler than NFA. We use OFS as the default implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Requirements for the injected noise</head><p>A basic and reasonable requirement is that, applying Equation 2 should make a close approximation to Equation 1. Since each iteration is based on backward propagation, we relax this requirement to having an unbiased gradient in terms of its expectation at each iteration.</p><p>Here we induce the requirement based on OFS for simplicity.</p><p>Design of ? We letx be the noise injected into the skip operation, and ? s be the corresponding architectural weight. The loss of a skip connection operation can be written as,</p><formula xml:id="formula_2">L = g(y), y = f (? s ) ? (x +x)<label>(3)</label></formula><p>where g(y) is the validation loss function and f (? s ) gives the softmax output for ? s . Approximately, when the noise is much smaller than the output features, we have</p><formula xml:id="formula_3">y ? f (? s ) ? x whenx x.<label>(4)</label></formula><p>In the noisy scenario, the gradient of the parameters via the skip connection operation becomes,</p><formula xml:id="formula_4">? L ? ? s = ? L ? y ? y ? ? s = ? L ? y ? f (? s ) ? ? s (x +x) .<label>(5)</label></formula><p>As random noisex brings uncertainty to the gradient update, skip connections have to overcome this difficulty in order to win over other operations. Their unfair advantage is then much weakened. However, not all types of noise are equally effective in this regard. Formally, the expectation of its gradient can be written as,</p><formula xml:id="formula_5">Ex [? ? s ] = Ex ? L ? y ? f (? s ) ? ? s (x +x) ? ? L ? y ? f (? s ) ? ? s (x + Ex [x]) .<label>(6)</label></formula><p>Supposing that ? L ? y is smooth, we can use ? L ? y to approximate the its small neighbor hood. Based on the premise stated in Equation 4, we take ? L ? y out of the expectation in Equation 6 to make an approximation. As there is still an extra E [x] in the gradient of skip connection, to keep the gradient unbiased, E [x] should be 0. It's intuitive to see the unbiased injected noise can play a role of encouraging the exploration of other operations.</p><p>Design of ? The variance ? 2 controls the magnitude of the noise, which also represents the strength to step out of local minima. Intuitively, the noise should neither be too big (overtaking) nor too small (ineffective). For simplicity, we start with Gaussian noise and other options are supposed to work as well. Notably, applying Equation 2 when ? =0 falls back to Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Stepping out of the performance collapse by noise</head><p>Based on the above analysis, we propose NoisyDARTS to step out of the performance collapse. In practice, we inject Gaussian noisex ? N (?, ? ) into skip connections to weaken the unfair advantage. Formally, the edge e i, j from node i to j in each cell operates on i-th input feature x i and its output is denoted as o i, j (x i ). The intermediate node j gathers all inputs from the incoming edges: </p><formula xml:id="formula_6">x j = ? i&lt; j o i, j (x i ). Let O = {o 0 i, j , o 1 i, j , ? ? ? , o M?1 i, j }</formula><formula xml:id="formula_7">o i, j (x) = M?1 ? k=1 f (? o k )o k (x) + f (? o skip )o skip (x +x).<label>(7)</label></formula><p>The architecture search problem remains the same as the original DARTS, which is to alternately learn ? * and network weights w * that minimize the validation loss L val (? * , w * ). To summarize, NoisyDARTS (OFS) is shown in Algorithm 1 (supplementary). The NFA version is in Algorithm 2 (supplementary).</p><p>The role of noise. The role of the injected noise is threefold. Firstly, it breaks the unfair advantage so that the final chosen skip connections indeed have substantial contribution for the standalone model. Secondly, it encourages more exploration to escape bad local minima, whose role is akin to the noise in SGLD <ref type="bibr" target="#b51">[52]</ref>. Lastly, it smooths the loss landscape w.r.t ? s (NFA is similar). If we denote validation loss as L v , this role can be explained due to the fact that our approach implicitly controls the loss landscape. Supposing that the injected noise z is small and z ? N(0, ? 2 I), the expectation of the loss over z can be approximated by</p><formula xml:id="formula_8">E z [L v (w, ? s , z)] ? E z [L v (w, ? s , 0) + ? z L v (w, ? s , 0)z + 1 2 z T ? 2 z L v (w, ? s , 0)z] = L v (w, ? s , 0)E z 1 1 1 + ? z=0 L v (w, ? s , 0)E z z + E z 1 2 z T ? 2 z L v (w, ? s , 0)z] = L v (w, ? s , 0) + ? 2 2 Tr{? 2 z L v (w, ? s , 0)} ? L v (w, ? s , 0) + ? ? 2 ? 2 s 2 Tr{? 2 ? s L v (w, ? s , 0)} where z ? N(0, ? 2 I), ? = E 1 o skip (x) T o skip (x)</formula><p>, I is unit matrix.</p><p>Its role can be better understood via the visualization in <ref type="figure" target="#fig_2">Figure 2</ref>, where DARTS obtains a sharp landscape with oval contours and ours has round ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Search spaces and 15 benchmarks</head><p>To verify the validity of our method, we adopt several search spaces: the DARTS search space from <ref type="bibr" target="#b31">[32]</ref>, MobileNetV2's search space as in <ref type="bibr" target="#b0">[1]</ref>, four harder spaces (from S 1 to S 4 ) from <ref type="bibr" target="#b50">[51]</ref>. We use NAS-Bench-201 <ref type="bibr" target="#b13">[14]</ref> to benchmark our methods.</p><p>DARTS's search space (Benchmark 1) It consists of a stack of duplicate normal cells and reduction cells, which are represented by a DAG of 4 intermediate nodes. Between every two nodes there are several candidate operations (max pooling, average pooling, skip connection, separable convolution 3?3 and 5?5, dilation convolution 3?3 and 5?5).</p><p>MobileNetV2's search space (Benchmark 2) It is the same as that in ProxylessNAS <ref type="bibr" target="#b0">[1]</ref>. We search proxylessly on ImageNet in this space. It uses the standard MobileNetV2's backbone architecture <ref type="bibr" target="#b38">[39]</ref>, which comprises 19 layers and each contains 7 choices: inverted bottleneck blocks denoted as Ex_Ky (expansion rate x ? {3, 6}, kernel size y ? {3, 5, 7}) and a skip connection. The stem, the first bottleneck block and the tail is kept unchanged, see <ref type="figure">Figure 6</ref> (supplementary) for reference.</p><p>S 1 -S 4 (Benchmark 3-14) These are reduced search spaces introduced by RobustDARTS <ref type="bibr" target="#b50">[51]</ref>. S 1 is a preoptimized search space with two operation per edge, see <ref type="bibr" target="#b50">[51]</ref> for the detail. For each edge in the DAG, S 2 has only {3 ? 3 SepConv, SkipConnect}, S 3 has {3 ? 3 SepConv, SkipConnect, Zero (None)}, and S 4 has {3 ? 3 SepConv, Noise}. We search on three datasets for each search space, which makes 12 benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Params ?+  <ref type="table">Table 1</ref>: Results on CIFAR-10 (left) and ImageNet (right). NoisyDARTS-a and b are the models searched on CIFAR-10 when ? = 0.2 and ? = 0.1 respectively ( <ref type="figure" target="#fig_1">Figure 11</ref> and 12 in the supplementary). NoisyDARTS-A ( <ref type="figure">Figure 6</ref> in the supplementary) is searched on ImageNet in the MobileNetV2-like search space as in <ref type="bibr" target="#b43">[44]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS-Bench</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>We use a set of standard image classification datasets CIFAR-10, CIFAR-100 <ref type="bibr" target="#b22">[23]</ref>, SVHN <ref type="bibr" target="#b34">[35]</ref> and ImageNet <ref type="bibr" target="#b10">[11]</ref> for both searching and training. We also search for GCNs on ModelNet <ref type="bibr" target="#b45">[46]</ref> as in <ref type="bibr" target="#b24">[25]</ref> (see Section C.1 in the supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Searching Results</head><p>Searching on CIFAR-10. In the search phase, we use similar hyperparameters and tricks as <ref type="bibr" target="#b31">[32]</ref>. All experiments are done on a Tesla V100 with PyTorch 1.0 <ref type="bibr" target="#b35">[36]</ref>. The search phase takes about 0.4 GPU days. We only use the first-order approach for optimization since it is more efficient. The best models are selected under the noise with a zero mean and ? = 0.2. An example of the evolution of the architectural weights during the search phase is exhibited in <ref type="figure" target="#fig_6">Figure 5</ref> (see Section C in the supplementary). For training a single model, we use the same strategy and data processing tricks as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>, and it takes about 16 GPU hours. The results are shown in <ref type="table">Table 1</ref>. The best NoisyDARTS model (NoisyDARTS-a) achieves a new state-of-the-art result of 97.63% with only 534M FLOPS and 3.25M parameters, whose genotypes are shown in <ref type="figure" target="#fig_1">Figure 11</ref> (see Section D in the supplementary).</p><p>Searching in the reduced search spaces of RobustDARTS. We also study the performance of our approach under reduced search spaces, compared with DARTS <ref type="bibr" target="#b31">[32]</ref>, RDARTS <ref type="bibr" target="#b50">[51]</ref> and SDARTS <ref type="bibr" target="#b2">[3]</ref>. Particularly we use OFS for S 1 , S 2 , and S 3 (from <ref type="bibr" target="#b50">[51]</ref>), where DARTS severely suffers from the collapse owing to an excessive number of skip connections. For S 4 where skip operations are not present, we apply NFA. We kept the same hyper-parameters as <ref type="bibr" target="#b50">[51]</ref> for training every single model to make a fair comparison. Since the unfair advantage is intensified in the reduced search spaces, we use stronger Gaussian noise (e.g. ? = 0.6, 0.8). As before, we don't utilize any regularization tricks. The results are given in <ref type="table" target="#tab_3">Table 2</ref> and  Searching proxylessly on ImageNet. In the search phase, we use ? = 0 and ? = 0.2 and we don't optimize the hyper-parameters regarding cost. It takes about 12 GPU days on Tesla V100 machines (more details are included in Section C.5 in the supplementary). As for training searched models, we use similar training tricks as EfficientNet <ref type="bibr" target="#b40">[41]</ref>. The evolution of dominating operations during the search is illustrated in <ref type="figure">Figure 7</ref> (supplementary). Compared with DARTS (66.4%), the injected noise in NoisyDARTS successfully eliminates the unfair advantage. Our model NoisyDARTS-A (see <ref type="figure">Figure 6</ref> in the supplementary) obtains the new state of the art results: 76.1% top-1 accuracy on ImageNet with 4.9M number of parameters. After being equipped with more tricks as in EfficientNet, such as squeeze-and-excitation <ref type="bibr" target="#b19">[20]</ref> and AutoAugment <ref type="bibr" target="#b9">[10]</ref>, it obtains 77.9% top-1 accuracy.</p><p>Searching on NAS-Bench-201. We report the results (averaged on 3 runs of searching) on NAS-bench-201 <ref type="bibr" target="#b13">[14]</ref> in <ref type="table" target="#tab_5">Table 3</ref>. Our method surpasses SETN <ref type="bibr" target="#b11">[12]</ref> with a clear margin using 3 fewer search cost. This again proves NoisyDARTS to be a robust and powerful method. Learnable and decayed ? are used for ablation purposes (see Section 4.4).</p><p>Searching GCN on ModelNet10. We follow the same setting as SGAS <ref type="bibr" target="#b24">[25]</ref> to search GCN networks on ModelNet10 <ref type="bibr" target="#b45">[46]</ref> and evaluate them on ModelNet40. Our models (see <ref type="figure" target="#fig_2">Figure 23</ref> in the supplementary) are on par with SGAS as reported in <ref type="table" target="#tab_3">Table 12</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>With vs without noise. We compare the searched models with and without noises on two commonly used search spaces in <ref type="table" target="#tab_6">Table 4</ref>. NoisyDARTS robustly escapes from the performance collapse across different search spaces and datasets. Note that without noise, the differentiable approach performs severely worse and obtains only 66.4% top-1 accuracy on the ImageNet classification task. In contrast, our simple yet effective method can find a state-of-the-art model with 76.1%.</p><p>Noise vs. Dropout Dropout <ref type="bibr" target="#b17">[18]</ref> can be regarded as a type of special noise, which is originally designed to avoid overfitting. We use a special type of Dropout: DropPath <ref type="bibr" target="#b54">[55]</ref> to act as a baseline, which is a drop-in replacement of our noise paradigm. We search on NAS-Bench-201 using different DropPath rates r drop ? {0.1, 0.2} and report the results in <ref type="table" target="#tab_8">Table 5</ref>. It appears that Dropout produces much worse results.  Zero-mean (unbiased) noise vs. biased noise. Experiments in <ref type="table" target="#tab_6">Table 4</ref> and 8 verify the necessity of the unbiased design, otherwise it brings in a deterministic bias. We can observe that the average performance of the searched models decreases while the bias increases. Eventually, it fails to overcome the collapse problem because larger biases overshoot the gradient and misguide the whole optimization process.  <ref type="bibr" target="#b50">[51]</ref> during the optimization on CIFAR-10. <ref type="bibr" target="#b50">[51]</ref> suggests that the optimization should stop early at the marked points. SDARTS <ref type="bibr" target="#b2">[3]</ref> regularizes? ? max while searching. However, without doing so, we don't see the collapse in NoisyDARTS. DARTS has an average accuracy of 96.9% while we have 97.35%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion about the Single-point Hessian eigenvalue</head><p>According to <ref type="bibr" target="#b50">[51]</ref>, the collapse is likely to occur when the maximal eigenvalue ? ? max increases rapidly (whose Hessian matrix is calculated only on a snapshot of ?, i.e. single-point), under which condition some early stopping strategy was involved to avoid the collapse. To verify their claim, we search with DARTS and NoisyDARTS across 7 seeds and plot the calculated Hessian eigenvalues per epoch in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Remarkably, both DARTS and our method show a similar trend. We continue to train the supernet whilst eigenvalues keep increasing, but we still derive mostly good models with an average accuracy of 97.35%. It's surprising to see that no obvious collapse occurs. Although the Hessian eigenvalue criterion benefits the elimination of bad models <ref type="bibr" target="#b50">[51]</ref>, it seems to mistakenly reject good ones. We also find the similar result on CIFAR-100, see <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>We observe similar results in the reduced space too (see Section C.10 in the supplementary). We think that a single-point Hessian eigenvalue indicator at a local minimum cannot represent the curvatures of its wider neighborhood. It requires the wider landscape be smoother to avoid the collapse. It is more clearly shown in <ref type="figure" target="#fig_2">Figure 2</ref>, where NoisyDARTS has a tent-like shape that eases the optimization.</p><p>Comparison with SDARTS. SDARTS <ref type="bibr" target="#b2">[3]</ref>, which performs perturbation on architectural weights to implicitly regularize the Hessian norm. However, we inject noise only into the skip connections' output features or to all candidate operations, which suppresses the unfair advantage by disturbing the overly fluent gradient flow. Moreover, our method is efficient and nearly no extra cost is required. In contrast, SDARTS-ADV needs 2? search cost than ours.</p><p>Our method differs from SDARTS <ref type="bibr" target="#b2">[3]</ref> in Hessian eigenvalue trend, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. SDARTS enjoys decreasing hessian eigenvalues while ours can have growing ones. The validation landscape of SDARTS is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. SDARTS has a rather carpet-like landscape. It seems that too flat landscape of SDARTS may not correspond to a good model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a novel approach NoisyDARTS, to robustify differentiable architecture search. By injecting a proper amount of unbiased noise into candidate operations, we successfully let the optimizer be perceptible about the disturbed gradient flow. As a result, the unfair advantage is largely attenuated, and the derived models generally enjoy improved performance. Experiments show that NoisyDARTS can work effectively and robustly, regardless   <ref type="bibr" target="#b50">[51]</ref> during the optimization on CIFAR-100 in reduced search spaces S 1 , S 3 , S 5 from <ref type="bibr" target="#b50">[51]</ref>. We observe the similar growing trend. Notwithstanding, we achieve a state-of-the-art 16.28% test error rate in S 5 . of noise types. We achieved state-of-the-art results on several datasets and search spaces with low CO 2 emissions.</p><p>While most of the current approaches addressing the fatal collapse focus on designing various criteria to avoid stepping into the failure mode, our method stands out of the existing framework and no longer put hard limits as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>. We review the whole optimization process to find out what leads to the collapse and directly control the unfair gradient flow, which is more fundamental than a stationary failure point analysis. We hope this would bring a novel insight for the NAS community to shift attention away from criteria-based algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Analysis of multiplicative noise</head><p>We set the output of skip connection with multiplicative noise is x = x ?x, wherex is sampled from a certain distribution. Similar to Section 3.2 (main text), the expectation of the gradient under multiplicative noise can be written as:</p><formula xml:id="formula_10">E ? skip = E ? L ? y ? f (? s ) ? ? s x ? ? L ? y ? f (? s ) ? ? s (x ? E [x]) .<label>(9)</label></formula><p>Again notice that taking ? L ? y out of the expectation in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Experiments and Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 More Ablation Studies</head><p>Gaussian noise vs. uniform noise According to the analysis of Section 3.2 in the main text, unbiased Gaussian noise is an appropriate choice that satisfies Equation 6. In the same vein, unbiased uniform noise should be equally useful. We compare both types of noise in terms of effectiveness in <ref type="table" target="#tab_11">Table 6</ref>. Both have improved performance while Gaussian is slightly better. This can be loosely explained. As the output feature x from each skip connection tends to be Gaussian, i.e. x ? N (? 1 , ? 2 1 ) (see <ref type="figure" target="#fig_4">Figure 4</ref>), a Gaussian noisex ? N (0, ? 2 2 ) is preferred since the additive result shares the similar statistics, i.e., x +x ? N(? 1 , ? 2 1 + ? 2 2 ).   Additive noise vs. multiplicative noise Apart from additive noise, we also blend the noise (? = 1) by multiplying it with the output x of skip connections, which is approximately effective as additive noise, see <ref type="table" target="#tab_13">Table 7</ref>. In general, we also notice that searching with the biased noise also outperforms DARTS. This could be empirically interpreted as that resolving the aggregation of skip connections is more critical, while a slight deviation during the optimization matters less.    competitive architectures (no longer suffering performance collapse), but not as good as those found by state-of-the-art methods in <ref type="table" target="#tab_5">Table 3</ref> in the main text, for instance, it has a CIFAR-10 test accuracy 88.98% vs. NoisyDARTS's 93.49%. We suggest that skip connections play an indispensable role in neural architecture search and have to be carefully dealt with as we did in NoisyDARTS.  Noise For All (NFA) vs. Only For Skip connection (OFS) Applying noise to skip connections is not an ad-hoc decision. In theory, we can interfere with the optimization by injecting the noise to any operation. The underlying philosophy is that only those operations that can work robustly against noises will win the race without unfair advantage. We compare the settings of NFA, ES (noise for all but excluding skip), OFS in <ref type="table" target="#tab_19">Table 11</ref>. This proves noise injection to skip connection is critical for a better searching performance. Note that this approach also obtains much better result than DARTS. However, it requires a bit of trial-and-error to control the ? when there are many candidate operations. Therefore, if otherwise specified, we use OFS as the default choice throughout the paper. Searching GCN Architectures on ModelNet10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Training Setting on Transferred Results on CIFAR-10</head><p>For transferred learning, we train the ImageNet-pretrained NoisyDARTS-A on CIFAR-10 for 200 epochs with a batch size of 256 and a learning rate of 0.05. We set the weight decay    <ref type="table" target="#tab_3">Table 12</ref>: 3D classification on ModelNet40. OA: overall accuracy to be 0.0, a dropout rate of 0.1 and a drop connect rate of 0.1. In addition, we also use AutoAugment as <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Training Results on CIFAR-100</head><p>We show NoisyDARTS models searched in the DARTS search space and trained on CIFAR-100 in <ref type="table" target="#tab_5">Table 13</ref>. We set the initial channel as 36 and the number of layers as 20.  <ref type="table" target="#tab_5">Table 13</ref>: Comparison of searched models on CIFAR-100. : Reported by <ref type="bibr" target="#b12">[13]</ref>, : Reported by <ref type="bibr" target="#b50">[51]</ref>, ? :Rerun their code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Training Settings on COCO Object Detection</head><p>We use the MMDetection tool box since it provides a good implementation for various detection algorithms <ref type="bibr" target="#b1">[2]</ref>. Following the same training setting as <ref type="bibr" target="#b29">[30]</ref>, all models in <ref type="table" target="#tab_18">Table 10</ref> are trained and evaluated on the COCO dataset for 12 epochs. The learning rate is initialized as 0.01 and decayed by 0.1? at epoch 8 and 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Training Settings on ImageNet</head><p>We split the original training set into two datasets with equal capacity to act as our training and validation dataset. The original validation set is treated as the test set. We use the SGD optimizer with a batch size of 768. The learning rate for the network weights is initialized as 0.045 and it decays to 0 within 30 epochs following the cosine decay strategy. Besides, we utilize Adam optimizer (? 1 = 0.5, ? 2 = 0.999) and a constant learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Detailed Settings on NAS-Bench-201</head><p>For NAS-Bench-201 experiments, we adapt the code from <ref type="bibr" target="#b13">[14]</ref>. We only use the first-order DARTS optimization. We track the running statistics for batch normalization to be the same as DARTS <ref type="bibr" target="#b31">[32]</ref>. Each setting is run 3 times to obtain the average. We use a noise of ? = 0.8 regarding this particular search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Relations to Other Work</head><p>Comparison with PNI. PNI <ref type="bibr" target="#b16">[17]</ref> uses parametric noise to boost adversarial training. In contrast, we inject the fixed noise at the output of candidate operations and smooth the loss landscape of the bi-level search to avoid collapse. Moreover, parametric noise leads to collapse on NAS-Bench-201 (see learnable ? in <ref type="table" target="#tab_5">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 Transferred Results</head><p>Transferred results on object detection. We further evaluate the transferability of our searched models on the COCO objection task <ref type="bibr" target="#b28">[29]</ref>. Particularly, we utilize a drop-in replacement for the backbone based on Retina <ref type="bibr" target="#b29">[30]</ref>. As shown in <ref type="table" target="#tab_18">Table 10</ref> (supplementary), our model obtains the best transferability than other models under the mobile settings. Detailed setting is provided in Section C.4 (supplementary).</p><p>Transferring ImageNet models to CIFAR-10. We transferred our model NoisyDARTS-A searched on ImageNet to CIFAR-10. Specifically, the transferred model NoisyDARTS-A-t achieved 98.28% top-1 accuracy with only 447M FLOPS, as shown in <ref type="table">Table 1</ref>. Training details are listed in Section C.2 (Supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.9 Evolution of NoisyDARTS architectural parameters</head><p>We plot the evolution of architectural parameters during the NoisyDARTS optimization in <ref type="figure" target="#fig_6">Figure 5</ref>. The injected noise is zero-mean Gaussian with ? = 0.2. As normal cells are the main building blocks (18 out of 20) of the network, we see that the number of skip connections is much reduced. Compared with <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b4">[5]</ref>, we don't set any hard limits for it. We also don't compute expensive Hessian eigenspectrum <ref type="bibr" target="#b50">[51]</ref> as a type of regularization for skip connections. Neither do we use Scheduled DropPath <ref type="bibr" target="#b54">[55]</ref> or fixed drop-path during the searching. It confirms that by simply disturbing the gradient flow of skip connections, the unfair advantage is much weakened so that the optimization is fairer to deliver better performing models.  <ref type="table" target="#tab_2">stem  BOTTLE_K3  MBE6_K3  MBE3_K3  MBE6_K5  MBE6_K3  MBE6_K3  MBE6_K3  MBE6_K5  MBE6_K7  MBE6_K3  MBE6_K3  MBE6_K3  MBE6_K3  MBE6_K3  MBE6_K3  MBE6_K3  MBE6_K7  MBE6_K7   224x224x3  112x112x32  112x112x16  56x56x32  28x28x40  56x56x32  28x28x40  28x28x40  28x28x40  14x14x80  14x14x80  14x14x80  14x14x86  14x14x96  14x14x96  14x14x96  14x14x96  7x7x192  7x7x192  7x7x192</ref> MBE6_K7 MBE6_K7 7x7x192 7x7x320</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONV_K1</head><p>Global_Pooling + FC 7x7x1280 <ref type="figure">Figure 6</ref>: NoisyDARTS-A searched on ImageNet. Colors represent different stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.10 More discussions about Hessian Indicator in Reduced Search Space</head><p>Specifically, when training these models from supposed early-stop points, we only obtain a lower average performance 97.02?0.21. How about other search spaces? We further evaluate the Hessian eigenvalue trajectories of our method in the reduced search space, which are shown in <ref type="figure" target="#fig_8">Figure 8</ref>. When we search with injected Gaussian noise, we still observe an obvious growth of eigenvalues in both of two spaces. However, when being trained from scratch, the models derived from the last epoch (without early-stopping or any regularization tricks) perform much better than their proposed adaptive eigenvalue regularization method DARTS-ADA <ref type="bibr" target="#b50">[51]</ref>. Compared with their best effort L2 regularization <ref type="bibr" target="#b50">[51]</ref> and another method SDARTS <ref type="bibr" target="#b2">[3]</ref> based on implicit regularization of Hessian norm, we also have better performance in S 2 and comparable performance in S 3 (see <ref type="table" target="#tab_3">Table 2</ref> in the main text). Notice we only use 3x fewer searching cost. This reassures our observation that the Hessian norm <ref type="bibr" target="#b50">[51]</ref> may not be an ideal indicator of performance collapse, because it rejects good models by mistake, as illustrated in   <ref type="bibr" target="#b50">[51]</ref>. Compared with RDARTS, the eigenvalues still have a trend of increasing. Notice that better models can be found 3? faster than RDARTS (they run four times to get the best model while we produce better ones at each single run).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 9.</head><p>H G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NoisyDARTS RobustDARTS</head><p>SmoothDARTS H ? G <ref type="figure">Figure 9</ref>: Exemplary illustration on the relation of the set of models. Set H means models found with low Hessian norms. Set G are the models with better test accuracy. Robust-DARTS's Hessian norm criterion <ref type="bibr" target="#b50">[51]</ref> tends to reject a part of good models, e.g. blue models found by NoisyDARTS that are not in H ? G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.11 More Details on Reduced RobustDARTS Experiments</head><p>Like on CIFAR-10, we repeatedly find the Hessian eigenvalues are both growing when searching with NoisyDARTS on CIFAR-100 and SVHN datasets (see <ref type="figure" target="#fig_1">Figure 10</ref>), but models derived from these searching runs still outperform or are comparable to those from regularized methods like RDARTS <ref type="bibr" target="#b50">[51]</ref> and SDARTS <ref type="bibr" target="#b2">[3]</ref> (see <ref type="table" target="#tab_3">Table 2</ref> in the main text). These results again confirm that the eigenvalues are not necessarily a good indicator for finding betterperforming models.    We plot all the best models in different configurations of searching from <ref type="figure" target="#fig_1">Figure 11</ref> to <ref type="figure" target="#fig_2">Figure 20</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Models searched on CIFAR-10 in the reduced search spaces of RDARTS</head><p>We plot them in <ref type="figure" target="#fig_1">Figure 21</ref> and <ref type="figure" target="#fig_2">Figure 22</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 GCN Models searched on ModelNet10</head><p>They are depicted in <ref type="figure" target="#fig_2">Figure 23</ref>.  <ref type="figure" target="#fig_1">Figure 16</ref>: NoisyDARTS-f cells searched on CIFAR-10 with multiplicative Gaussian noise, ? = 0, ? = 0.1.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>-201 (Benchmark 15) NAS-Bench-201 [14] is a cell based search space with known evaluations of each candidate architecture, where DARTS severely suffers from the performance collapse. It includes 15625 sub architectures in total. Specifically, it has 4 intermediate nodes and 5 candidate operations (none, skip connection, 1?1 convolution, 3?3 convolution and 3?3 average pooling).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Smoothed maximal Hessian eigenvalues? ? max</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The landscape of validation accuracy w.r.t the architectural weights on CIFAR-10 and the corresponding contours. Following<ref type="bibr" target="#b2">[3]</ref>, axis x and y are orthogonal gradient direction of validation loss w.r.t. architectural parameters ?, axis z refers to the validation accuracy. The related stand-alone model accuracies and Hessian eigenvalues are 96.96%/0.3388, 97.21%/0.1735, 97.42%/0.4495 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Smoothed maximal Hessian eigenvalues? ? max</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The Gaussian-like distribution of output features on all skip edges in the original DARTS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>1 96.92?0.36 -1.0 0.2 97.14?0.15 Gaussian -0.5 0.1 97.13?0.20 -0.5 0.2 97.07?0.12 Gaussian 0.0 0.1 97.21?0.21 0.0 0.2 97.35?0.23 Gaussian 0.5 0.1 97.02?0.21 0.5 0.2 97.16?0.15 Gaussian 1.0 0.1 96.89?0.26 1.0 0.2 96.82?0.57</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Evolution of architectural weights during the NoisyDARTS searching phase on CIFAR-10. Skip connections in normal cells are largely suppressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :S 3</head><label>73</label><figDesc>Stacked plot of dominant operations during searching on ImageNet. The inferred model of DARTS (left) obtains 66.4% accuracy on ImageNet, while NoisyDARTS (right) obtains 76.1%. , ? = 0.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Evolution of maximal Hessian eigenvalue when searching with NoisyDARTS on two reduced search spaces S 2 and S 3 proposed by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Evolution of maximal Hessian eigenvalue when searching with NoisyDARTS on CIFAR-100 and SVHN, in two reduced search spaces S 2 and S 3 proposed by<ref type="bibr" target="#b50">[51]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>NoisyDARTS-a cells searched on CIFAR-10. D NoisyDARTS architectures D.1 Models searched on CIFAR-10 in the DARTS search space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>NoisyDARTS-b cells searched on CIFAR-10 with additive Gaussian noise, ? = 0, ? = 0.1. NoisyDARTS-c cells searched on CIFAR-10 with additive uniform noise, ? = 0, ? = 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>NoisyDARTS-d cells searched on CIFAR-10 with additive uniform noise, ? = 0, ? = 0.1. NoisyDARTS-e cells searched on CIFAR-10 with multiplicative Gaussian noise, ? = 0, ? = 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 17 :</head><label>17</label><figDesc>NoisyDARTS-g cells searched on CIFAR-10 with additive Gaussian noise, ? = 0.5, ? = 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 18 :Figure 19 :Figure 20 :Figure 21 :</head><label>18192021</label><figDesc>NoisyDARTS-h cells searched on CIFAR-10 with additive Gaussian noise, ? = 1.0, ? = 0.2. NoisyDARTS-i cells searched on CIFAR-10 with additive Gaussian noise, ? = 0.5, ? = 0.1. NoisyDARTS-j cells searched on CIFAR-10 with additive Gaussian noise, ? = 1.0, ? = 0.1. NoisyDARTS cells searched on CIFAR-10 with additive Gaussian noise ? = 0, ? = 0.6, in S 2 of RobustDARTS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 22 :Figure 23 :</head><label>2223</label><figDesc>NoisyDARTS cells searched on CIFAR-10 with additive Gaussian noise ? = 0, ? = 0.6, in S 3 of RobustDARTS. NoisyDARTS GCN cells searched on ModelNet-10 with additive Gaussian noise ? = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 14 (</head><label>14</label><figDesc>see Section C.11 in the supplementary). Each search is repeated only three times to obtain the average.</figDesc><table><row><cell cols="2">Data Space</cell><cell>DARTS</cell><cell>DARTS ADA DARTS ES</cell><cell>Ours</cell><cell cols="3">RDARTS L2 SDARTS RS SDARTS ADV Ours</cell></row><row><cell></cell><cell>S 1</cell><cell cols="3">95.34?0.71 96.97?0.08 96.95?0.07 97.05?0.18</cell><cell>97.22</cell><cell>97.22</cell><cell>97.27</cell><cell>97.27</cell></row><row><cell>C10</cell><cell>S 2 S 3</cell><cell cols="3">95.58?0.40 96.41?0.31 96.59?0.14 96.59?0.11 95.88?0.85 97.01?0.34 96.29?1.14 97.42?0.08</cell><cell>96.69 97.49</cell><cell>96.67  ? 97.47</cell><cell>96.59  ? 97.51</cell><cell>96.71 97.53</cell></row><row><cell></cell><cell>S 4</cell><cell cols="3">93.05?0.18 96.11?0.67 95.83?0.21 97.22?0.08</cell><cell>96.44</cell><cell>97.07</cell><cell>97.13</cell><cell>97.29</cell></row><row><cell></cell><cell>S 1</cell><cell cols="3">70.07?0.41 75.06?0.81 71.10?0.81 77.89?0.88</cell><cell>75.75</cell><cell>76.49</cell><cell>77.67</cell><cell>78.83</cell></row><row><cell>C100</cell><cell>S 2 S 3</cell><cell cols="3">71.25?0.92 73.12?1.11 75.32?1.43 78.15?0.44 70.99?0.24 75.45?0.63 73.01?1.79 79.48?0.59</cell><cell>77.76 76.01</cell><cell>77.72 78.91</cell><cell>79.44 78.92</cell><cell>78.82 79.93</cell></row><row><cell></cell><cell>S 4</cell><cell cols="3">75.23?1.51 76.34?0.90 76.10?2.01 78.37?0.42</cell><cell>78.06</cell><cell>78.54</cell><cell>78.75</cell><cell>78.84</cell></row><row><cell></cell><cell>S 1</cell><cell cols="3">90.12?5.50 97.41?0.07 97.20?0.09 97.44?0.06</cell><cell>95.21</cell><cell>97.14  ?</cell><cell>97.51  ?</cell><cell>97.51</cell></row><row><cell>SVHN</cell><cell>S 2 S 3</cell><cell cols="3">96.31?0.12 97.21?0.22 97.32?0.18 97.60?0.08 96.00?1.01 97.42?0.07 97.22?0.19 97.58?0.06</cell><cell>97.49 97.52</cell><cell>97.61 97.64</cell><cell>97.65 97.60</cell><cell>97.66 97.63</cell></row><row><cell></cell><cell>S 4</cell><cell cols="3">97.10?0.02 97.48?0.06 97.45?0.15 97.59?0.09</cell><cell>97.50</cell><cell>97.54</cell><cell>97.58</cell><cell>97.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison in the reduced spaces of RobustDARTS<ref type="bibr" target="#b50">[51]</ref>. For NoisyDARTS, we use NFA for S 4 since there is no skip connection in it, and OFS for all the rest.</figDesc><table /><note>? : 16 initial channels (retrained). Four right columns are the best out of three runs. ADA: adaptive regularization, ES: early-stop, L2: L2 regularization (ADA, ES, L2 are from RDARTS [51]).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison on NAS-Bench-201. Averaged on 3 searches. The best for is in bold and underlined, while the second best is in bold. : reported by<ref type="bibr" target="#b3">[4]</ref> </figDesc><table><row><cell>Method</cell><cell>Type</cell><cell>Dataset</cell><cell>Benchmark</cell><cell>Acc (%)</cell></row><row><cell cols="2">NoisyDARTS w/ Noise</cell><cell>CIFAR-10</cell><cell>1</cell><cell>97.35?0.23</cell></row><row><cell>DARTS</cell><cell cols="2">w/o Noise CIFAR-10</cell><cell>1</cell><cell>96.62?0.23</cell></row><row><cell cols="2">NoisyDARTS w/ Noise</cell><cell>ImageNet</cell><cell>2</cell><cell>76.1</cell></row><row><cell>DARTS</cell><cell cols="2">w/o Noise ImageNet</cell><cell>2</cell><cell>66.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>NoisyDARTS is robust across CIFAR-10 and ImageNet. : Reported by [50]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>10?17.68 66.02?18.63 38.66?15.72 38.75?15.72 18.59?11.61 18.05?11.47 0.2 51.54?0.00 55.15?0.00 28.74?0.00 28.86?0.00 11.60?0.00 10.87?0.00 ours 90.26?0.22 93.49?0.25 71.36?0.21 71.55?0.51 42.47?0.00 42.34?0.06</figDesc><table><row><cell>r drop</cell><cell>CIFAR-10</cell><cell cols="2">CIFAR-100</cell><cell cols="2">ImageNet-16</cell></row><row><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell></row><row><cell>0.1 63.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Applying Drop-path in replace of noise on NAS-Bench-201.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Architecture parameters ? i, j , network weights w , noise's standard variance ? , E poch max . 2: while not reach E poch max do Update architecture parameters ? by ? ? L val (w, ?) 6: end while 7: Derive the final architecture according to learned ?. give the NFA version of NoisyDARTS in Algorithm 2. Input: Architecture parameters ? i, j , network weights w , noise's standard variance ? , E poch max . 2: while not reach E poch max do Update architecture parameters ? by ? ? L val (w, ?) 6: end while 7: Derive the final architecture according to learned ?.</figDesc><table><row><cell></cell><cell>Equation 9 requires Equation 4 (main</cell></row><row><cell cols="2">text) be satisfied. To keep the gradient unbiased,x should be close to 1. Thus, we use Gaussian</cell></row><row><cell cols="2">distributionx ? N (1, ? 2 ).</cell></row><row><cell cols="2">B Algorithm</cell></row><row><cell cols="2">Algorithm 1 NoisyDARTS-OFS (default and recommended)</cell></row><row><cell cols="2">1: Input: 3: Inject random Gaussian noisex into the skip connections' output.</cell></row><row><cell>4:</cell><cell>Update weights w by ? w L train (w, ?)</cell></row><row><cell>5:</cell><cell></cell></row><row><cell cols="2">We Algorithm 2 NoisyDARTS-NFA</cell></row><row><cell>1: 3:</cell><cell>Inject random Gaussian noisex into all candidate operations' output.</cell></row><row><cell>4:</cell><cell>Update weights w by ? w L train (w, ?)</cell></row><row><cell>5:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Experiments on different types of noise. Each search is run 8 times</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Experiments on different mixing operations. Each search is run 8 times</figDesc><table><row><cell>Remove</cell></row></table><note>Skip Connection from the search space Skip connections are a necessary compo- nent but they are troublesome for DARTS. We remove this operation from the NAS-Bench-201 search space to study how well DARTS performs. Table 9 hints that DARTS can find relatively</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Ablation on additive Gaussian noise on CIFAR-10 (each search is run 8 times)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Removing skip connection from search space on NAS-Bench-201.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>BackbonesParams Acc AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell></cell><cell>(M)</cell><cell>(%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell cols="2">MobileNetV2 3.4</cell><cell>72.0 28.3 46.7 29.3 14.8 30.7 38.1</cell></row><row><cell cols="2">SingPath NAS 4.3</cell><cell>75.0 30.7 49.8 32.2 15.4 33.9 41.6</cell></row><row><cell cols="2">MobileNetV3 5.4</cell><cell>75.2 29.9 49.3 30.8 14.9 33.3 41.1</cell></row><row><cell>MnasNet-A2</cell><cell>4.8</cell><cell>75.6 30.5 50.2 32.0 16.6 34.1 41.1</cell></row><row><cell>SCARLET-A</cell><cell>6.7</cell><cell>76.9 31.4 51.2 33.0 16.3 35.1 41.8</cell></row><row><cell>MixNet-M</cell><cell>5.0</cell><cell>77.0 31.3 51.7 32.4 17.0 35.0 41.9</cell></row><row><cell>FairNAS-A</cell><cell>5.9</cell><cell>77.</cell></row></table><note>5 32.4 52.4 33.9 17.2 36.3 43.2 NoisyDARTS-A 5.5 77.9 33.1 53.4 34.8 18.5 36.6 44.4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>V1 39.77?0.00 54.30?0.00 15.03?0.00 15.61?0.00 16.43?0.00 16.32?0.00 ES, ? =0.2 39.77?0.00 54.30?0.00 15.03?0.00 15.61?0.00 16.43?0.00 16.32?0.00 ES, ? =0.4 49.27?16.46 59.84?9.59 22.87?13.59 23.39?13.48 17.24?1.40 17.01?1.20 NFA 88.17?2.02 91.60?1.74 67.71?2.35 68.26?1.59 41.91?2.00 41.57?2.59 OFS 90.26?0.22 93.49?0.25 71.36?0.21 71.55?0.51 42.47?0.00 42.34?0.06</figDesc><table><row><cell>Method</cell><cell cols="2">CIFAR-10</cell><cell cols="2">CIFAR-100</cell><cell cols="2">ImageNet-16</cell></row><row><cell></cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell></row><row><cell>DARTS-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>COCO Object detection of various drop-in backbones.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 11 :</head><label>11</label><figDesc>Comparison of NoisyDARTS (NFA, ES, OFS) on NAS-Bench-201.</figDesc><table><row><cell>Methods</cell><cell cols="2">Params (M) OA (%)</cell></row><row><cell>SGAS (Cri. 1 avg.)</cell><cell>8.78</cell><cell>92.69?0.20</cell></row><row><cell>SGAS (Cri. 1 best)</cell><cell>8.63</cell><cell>92.87</cell></row><row><cell>NoisyDARTS (? = 0.3 avg.)</cell><cell>8.68</cell><cell>92.85?0.36</cell></row><row><cell>NoisyDARTS (? = 0.3 best)</cell><cell>8.33</cell><cell>93.11</cell></row><row><cell>NoisyDARTS (? = 0.4 avg.)</cell><cell>8.68</cell><cell>92.70?0.43</cell></row><row><cell>NoisyDARTS (? = 0.4 best)</cell><cell>8.93</cell><cell>93.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 14 :</head><label>14</label><figDesc>Test accuracy and the maximum Hessian eigenvalue (in the final searching epoch) of NoisyDARTS models searched with different ? in the reduced search spaces of RobustDARTS on CIFAR-10. Notice here we train models in S 2 with the same settings as in S 3 . It's interesting to see that ? ? max = 0.418 in S 3 is the best model with 97.47% top-1 accuracy. However, such similar value in [51] indicates a failure (94.70%) under the same setting</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stabilizing Differentiable Architecture Search via Perturbation-based Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DrNAS: Dirichlet neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DARTS-: Robustly stepping out of performance collapse without indicators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search. ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">SCARLET-NAS: Bridging the Gap Between Stability and Fairness in Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<publisher>ICCVW</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning Augmentation Policies from Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via self-evaluated template network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3681" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searching for a Robust Neural Architecture in Four GPU Hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">NAS-Bench-102: Extending the Scope of Reproducible Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mollifying networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness Against Adversarial Attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhezhi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adnan</forename><surname>Siraj Rakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DSNAS: Direct Neural Architecture Search without Parameter Retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12084" to="12092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An Alternative View: When Does SGD Escape Local Minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2698" to="2707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Certified robustness to adversarial examples with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lecuyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaggelis</forename><surname>Atlidakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Geambasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Jana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="656" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6389" to="6399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kechen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">DARTS+: Improved Differentiable Architecture Search with Early Stopping</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive Neural Architecture Search. In ECCV</title>
		<imprint>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards robust neural networks via random self-ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="369" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">XNAS: Neural Architecture Search with Expert Advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>NIPSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks. In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mixed Depthwise Convolutional Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-Aware Neural Architecture Search for Mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05884</idno>
		<title level="m">Revisiting Loss Landscape for Adversarial Robustness</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SNAS: Stochastic Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PC-DARTS: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJlS634tPr" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1gDNyrKDS" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multinomial Distribution Learning for Effective Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1304" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Theory-Inspired Path-Regularized Differential Network Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning Transferable Architectures for Scalable Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
