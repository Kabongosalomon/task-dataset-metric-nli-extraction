<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatial-Temporal Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Key Laboratory of Information Security Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peigen</forename><surname>Huang</surname></persName>
							<email>huangpg@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
							<email>xiexiaoh6@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Key Laboratory of Information Security Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spatial-Temporal Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of current person re-identification (ReID) methods neglect a spatial-temporal constraint. Given a query image, conventional methods compute the feature distances between the query image and all the gallery images and return a similarity ranked table. When the gallery database is very large in practice, these approaches fail to obtain a good performance due to appearance ambiguity across different camera views. In this paper, we propose a novel two-stream spatialtemporal person ReID (st-ReID) framework that mines both visual semantic information and spatial-temporal information. To this end, a joint similarity metric with Logistic Smoothing (LS) is introduced to integrate two kinds of heterogeneous information into a unified framework. To approximate a complex spatial-temporal probability distribution, we develop a fast Histogram-Parzen (HP) method. With the help of the spatial-temporal constraint, the st-ReID model eliminates lots of irrelevant images and thus narrows the gallery database. Without bells and whistles, our st-ReID method achieves rank-1 accuracy of 98.1% on Market-1501 and 94.4% on DukeMTMC-reID, improving from the baselines 91.2% and 83.8%, respectively, outperforming all previous state-of-the-art methods by a large margin. Code</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Person ReID aims to re-target pedestrian images across nonoverlapping camera views given a query image. Recently, state-of-the-art person ReID methods <ref type="bibr" target="#b35">Zhong et al. 2017a;</ref><ref type="bibr" target="#b0">Bai, Bai, and Tian 2017;</ref><ref type="bibr" target="#b25">Tang et al. 2017;</ref><ref type="bibr" target="#b37">Zhuo et al. 2018;</ref><ref type="bibr" target="#b16">Lin et al. 2017a</ref>) gained a significant improvement (e.g., rank-1 accuracy of 80-90% on Market-1501) by using deep learning for feature representation. However, these methods are still far from applied in real-world scenarios that may contain a large amount of gallery images. It is hard to further improve the performance using only general visual features due to appearance ambiguity. For example, different persons may share a similar appearance, a lighting condition or a human pose. How to exploit extra information to get around this bottleneck becomes a hot topic in person ReID community. Recent studies attempt to exploit person structure information to improve the performance of ReID methods. They believe that person structure information, such as body parts, human poses, person attributes, and background context information, can help ReID methods capture discriminative local visual features. For example, part-based methods <ref type="bibr" target="#b31">Zhao et al. 2017b</ref>) make a assumption that a person image consists of head, upper body, lower body and foot from top to bottom. Considering the person structure information, they can jointly learn both the global full-body and local body-part features for person ReID. Pose-based methods <ref type="bibr" target="#b22">(Su et al. 2017;</ref><ref type="bibr" target="#b30">Zhao et al. 2017a</ref>) aim to extract pose-invariant features by exploiting keypoint annotations to localize and align the poses. Other methods mine the cues of attribute, semantic segmentation or background context <ref type="bibr" target="#b9">(Kalayeh et al. 2018;</ref><ref type="bibr" target="#b20">Song et al. 2018)</ref> for person ReID. However, these models obtain a limited improvement for person ReID to address the appearance ambiguity problem.</p><p>Instead of using person structure information, a wide variety of approaches also attempt to exploit spatial-temporal information. The straightforward way is to exploit both spatial and temporal information from videos. Image-to-video and video-based person ReID methods <ref type="bibr" target="#b28">(Wang, Lai, and Xie 2017;</ref><ref type="bibr" target="#b13">Li et al. 2018)</ref> aim to learn spatial-and temporalinvariant visual features. However, these approaches still focus on visual feature representations, but not a spatialtemporal constraint across different cameras. For example, a person captured by Camera 1 at t should not be captured by Camera 2 that is far away from Camera 1 at t + ?t (?t is a small value). Such a spatial-temporal constraint eliminates lots of irrelevant target images in gallery, and thus significantly alleviates the appearance ambiguity problem. To distinguish the spatial-temporal concept of video-based methods, we call this the spatial-temporal person ReID (st-ReID), as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>St-ReID is explicitly or implicitly investigated in distributed camera network topology inference <ref type="bibr" target="#b7">(Huang et al. 2016;</ref><ref type="bibr" target="#b3">Cho et al. 2017</ref>) and cross-camera multiple object tracking. However, these approaches either make some strong assumptions for model simplification or do not focus on how to build an effective joint metric for the visual similarity and the spatial-temporal distribution. Formally, st-ReID is to learn a mapping f :</p><formula xml:id="formula_0">X ? Y from a train- ing set {(x v i , x s i , x t i , y i )}, where x v i , x s i ,</formula><p>x t i and y i represent a visual feature vector, a camera ID (spatial information), a timestamp, and a person ID, respectively. St-ReID has three properties: 1) extra information of st-ReID (i.e., x s i ,x t i ) widely exists in video surveillance and can be easily collected without any manual annotation (see <ref type="figure" target="#fig_0">Figure 1)</ref>; 2) with the cheap spatial-temporal information, the performance of ReID can be significantly improved (6.9% and 10.6% improvement on Market1501 and DukeMTMC-reID, respectively); 3) st-ReID can be thought of as analogous to the more difficult version of cross-camera multiple object tracking, which misses lots of in-between frames. St-ReID bridges the gap between the conventional person ReID and the cross-camera multiple object tracking.</p><p>There are three key challenges to model the spatialtemporal pattern in person ReID. First, it is extremely difficult to estimate the spatial-temporal pattern of person ReID that follows a complex distribution. Take the DukeMTMC-reID dataset as an example <ref type="figure">(Figure 2 (a)</ref>), there are several paths between Camera 1 and Camera 6 and therefore several peaks exist in the spatial-temporal distribution from Camera 1 to Camera 6 (Figure 2 (b)). Second, even though we can find a good formulation to describe the complex spatialtemporal distribution based on a finite dataset, it is still unreliable due to uncertain walking trajectories and velocities. That is, a person may appear at anytime and from anywhere. Third, given a reliable visual appearance similarity and an unreliable spatial-temporal distribution, it is difficult to build a reliable joint metric because the spatial-temporal distribution is unreliable and it is hard to assign appropriate weighting factors for these two types of metrics.</p><p>Considering these intractable problems, a novel joint similarity metric with Logistic Smoothing (LS) is proposed to integrate both visual feature similarity and spatial-temporal patterns into a unified metric function. Specially, we first train a deep convolutional neural network for visual feature representation based on the PCB model <ref type="bibr" target="#b24">(Sun et al. 2017b)</ref>. A fast Histogram-Parzen method (HP) is then introduced to describe the probability of positive image pairs with respect to time difference for each camera pair. To avoid missing low-probability positive gallery images, we propose to use logistic smoothing (LS) to alleviate the problem of uncertain walking trajectories and velocities in person ReID.</p><p>Overall, this paper makes three main contributions: ? First, we propose a novel two-stream spatial-temporal person ReID (st-ReID) framework that takes both visual semantic information and spatial-temporal information into consideration. With the help of the cheap spatialtemporal information that can be easily collected without any manual annotation, the st-ReID model eliminates lots of irrelevant images and thus to alleviate the problem of appearance ambiguity in person ReID. ? Second, we propose a joint similarity metric with Logistic</p><p>Smoothing (LS) to integrate two kinds of heterogeneous information into a unified framework. Furthermore, we develop a fast Histogram-Parzen (HP) method to approximate the spatial-temporal probability distribution. ? Third, without bells and whistles, our st-ReID method achieves rank-1 accuracy of 98.1% on Market-1501 and 94.4% on DukeMTMC-reID, improving from the baselines 91.2% and 83.8%, respectively, outperforming all previous state-of-the-art methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Recent person ReID methods concentrate on deep learning for visual feature representation. Basically, these deep models either attempt to design effective convolutional neural networks or adopt different kinds of loss functions, e.g., classification loss <ref type="bibr" target="#b5">Feng, Lai, and Xie 2018;</ref><ref type="bibr" target="#b14">Liang et al. 2018)</ref>, verification loss <ref type="bibr" target="#b11">(Li et al. 2014;</ref><ref type="bibr" target="#b1">Chen, Guo, and Lai 2015)</ref>, and triplet loss <ref type="bibr" target="#b4">(Ding et al. 2015;</ref><ref type="bibr" target="#b28">Wang, Lai, and Xie 2017;</ref><ref type="bibr" target="#b6">Hermans, Beyer, and Leibe 2017;</ref><ref type="bibr" target="#b27">Wang et al. 2016</ref>). Due to the remarkable ability of CNN representation, state-of-the-art approaches achieve a good performance, e.g., rank-1 accuracy of 80-90% on Market-1501. However, these methods can hardly address the appearance ambiguity problem. In order to achieve this goal, many studies try to exploit person structure information <ref type="bibr" target="#b31">Zhao et al. 2017b;</ref><ref type="bibr" target="#b22">Su et al. 2017;</ref><ref type="bibr" target="#b30">Zhao et al. 2017a;</ref><ref type="bibr" target="#b9">Kalayeh et al. 2018;</ref><ref type="bibr" target="#b20">Song et al. 2018)</ref>. For example, a multi-scale context-aware network ) is used to learn powerful features over full body and body parts to capture the local context information. A pose-driven deep convolutional model <ref type="bibr" target="#b22">(Su et al. 2017</ref>) is introduced to alleviate the pose variations and learn robust feature representations from both the global images and different local parts. A human parsing method <ref type="bibr" target="#b20">(Song et al. 2018</ref>) is adopted to improve the performance of person ReID with the help of the pixel-level accuracy.</p><p>Rather than using the person structure information, another group of researchers pay attention to spatial-temporal information. According to different kinds of annotations, spatial-temporal methods can be categorized into two subgroups. In the first sub-group, spatial-temporal information is implicitly hidden in videos, e.g., image-to-video (Wang, Lai, and Xie 2017) and video-based person ReID <ref type="bibr">(Li et</ref>   ) introduced a spatial-temporal attention model to discover a diverse set of distinctive body parts for the video-based person ReID. In the second sub-group, spatialtemporal information is explicitly used as a constraint that eliminate the irrelevant gallery images <ref type="bibr" target="#b3">(Cho et al. 2017;</ref><ref type="bibr" target="#b7">Huang et al. 2016;</ref><ref type="bibr" target="#b18">Lv et al. 2018)</ref>. For example, camera network topology inference methods <ref type="bibr" target="#b3">(Cho et al. 2017;</ref><ref type="bibr" target="#b18">Lv et al. 2018</ref>) aim to perform the person ReID and camera network topology inference alternately in an online or unsupervised learning manner. Given a person image with a timestamp t, they make a strong assumption that this person should be appear at (t ? ?t, t + ?t). Different from these methods, our st-ReID approach seeks an effective joint metric that naturally integrates spatial-temporal information into the visual feature representation for supervised person ReID. Besides, a Camera Network based Person ReID (CNPR) <ref type="bibr" target="#b7">(Huang et al. 2016</ref>) is introduced to consider both the visual feature representation and the spatial-temporal constraint. However, the CNPR model makes a strong assumption that the time difference for the transition between cameras follows a Weibull distribution that contains a peak value and thus is unavailable in complex scenarios, e.g., DukeMTMC-reID. Besides, CNPR fails to address the problem of uncertain walking trajectories and velocities. Different from the CNPR model, we propose to use a Histogram-Parzen window method for the Probability Density Function (PDF) approximation and introduce a logistic smoothing approach to solve the uncertainty problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>St-ReID aims to exploit both the visual feature similarity and the spatial-temporal constraint in a unified framework. To this end, we propose a two-stream architecture which consists of three sub-modules, i.e., a visual feature stream, a spatial-temporal stream, and a joint metric sub-module. <ref type="figure">Figure 3</ref> shows the two-stream architecture for the st-ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Feature Stream</head><p>Visual feature representation approaches are investigated in lots of studies. We do not focus on how to extract a dis-criminative and robust feature representation in this paper. Therefore, we use a clear Part-based Convolutional Baseline (PCB) <ref type="bibr" target="#b24">(Sun et al. 2017b</ref>) as a visual feature stream without considering a refined part pooling. This stream contains a ResNet backbone network, a stripe-based average pooling layer, six 1 ? 1 kernel-sized convolutional layers, six fullyconnected layers, and six classifiers (Cross-Entropy loss).</p><p>During the training phase, each classifier is used to predict the class (person identity) of a given image. With the part-level feature representation learning scheme, PCB can learn local discriminative features and thus achieve competitive accuracy. During the test phase, six stripe-based features are concatenated into a column vector for the visual feature representation. In <ref type="figure">Figure 3</ref>, we only show the test phase of the visual feature stream.</p><p>Given two images I i and I j (i and j denote image indexes in a dataset), we extract visual features by using the PCB model and obtain two feature vectors, denoting x i and x j , respectively. We compute a similarity score according to the cosine distance</p><formula xml:id="formula_1">s(x i , x j ) = x i ? x j ||x i ||||x j || (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial-temporal Stream</head><p>A spatial-temporal stream is to capture spatial-temporal complementary information to assist the visual feature stream. Instead of using a closed form probability distribution function <ref type="bibr" target="#b7">(Huang et al. 2016</ref>) that follows a strong assumption, we estimate the spatial-temporal distribution by using a non-parameter estimation approach, i.e., Parzen Window approach. However, it costs much time to directly estimate a PDF because there are too much spatial-temporal data points.</p><p>To alleviate the expensive computation problem, we develop a Histogram-Parzen approach. That is, we first estimate spatial-temporal histograms and then use the Parzen Window method to smooth it. Let (ID i , c i , t i ) and (ID j , c j , t j ) (t i &lt; t j ) denote the identity labels, camera IDs, timestamps of two images I i and I j , respectively. We create coarse spatial-temporal histograms to describe the probability of a positive image pair b?</p><formula xml:id="formula_2">p(y = 1|k, c i , c j ) = n k cicj l n l cicj<label>(2)</label></formula><p>where k indicates the kth bin of a histogram, i.e., the time interval t j ? t i ? ((k ? 1)?t, k?t). n k cicj represents the number of person image pairs whose time differences are at the kth bin from c i to c j . y = 1 denotes that I i and I j (i.e., ID i = ID j ) share the same person identity, while y = 0 for different person identities ((i.e., ID i = ID j )).</p><p>With the Parzen Window method, we smooth the histogram by</p><formula xml:id="formula_3">p(y = 1|k, c i , c j ) = 1 Z lp (y = 1|l, c i , c j )K(l ? k) (3)</formula><p>where K(.) is a kernel and Z = k p(y = 1|k, c i , c j ) is a normalized factor. In this work, we use a gaussian function as a kernel K, namely</p><formula xml:id="formula_4">K(x) = 1 ? 2?? e ?x 2 2? 2 (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Metric</head><p>After we obtain two kinds of heterogeneous patterns, it is intuitively assumed that the visual similarity probability is independent of the spatial-temporal probability. The joint probability can be simply formulated as</p><formula xml:id="formula_5">p(y = 1|x i , x j , k, c i , c j ) = s(x i , x j )p(y = 1|k, c i , c j ) (5)</formula><p>however, Eqn. 5 neglects two points. First, it is unreasonable to directly use the similarity score as the visual similarity probability, i.e., p(y = 1|x i , x j ) = s(x i , x j ). Second, the spatial-temporal probability p(y = 1|k, c i , c j ) is unreliable and uncontrollable because the walking trajectory and velocity of a person is uncertain, i.e., a person may appear at anytime and from anywhere. Directly using p(y = 1|k, c i , c j ) as the spatial-temporal probability function leads to a lower recall rate while keeping the same precision. As an example, given a query image, one gallery image is with a 0.9 similarity score, and a 0.01 spatial-temporal probability, while another gallery image is with 0.3, 0.1. Eqn. 5 tends to return the second gallery image. Those who have low spatialtemporal probabilities may be regarded as irrelevant images. However, this is impractical in real-world scenarios, especially video surveillance systems. For example, when retrieving the images of a thief, (s)he may not be retrieved because (s)he may walk faster than common person and has a low spatial-temporal probability. So, can we transform the similarity score as the visual similarity probability or can we build a robust spatial-temporal probability? Our observation is two-fold: Observation 1: Laplace smoothing. Laplace smoothing is a technique which is widely used to estimate a prior probability in Naive Bayes</p><formula xml:id="formula_6">p ? (Y = d k ) = m k + ? M + D?<label>(6)</label></formula><p>where d k indicates the label of the kth category, m k indicates the number of the kth category, M is the total number of examples, D is the total number of categories, ? is the smoothing parameter. As a special case, the number of categories D is 2 and ? = 1, we obtain</p><formula xml:id="formula_7">p ? (Y = d k ) = m k + 1 M + 2<label>(7)</label></formula><p>We can see that Laplace smoothing is used to adjust the probability of rare (but not impossible) events so those probabilities are not exactly zero and zero-frequency problems are avoided. It serves as a type of shrinkage estimator, as the smoothing result will be between the empirical estimate m k M , and the uniform probability 1 2 . Observation 2: Logistic function. The logistic model is widely applied for the binary classification problem. Specially, it is defined as</p><formula xml:id="formula_8">f (x; ?, ?) = 1 1 + ?e ??x<label>(8)</label></formula><p>where ? and ? are constant coefficients, ? is a smoothing factor and ? is a shrinking factor. Observation 1 shows the basic idea of a smoothing operator to alleviate unreliable probability estimation. Observation 2 shows logistic function can be used for the binary classification problem. Based on these two observations, we propose a logistic smoothing approach that both adjusts the probability of rare events and compute the probability of two images belonging to the same ID given the certain information. We modify Eqn. <ref type="formula">(5)</ref> as</p><formula xml:id="formula_9">p joint = f (s; ? 0 , ? 0 )f (p st ; ? 1 , ? 1 )<label>(9)</label></formula><p>For notation simplicity, we use p joint , s and p st to denote p(y = 1|x i , x j , k, c i , c j ), s(x i , x j ) and p(y = 1|k, c i , c j ), respectively. According to Eqn. <ref type="formula">(1)</ref> and <ref type="formula">(3)</ref>, we can see that s ? (?1, 1) is shrunk by the logistic function like the Laplace smoothing, but not so much. Differently, p st ? (0, 1) is truncated and lifted up largely. Even the spatialtemporal probability p st is close to zero, f (p st ; ? 1 , ? 1 ) ? f (0) = 1 1+?1 . With the logistic smoothing, Eqn. <ref type="formula" target="#formula_9">(9)</ref> is robust to rare events. This is reasonable because the spatialtemporal probability is unreliable as discussed above while visual similarity are relatively reliable. Besides, using the logistic function to transform the similarity score (spatialtemporal probability) into a binary classification probability (positive pair or negative pair) is intuitive and self-evident as described in Observation 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>As for the visual feature stream, we set the hyper-parameters following the PCB method <ref type="bibr" target="#b24">(Sun et al. 2017b</ref>) without considering the refined pooling scheme. The training images are augmented with horizontal flip and normalization and resized to 384 ? 192. We use SGD with a mini-batch size of 32. We train the visual feature stream for 60 epochs. The learning rate starts from 0.1 and is decayed to 0.01 after 40 epochs. The backbone model is pre-trained on ImageNet and the learning rate for all the pre-trained layers are set to 0.1? of the base learning rate. As for the spatial-temporal stream, we set the time interval ?t to 100 frames. We set the gaussian kernel parameter ? to 50 and use the three-sigma rule to further reduce the computation. As for the joint metric, we set ? 0 , ? 1 , ? 0 and ? 1 to 1, 2, 5 and 5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we evaluate our st-ReID method on two large-scale person ReID benchmark datasets, i.e., Market-1501 and DukeMTMC-reID, and show the superiority of the st-ReID model compared with other state-of-the-art methods. We then present ablation studies to reveal the benefits of each main component/factor of our method.</p><p>Datasets. The Market-1501 dataset is collected in front of a supermarket in Tsinghua University. A total of six cameras are used, including 5 high-resolution cameras, and one low-resolution camera. Overlap exists among different cameras. Overall, this dataset contains 32,668 annotated bounding boxes of 1,501 identities. Among them, 12,936 images from 751 identities are used for training, and 19,732 images from 750 identities plus distractors are used for gallery.  <ref type="table">Table 1</ref>: Comparison of the proposed method with the stateof-the-arts on Market-1501. The compared methods are categorized into seven groups. Group 1: handcrafted feature methods. Group 2: clear deep learning based methods. Group 3: attribute-based methods. Group 4: mask-guided methods. Group 5: part-based methods. Group 6: pose-based methods. Group 7: spatial-temporal methods. * denotes the methods that are reproduced by ourselves.</p><p>As for query, 3,368 hand-drawn bounding boxes from 750 identities are adopted. In this open system, images of each identity are captured by at most six cameras. Each annotated identity is present in at least two cameras. Each image contains its camera id and frame num (time stamp).</p><p>DukeMTMC-reID is a subset of the DukeMTMC dataset for image-based re-identification. There are 1,404 identities appearing in more than two cameras and 408 identities (distractor ID) who appear in only one camera. Specially, 702 IDs are selected as the training set and the remaining 702 IDs are used as the testing set. In the testing set, one query image is picked for each ID in each camera and the remaining images are put in the gallery. In this way, there are 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images (702 ID + 408 distractor ID). Each image contains its camera id and frame num (time stamp).</p><p>Evaluation Protocol. For each query, an algorithm computes the distances between the query image and all the gallery images and return a ranked table from small to large. Top-k accuracy is computed by checking if top-k gallery images contain the query identity. For each individual query identity, his/her gallery samples from the same camera are  <ref type="table">Table 2</ref>: Comparison of the proposed method with the stateof-the-arts on DukeMTMC-reID. The compared methods are categorized into seven groups. Group 1: handcrafted feature methods. Group 2: clear deep learning based methods. Group 3: attribute-based methods. Group 4: mask-guided methods. Group 5: part-based methods. Group 6: pose-based methods. Group 7: spatial-temporal methods. * denotes the methods that are reproduced by ourselves.</p><p>excluded due to the setting of cross-view matching in person ReID.</p><p>Mean average precision (mAP) is used to evaluate the overall performance. For each query, we calculate the area under the Precision-Recall curve, i.e., average precision (AP). Then, the mean value of APs of all queries, i.e., mAP, is calculated, which considers both precision and recall of an algorithm, thus providing a more comprehensive evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons to the State-of-the-Art</head><p>In this sub-section, we evaluate our st-ReID approach compared with lots of existing state-of-the-arts on two largescale person ReID benchmark datesets to shows the superiority of the st-ReID approach.</p><p>Evaluations on Market-1501. We evaluated the proposed st-ReID model against twenty existing state-of-theart methods, which can be grouped into seven categories, i.e., 1) handcrafted feature methods including BoW+kissme <ref type="bibr" target="#b32">(Zheng et al. 2015)</ref>, KLFDA <ref type="bibr">(Karanam et al. 2016)</ref>, Null Space <ref type="bibr" target="#b29">(Zhang, Xiang, and Gong 2016)</ref> and WARCA <ref type="bibr" target="#b8">(Jose and Fleuret 2016)</ref>; 2) clear deep learning based methods including PAN <ref type="bibr" target="#b34">(Zheng, Zheng, and Yang 2016)</ref>, SVDNet , and HA-CNN; 3) attribute-based methods including SSDAL ) and APR <ref type="bibr" target="#b17">(Lin et al. 2017b)</ref>; 4) mask-guided methods including Human Parsing <ref type="bibr" target="#b9">(Kalayeh et al. 2018)</ref>, Mask-guided <ref type="bibr" target="#b20">(Song et al. 2018)</ref>, and Background <ref type="bibr" target="#b26">(Tian et al. 2018)</ref>; 5) part-based methods including MultiScale <ref type="bibr" target="#b2">(Chen, Zhu, and Gong 2017)</ref>, PDC <ref type="bibr" target="#b22">(Su et al. 2017)</ref> and <ref type="bibr">PSE+ECN (Saquib Sarfraz et al. 2018)</ref>; 6) pose-based methods including Spindle Net <ref type="bibr" target="#b30">(Zhao et al. 2017a)</ref>, Latent Parts , Part-Aligned <ref type="bibr" target="#b31">(Zhao et al. 2017b</ref>) and PCB 7) spatial-temporal methods including TFusion-sup <ref type="bibr" target="#b18">(Lv et al. 2018</ref>).</p><p>Among them, attribute-based methods use person attribute annotations, mask-guided methods use the person masks or human body parsing annotations, part-based methods make the person body assumption or use body part detectors, and pose-based methods use keypoint annotations. These methods obtain a good performance compared with handcrafted feature methods and clear deep learning based methods, but they need expensive annotations and are quite time-consuming, e.g., pixel-level human parsing annotations, eighteen keypoints, and body part annotations.</p><p>Our st-ReID method uses the cheap spatial-temporal information (i.e., camera ID and timestamp) and achieves the rank-1 accuracy of 97.2% and mAP of 87.6%, outperforming all the existing state-of-the-art methods by a large margin. With random erase (RE) <ref type="bibr" target="#b37">(Zhong et al. 2017c</ref>), our st-ReID achieves the rank-1 accuracy of 98.1% and mAP of 87.6%. With the re-ranking scheme <ref type="bibr" target="#b36">(Zhong et al. 2017b</ref>), our st-ReID obtains mAP of 95.5%. TFusion-sup also use the spatial-temporal constraint. but it makes a strong assumption that a gallery person always appears in (t??t, t+ ?t) when given a query image with a timestamp t. Such a method may be not effective in complex scenarios, especially DukeMTMC-reID. Besides, TFusion-sup focuses on cross-dataset unsupervised learning for the person ReID by alternately iterating between learning visual feature representations and estimating spatial-temporal patterns. Therefore, TFusion-sup actually does not investigate how to estimate the spatial-temporal probability distribution and how to model the joint probability of the visual similarity and the spatial-temporal probability distribution.</p><p>Evaluations on DukeMTMC-reID. DukeMTMC-reID is a new dataset and manifests itself as one of the most challenging reID datasets up to now. We compare our st-ReID method with ten state-of-the-art methods on the DukeMTMC-reID dataset. All of the competing methods are also evaluated on the Market-1501 dataset except LOMO+XQDA <ref type="bibr" target="#b15">(Liao et al. 2015)</ref>. As shown in <ref type="table">Table 2</ref>, it is encouraging to see that our approach (without any reranking scheme) significantly outperforms the competing methods by a large margin, e.g., by improving the stateof-the-art rank-1 accuracy from 85.2% to 94.0% and mAP from 79.8% to 82.8% compared with PSE+ECN (with a re-ranking scheme). With random erase (RE), our st-ReID achieves the rank-1 accuracy of 94.4% and mAP of 83.9%.</p><p>With the re-ranking scheme, our st-ReID obtains mAP of 92.7%.</p><p>Remarks. Without bells and whistles, our st-ReID model outperforms all of the previous state-of-the-art person ReID methods, e.g., a rank-1 accuracy of 98.1% and 94.4% on Market-1501 and DukeMTMC-reID datasets, respectively. While outside the scope of this work, we expect many such techniques (e.g., refined pooling) to be applicable to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies and Model Analysis</head><p>To provide more insights on the performance of our approach, we conduct a lot of ablation studies on the most challenging DukeMTMC-reID dataset by isolating each key component, i.e., the visual feature representation stream, the      spatial-temporal probability estimation stream and the joint metric sub-module. Effect of the visual feature stream. To show the benefit of the visual feature stream (VIS stream), we conduct an ablation study by isolating this sub-module. To achieve this, we remove the visual feature stream and thus we only use the spatial-temporal stream. It is observed that the rank-1 accuracy drops by 88.5% (from 5.5% to 94.0%) when removing the VIS stream, shown in <ref type="figure" target="#fig_4">Figure 4</ref> (a). In this experiment, we confirm that the VIS stream plays in a key role in the st-ReID approach.</p><p>Effect of the spatial-temporal stream. To show the benefit of the spatial-temporal stream (ST stream), we remove this sub-module to see how the spatial-temporal stream makes an effect in the st-ReID. In this case, the st-ReID model is degraded as the PCB model. As shown in 4 (a), we can see that without the spatial-temporal probability estimation stream, the rank-1 accuracy drops 10.2% (from 94.0% to 83.8%).</p><p>Effectiveness of the joint metric. To show the effectiveness of the joint metric, we set a baseline by using Eqn. 5. In the baseline, both the VIS stream and the ST stream are normalized. For the fair comparison, we use the same VIS and ST streams. As shown in <ref type="figure" target="#fig_4">Figure 4 (b)</ref>, our joint metric method improves the performance from 86.9% to 94.0%. Compared with the VIS stream (PCB model), the baseline also obtains a 3.1% improvement because it integrates the spatial-temporal information.</p><p>Influence of parameters. To investigate the impact of two important parameters in our st-ReID, i.e., the smoothing factor ? and the shrinking factor ?, we conduct two sensitivity analysis experiments. As shown in <ref type="figure" target="#fig_4">Figure 4</ref> (c) and (d), when ? is in the range of 0.4?2.8 or ? is in the range of 1?7, our model nearly keeps the best performance.</p><p>Generalization of the st-ReID. To show the good generalization of the st-ReID, we further use different deep models as the VIS streams, respectively. The deep models are ResNet-50 (a clear model with the cross entropy loss), DenseNet-121 (a clear model with the cross entropy loss) and PCB. As shown in <ref type="table" target="#tab_4">Table 3</ref>, it is observed that when adding the ST stream into these VIS streams and using our joint metric, we can achieve more than 10% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel two-stream spatialtemporal person ReID (st-ReID) framework that mines both the visual semantic similarity and the spatial-temporal information. Without bells and whistles, our st-ReID method achieves rank-1 accuracy of 98.1% on Market-1501 and 94.4% on DukeMTMC-reID, improving from the baseline 91.2% and 83.8%, respectively, outperforming all previous state-of-the-art methods by a large margin.</p><p>We intend to extend this work in two directions. First, the st-ReID builds a bridge between the conventional ReID and the cross-camera multiple object tracking and thus can be easily generalized to the cross-camera multiple object tracking. Second, we intend to further improve the performance of the st-ReID method using an end-to-end training manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Conventional person ReID vs. our st-ReID. (a) Retrieval results of the conventional person ReID. Without the help of spatial-temporal information, it is difficult for the conventional ReID to deal with the appearance ambiguity (red boxes denote false alarms). (b) Retrieval results of our st-ReID. With spatial-temporal information, st-ReID can eliminate irrelevant images. Besides, spatial-temporal information (camera ID and timestamp) of st-ReID widely exists in video surveillance and can be easily collected without any manual annotation. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>(a) Camera topology of DukeMTMC-reID. (b) Spatial-temporal distribution, i.e, frequency of positive image pairs (an image pair with the same person identity denotes a positive pair) with respect to time interval. (Best viewed in color) The proposed two-stream architecture. It consists of three sub-modules, i.e., a visual feature stream, a spatial-temporal stream, and a joint metric sub-module. (Best viewed in color) al. 2018; Zheng et al. 2016). For example, Wang et al. (Wang, Lai, and Xie 2017) proposed a point-to-set network for the image-to-video person ReID. Li et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Effectiveness of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Generalization of the st-ReID on DukeMTMC-reID.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project was supported by the National Natural Science Foundation of China (U1611461, 61573387, 61672544).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable person reidentification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.06821</idno>
		<title level="m">Deep ranking for person re-identification via joint representation learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Person reidentification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Joint person re-identification and camera network topology inference in multiple cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00983</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning view-specific deep networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3472" to="3483" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Camera network based person reidentification by leveraging spatial-temporal constraint and multiple cameras relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable metric learning via weighted approximate rank component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?kmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Rates-Borras, A.; O.Camps</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09653</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
	<note>A comprehensive evaluation and benchmark for person re-identification</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">M2m-gan: Many-to-many generative adversarial transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03768</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Person reidentification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-domain visual matching via generalized similarity measure and feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiangchu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1089" to="1102" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07220</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7948" to="7956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person reidentification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saquib Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep attributes driven multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03259</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3800" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09349</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3539" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Eliminating background-bias for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5794" to="5803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dari: Distance metric and representation integration for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">P2snet: Can an image match a video for person re-identification in an end-to-end way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2777" to="2787" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00408</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<idno>arXiv:1804.02792</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Random erasing data augmentation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
