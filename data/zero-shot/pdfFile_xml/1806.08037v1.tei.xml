<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixel-level Reconstruction and Classification for Noisy Handwritten Bangla Characters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Karki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dibiano</surname></persName>
							<email>robert@ailectric.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Ailectric LLC</orgName>
								<address>
									<addrLine>Baton Rouge</addrLine>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Basu</surname></persName>
							<email>sbasu8@lsu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supratik</forename><surname>Mukhopadhyay</surname></persName>
							<email>supratik@csc.lsu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Louisiana State University</orgName>
								<address>
									<settlement>Baton Rouge</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pixel-level Reconstruction and Classification for Noisy Handwritten Bangla Characters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classification techniques for images of handwritten characters are susceptible to noise. Quadtrees can be an efficient representation for learning from sparse features. In this paper, we improve the effectiveness of probabilistic quadtrees by using a pixel level classifier to extract the character pixels and remove noise from handwritten character images. The pixel level denoiser (a deep belief network) uses the map responses obtained from a pretrained CNN as features for reconstructing the characters eliminating noise. We experimentally demonstrate the effectiveness of our approach by reconstructing and classifying a noisy version of handwritten Bangla Numeral and Basic Character datasets [1], <ref type="bibr" target="#b1">[2]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>We build upon recent developments in Deep Learning to develop techniques for efficient representation of sparse features <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Sparse representations are usually compact representations of signals or features that have unnecessary default values <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b6">[7]</ref>. Quadtrees can be an efficient representation for learning from sparse features. Real world images, especially those of handwritten characters, are usually noisy. Presence of noise can diminish the recognition power of classifiers <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b8">[9]</ref>. Efficient algorithms to remove noise can help in classification. We improve the effectiveness of probabilistic quadtrees by using a pixel level classifier to extract the character pixels and remove noise from handwritten character images. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture of our approach. Our approach uses a Convolutional Neural Network (CNN) pretrained on the ImageNet <ref type="bibr" target="#b9">[10]</ref> collection to extract features from images of handwritten characters. These extracted features help learn the shape of the characters and segment out those pixels that do not belong to the characters. The information acquired from the pre-trained CNN helps train a Deep Belief Network (DBN) (called the reconstruction network) that reconstructs the handwritten characters through transfer learning <ref type="bibr" target="#b2">[3]</ref>, segmenting out noisy pixels.</p><p>The reconstruction network takes as input a noisy handwritten character image and produces as output a denoised binary version of it. It segments out those pixels that do not belong to the character. A probabilistic quadtree is then used to learn the sparse features from the resulting denoised binary image obtained from the reconstruction network and is used to train a Character Classification Network (another DBN) to classify the character images.</p><p>We experimentally demonstrate the effectiveness of our algorithm at multiple resolutions using a recognition scheme similar to the one used in <ref type="bibr" target="#b0">[1]</ref> while improving on their performance. We also use the quadtree decomposition technique used in <ref type="bibr" target="#b3">[4]</ref> and improve upon it by using a saliency map to eliminate blocks that do not provide discriminating power. The saliency map helps reduce the dimension of the training data enabling efficient learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contributions</head><p>This paper makes the following contributions:</p><p>? It develops an efficient framework that removes noise from noisy images of handwritten characters ? It improves the effectiveness of probabilistic quadtrees by using a pixel level classifier to extract the character pixels removing noise from handwritten character images. ? It introduces a dataset comprising noisy Bangla basic characters and numerals with three different noise types. This dataset can serve as the basis and benchmark for future research in noisy Bangla character recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Data dimensionality reduction can help make learning algorithms more efficient while making the models simpler and more robust <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> [13], <ref type="bibr" target="#b13">[14]</ref>. Quadtrees have been used previously to compress images <ref type="bibr" target="#b14">[15]</ref> and represent spatial data <ref type="bibr" target="#b15">[16]</ref>. In <ref type="bibr" target="#b3">[4]</ref>, the authors use probabilistic quadtrees to represent character images and classify them using a deep belief network (DBN). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Map responses from intermediate layers of a Convolutional</head><p>Neural Network have been used as features in <ref type="bibr" target="#b16">[17]</ref> to segment images.</p><p>Chain code histogram features are used to discriminate classes in a multi stage approach in <ref type="bibr" target="#b0">[1]</ref> where rejected images are classified again at higher resolutions. We follow a similar approach of using higher resolutions when classifying the Bangla Numeral Dataset <ref type="bibr" target="#b0">[1]</ref>. In <ref type="bibr" target="#b1">[2]</ref> the authors classify handwritten Bangla Basic Characters that have 50 classes. They use a two stage approach where they employ chaincode and gradient based features. The initial stage uses a modified quadratic discriminant function (MQDF) based classifier which is followed by a Multi-layer Perception based classifier that helps to improve recognition on confused classes.</p><p>On the Noisy Bangla Datasets that we focus on, in <ref type="bibr" target="#b3">[4]</ref> the authors have used probablistic quadtrees to learn sparse representations of handwritten character images and have used a two layer DBN for the classification.</p><p>For handwritten character images containing significant amounts of noise, the sparse representations learnt using quadtrees in <ref type="bibr" target="#b3">[4]</ref> are not efficient. In <ref type="bibr" target="#b3">[4]</ref>, the authors also introduce the handwritten Bangla Numeral Dataset which includes the 10 Bangla Numeric Characters and three types of noise: white gaussian noise, motion blur, and reduced contrast. We inject these three types of noises into the Bangla Basic Character dataset consisting of 50 classes resulting in the Noisy Bangla Basic Character Dataset. In <ref type="bibr" target="#b3">[4]</ref>, once a block has been chosen for decomposition in any one image based on the homogeneity criterion, that block is identically decomposed for every other image. We use a saliency map instead which improves the representation and we also use another DBN for character reconstruction removing noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PREPROCESSING DATA</head><p>The datasets that we evaluate our algorithm on consist of two types of handwritten Bangla characters: a) Bangla Basic Characters b) Bangla Numeric Characters. There are 50 classes in the first type and 10 in the second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Standardize Raw Data</head><p>The raw images are first processed using the non-local means denoising algorithm <ref type="bibr" target="#b17">[18]</ref>. The resulting images are bimodal in nature, with the pixels belonging to the background having value around one and the rest, belonging to the character or noise, having lower values. The next step is to use Otsu's binarization scheme <ref type="bibr" target="#b18">[19]</ref> to threshold the images to binary. In the binarized images, we set the values of the pixels in the background to 0 and those in the foreground to 255. Following the procedure described in <ref type="bibr" target="#b3">[4]</ref>, we then find the largest connected component for each image and center the image around that component. We pad the images, to create square images, resulting in at least 10 pixel long borders on all sides. To avoid large boundaries, we crop images that have too many background pixels surrounding the characters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Noisy Data Creation</head><p>Three kinds of noisy handwritten character image datasets are created from the preprocessed dataset by adding three distinct types of noise. Similar to <ref type="bibr" target="#b3">[4]</ref>, we create: a) the awgn noisy dataset by adding white Gaussian noise with a signal to noise ratio of 9.5 to the preprocessed dataset, b) the contrast noisy dataset by dividing the intensity of the preprocessed images by 2 and adding white Gaussian noise with a signal to noise ratio of 12, and (c) the motion blurred noisy dataset by blurring the images with a linear motion of 5 and an angle of 15 degrees in the counterclockwise direction. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the samples from the noisy Bangla Numeral dataset for each of the three noise types and <ref type="figure">Fig. 5</ref> shows the samples from the noisy Bangla Basic Characters dataset for each of the three noise types. The three noise types we added are commonly found in images due to camera movements, poor illumination, high temperature, and movement of objects <ref type="bibr" target="#b19">[20]</ref>. Natural images taken from cameras, scanner etc. often contain such noises </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ground Truth for the Character Reconstruction Network</head><p>As we do a pixel-level character reconstruction to clean the noisy images using the character reconstruction network, we use the binarized version of the images as labels for training the network. To keep the images binary after resampling, we use the nearest neighbor method. Each pixel in the ground truth can be grouped into one of the following two classes: a) belonging to the background or b) belonging to the character/foreground. <ref type="figure">Fig. 6</ref> shows binary images used as ground truth for the Character Reconstruction Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DEEP BELIEF NETWORKS</head><p>A deep belief network is trained by first greedily pretraining stacked Restricted Boltzmann Machines (RBMs) in an unsupervised way <ref type="bibr" target="#b11">[12]</ref>. A weight matrix for an RBM is obtained by training it in an unsupervised manner. This weight matrix is then used to initialize the weights of the links connecting the first and the second layers of the DBN. The RBM then transforms the matrix of input feature vectors <ref type="bibr">Fig. 6</ref>. Ground truth sample images of the the basic bangla characters for the Character Reconstruction Network by calculating the mean activation of the hidden unit. The transformed data is then used to initialize the weights of the connections between the second and the third layer in a similar way. This process continues until the weights for the connections between the output layer and the one before it are initialized. These weights are later fine-tuned using supervised training to change the learned representation to a classifier. Because of the unsupervised training in the beginning, DBNs are useful even with limited labeled data. Skip architectures are common in modern CNNs for pixel level classification for semantic segmentation <ref type="bibr" target="#b20">[21]</ref> that connect higher level layers to lower level ones. The use of DBN in our case is an alternative way to connect the map responses from different layers. There is flexibility that comes with the network itself learning the combination of layer filters to use instead of explicit connections.</p><p>We use Deep Belief Networks at two stages of our approach. First, we use a DBN as a Character Reconstruction Network (CRN) to segment the pixels belonging to characters from the background and then we also use a DBN as a Character Classification Network (CCN) to generate the final classification using the feature vector representation provided by the probabilistic quadtrees. The likelihood (L) and loss (J) <ref type="bibr" target="#b21">[22]</ref> functions that we use in both the CRN and the CCN are given by:</p><formula xml:id="formula_0">L(? = {M, B}, I) = |I| i=0 log(P (Y = y i |x i , M, B)) J(? = {M, B}, I) = ?L(? = {M, B}, I),<label>(1)</label></formula><p>where, I represents the dataset, and given an input x i , the weights matrix M , and a bias vector B, L represents the likelihood that the input belongs to a class y i . And, J is the cost function, the negative of the log-likelihood of x i belonging to y i <ref type="bibr" target="#b21">[22]</ref>. For the CRN, this represents the likelihood that a pixel belongs to the character whereas for the CCN, this represents the likelihood that the whole image belongs to one of the character classes (either 10 or 50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CHARACTER RECONSTRUCTION NETWORK</head><p>The Character Reconstruction Network uses the map responses from the hidden layers of a previously trained CNN as features for pixel-wise classification. The CRN segments the pixels representing the characters from the rest of the pixels. It uses the map responses to the noisy images obtained when they are fed to the pretrained CNN as features. Each pixel is treated as a single data point and the whole character image is reconstructed based on the classification of each pixel in that image. We do not know beforehand the type of noise present in an image. A single simple filter may not be enough to denoise images with unknown types of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transfer Learning</head><p>We use transfer learning to extract information from the character images using the map responses from the pre-trained CNN as features to the CRN. The data that we feed in to either the CRN or the CCN is not suitable for a convolutional network.</p><p>The ImageNet <ref type="bibr" target="#b9">[10]</ref>, used to pretrain the CNN, has 1000 object classes and more than a million images. It is trained mostly on objects, animals, scenes, and some geometric shapes. Though we are dealing with a different type of data, it has been found that some of the higher level features learned by the layers of the convolutional layers are applicable in other types of images as well <ref type="bibr" target="#b2">[3]</ref>. Since the objective of the CRN is to use pixel-wise classification to segment the pixels belonging to the character from the rest and the pre-trained CNN already takes into account the contextual information in the image, we use a DBN as the CRN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training the CRN</head><p>Our input to the Character Reconstruction Network is the preprocessed character images along with the maps extracted from the pre-trained CNN when these images are passed through it. The ground truth, as explained in Section III-C, is the binarized version of the original images. For the training of this network, all three types of noisy images, that includes both Numeral and Basic Character images, are used. We also train the CRN with images without noise to make it more robust to images containing very little noise as well. We only use images generated from subset of the original dataset reserved for training (and not the testing or validation set) for the training, validation, and testing of this network. We sample 30 images each from all the three noisy versions of each of the two types of data: Bangla Numeral and Bangla Basic Character and 14 images each from the images without noise. We use the framework described in <ref type="bibr" target="#b16">[17]</ref> to train the CRN where map responses from the pre-trained CNN are all rescaled to our input image size. Now, every input image pixel corresponds to a pixel in each of the map responses. The values of the pixels in the corresponding map responses are used as features that when aligned together form hypercolumns <ref type="bibr" target="#b22">[23]</ref>. We train the Character Reconstruction Network by taking random samples from the pool of all available pixels along with their respective features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. FEATURE REPRESENTATION USING QUADTREES</head><p>Decomposing an image window into maximal quadtree blocks has been used as an efficient way to represent sparse  <ref type="bibr" target="#b15">[16]</ref>. We use this technique to represent the images. Considering that the use of quadtree decomposition is more effective in representing images with less noise, the previous step is done to denoise noisy images. However, it is also beneficial to reduce dimensionality. Hence, we improve the sparse representation offered by probabilistic quadtrees <ref type="bibr" target="#b3">[4]</ref> further by eliminating a) blocks that have been decomposed in only a small fraction of samples and b) blocks that are present in almost all samples. This step would make the dataset itself less redundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Saliency Map</head><p>The homogeneity criterion described in <ref type="bibr" target="#b3">[4]</ref> guides the process of reducing the character images into a vector of intensity values. Blocks containing textural details would be decomposed into smaller blocks. The quadtree representation is then converted to a linear vector by performing a depth first search (DFS). The features used consist of the averages of the pixel values of the decomposed blocks. While this approach reduces the number of features, when the data is noisy the quadtrees tend to be broken down into smaller blocks. Also, using this approach, whenever an image is broken into smaller blocks, all other images use this block in the final feature vector.</p><p>To mitigate this problem, we only use salient blocks that help in discrimination of characters. We use a saliency mask and prune smaller blocks that are not decomposed in ? percentage of the total number of training images or if they are contained in more than ? percentage of the samples. This technique decreases the number of features while not removing key blocks helpful to distinguish characters. The decomposition map on <ref type="figure" target="#fig_6">Fig. 8 (a)</ref> shows the normalized recurrence of the decomposed blocks for the entire training dataset for motion  blurred (noisy) Bangla Numeral images. The map shows that a lot of the blocks on the edges are hardly ever used, and some blocks in the middle are present in almost all of the images. These blocks are not very likely to be useful in discriminating the characters. The saliency mask obtained in <ref type="figure" target="#fig_6">Fig. 8 (b)</ref> shows the blocks actually used. <ref type="table">Table.</ref> I shows the reduction of feature vectors using our technique compared to the probabilistic quadtree based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Character Classification Network</head><p>The Character Classification Network is a DBN that uses the average pixel values of the different blocks that have been decomposed in the quadtree. We train the CCN to recognize the handwritten characters as belonging to individual character classes. In this case, the output is a probability value that represents what character is present in the entire image instead of each pixel. The training data consists of the pixel values at all quadtree blocks. The labels are derived from the labels of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTAL RESULTS AND DISCUSSION</head><p>We trained both the CRN and the CCN by first performing a pre-training step with persistent chain contrastive divergence (P-CD) and then fine tuned the network using backpropagation. We used L1 and L2 norms for regularization and implemented dropout on the hidden layers. We ran our algorithms on an Intel i7 six core server with TITAN X GPU and used the Theano deep learning library for the Deep Belief Networks <ref type="bibr" target="#b23">[24]</ref>. We used mu = 5% and nu = 95% for our experiments.</p><p>On the Noisy Numeral Dataset, <ref type="table" target="#tab_0">Table II</ref> shows that we significantly improve on the results for each of the noise types in the Noisy Numeral dataset compared to <ref type="bibr" target="#b3">[4]</ref>. The use of the same network for reconstruction of all three types of noise hurt the performance on the motion blurred images. We still make further improvements using the saliency mask. Observe that the classification accuracy increased even with a decrease in the number of features. We also compare our results against a multi layer neural network that makes use of Dropconnect for regularization <ref type="bibr" target="#b24">[25]</ref>. The approach has excellent results on the MNIST classification with &lt; .5% classification error <ref type="bibr" target="#b24">[25]</ref>. Interestingly the motion blurred images were very accurately classified by this approach. In <ref type="table" target="#tab_0">Table III</ref>, we show the classification error rate on the numeral and basic character images after they have been reconstructed using the CRN. The results show improvements especially on the images with added gaussian noise (awgn and contrast). In <ref type="table" target="#tab_0">Table IV</ref>, we compare our results on the Noisy Bangla Characters dataset with that obtained by just using a traditional DBN using raw pixels as features (similar to <ref type="bibr" target="#b3">[4]</ref>) as well as those obtained using a multilayer neural network employing Dropconnect for regularization. We obtained significant improvements in overall accuracy for each of the noise types using our approach. We also studied the effect of various CCN architectures on the classification accuracy on the noisy Bangla Basic Characters dataset in <ref type="table" target="#tab_2">Table  V</ref>. For the awgn noise type, an architecture consisting of two hidden layers with five hundred neurons per layer performed best; for the motion noise type an architecture consisting of two hidden layers with three hundred neurons per layer performed best; for the contrast noise type, the architectures consisting of three hidden layers with five hundred neurons per layer performed best indicated in bold in the three columns of the <ref type="table" target="#tab_2">Table V.</ref>   <ref type="table" target="#tab_0">Table VI</ref> compares the results on the n-MNIST dataset provided in <ref type="bibr" target="#b3">[4]</ref>. We improve the recognition rates on the datasets with added white gaussian noise (awgn) and with reduced contrast and comparable results on the motion blurred image. Our approach yields better results than using the method in <ref type="bibr" target="#b24">[25]</ref> in the same two types of noise. Our reconstruction algorithm was able to improve upon the results achieved from just noisy images using <ref type="bibr" target="#b24">[25]</ref> as well.</p><p>In both n-MNIST and noisy Bangla Basic Characters, reduced contrast images had the worst recognition rates. The reconstruction network eroded many of the pixels from the characters of the reduced contrast images. The random noise pixels had similar intensity values to the character pixels. Because of that edges were not as sharp as the other two types of noisy images. Our approach was not the best on motion blurred images. Motion blurred images were easily recognized by architectures that already work great on normal (non-noisy) characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>We improved the efficiency of probabilistic quadtrees by using a pixel level classifier to reconstruct noisy handwritten character images by segmenting out the noisy pixels. The pixel level denoiser (a deep belief network) uses the map responses obtained from a pretrained CNN trained on Imagenet as features for reconstructing the characters eliminating noise. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Architecture of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Original Images Bangla Numeral Characters. One image for each Bangla Numeral.<ref type="bibr" target="#b0">[1]</ref> Original Images Bangla Basic Characters. One image each of the 50 Bangla Basic Characters.<ref type="bibr" target="#b1">[2]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>(a) added white gaussian noise (awgn) (b) motion blurred (c) decreased contrast and awgn Noisy Version of the Bangla Numerals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 3shows one sample image each from the 50 different classes of the Bangla Basic Characters dataset andFig. 2shows the 10 different Bangla Numerals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) added white gaussian noise (awgn) (b) decreased contrast and awgn (c) motion blurred Fig. 5. Training Data for the Character Reconstruction Network. The noisy images of the the basic bangla characters that need to be taken into consideration during classification tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Noisy Samples of images (awgn) (b) Corresponding output Fig. 7. Results from the Character Reconstruction Network features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Using saliency map to reduce dimensionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">OF THE NUMBER OF FEATURES USED</cell></row><row><cell>Noise Type</cell><cell cols="3">Original Ours Basu et. al.[4]</cell></row><row><cell>awgn</cell><cell>1024</cell><cell>208</cell><cell>244</cell></row><row><cell>contrast</cell><cell>1024</cell><cell>203</cell><cell>244</cell></row><row><cell>motion</cell><cell>1024</cell><cell>196</cell><cell>202</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF ERROR(%) ON THE NOISY BANGLA NUMERAL</figDesc><table><row><cell>Noise</cell><cell>Ours</cell><cell></cell><cell>Ours</cell><cell cols="2">Basu et. al. Dropconnect</cell></row><row><cell></cell><cell cols="2">(Saliency)</cell><cell>(w/o Saliency)</cell><cell>[4]</cell><cell>[25]</cell></row><row><cell>awgn</cell><cell>4.54</cell><cell></cell><cell>4.92</cell><cell>8.66</cell><cell>8.82</cell></row><row><cell>motion</cell><cell>4.96</cell><cell></cell><cell>5.12</cell><cell>7.34</cell><cell>2.95</cell></row><row><cell>contrast</cell><cell>7.15</cell><cell></cell><cell>7.4</cell><cell>12.69</cell><cell>14.21</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell></row><row><cell cols="6">CLASSIFICATION ERROR(%) USING DROPCONNECT NETWORK AFTER</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">NOISE REMOVAL</cell></row><row><cell></cell><cell>Noise</cell><cell cols="4">Bangla Numeral Bangla Basic Characters</cell></row><row><cell></cell><cell>awgn</cell><cell></cell><cell>5.30</cell><cell>25.66</cell></row><row><cell></cell><cell>motion</cell><cell></cell><cell>5.07</cell><cell>24.96</cell></row><row><cell cols="2">contrast</cell><cell></cell><cell>6.94</cell><cell>34.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE IV</cell><cell></cell></row><row><cell cols="6">COMPARISON OF ERROR(%) ON NOISY BANGLA CHARACTERS</cell></row><row><cell>Noise</cell><cell>Ours</cell><cell></cell><cell>Ours</cell><cell>DBN</cell><cell>Dropconnect</cell></row><row><cell></cell><cell cols="4">(Saliency) (w/o Saliency) (Raw Pixels)</cell><cell>[25]</cell></row><row><cell>awgn</cell><cell>23.26</cell><cell></cell><cell>29.36</cell><cell>42.69</cell><cell>38.86</cell></row><row><cell>motion</cell><cell>22.78</cell><cell></cell><cell>25.64</cell><cell>41.20</cell><cell>16.41</cell></row><row><cell>contrast</cell><cell>30.34</cell><cell></cell><cell>41.11</cell><cell>53.37</cell><cell>51.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V RESULTS</head><label>V</label><figDesc>ON THE NOISY BANGLA BASIC CHARACTERS DATASET WITH VARIOUS ARCHITECTURE</figDesc><table><row><cell></cell><cell></cell><cell>Error (%)</cell><cell></cell></row><row><cell>Architecture (Neurons)</cell><cell>awgn</cell><cell cols="2">motion contrast</cell></row><row><cell>100 -100</cell><cell>25.29</cell><cell>24.43</cell><cell>33.63</cell></row><row><cell>200 -200</cell><cell>23.57</cell><cell>23.26</cell><cell>31.90</cell></row><row><cell>300 -300</cell><cell>23.47</cell><cell>22.78</cell><cell>31.16</cell></row><row><cell>400 -400</cell><cell>23.35</cell><cell>23.09</cell><cell>30.71</cell></row><row><cell>500 -500</cell><cell>23.26</cell><cell>23.35</cell><cell>30.82</cell></row><row><cell>1000 -1000</cell><cell>23.28</cell><cell>23.03</cell><cell>30.88</cell></row><row><cell>100 -100 -100</cell><cell>26.81</cell><cell>26.03</cell><cell>39.90</cell></row><row><cell>300 -300 -300</cell><cell>24.08</cell><cell>23.49</cell><cell>32.20</cell></row><row><cell>500 -500 -500</cell><cell>23.28</cell><cell>23.24</cell><cell>30.34</cell></row><row><cell>1000 -1000 -1000</cell><cell>23.49</cell><cell>23.27</cell><cell>31.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">ERROR (%) ON NOISY MNIST</cell><cell></cell></row><row><cell>Noise</cell><cell>Ours (Saliency)</cell><cell>Basu et. al. [4]</cell><cell>Dropconnect (Noisy) [25]</cell><cell>Dropconnect (Reconstructed)</cell></row><row><cell>awgn</cell><cell>2.38</cell><cell>9.93</cell><cell>3.98</cell><cell>2.43</cell></row><row><cell>motion</cell><cell>2.80</cell><cell>2.60</cell><cell>1.42</cell><cell>2.80</cell></row><row><cell>contrast</cell><cell>4.96</cell><cell>7.84</cell><cell>6.76</cell><cell>5.07</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Handwritten numeral databases of indian scripts and multistage recognition of mixed numerals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="444" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Offline recognition of handwritten bangla characters: an efficient twostage approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Parui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="445" to="458" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning sparse feature representations using probabilistic quadtrees and deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gayaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theoretical analysis of deep neural networks for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gayaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks, IJCNN 2016</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="992" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse representation for signal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aviyente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse feature learning for deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comprehensive foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Network</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepsat: a learning framework for satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Nemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems<address><addrLine>Bellevue, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A semiautomated probabilistic framework for treecover delineation from 1-m NAIP imagery using a high-performance computing architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Milesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Votava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dubayah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duncanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saatchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5690" to="5708" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quad tree structures for image compression applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Markas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="707" to="721" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decomposing a window into maximal quadtree blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Aref</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="425" to="439" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Core sampling framework for pixel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2017 -26th International Conference on Artificial Neural Networks</title>
		<meeting><address><addrLine>Alghero, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="617" to="625" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="27" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On classification of distorted images with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01924</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Classifying mnist digits using logistic regression</title>
		<ptr target="http://deeplearning.net/tutorial/logreg.html" />
		<imprint/>
	</monogr>
	<note type="report_type">deeplearning.net</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Le</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
