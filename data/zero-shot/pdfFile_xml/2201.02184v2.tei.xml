<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING AUDIO-VISUAL SPEECH REPRESENTATION BY MASKED MULTIMODAL CLUSTER PREDICTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Shi</surname></persName>
							<email>bshi@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
							<email>wnhsu@fb.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
							<email>kushall@fb.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING AUDIO-VISUAL SPEECH REPRESENTATION BY MASKED MULTIMODAL CLUSTER PREDICTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours) <ref type="bibr" target="#b34">(Makino et al., 2019)</ref>. The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%). Our code and models are available at https://github.com/ facebookresearch/av_hubert * Work done at Meta AI Published as a conference paper at ICLR 2022 including but not limited to keyword spotting in sign language (Albanie et al., 2020), speech enhancement <ref type="bibr" target="#b49">(Xu et al., 2020)</ref> and talking face generation <ref type="bibr" target="#b14">(Chen et al., 2018)</ref>.</p><p>In this paper, we present Audio-Visual Hidden Unit BERT (AV-HuBERT), a multimodal selfsupervised speech representation learning framework. It encodes masked audio and image sequences into audio-visual features via a hybrid ResNet-transformer architecture to predict the predetermined sequence of discrete cluster assignments. The target cluster assignments are initially generated from signal processing-based acoustic features (e.g., MFCC) and iteratively refined using the features learned by the audio-visual encoder via k-means clustering. AV-HuBERT simultaneously captures linguistic and phonetic information for unmasked regions from both the lipmovement and audio streams into its latent representations, then encodes their long-range temporal relationships to solve the masked-prediction task.</p><p>The contextualized representations learned by AV-HuBERT show excellent transferability to the lipreading task, where only the visual modality is available. Pre-training on audio and visual input streams led to substantially better results than only visual input. In the low-resource setup using only 30 hours of labeled data from LRS3 (Afouras et al., 2018b), our model achieves a lip-reading WER of 32.5%, outperforming the previous state-of-the-art model (33.6%) trained on 31,000 hours of transcribed videos <ref type="bibr" target="#b34">(Makino et al., 2019)</ref>. Using the complete 433 hours from LRS3 further reduces WER to 28.6%. We further show AV-HuBERT and self-training are complementary to each other: combining both sets a new lip-reading WER record of 26.9%. In addition, we show that the multimodal clusters derived from AV-HuBERT can be used to pre-train a HuBERT model for audio-based speech recognition, outperforming the previous state-of-the-art model (2.3%) and the unimodal HuBERT pre-trained on audio clusters (1.5%) by a large margin (1.3%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The strong correlation between video modalities provides an effective means for self-supervised representation learning on videos, which has been explored in many prior works and is still an active research area. This work draws inspiration from two lines of previous research:</p><p>Multimodal general video representation focuses on learning self-supervised audio-visual representations of general videos to solve high-level semantic tasks, e.g., action recognition and audio event detection <ref type="bibr" target="#b7">(Arandjelovi? &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b11">Bruno et al., 2018;</ref> Morgado et al., 2021;<ref type="bibr" target="#b15">Chen et al., 2020;</ref> Lee et al., 2021). Owens &amp; Efros <ref type="formula">(2018)</ref> learns a multimodal network to predict whether the audio and visual streams of a video are temporally synchronized while Pham et al. (2019) applies a cyclic translation between different modalities. Piergiovanni et al. (2020) learns the visual representation through a multi-tasking framework that incorporates a series of pretext tasks such as reconstruction and temporal ordering prediction. Sharing our work's inspiration of DeepClustering (Caron et al., 2018), XDC (Alwassel et al., 2020) and AV-BERT (Chan et al., 2021) learn cross-modal representations through predicting cross-modal or cluster assignments.</p><p>In contrast to XDC, AV-HuBERT is trained with a BERT-like masked prediction loss, which forces the model to learn the structure within the multimodal input and was shown in <ref type="bibr" target="#b25">Hsu et al. (2021c)</ref> to be more resilient to bad cluster assignments compared to unmasked cluster prediction. On the other hand, AV-BERT focuses on learning utterance-level multimodal environment embeddings that serves as the global context for ASR, while our objective is to learn frame-level audio-visual speech representations and pre-train a model that can be fine-tuned for downstream tasks with either modality. Semi-and self-supervised audio-visual speech representation learning focuses on improving lip-reading with untranscribed audio-visual speech data. To solve isolated visual word recognition, Chung et al. (2020) learns visual embeddings using a contrastive loss based on audio-visual synchronization. Ma et al. (2021a) learns visual speech representations by minimizing the distance between its latent features and off-the-shelf audio embeddings. Using an external supervised ASR to transcribe unlabeled audio, Afouras et al. (2020) trains their lip-reading model on the augmented labeled and pseudo-labeled data. Unlike Ma et al. (2021a) and Afouras et al. ( <ref type="formula">2020)</ref>, our model is trained from scratch and encouraged to learn contextualized representations with a masked prediction task. Moreover, our method does not rely on any external pre-trained models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human perception of speech is intrinsically multimodal, involving audition and vision. The speech production is accompanied by the movement of lips and teeth, which can be visually interpreted to understand speech. Visual cues of speech not only play an essential role in language learning for pre-lingual children <ref type="bibr" target="#b37">(Meltzoff &amp; Moore, 1977;</ref><ref type="bibr" target="#b19">Davies et al., 2008)</ref>, but also improve speech understanding in noisy environment <ref type="bibr" target="#b48">(Sumby &amp; Pollack, 1954)</ref> and provide patients of speech impairment with means of communication. Furthermore, perceptual studies <ref type="bibr" target="#b36">(McGurk &amp; MacDonald, 1976)</ref> have shown that such visual cues can alter the perceived sound.</p><p>For machine learning models, the tight coupling between audio and visual lip movement information emerges as a natural source for supervision to learn speech representations, which has not been extensively utilized yet in the self-supervised speech representation learning literature. Recent successful representation learning frameworks for speech (e.g., APC <ref type="bibr" target="#b18">(Chung et al., 2019)</ref>, <ref type="bibr">CPC (Oord et al., 2018;</ref><ref type="bibr" target="#b26">Kharitonov et al., 2021)</ref>, wav2vec 2.0 <ref type="bibr" target="#b9">(Baevski et al., 2020;</ref><ref type="bibr" target="#b24">Hsu et al., 2021b)</ref>, De-CoAR2.0 <ref type="bibr" target="#b31">(Ling &amp; Liu, 2020)</ref>, HuBERT <ref type="bibr" target="#b25">(Hsu et al., 2021c;</ref><ref type="bibr">a)</ref>) are mostly built entirely on audio. The fundamental research question addressed in this paper is whether a self-supervised audio-visual speech representation learned from the lip movement information, alongside the audio signal in video recordings, captures cross-modal correlations and improves downstream performance for visual speech recognition (i.e., lip reading) and automatic speech recognition (ASR) tasks. Existing ML models for lip-reading rely heavily on text transcriptions to achieve an acceptable level of accuracy. The state-of-the-art lip-reading model <ref type="bibr" target="#b34">(Makino et al., 2019)</ref> requires 31K hours of transcribed video data for training. Such large amounts of labeled data are expensive and hard to obtain for most of the world's 7,000 languages. The benefit from a robust visual speech representation learning framework goes beyond lip-reading. Additionally, it can benefit a vast range of applications, 3 METHOD 3.1 PRELIMINARY: AUDIO HUBERT Our research builds on Audio HuBERT <ref type="bibr" target="#b23">(Hsu et al., 2021a)</ref> which is a self-supervised learning framework for speech and audio. It alternates between two steps: feature clustering and masked prediction. In the first step, a discrete latent variable model (e.g., k-means) is applied to a sequence of acoustic frames A 1:T producing a sequence of frame-level assignments z a 1:T . Clusters of signalprocessing-based acoustic features, e.g., Mel-frequency cepstral coefficients (MFCC), exhibit nontrivial correlations with the inherent acoustic units of speech inputs. Using (A 1:T , z a 1:T ) pairs, the second step learns new feature representations by minimizing a masked prediction loss, similar to masked language modeling in BERT <ref type="bibr" target="#b20">(Devlin et al., 2019)</ref>. The pressure to predict cluster assignments of masked audio regions forces the model to learn good local acoustic representations for unmasked regions and long-range temporal dependencies between latent features. Repeating these two steps improves the cluster quality and consequently the quality of the learned representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SINGLE-MODAL &amp; CROSS-MODAL VISUAL HUBERT</head><p>Single-modal Visual HuBERT: The most na?ve way to extend HuBERT to the visual domain is by generating targets using visual features. Formally, given an image sequence I 1:T , we first cluster the image features into a sequence of discrete units z i 1:T via k-means: z i t = k-means(G(I t )) ? {1, 2, ..., V }, where G is an visual feature extractor and V is the codebook size. The cluster assignments z i 1:T serve as the prediction targets of the model. Initially, G can be an engineered image feature extractor such as Histogram of Oriented Gradients (HoG), analogous to MFCC in audio HuBERT. The intermediate layers of the HuBERT model are used as G in later iterations.</p><p>To perform the masked prediction task, the model first encodes I 1:T using a ResNet into an intermediate visual feature sequence f v 1:T , which is then corrupted intof v 1:T via a binary mask M . Specifically, ?t ? M ,f v t is replaced with a learned masked embedding. We adopt the same strategy in HuBERT to generate span masks. The masked visual featuresf v 1:T are encoded into a sequence of contextualized features e 1:T via a transformer encoder followed by a linear projection layer. The loss is computed over the masked regions and optionally over unmasked ones (when ? ? 0):</p><formula xml:id="formula_0">p t = Softmax(We t + b), 1 ? t ? T L = ? t?M log p t (z i t ) ? ? t ?M log p t (z i t ) (1) Where (W ? R d?V , b ? R V )</formula><p>are parameters of the projection layer which maps features into logits predicting the cluster assignments.</p><p>Cross-modal Visual HuBERT: The single-modal visual HuBERT aims to learn visual speech representation through gradually refined image features. However, it does not employ the audio stream of the video. Presumably, audio features, e.g., MFCC or a pre-trained audio HuBERT model, correlate with phones better than vanilla image features (e.g., HoG) do. To this end, we train an audio encoder based on the aligned audio frame sequence A 1:T in parallel to the visual encoder. The iterative training alternates between the two encoders. In each iteration, an audio encoder E a is utilized to generate target cluster assignments z a 1:T . The visual encoder E v is trained subsequently with (I 1:T , z a 1:T ). The z a 1:T is also used to train the next iteration of the audio encoder E a for refinement. The cross-modality visual HuBERT can be seen as modeling visual inputs by distilling knowledge from the audio stream, where z a 1:T represents the audio-side knowledge. We hypothesize that the audio feature is more favorable to speech representation learning than the visual feature, which is validated in the Section E.1. Critical for the lip-reading downstream task, the masked prediction objective used by HuBERT forces the model to capture temporal relationships, which facilitates prediction of homophemes, which are groups of sounds with identical visual shapes (e.g., <ref type="bibr">'p'-'b', 'f'-'v', 'sh'-'ch')</ref> that are impossible to distinguish using a single image frame.  <ref type="figure" target="#fig_1">Figure 1</ref>: Illustration of AV-HuBERT. Masked prediction losses are only computed for the three middle frames, because at least one modality is masked for those frames. See section A for its comparison between single-modal and cross-modal visual HuBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A S K E D F R A M E F R A M E # M ULT IM O DA L C L US T E R ID S C O NT E XT UA L IZ E D AUD IO -VIS UA L R E P R E S E NTAT IO NS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">AUDIO-VISUAL HUBERT</head><p>Our primary model in this work is Audio-Visual HuBERT (AV-HuBERT), shown in figure 1, which is trained iteratively by alternating between feature clustering and masked prediction in a similar way to the Visual HuBERT but with four main improvements:</p><p>Audio-visual input: The AV-HuBERT model consumes both acoustic and image frames for the masked prediction training, which enables better modeling and distillation of the correlations between the two modalities. Specifically, image sequences and acoustic features pass through their light-weight modality-specific encoders to produce intermediate features, which are then fused and fed into a shared backbone transformer encoder to predict masked cluster assignments. The targets are generated from clustering audio features or features extracted from the previous iteration of the AV-HuBERT model. When fine-tuned for lip-reading, we drop the audio input to work solely with the visual input. The input discrepancy is addressed by modality dropout described next.</p><p>Modality dropout: Audio-visual speech recognition models can relate audio input to lexical output more effortlessly than the visual input stream, as observed in the literature <ref type="bibr" target="#b2">(Afouras et al., 2018a;</ref><ref type="bibr" target="#b33">Ma et al., 2021b)</ref>. This causes the audio modality to dominate model decisions. The problem is aggravated in our setting because the target cluster assignments are initially generated from acoustic features. To prevent the model's over-reliance on the audio stream in our joint model, we only use a linear layer to encode acoustic input to force the audio encoder to learn simple features.</p><p>Additionally, before fusing audio and visual inputs into the backbone transformer encoder, dropout is applied to mask the full features of one modality; we refer to it as modality dropout. With a probability p m , both modalities are used as input. When only one modality is used, the audio stream is selected with a probability of p a . Formally, given the encoded audio and visual feature sequence f a 1:T and f v 1:T , equation 2 shows feature fusion equipped with modality dropout:</p><formula xml:id="formula_1">f av t = ? ? ? concat(f a t , f v t ) with p m concat(f a t , 0) with (1 ? p m )p a concat(0, f v t ) with (1 ? p m )(1 ? p a )<label>(2)</label></formula><p>where concat denotes channel-wise concatenation. Note that modality drop out is applied at the sequence level instead of at the frame-level, which effectively tasks AV-HuBERT to perform masked prediction with visual-only, audio-only, or audio-visual input. Modality dropout prevents the model from ignoring video input and encourages the model to produce the prediction regardless of what modalities are used as input. Furthermore, since the fine-tuning and inference phases use the visual stream alone (no audio input), this modality dropout mechanism bridges the gap between pretraining (multimodal) and fine-tuning/inference (single-modality). A similar dropout mechanism is used in prior work <ref type="bibr" target="#b50">(Zhang et al., 2019a;</ref><ref type="bibr" target="#b34">Makino et al., 2019;</ref><ref type="bibr" target="#b39">Neverova et al., 2014;</ref><ref type="bibr" target="#b1">Abdelaziz et al., 2020)</ref> to increase the robustness in multi-modal settings. We verify the modality dropout effectiveness in Section D.</p><p>Audio-visual clustering: One benefit of pre-training on both modalities is the ability to generate multimodal cluster assignments that serve as target labels for the masked prediction task of the next iteration. In contrast to the Cross-modal Visual HuBERT where targets are generated from audio-based features or a prior Audio HuBERT model, the targets for AV-HuBERT are naturally multimodal after the first iteration. Lip movement sequences provide complementary information to the audio stream. Combining both modalities produces cluster assignments of higher quality for AV-HuBERT, as shown in Section E.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masking by substitution:</head><p>We propose a novel masking strategy for AV-HuBERT that masks segments in the visual stream by substituting them with random segments from the same video. More formally, given an input video I v 1:T , an imposter video I v,f 1:T f and a mask consisting of n intervals</p><formula xml:id="formula_2">M = {(s i , t i )} 1?i?n , we corrupted I v 1:T into? v 1:T by setting: I v si:ti = I v,f pi:pi+ti?si , ?1 ? i ? n<label>(3)</label></formula><p>where p i is a sampled integer offset from the interval [0, T f ? t i + s i ]. Now, to solve the task, the model needs to first identify the fake frames and then infer the labels belonging to the original frames. Since the "filled-in" segments are from real video segments and temporally smooth, the fake segment detection sub-task becomes less trivial compared to when using vanilla masking or substitution with non-consecutive frames. We show an ablation confirming the advantage of the proposed masking strategy in Section D.</p><p>The audio and visual segments are masked independently using two different masking probabilities m a and m v . We hypothesize that the difficulty of the masked prediction task differs for each modality: inferring the masked targets given the audio stream is more straightforward than using the lip movement stream. Setting a high masking probability for acoustic frames is essential to help the whole model capture the language characteristics. On the contrary, setting a high masking probability for the visual input provide the model with more imposter segments than the original ones, hurting its ability to learn meaningful features (studied in Section D of the appendix). Given the output probability p 1:T and target cluster assignments z 1:T , the AV-HuBERT pre-training loss is:</p><formula xml:id="formula_3">L = ? t?M a ?M v log p t (z t ) ? ? t ?M a ?M v log p t (z t )<label>(4)</label></formula><p>where M a and M v denotes the frames that are masked for the audio and the visual stream. ? is a hyperparameter weighting the contribution of the unmasked regions in the overall objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">FINE-TUNING</head><p>The proposed pre-training approaches can be fine-tuned for visual speech recognition using any sequence classification loss. In this paper, we focus on fine-tuning with the connectionist temporal classification (CTC) <ref type="bibr" target="#b22">(Graves et al., 2006)</ref> and attention-based sequence-to-sequence cross-entropy loss <ref type="bibr" target="#b10">(Bahdanau et al., 2016</ref>) (S2S for brevity), which are the most popular choices. Assume that the feature sequence output of our pre-trained model is e 1:T and the ground-truth transcription is w = w 1 , w 2 , ..., w s . For CTC, a projection layer is used to map the visual feature sequence into the output probabilities:</p><formula xml:id="formula_4">p t = Softmax(W f t e t + b f t ), where W f t ? R d?(U +1) , b f t ? R U +1</formula><p>and U is the output vocabulary size (+1: plus blank symbol). The model is trained with CTC loss: L ctc = ? log ??B ?1 (w) p(?|e 1:T ), where B maps an alignment sequence ? to w. For S2S, a tranformer decoder is appended to the pre-trained encoder to autoregressively decode the feature sequence e 1:T into target probabilities p(w t |w 1:t?1 , e 1:T ). The whole model is trained with cross entropy loss: L s2s = ? s t=1 log p(w t |w 1:t?1 , e 1:T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SETUP</head><p>We conduct experiments on two datasets: LRS3 <ref type="bibr" target="#b3">(Afouras et al., 2018b)</ref> with 433 hours of transcribed English videos and VoxCeleb2 <ref type="bibr" target="#b16">(Chung et al., 2018)</ref> with 2442 hours of unlabeled multilingual videos. We only use the English portion of VoxCeleb2, which amounts to 1,326 hours of content. The inputs to our backbone model are lip Regions-Of-Interest (ROIs) for the visual stream and log filterbank energy feature for the audio stream. The image encoder is a modified ResNet-18, which has been used in prior work <ref type="bibr" target="#b33">(Ma et al., 2021b;</ref><ref type="bibr" target="#b35">Martinez et al., 2020;</ref><ref type="bibr" target="#b47">Stafylakis &amp; Tzimiropoulos, 2017)</ref>. The audio encoder is simply a linear projection layer. We consider two model configurations: BASE with 12 transformer blocks and LARGE with 24 transformer blocks. For BASE and LARGE, the embedding dimension/feed-forward dimension/attention heads in each transformer block are 768/3072/12 and 1024/4096/16 respectively. The number of parameters in BASE and LARGE are 103M and 325M respectively. ? in equation 4 is set to 0.</p><p>The model uses five iterations of feature clustering and masked prediction during pre-training. See Section B.4, E for details on clustering. For fine-tuning, we use phone targets for the CTC loss and unigram-based subword units <ref type="bibr" target="#b29">(Kudo, 2018)</ref> for the S2S loss. For decoding the CTC-trained model, we use a 4-gram language model trained on LRS3 training text. For the S2S fine-tuned model, we rely only on its own decoder module to incorporate language information, with no external language model employed during inference. To further show the complementary relationship between AV-HuBERT and existing approaches of using unlabeled data, we also experiment on combining AV-HuBERT with self-training. Specifically, we generate pseudo labels for unlabeled data using a finetuned HuBERT, and fine-tune the pre-trained AV-HuBERT model with the combination of pseudolabeled videos and original labeled videos. Note that no additional data is used when combined with self-training. More details about the used datasets, data pre-processing, and model training are in Section B. <ref type="table" target="#tab_1">Table 1</ref> compares the performance of our AV-HuBERT pre-training approach to previously published supervised, semi-supervised, and self-supervised lip-reading systems using different amounts of labeled and unlabeled data. Since the CTC and S2S fine-tuning approaches have similar trends, only S2S results are shown in table 1. Complete results of CTC fine-tuning are in <ref type="table" target="#tab_1">Table C.4. 1</ref> Using 1,759 hours unlabeled data for pre-training and only 30 hours of labeled data for fine-tuning, AV-HuBERT-LARGE outperforms all the prior lip-reading models, including the model in <ref type="bibr" target="#b34">(Makino et al., 2019)</ref> which is trained with 1000 times more labeled data. Fine-tuning with the whole training set of LRS3 further reduces WER. Combining our method and self-training achieves a new SOTA result with only 7% of the data used for training the model in <ref type="bibr" target="#b34">Makino et al. (2019)</ref>. Furthermore, it shows that AV-HuBERT and self-training are complementary to each other. Note the overall gain is attributed mainly to AV-HuBERT as self-training alone leads to much worse performance (&gt; 50% WER). More details can be found in section C.3. Many prior models pre-train their visual front-end, e.g., ResNet-18, using word-level annotated lip-reading videos, which is costly to collect since they require word boundary information. In contrast to these models, our models are fully pre-trained from scratch using the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MAIN RESULT</head><p>Compared to the semi-supervised approach Jasper-KD , which transcribed 334 hours of the English data in VoxCeleb2 using a pre-trained ASR system, 2 our best model achieves 29% lower absolute WER benefiting from VoxCeleb2 for pre-training. Even when limiting our model to the LRS3 data for pre-training and fine-tuning, our model surpasses their semi-supervised system by 18%. Compared to LiRA <ref type="bibr" target="#b32">(Ma et al., 2021a)</ref>, a recently proposed self-supervised model for lip-reading, AV-HuBERT-BASE provides 17.5% lower absolute WER on average for low-resource and high-resource settings. The implementation details of LiRA are provided in Section B.5.</p><p>With the same network architecture, our pre-training approach significantly reduces WER compared to training from scratch, in both low-resource (92.3% ? 32.5%, LARGE) and high-resource (62.3% ? 28.6%, LARGE) settings. A qualitative view of the improvement can be found in Section F. Additionally, we notice that our AV-HuBERT pre-training helps under a fully supervised setting. Using the LRS3 data only (433 hours), pre-training followed by fine-tuning (41.6%, LARGE) outperforms training a lip-reading model from scratch (62.3%, LARGE) to predict the output text. AV-HuBERT, pre-trained on video-audio pairs, learns better fine-grained visual representation than the scratch model trained on video-text pairs. The benefits of AV-HuBERT pre-training in various labeled data setups can be found in Section C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">AV-HUBERT VS. VISUAL HUBERT</head><p>We compare AV-HuBERT against a suite of alternatives, including the Single-modal and Crossmodal Visual HuBERT in <ref type="table" target="#tab_2">Table 2</ref>. All the models are BASE pretrained on 433 hours of unlabeled data and fine-tuned on 30 hours of labeled data. For this comparison, we use CTC fine-tuning due to its computational efficiency and given their similarity in results trends to S2S. </p><formula xml:id="formula_5">MFCC A/MFCC -&gt; A A/MFCC -&gt; A ... V/MFCC -&gt; A V/MFCC -&gt; A ... AV/MFCC -&gt; A AV/MFCC -&gt; A ... AV/MFCC -&gt; AV AV/MFCC -&gt; AV ... V/MFCC -&gt; V V/MFCC -&gt; V ... V/HoG -&gt; V V/HoG -&gt; V ..</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. HoG</head><p>Iter-1</p><p>Iter-2</p><p>As shown in table 2, target cluster assignments driven from a single modality, either audio or visual, do not provide much WER reduction beyond the second iteration. Training the AV-HuBERT model using targets driven from audio-visual features keeps improving for more iterations and achieves better final performance. As mentioned in Section 3, visual information is complementary to audio input; hence, using both produces higher quality target clusters. Measuring the quality of different target labels using cluster quality metrics such as purity and NMI shows the same trend observed from lip-reading WER (See appendix E.1).</p><p>Fixing target labels used for the masked-prediction pre-training, AV-HuBERT (AV/MFCC?A) outperforms the Cross-modal Visual HuBERT (V/MFCC?A) by a large margin. AV-HuBERT effectively transfers knowledge from audio into the visual encoder and the backbone transformer model to benefit visual-only fine-tuning and inference. In contrast to AV-HuBERT, iterative pre-training brings much smaller gains to single-modality visual HuBERT ("V/MFCC?V", "V/HoG?V").</p><p>Starting with audio features is critical for learning effective target labels for the masked-prediction task. Phonetic information, which is crucial for lip-reading, is primarily present in the audio stream. All the models considered so far are based on audio feature MFCC clustering in their initial iteration. As is shown in the "V/HoG?V" row, using hand-engineered visual features provides a lousy starting point for iterative learning. Visual features such as HoG mainly incorporate low-level visual cues such as edges and luminance, which is irrelevant to the downstream recognition task. Using features of a higher correlation with phonetic units are more likely to benefit the final lip-reading model. Indeed, clustering MFCC features show much higher purity (30.3%) than HoG clusters (16.4%) if one considers phone labels as the target units, as shown in <ref type="table" target="#tab_9">Table E</ref>.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">MULTILINGUAL VS. MONOLINGUAL</head><p>Since the correlation between lip movements and the produced sound is governed by the vocal apparatus that is language-agnostic, the proposed AV-HuBERT can utilize multi-lingual data for such learning. This would be particularly beneficial for low-resource languages. Nevertheless, the masked language modeling aspect in AV-HuBERT is still language-dependent, implying that mixing other languages would increase the language domain discrepancy. To study how these two factors affect AV-HuBERT, we compare using monolingual English data only versus multilingual videos in the pre-training phase. In particular, we vary the amount of English data we use in pre-training while the number of non-English utterances is fixed to 1,116 hours in all the settings. For simplicity, we train AV-HuBERT (BASE) for one iteration with MFCC clusters and fine-tune it with CTC using 30 hours of labeled data. As is shown in table 3, using non-English data in pre-training significantly reduces WER when there are no or very little English data in pre-training (? 100 hours). As we increase the amount of English data, the gain diminishes because the out-of-domain effect brought by non-English data outweighs the benefit of the overall increase in pre-training data. Using the whole English data only for pre-training is better than combining it with other languages (59.9% vs. 64.3%). Training with 5 iterations leads to similar results (47.3% vs. 48.9%). This experiment highlights the importance of the domain match between pre-training and fine-tuning data. For zero/low-resource settings, merging data from other languages in pre-training benefits the downstream task. When the unlabeled data from the target language is abundant, limiting the pre-training data to one language is beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">ASR PERFORMANCE</head><p>The multimodal clusters produced by AV-HuBERT, which have higher quality than audio-HuBERT targets, can also benefit speech pre-training. To test our hypothesis, we trained an audio-HuBERT, with only audio input during the masked-prediction pre-training, for one iteration with cluster assignments driven from AV-HuBERT features. We also pre-trained an audio-HuBERT from scratch using clusters driven from the MFCC features for three iterations. The two pre-trained models are evaluated on a downstream ASR task. <ref type="table" target="#tab_4">Table 4</ref> shows the performance of different models fine-tuned on the ASR task. We only include the performance of S2S fine-tuning for our models as it consistently outperforms the CTC fine-tuning. An audio-HuBERT pre-trained using targets generated by AV-HuBERT features outperforms the vanilla audio-HuBERT in low-resource (30h) and high-resource settings (433h) fine-tuning settings across different model architectures. With the same amount of labeled data, our best model (1.4%) outperforms the prior SOTA (2.3%) even without an external language model during inference.</p><p>Given that the AV-HuBERT model utilizes both modalities at its input, it can be fine-tuned, in principle, for the ASR downstream task. In practice, we notice pre-training an audio-HuBERT with audio-visual cluster leads to better ASR performance (3.8%) than a pre-trained AV-HuBERT (4.6%), potentially due to its hyperparameters being selected based on lip reading rather than ASR. In fact, audio-HuBERT can be treated as a special case of AV-HuBERT with p m = 0, p a = 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We presented multiple pre-training models for visual speech recognition. Our AV-HuBERT model leverages the strong correlation between the audio and lip movement streams for self-supervised audio-visual speech representation learning. Our pre-training approaches iteratively alternate between feature clustering and learning new features through a masked-prediction loss. The AV-HuBERT model consumes masked image and audio frames to predict target cluster assignments. The targets are initially generated from MFCC features and gradually refined through iterative training. Experiments on visual speech recognition show that AV-HuBERT achieves SOTA using 433 hours of text transcriptions, two orders of magnitude less than the 31,000 hours of labeled data used in the prior best approach. When using only one-thousandth of labeled data, the lip-reading performance outperforms the prior SOTA by more than 10% (relative). AV-HuBERT also improves the representation for the ASR downstream task. An audio-HuBERT model trained with targets generated by an AV-HuBERT model shows superior performance, achieving the SOTA in the audio-based speech recognition in the LRS3 dataset. As future work, AV-HuBERT can be applied for multilingual lip-reading in low-resource languages. Additionally, our approach can be extended to other applications of visual speech representation, such as speech enhancement and generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICAL STATEMENT</head><p>All the data used in this paper are publicly available and are used under the following three licenses: the TED terms of use, the Creative Commons BY-NC-ND 4.0 license and Creative Commons Attribution 4.0 International License. Through spot-checking, we find the datasets are gender balanced and cover a wide range of races and ages. However, the distribution of speakers in the data may not be representative of the global human population. Please be cautious of unintended societal, gender, racial and other biases caused by the fact. To maintain anonymity, only the mouth area of a speaker is visualized wherever used in the paper. The proposed method can be applied in several areas including security and crime investigations. However it can also be used for malicious purposes such as surveillance and wiretapping. We are committed to distributing our code and model carefully, with special attention to any potential security and privacy concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>Our code and models are publicly available. In the meantime, we include as many implementation details as we can in the paper.</p><p>A MODEL ILLUSTRATION <ref type="figure">Figure</ref>  In the original dataset, the training data is split into two partitions: pretrain (403 hours) and trainval (30 hours). Both parts are transcribed at the sentence level and come from the same source as the test set. The pretrain set differs from trainval in that the duration of its video clips are of a much wider range and can be shorter or longer than a full sentence. In the low-resource setup, we only use trainval as the labeled data. As no official development set is provided, we randomly select 1,200 sequences from trainval as the validation set (about 1 hour) for early stopping and hyperparameter tuning.</p><p>VoxCeleb2 <ref type="bibr" target="#b16">(Chung et al., 2018)</ref> is originally created for multilingual audio-visual speaker recognition and it contains over 2,442 hours of utterances of over 6,000 speakers extracted from YouTube videos. The dataset naturally serves our purpose as it does not contain ground-truth transcriptions. The VoxCeleb2 has substantial domain discrepency to the LRS3 data as its utterances are from multiple languages and includes videos in a larger variety of domains including interviews, talks, excerpts under indoor and outdoor environments. In VoxCeleb2, by default we only use the English portion for pre-training. As no ground-truth language label is given in the VoxCeleb2, we use a simple heuristic to choose the English samples. Specifically, we use an off-the-shelf character-based ASR model <ref type="bibr" target="#b23">(Hsu et al., 2021a)</ref> trained on Librispeech which achieves 1.9%/3.5% WER on clean/other test set. We run greedy decoding on the VoxCeleb2 and use the proportion of valid English words to determine if a target utterance is English or not. An utterance is only selected if the proportion of valid English words is higher than 60%. The total amount of unlabeled data after filtering is 1,326 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 DATA PREPROCESSING</head><p>For each video clip, we detect the 68 facial keypoints using dlib <ref type="bibr" target="#b27">(King, 2009</ref>) and align each frame to a reference face frame via affine transformation. We crop a 96?96 region-of-interest (ROI) centered on the mouth. Each image frame is converted to grayscale. We randomly cropped 88 ? 88 from the whole ROI and randomly flipped it horizontally with probablity of 0.5 during training. At test time, 88 ? 88 ROI is center cropped and does not go through horizontal flipping. The preprocessing steps remain same as prior works in lip reading <ref type="bibr" target="#b35">(Martinez et al., 2020;</ref><ref type="bibr" target="#b33">Ma et al., 2021b)</ref>. For the associated audio, we extract the 26-dimensional log filterbank energy feature at a stride of 10 ms from the raw waveform and use it as input to the model. As the image frames are sampled at 25Hz, we stack the 4 neighboring acoustic frames to synchronize the two modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 AV-HUBERT MODEL ARCHITECTURE</head><p>In the modified ResNet-18 <ref type="bibr" target="#b33">(Ma et al., 2021b;</ref><ref type="bibr" target="#b35">Martinez et al., 2020;</ref><ref type="bibr" target="#b47">Stafylakis &amp; Tzimiropoulos, 2017)</ref>, the first convolutional layer is substituted by a 3D convolutional layer with kernel size 5 ? 7 ? 7. The visual feature tensor is flattened into a single-dimensional vector through a 2D average pooling layer in the end. We use one linear projection layer as the audio encoding module. The acoustic features are normalized by per-frame statistics before feeding into the network <ref type="bibr" target="#b8">(Ba et al., 2016)</ref>. We use a dropout of p = 0.1 after the self-attention block within each transformer layer, and each transformer layer is dropped <ref type="bibr" target="#b21">(Fan et al., 2020)</ref> at a rate of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 TRAINING AND INFERENCE</head><p>Pretraining Our models are implemented with fairseq <ref type="bibr" target="#b41">(Ott et al., 2019)</ref>. The whole network is randomly initialized before pre-training. In pre-training, the model is trained for five iterations in total. For the initial iteration, we generate the targets by running k-means clustering algorithm on 39-dimensional MFCC feature (13 coefficients with its first-and second-order derivatives) extracted from raw audio waveform. We set both p m and p a to 0.5 for modality dropout at training time. To extract features for clustering, both modalities are used. We adopt the strategy used in wav2vec 2.0 <ref type="bibr" target="#b9">(Baevski et al., 2020)</ref> to generate masks, where p% of all frames are randomly selected as start and subsequent l frames are masked. In iteration 1-4, we mask the fused features and set p/l to be 8/10 respectively as we observe such practice generates higher quality cluster assignments (see section E.2). In the last iteration, we set p/l to be 6/5 for video and 8/10 for audio (see section D).</p><p>We train the model with Adam <ref type="figure" target="#fig_1">(Kingma &amp; Ba, 2015)</ref>, warming up the learning rate for the first 8% of updates to a peak of 0.002 and then linearly decay it. Videos are batched together to not exceed 1,000 image frames (40 seconds) per GPU. Both BASE and LARGE models are updated for 400K and 600K steps at each iteration, respectively in 433h/1759h unlabeled settings. We train on 32 and 64 V100-GPUs for BASE and LARGE. On average, each iteration takes ? 2.0/3.0 days for BASE and ? 2.4/3.6 days for LARGE in using 433h/1759h unlabeled data for pre-training.</p><p>Fine-tuning After pre-training, we fine-tune the AV-HuBERT model on labeled (video, text) pairs. The audio encoder is removed and its output is replaced by a zero-vector. In CTC fine-tuning, a randomly initialized projection layer is added on top of the transformer to map features into phonemes. The lexicon is constructed with CMUDict (cmu). In fine-tuning with S2S, we use a 6-layer/9-layer transformer decoder on BASE and LARGE model to decode features into unigram-based subword units <ref type="bibr" target="#b29">(Kudo, 2018)</ref>. The vocabulary size in CTC and S2S are 46 and 1000 respectively.</p><p>In CTC, the pre-trained model is updated from the initial iteration without any freezing. The model is fine-tuned for 30K/100K steps respectively in 30h/433h setting. In S2S, the pre-trained model (i.e., encoder) is frozen for the first N % updates . N is 100 and 50 for 30h and 433h labeled setting respectively. The entire model is trained for 18K/45K steps in the 30h/433h setting. Both models are trained with Adam, with the learning rate being warmed up for the first P % of updates to a peak of 0.001 and linearly decayed. P is tuned among {10, 30, 50}. All hyperparamters are tuned on the validation set.</p><p>Decoding For CTC, we use a 4-gram language model trained on text data in the LRS3 training set. The perplexity of the 4-gram LM on test set is 110.5. No language model is used for S2S decoding. For CTC, we tune the beam width among {5, 10, 20, 50, 100, 150}, the language model weight among {0, 1, 2, 4, 8} and word insertion penalty among {?4, ?2, ?1, 0}. For S2S, The beam width and length penalty are tuned among {5, 10, 20, 50} and {0, ?1}. The tuning is done on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Training</head><p>We apply self-training on LARGE AV-HuBERT. Specifically, the fine-tuned LARGE HuBERT model(A/MFCC?AV, <ref type="table" target="#tab_4">Table 4</ref>) is used to assign pseudo-labels to the unlabeled audiovisual data. In 30h/433h setting, the amount of data for fine-tuning A/MFCC?AV are 30h and 433h respectively. The pre-trained AV-HuBERT LARGE is fine-tuned with the pseudo-labeled videos and videos with ground-truth text labels (30h/433h). Note the data used here is exactly same with the case of using AV-HuBERT only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 LIRA IMPLEMENTATION</head><p>We re-implemented the LiRA <ref type="bibr" target="#b32">(Ma et al., 2021a)</ref> training objective in our framework, as there does not exist publicly available implementations and we aim to focus the comparison on the pre-training objective rather than the architectural difference. We use the same backbone architecture as the BASE AV-Hubert except the output layer being a linear project layer with an output dimension of 256. The 256-dimensional frame PASE+ feature is extracted from the audio with its official implementation in <ref type="bibr" target="#b45">(Ravanelli et al., 2020)</ref>. The original PASE+ feature is downsampled to 25Hz for synchronization with the visual stream. The pre-trained model is fine-tuned in both CTC and S2S. The optimizer and learning rate schedule in pre-training, hyperparameter search in fine-tuning and decoding remain the same as AV-HUBERT. Note with our implementation, the WER is reduced by 23% (94.3% ? 71.9%) using LiRA when the percentage of labeled data is 6.9% (30h labeled, 433h in total), while <ref type="bibr" target="#b32">Ma et al. (2021a)</ref> achieves ? 10% improvement in a similar setting.</p><p>C ADDITIONAL LIP-READING RESULTS C.1 AMOUNT OF LABELED DATA <ref type="table" target="#tab_6">Table C</ref>.1 shows the effect of pre-training on different amount of labeled data for fine-tuning. We use 433 hours of unlabeled data (LRS3 only) and randomly selected 1, 10 and 100 hours of labeled data for fine-tuning. Overall pre-training brings large and consistent gains across different amount of labeled data. Specifically CTC-based fine-tuning outperforms S2S-based fine-tuning in lowresource settings <ref type="figure" target="#fig_1">(1-hour and 10-hour)</ref>. The larger number of parameters as well as the lack of a language model for decoding makes S2S model more likely to overfit especially when the amount of fine-tuning data is small.  <ref type="table" target="#tab_6">Table C</ref>.3 shows the performance of only applying self-training. The WER of a self-training only model is significantly higher than AV-HuBERT and self-trained AV-HuBERT, which suggests that the gain of the combined approach is primarily from AV-HuBERT.</p><p>C.4 FULL RESULTS WITH CTC FINE-TUNING <ref type="table" target="#tab_6">Table C</ref>.4 shows the full results on LRS3, which includes the CTC fine-tuning performance for all the models we implemented. In general, the conclusions we draw from S2S (e.g., the benefits of our pre-training approach in different settings, the improvement over LiRA) in section 4.2 holds for CTC as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ABLATION STUDIES</head><p>The ablation studies in this section are done in the last iteration of the AV-HuBERT, pre-trained with 433 hours of unlabeled data. The model is fine-tuned with 30 hours of labeled data using CTC. Masking Strategy In the first part of table D.1, we compare the proposed masking strategy against several alternatives. Feature masking applies span mask at feature level and leads to the worst performance, which is due to the leakage of information to ResNet. Directly masking image sequence with random Gaussian noise or a learned embedding slightly improves the performance by preventing the prior issue. However, those artificial frames also corrupts the raw image sequence and enlarges the domain gap in videos between pre-training and fine-tuning. On the other hand, our proposed method achieves better performance. Specifically, using segments from the same utterance ("Sub, (same, seg)") as the imposter leads to the best result compared to sampling from a different utterance ("Sub, (diff, seg)") or sampling non-consecutive frames from the same utterance ("Sub, (same, frm)"). 3 The filled-in fake images are visually similar to the raw images and substitution with a segment keeps the temporal smoothness making the replaced segment more realistic, which enforces the ResNet to encode more fine-grained visual details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masking probability</head><p>We set two different masking probabilities for audio and visual stream in the last iteration. The probabilities of an acoustic frame and image frame being masked are 0.8 and 0.3 respectively. As is shown in the second part of table D.1, setting masks for audio and visual stream independently is essential because the optimal masking probability for audio and visual stream are different. Audio encoder tends to deteriorate into a simple acoustic feature extractor when mask length is small. On the other hand, long visual mask will lead to the model lacking context to distinguish between fake and real images.</p><p>Modality dropout The third part of table D.1 compares the model performance with and without modality dropout. Randomly dropping audio sequence prevents the model from over-relying on audio for masked prediction and helps the visual representation learning.</p><p>Where to compute prediction loss In the last part of table D.1, we compare the choice of masked prediction vs. prediction. The loss weight on unmasked region does not have a large impact on the fine-tuning performance. This is different from the findings in Audio HuBERT <ref type="bibr" target="#b23">Hsu et al. (2021a)</ref>, where masked prediction leads to much better performance. Given image frames as input, the prediction of cluster assignments, which are mostly determined by the accompanied audio stream, helps encode phonetic information into the visual representation. The task is much less trivial than a single-modal model (audio-HuBERT), where setting non-zero weight on unmasked prediction can easily make the model deteriorate into an acoustic feature extractor. In addition, the high quality of targets in the last iteration also makes such prediction more helpful.</p><p>E ANALYSIS ON CLUSTERING E.1 MEASURING CLUSTERING QUALITY For analysis, we use frame-level phonetic labels as the ground-truth and match its correlation between cluster assignments. The phonetic labels are obtained via forced alignement from a monophone based HMM-GMM ASR model trained on LRS3. In particular, we use clustering purity and Normalize Mutual Information (NMI) as the evaluation metrics. <ref type="table" target="#tab_9">Table E</ref>.1 shows that (1) The quality of cluster assignments is consistent with fine-tuning performance across different models <ref type="formula" target="#formula_1">(2)</ref> Hand-engineered audio feature (MFCC, NMI: 21.5%) has much stronger correlation with phonetic labels than the visual feature (HoG, NMI: 1.6%) (3) Audio-visual clusters (NMI: 44.2%) are of better quality than pure audio-based clusters (NMI: 39.7%). (4) In single-modality visual Hubert (V/HoG?V), feature quality is improved negligibly through iteration training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 FEATURE MASKING PRODUCES BETTER FEATURES FOR CLUSTERING</head><p>In iteration 1-4, we apply the mask in the fused feature. We observe such practice generates targets of higher quality, thus helping future iterations more. <ref type="table" target="#tab_9">Table E</ref>.2 shows a comparison between such two different masking strategies. Input-level masking enhances the learning of visual representation (see table D.1) while produces worse audio-visual feature (NMI: 27.2%). In contrast, the two streams of original input are better aligned in feature-level masking which is consistent with the cluster generation process, thus leading to better audio-visual clusters (NMI: 37.7%).  <ref type="figure">Figure E</ref>.1 shows the clustering quality of features of different layers in different iterations. The cluster assignment quality generally improves with more iterations. In the first iteration, features in the middle layers show higher quality than the other layers. The target for the first iteration (MFCC clusters) is of worse quality, thus later layers that are more correlated with targets do not yield the best cluster. Target quality improves with more training iterations, thus the best feature layer shifts towards the end. Setting a larger number of clusters increases the clustering quality as can be seen from the comparison between "varied clusters" and "2K clusters". In terms of the 12th layer which we choose, the highest NMI (44.2%) is achieved in the last iteration. In addition, more iterations of training improves the overall quality of clusters produced by a model though the highest NMI/purity among all layers does not necessarily increase in later iterations. Therefore, setting a larger number of iterations brings stable gains which are more robust to the index of layer chosen for clustering. It is important as the purity/NMI, whose measurement rely on a supervised model, are not used for hyperparameter search in practice.  <ref type="figure">Figure F</ref>.2 shows the example outputs from different models. Our self-supervised model is the LARGE AV-HUBERT pre-trained with 1,759 hours unlabeled data and fine-tuned with 433 hours labeled data. The baseline model is the supervised baseline trained with 433 hours labeled data. Both models use the S2S criterion for supervised training and have the same number of parameters. Qualitatively, our approach provides transcriptions with much higher quality. The baseline approach confuses among words of similar sound while our model output more semantically sound sentences. <ref type="figure">Figure F</ref>.2 also shows typical errors made by our model. We noticed many errors are on short sentences. This is mainly because lip reading relies heavily on the context for recognition due to the existence of homophomes. Thus the error rates in lip reading are notably higher in short utterances, which differs from ASR, as can be seen from figure F.1. Substitution among words with homophemes <ref type="bibr">('fiction' vs. 'vision' in a.4, 'part' vs. 'bunk' in b.5</ref>) is another source of error made by the model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A.1: Comparison between the proposed AV-HuBERT with single-modal and cross-et al., 2018b) is the largest publicly available sentence-level lip reading dataset to date. It consists of over 400 hours of video, extracted from TED &amp; TEDx talks in English from YouTube.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure E. 1 :</head><label>1</label><figDesc>Quality of feature clusters from different layers across different iterations (BASE, 433 hours unlabeled data). (Iter i, Layer j): cluster quality of layer-j feature of iter-i model. Upper row: 100, 500, 1K, 2K clusters for 4 iterations. Bottom row: 2K clusters for all iterations. Purity/NMI of the initial MFCC clusters: 30.3%/21.5%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure F. 1 :</head><label>1</label><figDesc>WER vs. sentence length for lip reading (left) and ASR (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>WER (%) of our models and prior work on the LRS3 dataset. ?We re-implemented<ref type="bibr" target="#b32">Ma et al. (2021a)</ref> with the same architecture since the author source code was not provided.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Criterion</cell><cell>Labeled iso (hrs)</cell><cell>Labeled utt (hrs)</cell><cell>Unlabeled data (hrs)</cell><cell>WER (%)</cell></row><row><cell></cell><cell cols="2">Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Afouras et al. (2020)</cell><cell>CNN</cell><cell>CTC</cell><cell>157</cell><cell>433</cell><cell>-</cell><cell>68.8</cell></row><row><cell>Zhang et al. (2019b)</cell><cell>CNN</cell><cell>S2S</cell><cell>157</cell><cell>698</cell><cell>-</cell><cell>60.1</cell></row><row><cell>Afouras et al. (2018a)</cell><cell>Transformer</cell><cell>S2S</cell><cell>157</cell><cell>1,362</cell><cell>-</cell><cell>58.9</cell></row><row><cell>Xu et al. (2020)</cell><cell>RNN</cell><cell>S2S</cell><cell>157</cell><cell>433</cell><cell>-</cell><cell>57.8</cell></row><row><cell>Shillingford et al. (2019)</cell><cell>RNN</cell><cell>CTC</cell><cell>-</cell><cell>3,886</cell><cell>-</cell><cell>55.1</cell></row><row><cell>Ma et al. (2021b)</cell><cell>Conformer</cell><cell>CTC+S2S</cell><cell>-</cell><cell>433</cell><cell>-</cell><cell>46.9</cell></row><row><cell>Ma et al. (2021b)</cell><cell>Conformer</cell><cell>CTC+S2S</cell><cell>157</cell><cell>433</cell><cell>-</cell><cell>43.3</cell></row><row><cell>Makino et al. (2019)</cell><cell>RNN</cell><cell>Transducer</cell><cell>-</cell><cell>31,000</cell><cell>-</cell><cell>33.6</cell></row><row><cell></cell><cell cols="3">Semi-Supervised &amp; Self-Supervised</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Afouras et al. (2020)</cell><cell>CNN</cell><cell>CTC</cell><cell>157</cell><cell>433</cell><cell>334</cell><cell>59.8</cell></row><row><cell>Ma et al. (2021a) ?</cell><cell>Transformer-BASE</cell><cell>S2S</cell><cell>--</cell><cell>30 433</cell><cell>433 1,759</cell><cell>71.9 49.6</cell></row><row><cell cols="5">Proposed (Self-Supervised &amp; Self-Supervised + Semi-Supervised)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>-</cell><cell>94.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>433</cell><cell>51.8</cell></row><row><cell></cell><cell>Transformer-BASE</cell><cell>S2S</cell><cell>-</cell><cell>30</cell><cell>1,759</cell><cell>46.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>-</cell><cell>60.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>433</cell><cell>44.0</cell></row><row><cell>AV-HuBERT</cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>1,759</cell><cell>34.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>-</cell><cell>92.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>433</cell><cell>44.8</cell></row><row><cell></cell><cell>Transformer-LARGE</cell><cell>S2S</cell><cell>-</cell><cell>30</cell><cell>1,759</cell><cell>32.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>-</cell><cell>62.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>433</cell><cell>41.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>1,759</cell><cell>28.6</cell></row><row><cell cols="2">AV-HuBERT + Self-Training Transformer-LARGE</cell><cell>S2S</cell><cell>--</cell><cell>30 433</cell><cell>1,759 1,759</cell><cell>28.6 26.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Fine-tuning performance (in WER, %) of AV-HuBERT and visual HuBERT on different target labels. Init: feature in the initial iteration, sub: feature in subsequent iterations. AV: AV-</figDesc><table><row><cell cols="5">HuBERT, V: Visual-HuBERT, A: Audio-HuBERT.</cell><cell></cell></row><row><cell>Model/init?sub</cell><cell>1</cell><cell>2</cell><cell>Iteration 3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="6">AV/MFCC?AV 71.5 63.6 60.9 58.8 58.2</cell></row><row><cell>AV/MFCC?A</cell><cell cols="3">71.5 64.3 63.5</cell><cell>-</cell><cell>-</cell></row><row><cell>V/MFCC?A</cell><cell cols="3">75.4 69.4 69.1</cell><cell>-</cell><cell>-</cell></row><row><cell>V/MFCC?V</cell><cell cols="3">75.4 72.6 72.3</cell><cell>-</cell><cell>-</cell></row><row><cell>V/HoG?V</cell><cell cols="2">80.3 80.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Pre-train on En + 1,116 hr of non-En, WER (%) 70.6 68.4 67.4 66.6 64.3</figDesc><table><row><cell>Hours of En data for Pre-Training</cell><cell>0</cell><cell>100 400 800 1759</cell></row><row><cell>Pre-train on En only, WER (%)</cell><cell cols="2">84.1 77.8 68.9 67.9 59.9</cell></row></table><note>WER (%) with different amounts of unlabeled English utterances in pre-training. Non-En data: 1,116 hours. Labeled data for fine-tuning: 30 hours.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>ASR WER (%) of audio-HuBERT pre-trained with audio-only/audio-visual clusters and their comparison to prior work on the LRS3 dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Criterion</cell><cell>LM</cell><cell>Labeled data (hrs)</cell><cell>Unlabeled data (hrs)</cell><cell>WER (%)</cell></row><row><cell>Afouras et al. (2018a) Afouras et al. (2018a)</cell><cell>Transformer Transformer</cell><cell>Supervised S2S CTC</cell><cell>? ?</cell><cell>1,362 1,362</cell><cell>--</cell><cell>8.3 8.9</cell></row><row><cell>Xu et al. (2020) Ma et al. (2021b)</cell><cell>RNN Conformer</cell><cell>S2S CTC+S2S</cell><cell>-?</cell><cell>433 433</cell><cell>--</cell><cell>7.2 2.3</cell></row><row><cell></cell><cell></cell><cell>Self-Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>433</cell><cell>5.4</cell></row><row><cell></cell><cell>Transformer-Base</cell><cell>S2S</cell><cell>-</cell><cell>30</cell><cell>1,759</cell><cell>5.0</cell></row><row><cell>Hsu et al. (2021a)</cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>1,759</cell><cell>2.4</cell></row><row><cell>(A/MFCC?A)</cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>433</cell><cell>4.5</cell></row><row><cell></cell><cell>Transformer-Large</cell><cell>S2S</cell><cell>-</cell><cell>30</cell><cell>1,759</cell><cell>3.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>1,759</cell><cell>1.5</cell></row><row><cell></cell><cell cols="3">Proposed (Self-Supervised)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>433</cell><cell>4.9</cell></row><row><cell></cell><cell>Transformer-Base</cell><cell>S2S</cell><cell>-</cell><cell>30</cell><cell>1,759</cell><cell>3.8</cell></row><row><cell>A/MFCC?AV</cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>1,759</cell><cell>2.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>433</cell><cell>4.2</cell></row><row><cell></cell><cell>Transformer-Large</cell><cell>S2S</cell><cell>-</cell><cell>30</cell><cell>1,759</cell><cell>2.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>1,759</cell><cell>1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The number of features are clustered to {100, 100, 500, 1000, 2000} respectively for the 5 iterations. See section E.3 for analysis. To save training time, we always use the BASE model to generate clusters and LARGE model is only trained in the 5th iteration.</figDesc><table /><note>For the subsequent iterations, the feature from an intermediate layer of the model in the previous iteration is used for clustering. The layer index (one-based) used for clustering in iteration 1-4 are {9, 12, 12, 12}.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table C</head><label>C</label><figDesc>PERFORMANCE ON SEEN SPEAKERSThe current LRS3 benchmark is under the open-speaker setting, where the speaker identities in training and test set do not overlap. To test the lip reading performance for a fixed set of speakers which is the case for an early versions of LRS3 used before October 2018, we randomly choose 5 groups of utterances from the trainval partition of LRS3 as test set and repeat experiments for each group independently. Each group contains 1322 utterance, which is of the same amount as the original test set. The model we compare is the AV-HUBERT LARGE pre-trained with 1,759 unlabeled data. As is shown in table C.2, the average WER achieved by our model for seen speakers is 18.0 ? 0.5%, which is significantly lower than the WER for unseen speakers (30.5%) under the open-speaker setting.</figDesc><table><row><cell cols="7">.1: WER (%) in using different amount of labeled data for fine-tuning (BASE, 433 hours</cell></row><row><cell>unlabeled)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Labeled (hrs) Unlabeled (hrs) Criterion</cell><cell>LM</cell><cell></cell><cell cols="2">WER (%) w/o pretrain w/ pretrain</cell></row><row><cell>1</cell><cell>433</cell><cell>CTC</cell><cell>4-gram</cell><cell></cell><cell>98.6</cell><cell>68.8</cell></row><row><cell></cell><cell></cell><cell>S2S</cell><cell>-</cell><cell></cell><cell>98.9</cell><cell>92.0</cell></row><row><cell>10</cell><cell>433</cell><cell>CTC</cell><cell>4-gram</cell><cell></cell><cell>90.8</cell><cell>57.6</cell></row><row><cell></cell><cell></cell><cell>S2S</cell><cell>-</cell><cell></cell><cell>97.6</cell><cell>63.1</cell></row><row><cell>100</cell><cell>433</cell><cell>CTC</cell><cell>4-gram</cell><cell></cell><cell>77.8</cell><cell>54.2</cell></row><row><cell></cell><cell></cell><cell>S2S</cell><cell>-</cell><cell></cell><cell>84.3</cell><cell>48.1</cell></row><row><cell cols="7">C.2 Table C.2: WER (%) under closed-speaker setting for 5 randomly sampled test sets and their average</cell></row><row><cell></cell><cell>1</cell><cell cols="2">Test set (seen speakers) 2 3 4</cell><cell>5</cell><cell>AVG</cell></row><row><cell></cell><cell cols="5">WER (%) 17.4 18.6 18.5 17.5 18.3 18.0 ? 0.5</cell></row><row><cell cols="3">C.3 PERFORMANCE OF SELF-TRAINING ONLY</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table C</head><label>C</label><figDesc></figDesc><table><row><cell cols="8">Table C.4: WER (%) of our models and the comparison with prior works on LRS3-TED dataset.</cell></row><row><cell cols="8">?We re-implemented Ma et al. (2021a) using the same model architecture as our approach to have a</cell></row><row><cell>more fair comparison.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>Criterion</cell><cell cols="2">Labeled iso (hrs)</cell><cell>Labeled utt (hrs)</cell><cell>Unlabeled data (hrs)</cell><cell>WER (%)</cell></row><row><cell></cell><cell cols="2">Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Afouras et al. (2020)</cell><cell>CNN</cell><cell>CTC</cell><cell cols="2">157</cell><cell>433</cell><cell>-</cell><cell>68.8</cell></row><row><cell>Zhang et al. (2019b)</cell><cell>CNN</cell><cell>S2S</cell><cell cols="2">157</cell><cell>698</cell><cell>-</cell><cell>60.1</cell></row><row><cell>Afouras et al. (2018a)</cell><cell>Transformer</cell><cell>S2S</cell><cell cols="2">157</cell><cell>1,362</cell><cell>-</cell><cell>58.9</cell></row><row><cell>Xu et al. (2020)</cell><cell>RNN</cell><cell>S2S</cell><cell cols="2">157</cell><cell>433</cell><cell>-</cell><cell>57.8</cell></row><row><cell>Shillingford et al. (2019)</cell><cell>RNN</cell><cell>CTC</cell><cell></cell><cell>-</cell><cell>3,886</cell><cell>-</cell><cell>55.1</cell></row><row><cell>Ma et al. (2021b)</cell><cell>Conformer</cell><cell>CTC+S2S</cell><cell></cell><cell>-</cell><cell>433</cell><cell>-</cell><cell>46.9</cell></row><row><cell>Ma et al. (2021b)</cell><cell>Conformer</cell><cell>CTC+S2S</cell><cell cols="2">157</cell><cell>433</cell><cell>-</cell><cell>43.3</cell></row><row><cell>Makino et al. (2019)</cell><cell>RNN</cell><cell>Transducer</cell><cell></cell><cell>-</cell><cell>31,000</cell><cell>-</cell><cell>33.6</cell></row><row><cell></cell><cell cols="4">Semi-Supervised &amp; Self-Supervised</cell><cell></cell><cell></cell></row><row><cell>Afouras et al. (2020)</cell><cell>CNN</cell><cell>CTC</cell><cell cols="2">157</cell><cell>433</cell><cell>334</cell><cell>59.8</cell></row><row><cell cols="8">.3: Comparison of WER (%) among model trained from scratch, self-training only, AV-HuBERT only and self-trained AV-HuBERT. All models are Transformer-LARGE. Ma et al. (2021a) ? Transformer-BASE -30 433 72.8 CTC -433 1,759 58.4</cell></row><row><cell cols="3">Labeled (hrs) Unlabeled (hrs) Method S2S</cell><cell></cell><cell>--</cell><cell>30 433</cell><cell>WER (%) 433 1,759</cell><cell>71.9 49.6</cell></row><row><cell cols="6">Proposed (Self-Supervised &amp; Self-Supervised + Semi-Supervised)</cell><cell></cell></row><row><cell>30</cell><cell>1,759</cell><cell cols="4">w/o pre-training Self-training AV-HuBERT CTC AV-HuBERT + Self-training -30 -30 -30 -433 -433</cell><cell>-92.3 433 53.0 1,759 32.5 -28.6 433</cell><cell>83.7 55.3 47.3 62.5 49.3</cell></row><row><cell>433</cell><cell>1,759 Transformer-BASE</cell><cell cols="2">w/o pre-training</cell><cell>-</cell><cell>433</cell><cell>62.3 1,759</cell><cell>43.0</cell></row><row><cell></cell><cell></cell><cell cols="4">Self-training AV-HuBERT AV-HuBERT + Self-training -30 -30 S2S -30 -433</cell><cell>-51.7 433 28.6 1,759 26.9 -</cell><cell>94.3 51.8 46.1 60.3</cell></row><row><cell>AV-HuBERT</cell><cell></cell><cell></cell><cell></cell><cell>--</cell><cell>433 433</cell><cell>433 1,759</cell><cell>44.0 34.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>-</cell><cell>92.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>433</cell><cell>48.4</cell></row><row><cell></cell><cell></cell><cell>CTC</cell><cell></cell><cell>-</cell><cell>30</cell><cell>1,759</cell><cell>40.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>-</cell><cell>61.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>433</cell><cell>44.3</cell></row><row><cell></cell><cell>Transformer-LARGE</cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>1,759</cell><cell>38.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>-</cell><cell>92.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>30</cell><cell>433</cell><cell>44.8</cell></row><row><cell></cell><cell></cell><cell>S2S</cell><cell></cell><cell>-</cell><cell>30</cell><cell>1,759</cell><cell>32.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>-</cell><cell>62.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>433</cell><cell>41.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>433</cell><cell>1,759</cell><cell>28.6</cell></row><row><cell cols="2">AV-HuBERT + Self-Training Transformer-LARGE</cell><cell>S2S</cell><cell></cell><cell>--</cell><cell>30 433</cell><cell>1,759 1,759</cell><cell>28.6 26.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table D .</head><label>D</label><figDesc>1: Ablation study for hyper-parameters. The ablations are done in the last iteration of AV-HUBERT. m a /m v : the probability of an acoustic/image frame being masked.</figDesc><table><row><cell></cell><cell>Masking</cell><cell cols="3">Modality Dropout Loss</cell><cell cols="2">WER</cell></row><row><cell>Where</cell><cell>How</cell><cell>m a m v p m</cell><cell>p a</cell><cell>?</cell><cell>dev</cell><cell>test</cell></row><row><cell>Input</cell><cell>Sub (same, seg)</cell><cell>0.8 0.3 0.5</cell><cell>0.5</cell><cell>0.0</cell><cell cols="2">46.8 55.3</cell></row><row><cell></cell><cell>Sub (same, frm)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">47.2 55.8</cell></row><row><cell></cell><cell>Sub (diff, seg)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">47.6 56.1</cell></row><row><cell></cell><cell>Learned Embedding</cell><cell></cell><cell></cell><cell></cell><cell cols="2">52.6 57.8</cell></row><row><cell></cell><cell>Gauss. Noise</cell><cell></cell><cell></cell><cell></cell><cell cols="2">52.4 57.9</cell></row><row><cell cols="2">Feature Learned Embedding</cell><cell></cell><cell></cell><cell></cell><cell cols="2">55.2 58.2</cell></row><row><cell>Input</cell><cell>Sub (same, seg)</cell><cell>0.8 0.3 0.5</cell><cell>0.5</cell><cell>0.0</cell><cell cols="2">46.8 55.3</cell></row><row><cell></cell><cell></cell><cell>0.8 0.8</cell><cell></cell><cell></cell><cell cols="2">59.3 61.6</cell></row><row><cell></cell><cell></cell><cell>0.3 0.3</cell><cell></cell><cell></cell><cell cols="2">54.9 58.2</cell></row><row><cell>Input</cell><cell>Sub (same, seg)</cell><cell>0.8 0.3 0.5</cell><cell>0.5</cell><cell>0.0</cell><cell cols="2">46.8 55.3</cell></row><row><cell></cell><cell></cell><cell>1.0</cell><cell>n/a</cell><cell></cell><cell cols="2">55.2 57.0</cell></row><row><cell>Input</cell><cell>Sub (same, seg)</cell><cell>0.8 0.3 0.5</cell><cell>0.5</cell><cell>0.0</cell><cell cols="2">46.8 55.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell cols="2">46.7 55.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table E .</head><label>E</label><figDesc>1: Quality of different cluster assignments. Each number is in the format of Purity (NMI). The metrics of cluster assignments and WER in last iteration of each model are in boldface.</figDesc><table><row><cell>Model</cell><cell>Iter</cell><cell>Feature</cell><cell>Target K</cell><cell cols="2">Purity (%),? NMI (%),?</cell><cell>WER (%),?</cell></row><row><cell></cell><cell>1</cell><cell>MFCC</cell><cell>100</cell><cell>30.3</cell><cell>21.5</cell><cell>71.5</cell></row><row><cell>AV/MFCC?AV (Proposed)</cell><cell>2 3 4</cell><cell cols="2">AV/MFCC?AV (it1, L9) AV/MFCC?AV (it2, L12) 500 100 AV/MFCC?AV (it3, L12) 1000</cell><cell>47.3 61.5 65.6</cell><cell>37.7 42.6 43.7</cell><cell>63.6 60.9 58.8</cell></row><row><cell></cell><cell>5</cell><cell cols="2">AV/MFCC?AV (it4, L12) 2000</cell><cell>68.8</cell><cell>44.2</cell><cell>58.2</cell></row><row><cell></cell><cell>1</cell><cell>MFCC</cell><cell>100</cell><cell>30.3</cell><cell>21.5</cell><cell>71.5</cell></row><row><cell>AV/MFCC?A</cell><cell>2</cell><cell>A/MFCC?A (it1, L9)</cell><cell>100</cell><cell>47.0</cell><cell>36.7</cell><cell>64.3</cell></row><row><cell></cell><cell>3</cell><cell>A/MFCC?A (it2, L12)</cell><cell>500</cell><cell>56.5</cell><cell>39.7</cell><cell>63.5</cell></row><row><cell></cell><cell>1</cell><cell>MFCC</cell><cell>100</cell><cell>30.3</cell><cell>21.5</cell><cell>75.4</cell></row><row><cell>V/MFCC?A</cell><cell>2</cell><cell>A/MFCC?A (it1, L9)</cell><cell>100</cell><cell>47.0</cell><cell>36.7</cell><cell>69.4</cell></row><row><cell></cell><cell>3</cell><cell>A/MFCC?A (it2, L12)</cell><cell>500</cell><cell>56.5</cell><cell>39.7</cell><cell>69.1</cell></row><row><cell></cell><cell>1</cell><cell>MFCC</cell><cell>100</cell><cell>30.3</cell><cell>21.5</cell><cell>75.4</cell></row><row><cell>V/MFCC?V</cell><cell>2</cell><cell>V/MFCC?V (it1, L9)</cell><cell>100</cell><cell>32.8</cell><cell>22.9</cell><cell>72.6</cell></row><row><cell></cell><cell>3</cell><cell>V/MFCC?V (it2, L12)</cell><cell>500</cell><cell>33.0</cell><cell>22.8</cell><cell>72.3</cell></row><row><cell>V/HoG?V</cell><cell>1 2</cell><cell>HoG V/HoG?V (it1, L9)</cell><cell>100 100</cell><cell>16.4 16.4</cell><cell>1.6 1.8</cell><cell>80.3 80.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table E .</head><label>E</label><figDesc>2: Impact of masking strategy on quality of cluster assignments (purity/NMI: quality of cluster assignments used to train the model)</figDesc><table><row><cell>Feature</cell><cell>K</cell><cell cols="2">Purity (%) NMI (%)</cell></row><row><cell>MFCC</cell><cell>100</cell><cell>30.3</cell><cell>21.5</cell></row><row><cell cols="2">AV/MFCC?AV (it1, L9) w/ Feature Masking 100</cell><cell>47.3</cell><cell>37.7</cell></row><row><cell>AV/MFCC?AV (it1, L9) w/ Input Masking</cell><cell>100</cell><cell>34.5</cell><cell>27.2</cell></row><row><cell>E.3 CLUSTERING QUALITY ACROSS LAYERS</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The prior work in<ref type="bibr" target="#b33">(Ma et al., 2021b)</ref> uses an outdated version of LRS3 (before 2018) with speaker overlap in training and test data, which is no longer publicly available. Its best results on the current version are included in table 1&amp; C.4 . For comparison, we simulate the old closed-speaker setting in Section C.2.2  The gap in data amount (334hr vs 1,759hr) is due to the different parameters and ASR model used for filtering non-English data in VoxCeleb2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To avoid using original frames for substitution, pi in equation 3 is selected from [0, 2si ? ti] ? [ti, T f ? ti + si] if the imposter is from the same sequence.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure F</ref><p>.2: Transcriptions from different lip-reading models. GT: ground-truth, Proposed: selfsupervised model, Supervised: supervised model. Red: wrong words in the output (a) Self-supervised vs. Supervised</p><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT:</head><p>why not ask all of the states to do that instead Proposed:</p><p>why not ask all of these things to do that instead Supervised: why can't i actually all of these things do things and <ref type="formula">(2)</ref> GT:</p><p>indeed we run the risk of making things worse Proposed:</p><p>indeed we want the risk of making things worse Supervised: in india we roughly receive money in the health world <ref type="formula">(3)</ref> GT: my desire to disappear was still very powerful Proposed: my desire to disappear was still very powerful Supervised: my son is speaking with children about food <ref type="formula">(4)</ref> GT:</p><p>the silent majority does not need to be silent Proposed:</p><p>the same majority does not need to be silent Supervised: this time the total disaster needs to be designed <ref type="formula">(5)</ref> GT:</p><p>mortality is not going down it's going up Proposed:</p><p>mortality is not going down it's going up Supervised: we're seeing this not only carrying slowly how</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Failure cases</head><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT:</head><p>it's a win all around Proposed:</p><p>he's a win on the ground <ref type="formula">(2)</ref> GT: sort of leadership by humiliation Proposed: so the leadership by communication</p><p>GT: is it about equality Proposed:</p><p>ask about quality</p><p>GT: science fiction is one of the greatest and most effective forms of political writing Proposed:</p><p>stage vision is one of the greatest and most effective forms of political writing</p><p>GT: we can't identify with that part Proposed:</p><p>we can't identify with that bunk</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.speech.cs.cmu.edu/cgi-bin/" />
		<title level="m">The CMU pronouncing dictionary</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nicholas Apostoloff, and Sachin Kajareker. Modality dropout for improved performance-driven talking faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry-John</forename><surname>Ahmed Hussen Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">LRS3-TED: a large-scale dataset for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00496</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ASR is all you need: Cross-modal distillation for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Samuel Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliane</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Momeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Abdel rahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Endto-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korbar</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torresani</forename><surname>Lorenzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-modal pretraining for automated speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debmalya</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Hoffmeister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09890</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Seeing voices and hearing voices: learning discriminative embeddings using cross-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Whan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03240</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Investigating the psycholinguistic correlates of speechreading in preschool age children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Lander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of language and communication disorders</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="164" to="74" />
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>In ICML</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hubert: Self-supervised speech representation learning by masked prediction of hidden units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07447</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01027</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubert</surname></persName>
		</author>
		<title level="m">How much can a bad teacher benefit asr pre-training? In ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data augmenting contrastive learning of speech representations in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="215" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parameter Efficient Multimodal Transformers for Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Decoar 2.0: Deep contextualized acoustic representations with vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshi</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06659</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">LiRA: Learning visual speech representations from audio through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Mira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end audio-visual speech recognition with conformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Recurrent neural network transducer for audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaki</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basilio</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otavio</forename><surname>Braga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Siohan</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lipreading using temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hearing lips and seeing voices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Mcgurk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="746" to="748" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imitation of facial and manual gestures by human neonates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Keith</forename><surname>Meltzoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="page" from="75" to="78" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Audio-visual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ModDrop: adaptive multimodal gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Louis-Philippe Morency, and Barnabas Poczos</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Evolving losses for unlabeled video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised learning for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">L</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Senior, and Nando Freitas. Large-scale visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?an</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utsav</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorrayne</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Mulville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<meeting><address><addrLine>Ben Laurie, Andrew</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Ben Coppin</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Combining residual networks with lstms for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visual contribution to speech intelligibility in noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Sumby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="212" to="215" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Discriminative multi-modality speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust audio-visual speech recognition using bimodal dfsmn with multi-condition training and dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spatio-temporal fusion based convolutional sequence learning for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
