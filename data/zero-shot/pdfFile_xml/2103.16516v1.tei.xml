<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recognizing Actions in Videos from Unseen Viewpoints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
							<email>mryoo@cs.stonybrook.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recognizing Actions in Videos from Unseen Viewpoints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Standard methods for video recognition use large CNNs designed to capture spatio-temporal data. However, training these models requires a large amount of labeled training data, containing a wide variety of actions, scenes, settings and camera viewpoints. In this paper, we show that current convolutional neural network models are unable to recognize actions from camera viewpoints not present in their training data (i.e., unseen view action recognition). To address this, we develop approaches based on 3D representations and introduce a new geometric convolutional layer that can learn viewpoint invariant representations. Further, we introduce a new, challenging dataset for unseen view recognition and show the approaches ability to learn viewpoint invariant representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Activity recognition with convolutional neural networks (CNNs) has been very successful <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b12">13]</ref> when provided sufficient diverse labeled data, like Kinetics <ref type="bibr" target="#b21">[21]</ref>. However, one major limitation of these CNNs is that they are unable to recognize actions/data that are outside of the training data distribution. This is most notably observed for unseen classes (objects, activities, etc.) which has been heavily studied in zero-shot and few-shot learning literature. In this work, we look at a related, but different problem of unseen viewpoint activity recognition, where the actions are the same, but occur from different camera angles.</p><p>To motivate this problem, let us consider an example. Given a labeled dataset of a person performing actions with one camera angle, we train a CNN to recognize this action. Now, suppose we have new videos to recognize, but from a different camera view. This could be as simple as a different camera placement in the environment, or an entirely different camera and setting (e.g., <ref type="figure" target="#fig_0">Fig. 1</ref>). In this case, a trained CNN, in general, fails to recognize the action. As a simple experiment, we use the Human3.6M dataset <ref type="bibr" target="#b13">[14]</ref>, which contains videos of a person performing an action from 4 different camera angles. As shown in <ref type="table" target="#tab_0">Table 1</ref>, when training on one</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLB-YouTube</head><p>Seen View New Unseen Views view and testing on another, the model is unable to recognize the action. However, humans are able to recognize these actions regardless of viewpoint and studies have found that this is likely because humans build invariant representations of actions in their minds <ref type="bibr" target="#b39">[39]</ref> . Further, this problem frequently occurs in real data (e.g., YouTube videos). Existing smaller datasets such as Toyota SmartHome <ref type="bibr" target="#b7">[8]</ref>, Charades-Ego <ref type="bibr" target="#b38">[38]</ref>, NTU <ref type="bibr" target="#b35">[35]</ref> and others all provide videos in multiple viewpoints to study this effect. Large video datasets like Kinetics <ref type="bibr" target="#b21">[21]</ref> naturally contain many views, however, there is no annotation of the view and each video only provides a single view. Other datasets like MLB-YouTue <ref type="bibr" target="#b31">[31]</ref> only contains the single broadcast camera view baseball games. As collecting video data is already challenging, designing CNNs that generalize to unseen viewpoints is critical, especially for applications where diverse view data is limited or unavailable. It would be practically impossible to build datasets for many desirable settings that enumerate all possible (or sufficiently large number of) viewpoints to fully model activities.</p><p>There are many potential ways to address this problem. One hypothesis is that by training on a large-scale video datasets, such as Kinetics, the model could implicitly learn multi-view representations of actions. However, as shown in formance, it is still lacking. A second hypothesis is that by using 3D human pose information, we can recognize actions in a global representation space, unconstrained by camera views. A key drawback to this approach is estimating 3D pose from video itself is a challenging problem, especially when multiple people are present. It further requires estimating camera pose in order to build a 'world'/global camera invariant 3D representation. Further, it is unclear what the right representation of 3D pose is (e.g., coordinates of joints, limbs, motion difference of joints between frames, etc.). Building on this hypothesis and observation, we present and evaluate several approaches for recognizing actions in unseen viewpoints. The basic approach relies on estimating 3D pose directly from the videos, then explores using different representations of it for recognition. Since directly estimating accurate real-world 3D pose is often difficult, we also present an approach of learning latent 3D representations of an action and its multi-view 2D projections. This is done by imposing the latent action representations to follow 3D geometric transformations and projections, in addition to minimizing the action classification loss. We learn such view invariant action representations without any 3D or view ground truth labels.</p><p>We also introduce a challenging dataset building on the MLB-YouTube dataset <ref type="bibr" target="#b31">[31]</ref>. The MLB-YouTube dataset contains actions from a single camera and these actions are all in the same environment (e.g., a professional baseball stadium). Our extended dataset contains evaluation samples of the same actions, but from many different viewpoints and a variety of different settings: batting cages, little league (children's baseball games), high school games, etc. These use different camera (e.g., cell phones), in very different environments. The goal is to learn a representation from the single view dataset that generalizes to these challenging, unseen viewpoints. Examples are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>To summarize, the contributions of this paper are:</p><p>? A computationally efficient, geometric-based layer and learning to learn view invariant representation.</p><p>? Thorough evaluation of multiple approaches to unseen viewpoint action recognition.</p><p>? A challenging new dataset for unseen viewpoint recognition in unseen environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background -3D Geometric Camera Model</head><p>First we briefly review the standard 3D geometric camera model used in computer vision, which we build on in this work. We begin with a standard pinhole camera model. Given the pixel coordinates p, the 3D world coordinates p w are represented as</p><formula xml:id="formula_0">p = K [ R | t ] [ p w | 1 ] T<label>(1)</label></formula><p>where K is the 3 ? 4 camera projection matrix (intrinsic camera matrix) mapping a 3D point into a 2D camera view. R and t are the camera rotation (3?3) and translation (1?3) in the world space (extrinsic camera matrix) that transform the points between different 3D camera views. | is the matrix concatenation operator. In many cases in computer graphics and computer vision, it is assumed that K, R and t are known. These matrices can be used to compute the inverse as [ R T | R T t ] (the inverse of a rotation matrix is its transpose). Thus, given a 3D coordinates p, the view-invariant world coordinates can be computed as</p><formula xml:id="formula_1">p w = p ? [ R T | R T t ].</formula><p>Importantly, points in the 3D world coordinate system are, by design, viewpoint invariant. However, in many settings, including activity recognition, the camera matrices are unknown. In some cases the intrinsic camera matrix K might be known (e.g., when the camera has been previously calibrated), but the extrinsic matrices as well as the definition of the world coordinate system are not. The core of this paper is exploring methods to learn and represent these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Basics -Using 3D Pose</head><p>We first design and investigate a straight forward approach of using 3D human pose estimation and its projection for action classification <ref type="figure" target="#fig_1">(Fig. 2</ref>). Many works have explored estimating 3D human pose from videos <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28]</ref>, even multi-person 3D pose <ref type="bibr" target="#b27">[27]</ref>. We begin by using PoseNet <ref type="bibr" target="#b27">[27]</ref> to estimate 3D coordinates. PoseNet provides 3D coordinates in camera space, so directly using the coordinates will not yield viewpoint-invariant recognition. For this, the 3D coordinates need to be transformed into world-space.</p><p>However, estimating the extrinsic matrix from a single, random image is a challenging problem. We use CalibNet <ref type="bibr" target="#b15">[16]</ref> to obtain estimations of R and t. This approach is quite limited by the accuracy of CalibNet, if it provides a poor estimation, the rest of the network will fail. Since there is limited camera calibration training data, we observe than for in-the-wild videos, CalibNet often gives inaccurate results and does not generalize well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Recognition with 3D representation</head><p>Given the estimated 3D pose in world space, these values can be directly used as input to the model. However, directly using 3D coordinates may not be the best feature, as scale changes (e.g., person size), speed of which the action occurs, etc. will all impact the features. Previous works have studied representations of 3D pose for skeleton action recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>, such as joint angles <ref type="bibr" target="#b29">[29]</ref> showing the difficulty of this task.</p><p>Instead of directly using the 3D pose as input to a classification model, multi-view 2D projections of it could be used. Research focusing on designing strong CNNs for understanding 2D image input has been one of the mainstream areas, and it often is more advantageous to use 2D image inputs rather than 3D. Further, by using multiple views, the model can see the input from different angles, instead of a single one. To do this, we assume we have an intrinsic camera matrix K that projects the 3D coordinates into 2D. We follow the standard pinhole camera model and learn a camera rotation and projection to generate multiple 2D views. Inspired by previous works, like Potion <ref type="bibr" target="#b5">[6]</ref>, we take the 2D projections of pose and render skeletons capturing the motion. These images are used as input to the model for activity classification. An overview of this approach is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network for Latent 3D Representations</head><p>The previous approach is an engineered combination of existing CNNs (pose, camera estimation and action recognition) and relies on multiple components correctly functioning. If any of these networks fail or gives slightly incorrect results, the rest of the model will fail. However, we draw inspiration from the geometric based approach, and design geometric CNN layers to learn and replicate similar transformations. To do this, we begin by learning a representation that contains and uses both 3D information and the extrinsic information. We then combine this information to get a 3D world representations of the actions and provide a CNN architecture. We introduce a loss function terms to learn such 3D view-invariant representations from a dataset with (unlabeled) views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Neural Projection Layer (NPL): Building a Latent 3D World Representation</head><p>First, we take a feature map F which is a W ?H ?(C +3) tensor, where C is the channels in the feature map plus the CNN estimated 3D camera space coordinate. Formally,</p><formula xml:id="formula_2">F x ,y = [p x ,y , f x , y ], where p x ,y = [x, y, z]</formula><p>at location x , y in the feature map and f x ,y is the C-dimensional feature at the location.</p><p>Next, we use a fully-connected layer to estimate R and t (rotation and translation) from each video. These are used to transform the video camera view into the world coordinates as p w</p><p>x ,y = p x ,y ?[ R T | R T t ] for each p in the feature map. This gives the 3D world coordinates, i.e., camera invariant coordinate, of each point p. Note that the 3D world coordinate system is the same for all videos, thus R is different for each video, depending on the camera viewpoint. R plays the role of aligning features (e.g., humans) in different scenes, so that the losses are minimized. This allows the model to learn to map each video into the same global coordinates.</p><p>The world 3D representation is then created as:</p><formula xml:id="formula_3">F W x,y,z = W,H i=0,j=0 1(p w i,j = [x, y, z])F i,j<label>(2)</label></formula><p>where 1(p w i,j = [x, y, z]) is the indicator function for when p w matches location x, y, z.</p><p>That is, we can create a new feature map F W which has a shape of W ? H ? Z (in practice, W = H = Z, for example, 64). Given one of the points p x ,y and its associated feature vector f x ,y from the original feature map, we compute the 'world coordinate' of that point x, y, z = p w x ,y and then set F W x,y,z = F W p w</p><p>x ,y = f x ,y . I.e., we set the values in the feature map based on their location in the world coordinate reference. This transforms the original latent 3D representation into the rotation and translation invariant 3D world representation, resulting in a representation that is viewpoint/camera invariant.</p><p>However, since x, y, z are integers and p w i,j is likely not an integer and we want to implement this as a differentiable function to be learned with gradient descent, we slightly modify this to:  This is similar to Eq. 5 introduced in the spatial transformer network <ref type="bibr" target="#b17">[17]</ref>. In the implementation, we first set p w = tanh p w to ensure its values fit in the feature map space, similar to the transformer network.</p><formula xml:id="formula_4">F W c,x,y,z = W,H i=0,j=0 (1 ? |x ? p w i,j [x]|) (1 ? |y ? p w i,j [y]|)(1 ? |z ? p w i,j [z]|)F c,i,j<label>(3)</label></formula><p>Once we obtain F W , we use it directly as input to the remaining CNN for classification. We note that F W is a 4-dimensional tensor (channels followed by 3D coordinates), so we use 3D convolution on top of this representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Multi-view 2D Projections</head><p>Instead of directly working with a 3D feature map, we can follow the ideas from Section 3.1 and generate multiple 2D projections of the features. This is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>Assuming we have a camera matrix K, which we represent as</p><formula xml:id="formula_5">K = R ? ? s x 0 x 0 0 s y y 0 0 0 1 ? ?<label>(4)</label></formula><p>where s x , s y are the focal lengths and x 0 , y 0 are the offsets, and R is the 3 ? 3 camera rotation matrix. Note R can be represented with 3 parameters: yaw, pitch and roll which generate the full 3 ? 3 rotation matrix. This process uses the same components and projections as in Section 4.1, but instead of estimating R using a layer for each video, here these are learned parameters of the model. This allows the model to generate the same 2D views from the view-invariant world 3D space. We can then model a 2D projection of the 3D points as:  </p><formula xml:id="formula_6">(1?|x?Kp w i,j [x]|)(1?|y?Kp w i,j [y]|)F c,i,j<label>(5)</label></formula><p>As these operations are differentiable, we can learn all these parameters with gradient descent. This allows the model to learn the optimal arrangement of cameras to capture views from the latent 3D representations for action recognition. Further, by increasing the number of cameras (N ), we can learn multiple 2D projections of the representations, which can be stacked on the channel axis for recognition. In the next section, we describe the training loss that enables learning of the 3D representation without any 3D or camera calibration ground truth data.</p><p>We note that in this setting, some views will have occluded objects/features. An assumption of the approach is that using multiple cameras will naturally capture different viewpoints that will minimize the effects of occlusion. Another approach could be to use tomographies of each view to remove the effect of occlusions, which we leave as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Recognition with 3D Representation</head><p>The full model, as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>, begins by applying several conv. layers to a video input. At some point in the network (we experimentally evaluate where), we generate the 3D feature map and apply the geometric transformations described above. We then use either 2D or 3D conv. layers followed by a fully connected layer to classify the video clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning Latent 3D Representations</head><p>The key challenge with this approach is learning the camera matrices that generate view-invariant representations. We propose an approach to learn this view-invariant representation from a dataset with action videos (e.g., Kinetics <ref type="bibr" target="#b21">[21]</ref>). Importantly, we do not assume any ground truth viewpoint or 3D data is provided, but only that the videos naturally contain multiple views. Unlike the first approach where we first learn 3D human pose estimation and extrinsic camera calibration from specific datasets, this approach only requires action-labeled data, available in many large-scale public datasets.</p><p>Given two videos V and U of the same action, we compute their 3D representations F W (V ) and F W (U ) and use a loss to make their representations the same:</p><formula xml:id="formula_7">3d_loss(V, U ) = ||F W (V ) ? F W (U )|| F<label>(6)</label></formula><p>Intuitively, this loss term makes it so that two videos likely with different camera views have the same 3D representation after the projections. This encourages the representations from different viewpoints of the same action result in the same 3D representation.</p><p>For multi-view 2D projection, we apply that loss on each 2D view, as well as adding a regularizing term to make each camera different:</p><formula xml:id="formula_8">cam_reg(c 1 , c 2 ) = max(?||c 1 ? c 2 || F , ?)<label>(7)</label></formula><p>where c 1 , c 2 are camera matrices and ? is the margin, or desired max difference. We note that this constraint forces the cameras to be different, but does not ensure that they are facing the scene. However, we observed that during training, the cameras converged to views that were facing the scene, as shown in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training Loss Function</head><p>Our final loss is a combination of these terms plus a standard classification loss. Given the set of cameras in the model C, let V be a video and l be the binary vector indicating the class label of a video. Let M (V ) be the application of the network to video V giving the predictions for each class (e.g., a c-dimensional vector where c is the number of classes). In particular, M (V ) is some video CNN that produces the classification. More details are shown in <ref type="figure" target="#fig_3">Fig. 4</ref> and specific architecture details are in the appendix. Given the set of training videos and labels, V, The final loss function is:</p><formula xml:id="formula_9">L(V) = (V,l)?V ? c i l i log M (V ) i +? 1 ? ? (U,k)?V 3d_loss(V, U ) l = k 0 otherwise ? ? + ? 2 ? ? c1 =c2?C cam_reg(c 1 , c 2 ) ? ? (8)</formula><p>The combination of the geometric structure imposed by the NPL and the components of the loss function encourages the model to learn viewpoint invariant representations. This formulation enables learning 3D representations with only activity-level labels and the geometric constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We conduct various experiments to understand the various components of the approach on multiple datasets. The model was implemented in PyTorch (code in appendix) and pretrained on Kinetics-400 <ref type="bibr" target="#b21">[21]</ref>. We used Kinetics to pretrain as it is large and naturally has many viewpoints, allowing for the evaluation of this approach. We then use the network to extract features and train a small two-layer network on each specific dataset. Training was done for 25 epochs with the learning rate set to 0.01. For learning the rotation matrices, we learn 3 parameters: yaw, pitch and roll, which we convert to the rotation matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We evaluate this approach on three datasets containing multi-view data. On Human3.6M, we train the model on one camera view for one subject and test on one of the other views. The model is trained to recognize 11 different activities. We perform this experiment for 9 subjects and 2 different seen/unseen view combinations and report the average over each setting.</p><p>For the unseen MLB (baseball) videos, we train on the broadcast camera videos (original MLB-YouTube dataset <ref type="bibr" target="#b31">[31]</ref> and test on the new, unseen views. We newly created this dataset of unseen views for testing only. It consists of 500 videos from YouTube for testing of 4 different baseball actions (swing, hit, pitch, bunt). This data is quite challenging as it has drastically different viewpoints, people, backgrounds, activity speeds, etc. Examples of these views are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>For Toyota SmartHome (TSH), we follow the CV 1 protocol <ref type="bibr" target="#b7">[8]</ref> where the model is trained using camera 1 and tested on camera 2. We also report results on the CV 2 protocol where it is evaluated on camera 2 but trained on multiple cameras. This dataset has 16.1k videos taken from 7 different camera viewpoints. It contains 31 classes of human daily activities in real-world environments. Similarly, we also compare on NTU-RGB-D <ref type="bibr" target="#b35">[35]</ref> following the standard settings.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, we compare the different proposed approaches. We find that for some datasets, like Human3.6M, using 3D pose (Section 3) is a very good feature, as this dataset contains actions focused on human motion. However, on TSH, which has many object-dependent actions, using pose degrades performance. For example, actions such as 'pick up cup' and 'pick up plate', the pose motion is nearly identical for both of those, but the object is different. Using pose gives little indication which object is being used, thus when only using pose to recognize the action, the model cannot distinguish.</p><p>On the MLB dataset, using pose harms performance on the seen viewpoint, but slightly improves performance on the unseen views. In both cases, it is only slightly better than random. This is likely due to noisy data with many people present, and thus the 3D pose estimation is not accurate enough. When using the learned, latent 3D representation (Section 4), we find in all cases the performance on the unseen views (and often seen views) improves, showing the benefit of the proposed approach. We note that the latent 3D representation generalizes quite well to challenging data with very different views and backgrounds (e.g., MLB data) because it is trained on large-scale video data, the model is able to learn more general projections.</p><p>We also find that training from scratch (ResNet-50) gives very poor performance on the unseen views. Somewhat surprising, pre-training on Kinetics, which has many different views of people performing actions, does slightly improve performance on unseen views, but still remains low, especially in the MLB and Toyota SmartHome datasets. This suggests that standard video CNNs are not learning 3D rotation invariant representations, even when given training data from many views, further showing the benefit of learning 3D view invariant representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Ablation Experiments</head><p>To better understand the effect of the NPL, we conduct a set of experiments to analyze each component's impact. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Adding the multi-view projection without any of the loss constraints slightly reduces performance. Adding the 3D loss enforces the geometric constraints to learn the viewpoint invariant representation, without this, the model struggles to learn the representation. Further adding the the camera regularization loss improves performance. Based on this observation, we also try a baseline with representation matching (RepMatch) where we apply the 3D loss (Eq. 6) to feature maps from a ResNet-50 without using any of the geometric layers. The findings shown no real benefit of RepMatch over standard pre-training, while the proposed geometric approach shows meaningful benefit. We also study the effect of the number of cameras in <ref type="figure" target="#fig_6">Fig  6,</ref> finding that using just 1 camera projection is very helpful while more than 4 no longer improves performance. This intuitively makes sense, as a single camera will already result in a viewpoint invariant representation, and the amount of new data introduced with additional cameras decreases as more are added. In <ref type="figure" target="#fig_7">Fig. 7</ref>, we compare the effect of placing the geometric layer at different locations in the network. Overall, the performance is fairly stable regardless of where it is added. In <ref type="figure" target="#fig_5">Fig. 5</ref> we visualize the learned cameras from Sec 4.1.1. In the figure, the red rectangle represents the world 3D representations space which contains the CNN features F W (Eq. 3). The brown, blue and green markers indicate the different learned camera matrices that capture different 2D views of the space (Eq. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison to other approaches</head><p>We compare the proposed approach to other approaches for unseen viewpoint activity recognition. The results are  shown in <ref type="table" target="#tab_4">Table 4</ref>, showing that the proposed approach outperforms the existing ones. Importantly, the added runtime of our approach is small, processing a 3 second video clip in 120ms (ours) vs. 105ms (baseline ResNet-50), enabling practical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related Works</head><p>Representation Invariant Networks: Many works have studied representation invariant networks. The spatial transformer network <ref type="bibr" target="#b17">[17]</ref> and Equivarient CNNs <ref type="bibr" target="#b10">[11]</ref> introduced an operation to make CNNs invariant to 2D translation, scale, rotation and more generic warping. Spherical CNNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7]</ref> took advantage of spherical representation that are invariant to 3D rotation transformations of objects in the camera view.</p><p>Our approach shares some similar ideas and motivations to spherical CNNs, i.e., trying to learn a rotation invariant representation. But differs in the goal of learning object rotation invariance in spherical CNNs vs. world space representations in this work. Another difference is in the design of the representation: spherical CNNs rely on convolutions in the spherical harmonic domain, while the proposed approach uses traditional geometric computer vision to learn a representation. Other works like geometry-aware RNNs <ref type="bibr" target="#b4">[5]</ref> propose the related idea of 'unprojection' for learning 3D representations by utilizing ground truth 3D data.</p><p>View Invariant Action Recognition: Many works have studied view invariance in action recognition <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b14">15]</ref>. Several works have studied using multiple views during training to learn view invariant representations <ref type="bibr" target="#b45">[45]</ref> or 'hallucinating' features (e.g., HOG) in different viewpoints to recognize actions in unseen views <ref type="bibr" target="#b3">[4]</ref>. Several works explored using cross-view similarity to recognize actions in various viewpoints <ref type="bibr" target="#b18">[18]</ref> or trajectory curvature <ref type="bibr" target="#b0">[1]</ref>.</p><p>Other works <ref type="bibr" target="#b33">[33]</ref> explored using 3D Pose for recognition, but as described, pose has limitations when interacting with objects.</p><p>3D Representations: Other works have designed CNNs to specifically model 3D shapes, such as RotationNet <ref type="bibr" target="#b20">[20]</ref> and others (e.g., ShapeNet, PointNet, etc.) <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">32]</ref>. However, these works focused on learning 3D models, rather than applications to noisy, real videos in various environments where no 3D data is directly given. Works such as SynSin <ref type="bibr" target="#b44">[44]</ref> propose similar ideas of using geometric projections on CNN representations, showing the promise of such ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We presented a new geometric based layer to learn 3D viewpoint invariant representations within CNNs. We also introduced a new, challenging dataset to evaluate camera view invariance. We experimentally showed the benefit of the proposed layer on multiple datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Details</head><p>Our base model used a standard (2+1)D ResNet-50 <ref type="bibr" target="#b42">[42]</ref>. The camera transform is inserted into the network usually after the 3rd block (in the main paper we compared all locations). Usually this network used 256 channels for the representation and we used 3 cameras (i.e., 3 different 2D projections). The total number of parameters of the 3 main models is summarized in <ref type="table" target="#tab_5">Table 5</ref>. Our layer adds only 280k parameters (only about 1% of the parameters), but significantly improves performance on unseen views. It further has significantly better runtime performance than spherical CNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Full Results</head><p>The full numerical results from plots in the paper are provided here.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of the seen, static broadcast camera in MLB-YouTube and examples of the new, unseen viewpoints of the same actions. This dataset is quite challenging, adding new views, people, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the process to learn global 3D pose and 2d multi-view projections of it for classifying unseen viewpoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Learning latent 3D representation. A CNN produces a 3D feature: each location has a feature value and x, y, z location. The network also produces camera parameters, allowing the construction of a viewpoint invariant 3D feature. Then multiple cameras are learned, allowing the creation of multi-view 2D projections of the features. These are stacked on the channel axis and used for classification. The world 3D feature and 2D MultiView features are learned to be identical for the same action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the full viewpoint invariant recognition model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the learned 2D multi-view cameras (Sec. 4.1.1). The red square represents the origin of the world coordinate system, the cameras are drawn using their intrinsic and extrinsic matrices using the matlab PlotCamera function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>How many cameras to use.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Where in network to add layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 ,</head><label>1</label><figDesc>we empirically find that while it improves per-</figDesc><table><row><cell>Method</cell><cell>Seen</cell><cell>Unseen</cell></row><row><cell>Random</cell><cell>9.1%</cell><cell>9.1%</cell></row><row><cell>2D ResNet-50</cell><cell>86.4%</cell><cell>9.1%</cell></row><row><cell>3D ResNet-50</cell><cell>100%</cell><cell>9.1%</cell></row><row><cell cols="2">3D ResNet-50 + Kinetics 100%</cell><cell>38.2%</cell></row><row><cell>Ground Truth 3D Pose</cell><cell>100%</cell><cell>100%</cell></row><row><cell cols="3">Table 1: Experiments on Human3.6M with unseen view-</cell></row><row><cell cols="3">points. Standard CNNs are unable to recognize actions with</cell></row><row><cell cols="3">different viewpoints, however, using global 3D pose allows</cell></row><row><cell cols="2">the models to recognize the actions.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the approaches. While pre-training on Kinetics improves results, the use of the geometric NPL is quite beneficial. Multi-view projections 99.7% 87.5% 58.9% 42.7% 54.5% 39.6%</figDesc><table><row><cell>Method</cell><cell cols="2">H3.6M</cell><cell></cell><cell>MLB</cell><cell></cell><cell>TSH</cell></row><row><cell></cell><cell>Seen</cell><cell>Unseen</cell><cell>Seen</cell><cell>Unseen</cell><cell>Seen</cell><cell>Unseen</cell></row><row><cell>Random</cell><cell>9.1%</cell><cell>9.1%</cell><cell>25%</cell><cell>25%</cell><cell>5.2%</cell><cell>5.2%</cell></row><row><cell>ResNet-50</cell><cell>86.4%</cell><cell>9.1%</cell><cell cols="2">49.4% 27.3%</cell><cell cols="2">34.6% 33.7%</cell></row><row><cell>ResNet-50 + Kinetics</cell><cell cols="2">100% 38.2%</cell><cell cols="2">55.6% 30.2%</cell><cell cols="2">49.8% 34.2%</cell></row><row><cell>3D Pose Based (Section 3)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ground Truth 3D Pose</cell><cell cols="2">100% 100%</cell><cell>-</cell><cell>-</cell><cell cols="2">19.6% 14.5%</cell></row><row><cell>Estimated 3D Pose</cell><cell cols="2">97.8% 78.6%</cell><cell cols="2">36.5% 33.6%</cell><cell cols="2">17.9% 11.6%</cell></row><row><cell>MultiView 2D Pose</cell><cell cols="2">98.3% 81.3%</cell><cell cols="2">37.6% 34.6%</cell><cell cols="2">18.4% 12.2%</cell></row><row><cell cols="2">Latent 3D Representation (Section 4)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NPL</cell><cell cols="2">99.3% 84.4%</cell><cell cols="2">52.3% 34.5%</cell><cell cols="2">51.2% 34.7%</cell></row><row><cell>NPL +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the components of the approach.</figDesc><table><row><cell cols="5">All models are based on a ResNet-50 and pretrained on</cell></row><row><cell>Kinetics-400.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>MLB</cell><cell></cell><cell>TSH</cell></row><row><cell></cell><cell cols="4">Seen Unseen Seen Unseen</cell></row><row><cell>ResNet-50 Baseline</cell><cell>55.6</cell><cell>30.2</cell><cell>49.8</cell><cell>34.2</cell></row><row><cell>RepMatch</cell><cell>55.7</cell><cell>30.3</cell><cell>48.7</cell><cell>33.8</cell></row><row><cell>+ MultiView Proj. (MVP)</cell><cell>52.7</cell><cell>28.5</cell><cell>47.8</cell><cell>33.7</cell></row><row><cell>+ MVP + 3D loss (Eq. 6)</cell><cell>57.9</cell><cell>35.5</cell><cell>52.3</cell><cell>37.8</cell></row><row><cell>+ MVP + cam reg (Eq. 7)</cell><cell>54.7</cell><cell>30.8</cell><cell>50.7</cell><cell>34.4</cell></row><row><cell cols="2">+ MVP + 3D loss + cam reg 58.9</cell><cell>42.7</cell><cell>54.5</cell><cell>39.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison to other approaches on Toyota SmartHome (TSH) and the Unseen MLB Views and NTU-RGB+D.</figDesc><table><row><cell>Method</cell><cell>TSH</cell><cell></cell><cell>MLB</cell><cell>NTU</cell></row><row><cell></cell><cell cols="3">CV1 CV2 Unseen</cell><cell>CV</cell></row><row><cell>IDT [43]</cell><cell cols="2">20.9 23.7</cell><cell>27.3</cell><cell>-</cell></row><row><cell cols="3">Pose LSTM [8] 13.4 17.2</cell><cell>-</cell><cell>-</cell></row><row><cell>I3D [2]</cell><cell cols="2">34.9 45.1</cell><cell>30.1</cell><cell>-</cell></row><row><cell>STA [8]</cell><cell cols="2">35.2 50.3</cell><cell>-</cell><cell>94.6</cell></row><row><cell>PEM [26]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>95.2</cell></row><row><cell>Ours</cell><cell cols="2">39.6 54.6</cell><cell>42.7</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the number of parameters in the 3 main models. Adding the geometric projection layer only adds 280k parameters, but greatly improves performance.</figDesc><table><row><cell>Model</cell><cell># params</cell><cell>?</cell></row><row><cell>(2+1)D ResNet-50</cell><cell>21.3M</cell><cell>0</cell></row><row><cell>(2+1)D ResNet-50 + Ours</cell><cell>21.5M</cell><cell>280k</cell></row><row><cell>Spherical CNNs</cell><cell>21.2M</cell><cell>-123k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>How many cameras to use.</figDesc><table><row><cell>Method</cell><cell></cell><cell>MLB</cell><cell></cell><cell>TSH</cell></row><row><cell></cell><cell cols="4">Seen Unseen Seen Unseen</cell></row><row><cell cols="2">Baseline 55.6</cell><cell>30.2</cell><cell>49.8</cell><cell>34.2</cell></row><row><cell>1 Cam</cell><cell>57.4</cell><cell>38.6</cell><cell>53.2</cell><cell>38.5</cell></row><row><cell>2 Cams</cell><cell>58.1</cell><cell>41.8</cell><cell>53.9</cell><cell>39.1</cell></row><row><cell>4 Cams</cell><cell>58.9</cell><cell>42.7</cell><cell>54.5</cell><cell>39.6</cell></row><row><cell>8 Cams</cell><cell>58.7</cell><cell>42.7</cell><cell>54.5</cell><cell>39.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Where in network to add layer.</figDesc><table><row><cell>Method</cell><cell>MLB</cell><cell></cell><cell>TSH</cell></row><row><cell cols="4">Seen Unseen Seen Unseen</cell></row><row><cell>Block 1 57.8</cell><cell>42.1</cell><cell>54.3</cell><cell>39.2</cell></row><row><cell>Block 2 58.3</cell><cell>42.4</cell><cell>54.4</cell><cell>39.2</cell></row><row><cell>Block 3 58.9</cell><cell>42.7</cell><cell>54.5</cell><cell>39.6</cell></row><row><cell>Block 4 57.4</cell><cell>41.7</cell><cell>53.8</cell><cell>38.9</cell></row><row><cell>Block 5 57.1</cell><cell>40.9</cell><cell>53.3</cell><cell>37.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the National Science Foundation (IIS-2104404 and CNS-2104416).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. PyTorch Implementation</head><p>We provide the code here to implement the camera projection layer. import numpy as np import torch import torch.nn as nn import torch.nn.functional as F device = torch.device('cuda') def rotation_tensor(theta, phi, psi, b=1):</p><p>""" Takes theta, phi, and psi and generates the 3x3 rotation matrix. Works for batched ops As well, returning a Bx3x3 matrix. """ one = torch.ones(b, 1, 1).to(device) zero = torch.zeros(b, 1, 1).to(device) rot_x = torch.cat(( torch.cat((one, zero, zero), 1), torch.cat((zero, theta.cos(), theta.sin()), 1), torch.cat((zero, -theta.sin(), theta.cos()), 1), ), 2) rot_y = torch.cat(( torch.cat((phi.cos(), zero, -phi.sin()), 1), torch.cat((zero, one, zero), 1), torch.cat((phi.sin(), zero, phi.cos()), 1), ), 2) rot_z = torch.cat(( torch.cat((psi.cos(), -psi.sin(), zero), 1), torch.cat((psi.sin(), psi.cos(), zero), 1), torch.cat((zero, zero, one), 1) ), 2) return torch.bmm(rot_z, torch.bmm(rot_y, rot_x)) class CameraProps(nn.Module):</p><p>""" Generates the extrinsic rotation and translation matrix For the current camera. Takes some feature as input, then Returns the rotation matrix (3x3) and translation (3x1) """ def __init__(self, channels):</p><p>super(CameraProps, self This layer can easily be inserted anywhere into a CNN. For example, assume the following code generates a ResNet. Then the camera transform is used as:</p><p>class Net(nn.Module):</p><p>def __init__(self, ...):</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Viewinvariant motion trajectory-based activity classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashfaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Khokhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schonfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="54" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An informationrich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inferring unseen views of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Chao-Yeh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2003" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometry-aware recurrent neural networks for active visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricson</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5081" to="5091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7024" to="7033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10130</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Spherical cnns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toyota smarthome: Real-world activities of daily living</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srijan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Koperski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Garattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianpiero</forename><surname>Francesca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="833" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning so (3) equivariant representations with spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Equivariant multi-view networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinshuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1568" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to recognize activities from the wrong view point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa Kamali</forename><surname>Tabrizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="154" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">View-invariant action recognition based on artificial neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Alexandros Iosifidis, Anastasios Tefas, and Ioannis Pitas</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="412" to="424" />
		</imprint>
	</monogr>
	<note>IEEE transactions on neural networks and learning systems</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Calibnet: Geometrically supervised extrinsic calibration using 3d spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Karnik Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K Madhava</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (&apos;)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1110" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-view action recognition from temporal selfsimilarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="293" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshifumi</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5010" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ferdous Sohel, and Farid Boussaid. A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senjian</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximummargin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2823" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint angles similarities and hog2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshed</forename><surname>Ohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fine-grained activity recognition in baseball videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR Workshop on Computer Vision in Sports</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">View-invariance in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cen</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">View-invariant action recognition using fundamental ratios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuping</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">View-invariant action recognition from point triplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuping</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1898" to="1905" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Charades-ego: A large-scale dataset of paired third and first person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09626</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Invariant recognition drives neural representations of action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyla</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1005859</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-view 3d models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="322" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">C3d: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Synsin: End-to-end view synthesis from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7467" to="7477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Crossview action recognition over heterogeneous feature spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
