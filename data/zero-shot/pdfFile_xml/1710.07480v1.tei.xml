<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HDR image reconstruction from a single exposure using deep CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kronander</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">GYORGY DENES</orgName>
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?</forename><forename type="middle">K</forename><surname>Mantiuk</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Unger</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kronander</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyorgy</forename><surname>Denes</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?</forename><forename type="middle">K</forename><surname>Mantiuk</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename></persName>
							<affiliation key="aff4">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HDR image reconstruction from a single exposure using deep CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3130800.3130816</idno>
					<note>0730-0301/2017/11-ART178 $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Computing methodologies ? Image processing</term>
					<term>Neural networks</term>
					<term>Additional Key Words and Phrases: HDR reconstruction, inverse tone-mapping, deep learning, convolutional network ACM Reference format:</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reconstructed HDR image Input LDR image Input Reconstruction Input Reconstruction Ground truth Ground truth Fig. 1. The exposure of the input LDR image in the bottom left has been reduced by 3 stops, revealing loss of information in saturated image regions. Using the proposed CNN trained on HDR image data, we can reconstruct the highlight information realistically (top right). The insets show that the high luminance of the street lights can be recovered (top row), as well as colors and details of larger saturated areas (bottom row). The exposures of the insets have been reduced by 5 and 4 stops in the top and bottom rows, respectively, in order to facilitate comparisons. All images have been gamma corrected for display.</p><p>Camera sensors can only capture a limited range of luminance simultaneously, and in order to create high dynamic range (HDR) images a set of different exposures are typically combined. In this paper we address the problem of predicting information that have been lost in saturated image areas, in order to enable HDR reconstruction from a single exposure. We show that this problem is well-suited for deep learning algorithms, and propose a deep convolutional neural network (CNN) that is specifically designed taking into account the challenges in predicting HDR values. To train the CNN we gather a large dataset of HDR images, which we augment by simulating sensor saturation for a range of cameras. To further boost robustness, we pre-train the CNN on a simulated HDR dataset created from a subset of the MIT Places database. We demonstrate that our approach can reconstruct high-resolution visually convincing HDR results in a wide range of situations, and that it generalizes well to reconstruction of images captured with arbitrary and low-end cameras that use unknown camera response functions and post-processing. Furthermore, we compare to existing methods for HDR expansion, and show high quality results also for image based lighting. Finally, we evaluate the results in a subjective experiment performed on an HDR display. This shows that the reconstructed HDR images are visually convincing, with large improvements as compared to existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. The exposure of the input LDR image in the bottom left has been reduced by 3 stops, revealing loss of information in saturated image regions. Using the proposed CNN trained on HDR image data, we can reconstruct the highlight information realistically (top right). The insets show that the high luminance of the street lights can be recovered (top row), as well as colors and details of larger saturated areas (bottom row). The exposures of the insets have been reduced by 5 and 4 stops in the top and bottom rows, respectively, in order to facilitate comparisons. All images have been gamma corrected for display.</p><p>Camera sensors can only capture a limited range of luminance simultaneously, and in order to create high dynamic range (HDR) images a set of different exposures are typically combined. In this paper we address the problem of predicting information that have been lost in saturated image areas, in order to enable HDR reconstruction from a single exposure. We show that this problem is well-suited for deep learning algorithms, and propose a deep convolutional neural network (CNN) that is specifically designed taking into account the challenges in predicting HDR values. To train the CNN we gather a large dataset of HDR images, which we augment by simulating sensor saturation for a range of cameras. To further boost robustness, we pre-train the CNN on a simulated HDR dataset created from a subset of the MIT Places database. We demonstrate that our approach can reconstruct high-resolution visually convincing HDR results in a wide range of situations, and that it generalizes well to reconstruction of images captured with arbitrary and low-end cameras that use unknown camera response functions and post-processing. Furthermore, we compare to existing methods for HDR expansion, and show high quality results also for image based lighting. Finally, we evaluate the results in a subjective experiment performed on an HDR display. This shows that the reconstructed HDR images are visually convincing, with large improvements as compared to existing methods. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ? 2017 Association for Computing Machinery. 0730-0301/2017/11-ART178 $15.00 https://doi.org/10. <ref type="bibr">1145/3130800.3130816</ref> CCS Concepts: ? Computing methodologies ? Image processing; Neural networks;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>High dynamic range (HDR) images can significantly improve the viewing experience -viewed on an HDR capable display or by means of tone-mapping. With the graphics community as an early adopter, HDR images are now routinely used in many applications including photo realistic image synthesis and a range of post-processing operations; for an overview see <ref type="bibr" target="#b2">[Banterle et al. 2011;</ref><ref type="bibr" target="#b14">Dufaux et al. 2016;</ref><ref type="bibr" target="#b43">Reinhard et al. 2010</ref>]. The ongoing rapid development of HDR technologies and cameras has now made it possible to collect the data required to explore recent advances in deep learning for HDR imaging problems.</p><p>In this paper, we propose a novel method for reconstructing HDR images from low dynamic range (LDR) input images, by estimating missing information in bright image parts, such as highlights, lost due to saturation of the camera sensor. We base our approach on a 178:2 ? G. Eilertsen, J. Kronander, G. Denes, R. K. Mantiuk and J. Unger fully convolutional neural network (CNN) design in the form of a hybrid dynamic range autoencoder. Similarly to deep autoencoder architectures <ref type="bibr" target="#b24">[Hinton and Salakhutdinov 2006;</ref><ref type="bibr" target="#b54">Vincent et al. 2008]</ref>, the LDR input image is transformed by an encoder network to produce a compact feature representation of the spatial context of the image. The encoded image is then fed to an HDR decoder network, operating in the log domain, to reconstruct an HDR image. Furthermore, the network is equipped with skip-connections that transfer data between the LDR encoder and HDR decoder domains in order to make optimal use of high resolution image details in the reconstruction. For training, we first gather data from a large set of existing HDR image sources in order to create a training dataset. For each HDR image we then simulate a set of corresponding LDR exposures using a virtual camera model. The network weights are optimized over the dataset by minimizing a custom HDR loss function. As the amount of available HDR content is still limited we utilize transfer-learning, where the weights are pre-trained on a large set of simulated HDR images, created from a subset of the MIT Places database <ref type="bibr" target="#b58">[Zhou et al. 2014</ref>].</p><p>Expansion of LDR images for HDR applications is commonly referred to as inverse tone-mapping (iTM). Most existing inverse tone-mapping operators (iTMOs) are not very successful in reconstruction of saturated pixels. This has been shown in a number of studies <ref type="bibr" target="#b0">[Aky?z et al. 2007;</ref><ref type="bibr" target="#b35">Masia et al. 2009</ref>], in which na?ve methods or non-processed images were more preferred than the results of those operators. The existing operators focus on boosting the dynamic range to look plausible on an HDR display, or to produce rough estimates needed for image based lighting (IBL). The proposed method demonstrates a step improvement in the quality of reconstruction, in which the structures and shapes in the saturated regions are recovered. It offers a range of new applications, such as exposure correction, tone-mapping, or glare simulation.</p><p>The main contributions of the paper can be summarized as:</p><p>(1) A deep learning system that can reconstruct a high quality HDR image from an arbitrary single exposed LDR image, provided that saturated areas are reasonably small. (2) A hybrid dynamic range autoencoder that is tailored to operate on LDR input data and output HDR images. It utilizes HDR specific transfer-learning, skip-connections, color space and loss function. (3) The quality of the HDR reconstructions is confirmed in a subjective evaluation on an HDR display, where predicted images are compared to HDR and LDR images as well as a representative iTMO using a random selection of test images in order to avoid bias in image selection. (4) The HDR reconstruction CNN together with trained parameters are made available online, enabling prediction from any LDR images: https:// github.com/ gabrieleilertsen/ hdrcnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 HDR reconstruction</head><p>In order to capture the entire range of luminance in a scene it is necessary to use some form of exposure multiplexing. While static scenes commonly are captured using multiplexing exposures in the time domain <ref type="bibr" target="#b11">[Debevec and Malik 1997;</ref><ref type="bibr" target="#b34">Mann and Picard 1994;</ref><ref type="bibr" target="#b53">Unger and Gustavson 2007]</ref>, dynamic scenes can be challenging as robust exposure alignment is needed. This can be solved by techniques such as multi-sensor imaging <ref type="bibr" target="#b31">[Kronander et al. 2014;</ref><ref type="bibr" target="#b52">Tocci et al. 2011]</ref> or by varying the per-pixel exposure <ref type="bibr" target="#b38">[Nayar and Mitsunaga 2000]</ref> or gain <ref type="bibr" target="#b22">[Hajisharif et al. 2015]</ref>. Furthermore, saturated regions can be encoded in glare patterns <ref type="bibr" target="#b47">[Rouf et al. 2011]</ref> or with convolutional sparse coding <ref type="bibr" target="#b49">[Serrano et al. 2016</ref>]. However, all these approaches introduce other limitations such as bulky and custom built systems, calibration problems, or decreased image resolution.</p><p>Here, we instead tackle the problem by reconstructing visually convincing HDR images from single images that have been captured using standard cameras without any assumptions on the imaging system or camera calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inverse tone-mapping</head><p>Inverse tone-mapping is a general term used to describe methods that utilize LDR images for HDR image applications <ref type="bibr" target="#b5">[Banterle et al. 2006</ref>]. The intent of different iTMOs may vary. If it is to display standard images on HDR capable devices, maximizing the subjective quality, there is some evidence that global pixel transformations may be preferred <ref type="bibr" target="#b35">[Masia et al. 2009</ref>]. Given widely different input materials, such methods are less likely to introduce artifacts compared to more advanced strategies. The transformation could be a linear scaling <ref type="bibr" target="#b0">[Aky?z et al. 2007]</ref> or some non-linear function <ref type="bibr" target="#b35">[Masia et al. 2009</ref><ref type="bibr" target="#b36">[Masia et al. , 2017</ref>. These methods modify all pixels without reconstructing any of the lost information.</p><p>A second category of iTMOs attempt to reconstruct saturated regions to mimic a true HDR image. These are expected to generate results that look more like a reference HDR, which was also indicated by the pair-wise comparison experiment on an HDR display performed by . <ref type="bibr" target="#b37">Meylan et al. [2006]</ref> used a linear transformation, but applied different scalings in highlight regions. <ref type="bibr" target="#b5">Banterle et al. [2006]</ref> first linearized the input image, followed by boosting highlights using an expand map derived from the median cut algorithm. The method was extended for video processing, and with additional features such as automatic calibration and crossbilateral filtering of the expand map <ref type="bibr" target="#b6">[Banterle et al. 2008]</ref>. <ref type="bibr" target="#b44">Rempel et al. [2007]</ref> also utilized an expand map, but computed this from Gaussian filtering in order to achieve real-time performance. <ref type="bibr" target="#b55">Wang et al. [2007]</ref> applied inpainting techniques on the reflectance component of highlights. The method is limited to textured highlights, and requires some manual interaction. Another semi-manual method was proposed by <ref type="bibr" target="#b12">Didyk et al. [2008]</ref>, separating the image into diffuse, reflections and light sources. The reflections and light sources were enhanced, while the diffuse component was left unmodified. More recent methods includes the iTMO by <ref type="bibr">Kovaleski and Oliviera [2014]</ref>, that focus on achieving good results over a wide range of exposures, making use of a cross-bilateral expand map <ref type="bibr" target="#b29">[Kovaleski and Oliveira 2009]</ref>.</p><p>For an in-depth overview of inverse tone-mapping we refer to the survey by . Compared to the existing iTMOs, our approach achieves significantly better results by learning from exploring a wide range of different HDR scenes. Furthermore, the reconstruction is completely automatic with no user parameters and runs within a second on modern hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bit-depth extension</head><p>A standard 8-bit LDR image is affected not only by clipping but also by quantization. If the contrast or exposure is significantly increased, quantization can be revealed as banding artifacts. Existing methods for decontouring, or bit-depth extension, include dithering methods that use noise in order to hide the banding artifacts <ref type="bibr" target="#b9">[Daly and Feng 2003]</ref>. Decontouring can also be performed using low-pass filtering followed by quantization, in order to detect false contours <ref type="bibr" target="#b10">[Daly and Feng 2004]</ref>. There are also a number of edge-preserving filters used for the same purpose. In this work we do not focus on decontouring, which is mostly a problem in under-exposed images. Also, since we treat the problem of predicting saturated image regions, the bit depth will be increased with the reconstructed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Convolutional neural networks</head><p>CNNs have recently been applied to a large range of computer vision tasks, significantly improving on the performance of classical supervised tasks such as image classification <ref type="bibr" target="#b50">[Simonyan and Zisserman 2014]</ref>, object detection <ref type="bibr" target="#b45">[Ren et al. 2015]</ref> and semantic segmentation <ref type="bibr" target="#b33">[Long et al. 2015]</ref>, among others. Recently CNNs have also shown great promise for image reconstruction problems related to the challenges faced in inverse tone-mapping, such as compression artifact reduction <ref type="bibr" target="#b51">[Svoboda et al. 2016]</ref>, super-resolution <ref type="bibr" target="#b32">[Ledig et al. 2016]</ref>, and colorization <ref type="bibr" target="#b25">[Iizuka et al. 2016]</ref>. Recent work on inpainting <ref type="bibr" target="#b40">[Pathak et al. 2016;</ref><ref type="bibr" target="#b56">Yang et al. 2016]</ref> have also utilized variants of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b20">[Goodfellow et al. 2014</ref>] to produce visually convincing results. However, as these methods are based on adversarial training, results tend to be unpredictable and can vary widely from one training iteration to the next. To stabilize training, several tricks are used in practice, including restricting the output intensity, which is problematic for HDR generation. Furthermore, these methods are limited to a single image resolution, with results only shown so far for very low resolutions.</p><p>Recently, deep learning has also been successfully applied for improving classical HDR video reconstruction from multiple exposures captured over time <ref type="bibr" target="#b27">[Kalantari and Ramamoorthi 2017]</ref>. In terms of reconstructing HDR from one single exposed LDR image, the recent work by <ref type="bibr" target="#b57">Zhang and Lalonde [2017]</ref> is most similar to ours. They use an autoencoder <ref type="bibr" target="#b24">[Hinton and Salakhutdinov 2006]</ref> in order to reconstruct HDR panoramas from single exposed LDR counterparts. However, the objective of this work is specifically to recoverer high intensities near the sun in order to use the prediction for IBL. Also, the method is only trained using rendered panoramas of outdoor environments where the sun is assumed to be in the same azimuthal position in all images. Given these restrictions, and that predictions are limited to 128 ? 64 pixels, the results are only applicable for IBL of outdoor scenes. Compared to this work, we propose a solution to a very general problem specification without any such assumptions, and where any types of saturated regions are considered. We also introduce several key modifications to the standard autoencoder design <ref type="bibr" target="#b24">[Hinton and Salakhutdinov 2006]</ref>, and show that this significantly improves the performance.</p><p>Finally, it should be mentioned that the concurrent work by <ref type="bibr" target="#b15">Endo et al. [2017]</ref> also treats inverse tone-mapping using deep learning algorithms, by using a different pipeline design. Given a single <ref type="bibr">(a)</ref>  exposure input image, the method uses autoencoders in order to predict a set of LDR images with both shorter and longer exposures. These are subsequently combined using standard methods, in order to reconstruct the final HDR image.</p><formula xml:id="formula_0">f ?1 (D ) (b) exp(?) (c) ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (d)? (e) H</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HDR RECONSTRUCTION MODEL 3.1 Problem formulation and constraints</head><p>Our objective is to predict values of saturated pixels given an LDR image produced by any type of camera. In order to produce the final HDR image, the predicted pixels are combined with the linearized input image. The final HDR reconstructed pixel? i,c with spatial index i and color channel c is computed using a pixel-wise blending with the blend value ? i ,</p><formula xml:id="formula_1">H i,c = (1 ? ? i )f ?1 (D i,c ) + ? i exp(? i,c ),<label>(1)</label></formula><p>where D i,c is the input LDR image pixel and? i,c is the CNN output (in the log domain). The inverse camera curve f ?1 is used to transform the input to the linear domain. The blending is a linear ramp starting from pixel values at a threshold ? , and ending at the maximum pixel value,</p><formula xml:id="formula_2">? i = max(0, max c (D i,c ) ? ? ) 1 ? ? .<label>(2)</label></formula><p>In all examples we use ? = 0.95, where the input is defined to be in the range [0, 1]. The linear blending prevents banding artifacts between predicted highlights and their surroundings, as compared to a binary mask. It is also used to define the loss function in the training, as described in Section 3.4. For an illustration of the components of the blending, see <ref type="figure" target="#fig_0">Figure 2</ref>. Due to the blending predictions are focused on reconstructing around the saturated areas, and artifacts may appear in other image regions <ref type="figure" target="#fig_0">(Figure 2(b)</ref>).</p><p>The blending means that the input image is kept unmodified in the non-saturated regions, and linearization has to be made from either knowledge of the specific camera used or by assuming a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain transformation skip-connection</head><p>Skip-layer <ref type="figure">Fig. 3</ref>. Fully convolutional deep hybrid dynamic range autoencoder network, used for HDR reconstruction. The encoder converts an LDR input to a latent feature representation, and the decoder reconstructs this into an HDR image in the log domain. The skip-connections include a domain transformation from LDR display values to logarithmic HDR, and the fusion of the skip-layers is initialized to perform an addition. The network is pre-trained on a subset of the Places database, and deconvolutions are initialized to perform bilinear upsampling. While the specified spatial resolutions are given for a 320 ? 320 pixels input image, which is used in the training, the network is not restricted to a fixed image size.</p><p>certain camera curve f . We do not attempt to perform linearization or color correction with the CNN. Furthermore, information lost due to quantization is not recovered. We consider these problems separate for the following reasons:</p><p>(1) Linearization: The most general approach would be to linearize either within the network or by learning the weights of a parametric camera curve. We experimented with both these approaches, but found them to be too problematic given any input image. Many images contain too little information in order to evaluate an accurate camera curve, resulting in high variance in the estimation. On average a carefully chosen assumed transformation performs better. (2) Color correction: The same reasoning applies to color correction. Also, this would require all training data to be properly color graded, which is not the case. This means that given a certain white balancing transformation of the input, the saturated regions are predicted within this transformed color space.</p><p>(3) Quantization recovery: Information lost due to quantization can potentially be reconstructed from a CNN. However, this problem is more closely related to super-resolution and compression artifact reduction, for which deep learning techniques have been successfully applied <ref type="bibr" target="#b13">[Dong et al. 2015;</ref><ref type="bibr" target="#b32">Ledig et al. 2016;</ref><ref type="bibr" target="#b51">Svoboda et al. 2016</ref>]. Furthermore, a number of filtering techniques can reduce banding artifacts due to quantization <ref type="bibr" target="#b7">[Bhagavathy et al. 2007;</ref><ref type="bibr" target="#b10">Daly and Feng 2004]</ref>.</p><p>Although we only consider the problem of reconstructing saturated image regions, we argue that this is the far most important part when transforming LDR images to HDR, and that it can be used to cover a wide range of situations. Typical camera sensors can capture between 8 and 12 stops of dynamic range, which is often sufficient to register all textured areas. However, many scenes contain a small number of pixels that are very bright and thus saturated. These can be reconstructed with the proposed method, instead of capturing multiple exposures or using dedicated HDR cameras. Our method is not intended to recover the lower end of the dynamic range, which is below the noise floor of a sensor. Instead, the problem of underexposed areas is best addressed by increasing exposure time or gain (ISO). This will result in more saturated pixels, which then can be recovered using our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hybrid dynamic range autoencoder</head><p>Autoencoder architectures transform the input to a low-dimensional latent representation, and a decoder is trained to reconstruct the full-dimensional data <ref type="bibr" target="#b24">[Hinton and Salakhutdinov 2006]</ref>. A denoising autoencoder is trained with a corrupted input, with the objective of reconstructing the original uncorrupted data <ref type="bibr" target="#b54">[Vincent et al. 2008</ref>]. This is achieved by mapping to a higher level representation that is invariant to the specific corruption. We use the same concept for reconstruction of HDR images. In this case the corruption is clipped highlights, and the encoder maps the LDR to a representation that can be used by the decoder for HDR reconstruction. This means that the encoder and decoder work in different domains of pixel values, and we design them to optimally account for this. Since our objective is to reconstruct larger images than is practical to use in training, the latent representation is not a fully connected layer, but a lowresolution multi-channel image. Such a fully convolutional network (FCN) enables predictions at any resolution that is a multiple of the autoencoder downscaling factor. The complete autoencoder design is depicted in <ref type="figure">Figure 3</ref>. Convolutional layers followed by max-pooling encodes the input LDR in a W 32 ? H 32 ? 512 latent image representation, where W and H are the image width and height, respectively. The encoder layers correspond to the well-known VGG16 network <ref type="bibr" target="#b50">[Simonyan and Zisserman 2014]</ref>, but without the fully connected layers.</p><p>While the encoder operates directly on the LDR input image, the decoder is responsible for producing HDR data. For this reason the decoder operates in the log domain. This is accomplished using a loss function that compares the output of the network to the log of the ground truth HDR image, as explained in Section 3.4. For the image upsampling, we use deconvolutional layers with a spatial resolution of 4 ? 4 initialized to perform bilinear upsampling <ref type="bibr" target="#b33">[Long et al. 2015]</ref>. While nearest neighbor up-sampling followed by convolution has been shown to alleviate artifacts that are common in decoder deconvolutions <ref type="bibr" target="#b39">[Odena et al. 2016</ref>], we have not experienced such problems, and instead use the more general deconvolutional layers. All layers of the network use ReLU activation functions, and after each layer of the decoder a batch normalization layer <ref type="bibr" target="#b26">[Ioffe and Szegedy 2015]</ref> is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Domain transformation and skip-connections</head><p>The encoding of the input image means that much of the high resolution information in earlier layers of the encoder are lost. The information could potentially be used by the decoder to aid reconstruction of high frequency details in saturated regions. Thus, we introduce skip-connections that transfer data between both high and low level features in the encoder and decoder.</p><p>Skip-connections have been shown to be useful for constructing deeper network architectures which improve performance in a variety of tasks <ref type="bibr" target="#b23">[He et al. 2016</ref>]. For autoencoders, where layers have different spatial resolution, a separate residual stream can be maintained in full resolution, with connections to each layer within the autoencoder <ref type="bibr" target="#b41">[Pohlen et al. 2017]</ref>. Alternatively, skip-connections between layers of equal resolution in encoder and decoder have also been shown to boost performance in a variety of imaging tasks using autoencoders <ref type="bibr" target="#b46">[Ronneberger et al. 2015;</ref><ref type="bibr" target="#b57">Zhang and Lalonde 2017]</ref>.</p><p>Our autoencoder uses skip-connections to transfer each level of the encoder to the corresponding level on the decoder side. Since the encoder and decoder process different types of data (see Section 3.2), the connections include a domain transformation described by an inverse camera curve and a log transformation, mapping LDR display values to a logarithmic HDR representation. Since the camera curve is unknown, we have to assume its shape. Although a sigmoid function fits well with camera curves in general <ref type="bibr" target="#b21">[Grossberg and Nayar 2003</ref>], its inverse is not continuous over IR + . The linearization of the skip-connections is therefore done using a gamma function A skip-connected layer is typically added to the output of the layer at the connection point. However, to allow for additional freedom, we concatenate the two layers along the feature dimension. That is, given two W ? H ? K dimensional layers, the concatenated layer is W ? H ? 2K. The decoder then makes a linear combination of these, that reduces the number of features back to K. This is equivalent to using a convolutional layer with a filter size of 1 ? 1, where the number of input and output channels are 2K and K, respectively, as depicted in <ref type="figure">Figure 3</ref>. More specifically, the complete LDR to HDR skip connection is defined as</p><formula xml:id="formula_3">f ?1 (x) = x ? , where ? = 2. (a) Input (b) Without skip (c) With skip (d) Ground truth</formula><formula xml:id="formula_4">h D i = ? W h D i log f ?1 h E i + ? + b .<label>(3)</label></formula><p>The vectors h E i and h D i denote the slices across all the feature channels k ? {1, ..., K } of the encoder and decoder layer tensors y E , y D ? IR W ?H ?K , for one specific pixel i. Furthermore,h D i is the decoder feature vector with information fused from the skipconnected vector h E i . b is the bias of the feature fusion, and ? is the activation function, in our case the rectified linear unit (ReLU). A small constant ? is used in the domain transformation in order to avoid zero values in the log transform. Given K features, h E and h D are 1 ? K vectors, and W is a 2K ? K weight matrix, which maps the 2K concatenated features to K dimensions. This is initialized to perform an addition of encoder and decoder features, setting the weights as</p><formula xml:id="formula_5">W 0 = ? ? ? ? ? ? ? ? ? 1 0 . . . 0 1 0 . . . 0 0 1 . . . 0 0 1 . . . 0 . . . . . . . . . . . . . . . . . . . . . . . . 0 0 . . . 1 0 0 . . . 1 ? ? ? ? ? ? ? ? ? , b 0 = 0.<label>(4)</label></formula><p>During training, these weights can be optimized to improve the performance of the skip-connection. Since the linear combination of features is performed in the log domain, it corresponds to multiplications of linear HDR data. This is an important characteristic of the domain transformation skip-connections as compared to existing skip architectures.</p><p>-6 stops -6 stops -6 stops -6 stops An example of the impact of the described skip-connection architecture is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. The autoencoder design is able to reconstruct HDR information from an encoded LDR image. However, all information needed by the decoder has to travel trough the intermediate encoded representation. Adding the skip-connections enables a more optimal use of existing details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">HDR loss function</head><p>A cost function formulated directly on linear HDR values will be heavily influenced by high luminance values, leading to underestimation of important differences in the lower range of luminaces. The few existing deep learning systems that predict HDR have treated this problem by defining the objective function in terms of tonemapped luminance <ref type="bibr" target="#b27">[Kalantari and Ramamoorthi 2017;</ref><ref type="bibr" target="#b57">Zhang and Lalonde 2017</ref>]. In our system the HDR decoder is instead designed to operate in the log domain. Thus, the loss is formulated directly on logarithmic HDR values, given the predicted log HDR image? and the linear ground truth H ,</p><formula xml:id="formula_6">L(?, H ) = 1 3N i,c ? i ? i,c ? log H i,c + ? 2 ,<label>(5)</label></formula><p>where N is the number of pixels. Since H i,c ? IR + , the small constant ? removes the singularity at zero pixel values. The cost formulation is perceptually motivated by the the close to logarithmic response of the human visual system (HVS) in large areas of the luminance range, according to the Weber-Fechner law <ref type="bibr" target="#b16">[Fechner 1965</ref>]. The law implies a logarithmic relationship between physical luminance and the perceived brightness. Thus, a loss formulated in the log domain makes perceived errors spread approximately uniformly across the luminance range. As described in Section 3.1, we use only the information from the predicted HDR image? around saturated areas. This is also reflected by the loss function in <ref type="figure" target="#fig_3">Equation 5</ref> where the blend map ? from Equation 2 is used to spatially weight the error.</p><p>Treating the illuminance and reflectance components separately makes sense from a perceptual standpoint, as the visual system may indirectly perform such separation when inferring reflectance or discounting illumination <ref type="bibr" target="#b18">[Gilchrist and Jacobsen 1984]</ref>. We therefore also propose another, more flexible loss function that treats illuminance and reflectance separately. The illumination component I describes the global variations, and is responsible for the high dynamic range. The reflectance R stores information about details and colors. This is of lower dynamic range and modulates the illuminance to create the final HDR image, H i,c = I i R i,c . We approximate the log illuminance by means of a Gaussian low-pass filter G ? on the log luminance L? ,</p><formula xml:id="formula_7">log I? i = G ? * L? i , log R? i,c =? i,c ? log I? i .<label>(6)</label></formula><p>Since the estimation is performed in the log domain, the log reflectance is the difference between? and log illuminance. L? is a linear combination of the color channels,</p><formula xml:id="formula_8">L? i = log( c w c exp(? i,c )),</formula><p>where w = {0.213, 0.715, 0.072}. The standard deviation of the Gaussian filter is set to ? = 2. The resulting loss function using I and R is defined as</p><formula xml:id="formula_9">L I R (?, H ) = ? N i ? i log I? i ? log I y i 2 + 1 ? ? 3N i,c ? i log R? i,c ? log R y i,c 2 ,<label>(7)</label></formula><p>where y = log(H + ?) to simplify notation. The user-specified parameter ? can be tuned for assigning different importance to the illuminance and reflectance components. If not stated otherwise, we use the illuminance + reflectance (I/R) loss with ? = 0.5 for all results in this paper. This puts more importance on the illuminance since the error in this component generally is larger. <ref type="figure" target="#fig_3">Figure 5</ref> shows examples of predictions where the optimization has been performed with different values of ?. With more relative weight on illuminance, high intensity areas are in general better predicted, which e.g. could benefit IBL applications. If the reflectance is given more importance, local colors and details can be recovered with higher robustness, for better quality e.g. in post-processing applications. The visual improvements from using the I/R loss compared to the direct loss in Equation 5 are subtle. However, in general it tends to produce less artifacts in large saturated areas, as exemplified in <ref type="figure">Figure 6</ref>. One possible explanation is that the Gaussian low-pass filter in the loss function could have a regularizing effect, since it makes the loss in a pixel influenced by its neighborhood. This observation is further supported by the comparison in <ref type="table" target="#tab_0">Table 1,</ref>  Freq.</p><p>[norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of pixels]</head><p>Flickr Places Places non-saturated HDR database <ref type="figure">Fig. 7</ref>. Histograms over two LDR datasets and the pre-processed HDR dataset. The creation of the HDR data is described in Section 4. For the LDR data, the probabilities show a large increase close to 1, indicating saturated information. The HDR dataset contains such information, represented by the tail of decreasing frequency.</p><p>the I/R loss lowers the final error in Equation 5 by more than 5%, demonstrating better generalization to the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HDR IMAGE DATASET</head><p>A key challenge for a learning-based HDR reconstruction is to obtain a sufficiently large set of well structured training data. However, an increasing amount of HDR content has become available, in particular through recent HDR video datasets <ref type="bibr" target="#b1">[Azimi et al. 2014;</ref><ref type="bibr" target="#b8">Boitard et al. 2014;</ref><ref type="bibr" target="#b17">Froehlich et al. 2014;</ref><ref type="bibr" target="#b31">Kronander et al. 2014</ref>]. We were able to gather a total of 1121 HDR images and 67 HDR video sequences. The sources of the data are specified in the supplementary document. 4 video sequences and 95 images are separated from these to create the test set used throughout this paper, and the rest are used for training. Since consecutive frames from a video sequence are expected to be very similar, we use every 10th frame. Together with the static images, this results in a total of ?3700 HDR images. Using a virtual camera, a carefully designed set of data augmentation operations are then applied in order to improve robustness of the predictions.</p><p>Considering each HDR image a real-world scene, we set up a virtual camera that captures a number of random regions of the scene using a randomly selected camera calibration. This provides us with an augmented set of LDR and corresponding HDR images that are used as input and ground truth for training, respectively. The regions are selected as image crops with random size and position, followed by random flipping and resampling to 320 ? 320 pixels. The camera calibration incorporates parameters for exposure, camera curve, white balance and noise level. These are randomly selected, with the camera curve defined as a parametric function fitted to the database of camera curves collected by <ref type="bibr" target="#b21">Grossberg and Nayar [2003]</ref>. For details on the augmentation, we refer to Appendix A.</p><p>In total we capture ?125K training samples from the HDR dataset using the virtual camera. This augmentation is responsible for creating a final trained model that generalizes well to a wide range of images captured with different cameras. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image statistics</head><p>It is important to note that the dynamic range statistics of LDR and HDR images differ considerably. <ref type="figure">Figure 7</ref> shows averaged histograms over two typical LDR datasets, as well as our HDR dataset of 125K images. The LDR data are composed of around 2.5M and 200K images for Places <ref type="bibr" target="#b58">[Zhou et al. 2014]</ref> and Flickr, respectively. Inspecting the LDR histograms, they show a relatively uniform distribution of pixel values, except for distinct peaks near the maximum value representing information lost due to saturation. In the HDR histogram on the other hand, pixels are not saturated, and are instead represented by an exponentially decaying long tail. Although there are not many pixels with extreme intensities, these are very important to properly learn a plausible HDR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TRAINING</head><p>To initialize the weights in the network we use different strategies for different parts of the network. As we use the convolutional layers from the well-known VGG16 network, we can use pre-trained weights available for large scale image classification on the Places database <ref type="bibr" target="#b58">[Zhou et al. 2014</ref>] to initialize the encoder. The decoder deconvolutions are initiated for bilinear upsampling, and the fusions of skip-connection layers are initiated to perform addition of features <ref type="bibr">(Equation 4</ref>). For the convolutions within the latent image representation (right-most of <ref type="figure">Figure 3</ref>) and the final feature reduction (top-left in <ref type="figure">Figure 3</ref>) we use Xavier initializaion <ref type="bibr" target="#b19">[Glorot and Bengio 2010]</ref>. Minimization is performed with the ADAM optimizer <ref type="bibr" target="#b28">[Kingma and Ba 2014]</ref>, with a learning rate of 5 ? 10 ?5 , on the loss function in Equation 7. In total 800K steps of back-propagation are performed, with a mini-batch size of 8, taking approximately 6 days on an Nvidia Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pre-training on simulated HDR data</head><p>As we have a limited amount of HDR data at hand, we use transfer learning by pre-training the entire network on a large simulated HDR dataset. To this end, we select a subset of the images in the Places database <ref type="bibr" target="#b58">[Zhou et al. 2014]</ref>, requiring that the images should not contain saturated image regions. Given the set of all Places images P, this subset S ? P is defined as</p><formula xml:id="formula_10">S = {D | D ? P, p D (255) &lt; ? } ,<label>(8)</label></formula><p>where p D is the image histogram. For the threshold we use ? = 50/256 2 . Thus, if less than 50 pixels (0.076% of the 256 2 pixels of an image) have maximum value, we use this in the training set. For the Places database this gives ?600K images of the total set size of ?2.5M. The averaged histogram over the subset S, plotted in <ref type="figure">Figure 7</ref>, does not show the peak of saturated pixels as the original set P. By linearizing the images D ? S using the inverse of the camera curve f in Equation 10 and increasing the exposure, H = s f ?1 (D), we create a simulated HDR training dataset. The simulated HDR dataset is prepared in the same manner as in Section 4, but at 224 ? 224 pixels resolution and without resampling. The CNN is trained using the ADAM optimizer with learning rate 2 ? 10 ?5 for 3.2M steps with a batch size of 4.</p><p>The result of this pre-training on synthetic data leads to a significant improvement in performance. Small highlights, which sometimes are underestimated, are better recovered, as illustrated in <ref type="figure">Figure 8</ref>, and less artifacts are introduced in larger saturated regions. <ref type="table" target="#tab_0">Table 1</ref> shows that the error is reduced by more than 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>In this section we present a number of examples, verifying the quality of the HDR reconstructions. Additional visual examples can be found in the supplementary material and video. Furthermore, for prediction using any LDR image the CNN together with trained parameters can be downloaded from: https:// github.com/ gabrieleilertsen/ hdrcnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Test errors</head><p>To justify the different model and training strategies explained in Section 3 and 5, we evaluate the success of the optimization in Table 1. The errors of the different configurations have been averaged over the test set of 95 HDR images, reconstructed at 1024 ? 768 pixels resolution. The LDR images used for reconstruction use virtual exposures and clipping such that 5% of the pixels in each image are saturated. <ref type="table" target="#tab_0">Table 1</ref> shows how different errors are affected by the different training strategies. The CNN without skip-connections can drastically reduce the MSE of the input. However, adding the skipconnections reduces error by an additional 24%, and creates images with substantial improvements in details <ref type="figure" target="#fig_2">(Figure 4)</ref>. Comparing the two different loss functions in Equation 5 and 7, the latter I/R loss shows a lower error both in terms of I/R and direct MSE, with a reduction of 5.8%. Finally, with pre-training and the I/R loss the best training performance is accomplished, lowering the error by 10.7% as compared to no pre-training. All the trainings/pre-trainings that do no use our pre-trained parameters have been initialized  with VGG16 encoder weights trained for classification of the Places dataset <ref type="bibr" target="#b58">[Zhou et al. 2014</ref>]. <ref type="figure" target="#fig_7">Figure 9</ref> demonstrates a set of predictions on HDR images from the test set that have been transformed to LDR by the virtual camera described in Section 4. The examples demonstrate successful HDR reconstruction in a variety of situations. In night scenes, colors and intensities of street lights and illuminated facades can be restored with very convincing quality. The same goes for specular reflections and other smaller highlights in day scenes. Furthermore, in situations where there is some small amount of information left in any of the color channels, details and colors of larger areas can be reconstructed to be very close to the ground truth HDR image. For example, in the right-most column the large area light source does not contain any visual details when inspecting the input image. However, some small amount of invisible information enables recovery of details. Also, while all channels are saturated in the sky in the third column, the spatial context can be utilized to infer a blue color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparisons to ground truth</head><p>In order to visualize the information that is reconstructed by the CNN, <ref type="figure" target="#fig_8">Figure 10</ref> shows the residual,r = max 0,? ? 1 . That is, only the information in highlights is shown, which is not present in the input image D ? [0, 1]. The information corresponds well with the ground truth residual, r = max (0, H ? 1). The complete input and output signals are also plotted for two different scanlines across the images. Complex lighting of street lights and windows can be recovered convincingly (bottom left). However, in some situations of very intense light sources, the luminance is underestimated (bottom right). We elaborate on such limitations in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Reconstruction with real-world cameras</head><p>In order to show that the HDR reconstruction model generalizes to real-world cameras, <ref type="figure" target="#fig_9">Figure 11</ref> shows two of the scenes from <ref type="figure" target="#fig_7">Figure 9</ref>, captured using a Canon 5DS R camera's JPEG mode (top row) and with an iPhone 6S camera (bottom row). Both these cameras provide more realistic scenarios as compared to the virtual camera. Nevertheless, reconstructions of equal quality can be done from camera JPEGs. The iPhone images are more degraded, shot in dark conditions without a tripod, but the reconstructed information comply well with the image characteristics. To further explore the possibilities in reconstructing everyday images, <ref type="figure" target="#fig_0">Figure 12</ref> displays a set of iPhone images, taken in a variety of situations. The examples not only demonstrate the method's ability to generalize to a different camera, but also to a wide range of situations, such as skin tones, fire and caustics. The most limiting factor when reconstructing from iPhone images is the hard JPEG compression that has been applied, which results in small blocking artifacts close to highlights, affecting the final reconstruction. In order to compensate for these, the brightness of the images has been increased by a small factor, followed by clipping. This removes the artifacts caused by harder compression in the brightest pixels, which improves the final reconstruction quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Changing clipping point</head><p>In order to demonstrate the behavior of the reconstruction with varying amount of information loss, <ref type="figure" target="#fig_11">Figure 13</ref> shows predictions using different virtual exposure times. As the training of the CNN uses a virtual camera with different exposure settings, part of the objective is to minimize the difference between these, apart from a scaling factor. However, since more information is available in highlights with shorter exposure, in most cases there will be visible differences in the reconstruction quality, as exemplified by the figure. <ref type="figure" target="#fig_2">Figure 14</ref> shows the HDR reconstruction compared to three existing methods for inverse tone-mapping. These are examples of local methods that apply different processing in saturated areas in order Comparison to some existing iTMOs. Since the iTMO results usually are calibrated for an HDR display, they have been scaled to the same range for comparison. Although the they can deliver an impression of increased dynamic range by boosting highlights, when inspecting the saturated image regions little information have actually been reconstructed. The CNN we use can make a prediction that is significantly closer to the true HDR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison to iTMOs</head><p>to boost the dynamic range of an LDR image. The results can successfully convey an impression of HDR when viewed on an HDR capable display. However, when inspecting the highlights, local information is not recovered by a na?ve scaling of image highlights. With our CNN we are able to predict saturated regions based on a high level understanding of the context around the area, which makes it possible to actually recover convincing colors and structural content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Image based lighting</head><p>An important application of HDR imaging is image based lighting (IBL), where the omnidirectional lighting of a particular scene is captured in a panorama and used for re-lighting of a computer graphics model. For this task it is of major importance that the entire range of luminances are present within the image, in order to convey a faithful and dynamic rendering. <ref type="figure" target="#fig_3">Figure 15</ref> shows a panorama from an indoor scene, where the majority of illuminance is due to two windows. In the LDR image the windows are saturated, and the result when used for IBL is overall dark and of low contrast. The iTMO by <ref type="bibr" target="#b6">Banterle et al. [2008]</ref> can accomplish a more dynamic rendering by boosting image highlights, although the result is far from the ground truth. With our learning-based method, the reconstructed panorama shows some loss of details in the large saturated areas of the windows, but the illuminance is estimated convincingly. This makes it possible to render a result that is very close to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EVALUATION</head><p>In order to assess the perceived visual quality of our novel HDR reconstruction, we performed a subjective pairwise comparison experiment. 15 participants took part in the experiment, aged 19 -40 with normal or corrected-to-normal full color vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Setup</head><p>We used a calibrated projector-based HDR display, similar to the one described by <ref type="bibr" target="#b48">Seetzen et al. [2004]</ref>, at a viewing distance of 80 cm. The display consisted of an Acer P1276 1024 ? 768 DLP projector with removed color wheel, and a 9.7" 2048 ? 1536 iPad Retina display panel with removed backlight. The maximal and minimal luminance of the display was 5000 cd/m 2 and 0.1 cd/m 2 , yielding a maximal contrast range of 50 000:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Stimuli</head><p>Each stimulus consisted of a pair of images identical in terms of content, but reproducing one of the four processing or display scenarios:</p><p>(1) LDR image with its dynamic range clamped to that of a typical DSLR camera (10.5 stops); (2) ground truth HDR image;</p><p>(3) iTMO technique by <ref type="bibr" target="#b6">Banterle et al. [2008]</ref>, shown to perform the best in the evaluation ]; (4) output from our CNN-based HDR reconstruction (Pre-train + I/R loss). To prevent overall image brightness from affecting the outcome of the experiment, we fixed the luminance of the 90 th percentile pixel value for all methods to 180 cd/m 2 . To avoid bias in the selection of images, we used a randomly selected sample of 25 images from a pool of the 95 images in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Task</head><p>We used a two-alternative forced choice experiment, where in each trial the participants were shown a pair of images side-by-side. The task was to select the image that looked more natural. Our definition of natural involved "the depth and vividness that bears most resemblance to a view that you could experience looking through a window". Participants were given unlimited time to look at the images and to make their decisions. Before each session, participants were briefed about their task both verbally and in writing, followed by a short training session to gain familiarity with the display equipment and the task. We used a full pairwise comparison design, in which all pairs were compared. Each observer compared each pair three times, resulting in 450 trials per observer. The order of the stimuli as well as their placement on the screen were randomized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Results</head><p>The result of the pairwise comparison experiment is scaled in justobjectionable-differences (JODs), which quantify the relative quality differences. The scaling is performed using publicly available software 1 , which formulates the problem as a Bayesian inference under the Thurstone Case V assumptions, and uses a maximumlikelihood-estimator to find relative JOD values. Unlike standard scaling procedures, the Bayesian approach robustly scales pairs of conditions for which there is unanimous agreement. Since JOD values are relative, we fix the starting point of the JOD scale at 0 for the LDR images. When two points are a single unit apart on the JOD space, approximately 75% of the population are expected to perceive an objectionable quality difference.</p><p>The results of the subjective quality experiment are shown in <ref type="figure">Figure 16</ref>. Unexpectedly, the iTMO technique by Banterle et al. was judged to be the least natural, even less so than the LDR images. This can be explained by the operator's attempt to inverse the camera response curve, which often results in reduced contrast and inaccurate colors. <ref type="figure" target="#fig_13">Figure 17 row (a)</ref> shows the 19th image from the evaluation, where the effect of over-boosted colors can be easily observed. As expected, LDR images were rated worse than the original HDR images in almost all cases. Most participants were mainly accustomed to standard display monitors, which, as reflected upon by some subjects during the unstructured post-experiment interview, might have affected their perception of "natural". With more exposure to HDR displays we expect the perceived quality of LDR images to drop in the future. According to the data, our CNN-based images are very likely to perform better than their original LDR counterparts. The number of times our CNN images were picked is slightly less but comparable to the original HDR images. <ref type="figure" target="#fig_13">Figure 17 rows (b)</ref> and (c) illustrate two scenes, where the algorithm succeeds in estimating the spatial texture of high luminance areas, producing plausible results. The performance of the CNN is scene-dependent, and can sometimes introduce artifacts in the highlights that affect perceived naturalness. One example is depicted in <ref type="figure" target="#fig_13">Figure 17 row (d)</ref>, where the artificial texture of the sun is immediately obvious. Overall, the CNN-based reconstruction improves the subjective quality as compared to the input LDR images, which is not always the case for the state-of-the-art iTMO techniques. The reconstructed images are in most cases comparable to ground truth HDR images, as evidenced by the quality differences of less than 1 JOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>HDR reconstruction from an arbitrary single exposed LDR image is a challenging task. To robustly solve this problem, we have presented a hybrid dynamic range autoencoder. This is designed and trained taking into account the characteristics of HDR images in the model architecture, training data and optimization procedure. The quality and versatility of the HDR reconstruction have been demonstrated through a number of examples, as well as in a subjective experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Limitations</head><p>There is a content-dependent limit on how much missing information the network can handle which is generally hard to quantify. <ref type="figure">Figure 18</ref> shows two examples of difficult scenes that are hard to reconstruct. The first row has a large region with saturation in all color channels, so that structures and details cannot be inferred. However, illuminance may still be estimated well enough in order to allow for high quality IBL, as demonstrated in <ref type="figure" target="#fig_3">Figure 15</ref>. The second row of <ref type="figure">Figure 18</ref> shows a situation where besides a similar loss of spatial structures, extreme intensities are also underestimated. This is also demonstrated in <ref type="figure" target="#fig_8">Figure 10</ref> with the intense spotlight. The plot also shows that the problem can be alleviated by altering the illuminance weight ? in Equation 7. However, underestimation of highlights is also an inherent problem of the training data. Some of the HDR images used to create the training data show saturated pixels in high intensity regions. For example, the sun in <ref type="figure">Figure 18</ref> is saturated in the ground truth HDR image.</p><p>There is also a limitation on how much compression artifacts that can be present in the input image. If there are blocking artifacts around highlights, these will impair the reconstruction performance to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Future work</head><p>Recovering saturated image regions is only one of the problems in reconstructing HDR images. Another, less prominent issue is quantization artifacts, which can be alleviated using existing methods <ref type="bibr" target="#b9">[Daly and Feng 2003</ref>]. However, we believe that bit-depth extension also can be achieved by means of deep learning. For this purpose existing architectures for compression artifact reduction <ref type="bibr" target="#b51">[Svoboda et al. 2016]</ref> or super resolution <ref type="bibr" target="#b32">[Ledig et al. 2016</ref>] are probably better suited. The complementary problem of reconstructing saturated pixels, is the recovery of dark regions of an image, that have been lost due to quantization and noise. This problem is also significantly different from ours in that noise will be a main issue when increasing the exposure of an image.</p><p>Another direction for future work is to investigate how to improve reconstruction of images that are degraded by compression artifacts. The most straightforward solution would be to augment the training data with compression artifacts. However, this also runs the risk of lowering the quality for reconstructions of images without any compression applied.</p><p>Finally, although recent development in generative adversarial networks (GAN) <ref type="bibr" target="#b20">[Goodfellow et al. 2014;</ref><ref type="bibr" target="#b42">Radford et al. 2015]</ref> shows promising result in a number of imaging tasks <ref type="bibr" target="#b32">[Ledig et al. 2016;</ref><ref type="bibr" target="#b40">Pathak et al. 2016]</ref>, they have several limitations. An important challenge for future work is to overcome these, in order to enable high-resolution and robust estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Francesco Banterle for the invaluable discussions and help with inverse tone-mapping operators, and the anonymous reviewers for helping in improving the manuscript. This work was funded through Link?ping University Center for Industrial Information Technology (CENIIT), the Swedish Science Council through Grant 2015-05180, and the Wallenberg Autonomous Systems Program (WASP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A: DATA AUGMENTATION</head><p>In this appendix we specify the details of the virtual camera used in Section 4 for augmentation of HDR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Random cropping</head><p>For each mega-pixel of HDR data, N sub-images are selected at random positions and sizes. For the trainings we perform, we choose N = 10, which results in a final training set of ?125K images. The sizes are drawn uniformly from the range [20%, 60%] of the size of an input image, followed by bilinear resampling to 320x320 pixels. Since 320 pixels corresponds to 20%-60% of the original images, the training is optimized to account for images that are between 320/0.6 = 533 and 320/0.2 = 1600 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Exposure</head><p>The exposure of each cropped image is set so that clipping removes a fraction v of the image information. The fraction is uniformly drawn in the range v ? [0.05, 0.15]. To accomplish this, v is used to define an exposure scaling s,</p><formula xml:id="formula_11">s = 1 H th , s.t . H t h i=H min p H (i) = 1 ? v,<label>(9)</label></formula><p>where p H is the histogram of the HDR image H . Thus, H th is the 1 ?v percentile, and this defines the scaling s which is applied to the image in order to remove 5 ? 15% of the information when clipping is performed (see <ref type="formula" target="#formula_1">Equation 11</ref>).  <ref type="figure" target="#fig_7">Fig. 19</ref>. A sigmoid function fits well to the mean of the collected camera curves by <ref type="bibr" target="#b21">Grossberg and Nayar [2003]</ref> (a). We simulate camera curves from a normal distribution centered at the fitted parameters (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Camera curve</head><p>To approximate different camera curves we use a parametric function, in form of a sigmoid,</p><formula xml:id="formula_12">f (H i,c ) = (1 + ? ) H n i,c H n i,c + ? .<label>(10)</label></formula><p>The scaling 1 + ? is used to normalize the output so that f (1) = 1. We fit the parameters n and ? to the mean of the database of camera curves collected by <ref type="bibr" target="#b21">Grossberg and Nayar [2003]</ref>, where n = 0.9 and ? = 0.6 gives a good fit as shown in <ref type="figure" target="#fig_7">Figure 19</ref>. For random selection of camera curves in the training data preparation, we draw the parameters from normal distributions around the fitted values, n ? N (0.9, 0.1) and ? ? N (0.6, 0.1). As demonstrated in <ref type="figure" target="#fig_7">Figure 19</ref> this creates a continuous range that do not include extreme functions such as gamma curves with ? &gt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Other</head><p>Augmentation in terms of colors is accomplished in the HSV color space, where we modify the hue and saturation channels. The hue is altered by means of adding a random perturbationh ? N (0, 7). The same is done for the saturation, but with a narrower distribution, s ? N (0, 0.1). Finally, a small amount of additive Gaussian noise is injected, with a standard deviation randomly selected in the range ? ? [0, 0.01]. Also, images are flipped horizontally with a probability of 0.5. The processed linear images H represent the ground truth data, while the inputs for training are clipped at 1 and quantized, D i,c = ?255 min(1, f (H i,c )) + 0.5?/255.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Zoom-in of an example of the components of the blending operation in Equation 1, compared to the ground truth HDR image. (a) is the input image, (b) is prediction, (c) is the blending mask, (d) is the blending of (a-b) using (c), and (e) is ground truth. Gamma correction has been applied to the images, for display purpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Zoom-ins of reconstruction without (b) and with (c) the domain transformation skip-connections. The plain autoencoder architecture can reconstruct high luminance, but without skip-connections the detail information around saturated regions cannot be fully exploited.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Zoom-ins of reconstructions with different relative weight of illuminance and reflectance, ? in Equation 7. A higher weight of illuminance will in general better predict high intensity regions (b), while a higher weight of reflectance is better at deducing local colors and details (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>loss (eq. 5) (c) I/R loss (eq. 7) (d) Ground truthFig. 6. Zoom-in of a reconstruction with different loss functions. The input (a) is exposure corrected and clipped to have a large amount of information lost. The direct pixel loss (b) is more prone to generating artifacts as compared to the illuminance + reflectance loss (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>A 150?150 pixels zoom-in of a reconstruction. Using pre-training the CNN is in general more consistent and can reconstruct smaller highlights better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>The input images (a) have been exposure corrected, followed by camera transformation, quantization and clipping. 5% of the pixels are saturated and contain no information. Visually convincing reconstructions (b) can be made in a wide range of situations. The reconstructions correspond well to the ground truth HDR images (c). The exposures of the images have been reduced to show the differences, and all images have been gamma corrected for display.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>The learned residual of the image in Figure 1 (top left), together with the ground truth residual (top right). The plots show relative luminance values across the two scanlines marked in the images. Complex regions are predicted convincingly (bottom left), but for the very intense spotlight the signal is underestimated (bottom right). The underestimation is less pronounced when training with higher illuminance weight (? in Equation 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Reconstruction from Canon 5DS R camera JPEG(top). The examples are the same as inFigure 9, but with the camera set to store JPEG, applying unknown image transformations and compression. Reconstruction can also be performed with hand-held low-end cameras, such as the iPhone 6S (bottom), where the input image is more degraded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Predictions on iPhone camera images. Plausible reconstructions can be made of skin tones, caustics and fire (row 1-3). Large saturated areas can be recovered if there still is information in one of the color channels (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Zoom-ins of reconstructions (bottom row) with different exposure settings of the input (top row). The numbers indicate how large fraction of the total number of pixels are saturated in the input. The images have then been scaled to have the same exposure after clipping has been applied. Although illuminance is predicted at approximately the same level, more details are available in reconstruction of the shorter exposure images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>IBL using reconstructed highlights. The top row shows the panoramas that are used for IBL in the bottom row. Rendering with the LDR input gives a dark and undynamic result. The iTMO boosts brightness to alleviate the problems. With our reconstruction, although all details cannot be recovered in the large saturated areas of the windows, the estimated luminance enables a visually convincing rendering that is much closer to the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 .</head><label>17</label><figDesc>Examples of typical images in the subjective experiment. a) image 19 demonstrates how the inaccurate colors of iTMO reduces perceived realism. b) image 21 with a successful HDR reconstruction. c) image 4 shows how even an inaccurate estimate of the highlight luminance still produces plausible results. d) image 8 is an example of unsuccessful reconstruction, producing artifacts that heavily affect the perceived naturalness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>. Zoom-ins of areas where reconstruction fails. (Top) A large fraction of the pixels are saturated, and the structures cannot be recovered properly. (Bottom) the intensity and shape of the sun are not estimated correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Different MSEs evaluated over the test set. Rows show different training strategies, while columns evaluate with different errors. The direct MSE is from Equation 5, while the I/R, I and R MSEs use Equation 7with ? = 0.5, 1 and 0, respectively. The reference is the input image without reconstruction. Adding skip-connections improves the result to a large extent. The illuminance + reflectance loss lowers both direct MSE and in terms of illuminance and reflectance, as compared to optimizing for only the direct loss. Pre-training has a significant impact on the result.</figDesc><table><row><cell></cell><cell>Direct</cell><cell>I/R</cell><cell>I</cell><cell>R</cell></row><row><cell>Reference</cell><cell>0.999</cell><cell>0.890</cell><cell>0.712</cell><cell>0.178</cell></row><row><cell>Without skip-conn.</cell><cell>0.249</cell><cell>0.204</cell><cell>0.102</cell><cell>0.102</cell></row><row><cell>Direct loss (eq. 5)</cell><cell>0.189</cell><cell>0.159</cell><cell>0.090</cell><cell>0.069</cell></row><row><cell>I/R loss (eq. 7)</cell><cell>0.178</cell><cell>0.150</cell><cell>0.081</cell><cell>0.068</cell></row><row><cell>Pre-train. + I/R loss</cell><cell>0.159</cell><cell>0.134</cell><cell>0.069</cell><cell>0.066</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig. 16. Results of the subjective quality experiment. The error bars represent 95% confidence intervals computed by bootstrapping. All values on the JOD scale are relative to the LDR images. Negative scores indicate a lower perceived naturalness for the iTMO technique when compared with LDR images. The output of our CNN-based HDR reconstruction method surpasses LDR and is comparable to the original HDR images in most cases.</figDesc><table><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>HDR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4</cell><cell>Ours iTMO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(JOD)</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Quality</cell><cell>0 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>20</cell><cell>21</cell><cell>22</cell><cell>23</cell><cell>24</cell><cell>25</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Transactions on Graphics, Vol. 36, No. 6, Article 178. Publication date: November 2017.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Pairwise comparison scaling software for Matlab: https://github.com/mantiuk/pwcmp</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Do HDR Displays Support LDR Content?: A Psychophysical Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Aky?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Riecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>B?lthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating the Performance of Existing Full-Reference Quality Metrics on High Dynamic Range (HDR) Video Content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banitalebi-Dehkordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Pourazad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nasiopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Signal Processing (ICMSP &apos;14)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">789</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Advanced High Dynamic Range Imaging: Theory and Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Artusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High Dynamic Range Imaging and Low Dynamic Range Expansion for Generating HDR Content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Artusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2343" to="2367" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Psychophysical Evaluation of Inverse Tone Mapping Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bloj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Artusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="13" to="25" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inverse Tone Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia (GRAPHITE &apos;06)</title>
		<meeting>the 4th International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia (GRAPHITE &apos;06)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Expanding Low Dynamic Range Videos for High Dynamic Range Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Spring Conference on Computer Graphics (SCCG &apos;08)</title>
		<meeting>the 24th Spring Conference on Computer Graphics (SCCG &apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-Scale Probabilistic Dithering for Suppressing Banding Artifacts in Digital Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhagavathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Llach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Zhai</surname></persName>
		</author>
		<idno>IV -397-IV -400</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Image Processing (ICIP &apos;07)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Survey of Temporal Brightness Artifacts in Video Tone Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boitard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cozot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thoreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bouatouch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference and SME Workshop on HDR imaging</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bit-depth extension using spatiotemporal microdither based on models of the equivalent input noise of the visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5008</biblScope>
			<biblScope unit="page" from="455" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decontouring: prevention and removal of false contour artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5292</biblScope>
			<biblScope unit="page" from="130" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recovering High Dynamic Range Radiance Maps from Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;97</title>
		<meeting>the 24th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;97</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enhancement of Bright Video Features for HDR Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Didyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1265" to="1274" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compression Artifacts Reduction by a Deep Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Change</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV &apos;15)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High Dynamic Range Video: From Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Mantiuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Display and Applications</title>
		<editor>and M. Mrak</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2016" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Reverse Tone Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">177</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Elements of psychophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fechner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<pubPlace>Holt, Rinehart &amp; Winston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Creating Cinematic Wide Gamut HDR-Video for the Evaluation of Tone Mapping Operators and HDR-Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Froehlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grandinetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE 9023, Digital Photography X. 90230X-90230X-10</title>
		<meeting>SPIE 9023, Digital Photography X. 90230X-90230X-10</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perception of lightness and illumination in a world of one reflectance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilchrist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="5" to="19" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS &apos;10)</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What is the space of camera response functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;03)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>II-602-9</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive dualISO HDR reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hajisharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Let There Be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (PMLR &apos;15)</title>
		<meeting>the 32nd International Conference on Machine Learning (PMLR &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep High Dynamic Range Imaging of Dynamic Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High-quality brightness enhancement functions for real-time reverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kovaleski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="539" to="547" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">High-Quality Reverse Tone Mapping for a Wide Range of Exposures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kovaleski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Conference on Graphics, Patterns and Images (SIBGRAPI &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Unified Framework for Multi-Sensor HDR Video Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gustavson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="203" to="215" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;15)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Being &apos;undigital&apos; with cameras: Extending Dynamic Range by Combining Differently Exposed Pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<idno>323</idno>
	</analytic>
	<monogr>
		<title level="m">M.I.T. Media Lab Perceptual Computing Section</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="422" to="428" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluation of Reverse Tone Mapping Through Varying Exposure Conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Masia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agustin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="160" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic range expansion based on image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Masia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="631" to="648" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
		<title level="m">The Reproduction of Specular Highlights on High Dynamic Range Displays. Color and Imaging Conference</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="333" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High dynamic range imaging: spatially varying pixel exposures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitsunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;00)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;00)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deconvolution and Checkerboard Artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="http://distill.pub/2016/deconv-checkerboard" />
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Context Encoders: Feature Learning by Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;17)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<title level="m">High dynamic range imaging: acquisition, display, and image-based lighting</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ldr2Hdr: On-the-fly Reverse Tone Mapping of Legacy Video and Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Rempel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seetzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI &apos;15</title>
		<meeting>Medical Image Computing and Computer-Assisted Intervention (MICCAI &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Glare encoding of high dynamic range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;11</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">High Dynamic Range Display Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seetzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stuerzlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorozcovs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="760" to="768" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Convolutional Sparse Coding for High Dynamic Range Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Masia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="153" to="163" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Compression Artifacts Removal Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hradis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zemcik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of WSCG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Versatile HDR Video Production System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Tocci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tocci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">High-dynamic-range video for photometric measurement of illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gustavson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">6501</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Extracting and Composing Robust Features with Denoising Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML &apos;08)</title>
		<meeting>the 25th International Conference on Machine Learning (ICML &apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">High Dynamic Range Image Hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Eurographics Conference on Rendering Techniques (EGSR&apos;07</title>
		<meeting>the 18th Eurographics Conference on Rendering Techniques (EGSR&apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09969</idno>
		<title level="m">High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Learning High Dynamic Range from Outdoor Panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10200</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Scene Recognition using Places Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
