<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual-Awareness Attention for Few-Shot Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung-I</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Cheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Ting</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Cheng</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsiang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Fong</forename><surname>Yeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Chin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Mobile Drive Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dual-Awareness Attention for Few-Shot Object Detection</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON MULTIMEDIA</title>
						<imprint>
							<biblScope unit="volume">23</biblScope>
							<biblScope unit="page">2021</biblScope>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep learning</term>
					<term>object detection</term>
					<term>visual attention</term>
					<term>few-shot object detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While recent progress has significantly boosted fewshot classification (FSC) performance, few-shot object detection (FSOD) remains challenging for modern learning systems. Existing FSOD systems follow FSC approaches, ignoring critical issues such as spatial variability and uncertain representations, and consequently result in low performance. Observing this, we propose a novel Dual-Awareness Attention (DAnA) mechanism that enables networks to adaptively interpret the given support images. DAnA transforms support images into query-positionaware (QPA) features, guiding detection networks precisely by assigning customized support information to each local region of the query. In addition, the proposed DAnA component is flexible and adaptable to multiple existing object detection frameworks. By adopting DAnA, conventional object detection networks, Faster R-CNN and RetinaNet, which are not designed explicitly for few-shot learning, reach state-of-the-art performance in FSOD tasks. In comparison with previous methods, our model significantly increases the performance by 47% (+6.9 AP), showing remarkable ability under various evaluation settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Few-shot object detection (FSOD) is a recently emerging and rapidly growing research topic, which has great potential in many real-world applications. Unlike conventional object detectors, few-shot object detectors can be adapted to novel domains with only few annotated data, saving the costly data re-collection and re-training processes whenever the downstream task changes.</p><p>However, though considerable effort in recent years has been devoted, existing FSOD methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> suffer extremely low performance in comparison with traditional object detectors <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In addition, it seems previous methods suffer performance drop not only on the novel domain but also on the base (training) domain <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Moreover, by carrying out experiments, we discovered that previous FSOD models based on Faster R-CNN <ref type="bibr" target="#b5">[6]</ref> are incapable of reaching the performance that Faster R-CNN can achieve when being evaluated on base classes (see Tab. II). Though the models have been trained on a lot of annotated data, they still have difficulty in recognizing the objects they have seen. We therefore assume the modifications associated with fewshot learning applied in prior attempts somehow undermine the ability of detection networks, resulting in limited performance.</p><p>To recast traditional object detectors into few-shot object detectors, prior works tend to leverage the methods that have been proved effective at few-shot classification (FSC). The techniques such as building category prototypes <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, ranking similarity between inputs <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b2">[3]</ref> and feature map concatenation <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> are all widely adopted. However, unlike FSC aiming to classify images, FSOD is a much more complicated task requiring to identify and locate objects precisely. In this work, we summarize three potential concerns that could restrict the performance of FSOD (see <ref type="figure" target="#fig_0">Fig. 1</ref>). (a) First of all, prior methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref> performed global pooling on support features to ensure computational efficiency. However, the lack of spatial information would cause difficulty in measuring objectwise correlations. (b) Secondly, convolutional neural networks (CNN) are physically inefficient at modeling varying spatial distributions <ref type="bibr" target="#b14">[15]</ref>, so the methods using convolution-based attention <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b12">[13]</ref> would suffer the same restriction. (c) Furthermore, previous works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> took mean features across multiple support images as classspecific representations, which heavily relies on an implicit assumption that the mean features will still be representative in the embedding space.</p><p>To verify the aforementioned concerns are worth studying, we conduct a pilot study with the hypothesis: If the spatial information, variability and feature uncertainty are all trivial in FSOD, the choice of support images will not severely affect the performance. In the experiment, the four well-trained FSOD models are tested on the same query set 100 times but the given support images will be different each time. As shown in <ref type="figure">Fig. 2</ref>, even though we fix the query set, previous methods can easily be influenced by the choice of supports, resulting in a huge range of performance (up to 4.4 AP). On the other hand, our method, which solves the concerns summarized in <ref type="figure" target="#fig_0">Fig. 1</ref>, achieves the highest and firmest performance.</p><p>In this work, we present a novel Dual-Awareness Attention (DAnA) mechanism comprised of Background Attenuation (BA) and Cross-Image Spatial Attention (CISA) modules. Inspired by wave interference, we propose a BA module where each feature vector (1 ? 1 ? C) of a high-level feature map (H ?W ?C) is viewed as a wave along the channel dimension. By extracting the most representative feature vector from a feature map and adding it back to the feature map, those local features having different wave patterns from the extracted feature will be blurred and therefore can be easily recognized as noise by the detection network. Those foreground features, on the other hand, can maintain the wave patterns and be preserved after the addition. Thus, the BA module not only attenuates irrelevant features but preserves the target informa- tion as well.</p><p>To determine whether two objects belong to the same class, a person might first determine the most representative features among objects (e.g., dog paws, bird wings) and then make his decision according to these features. Inspired by such a nature, we propose CISA to adaptively transform support images into various query-position-aware (QPA) vectors. To be more specific, each QPA vector carries specific support information that is considered the most relevant to each local query region. By measuring correlations between the query regions and their corresponding QPA vectors, the model can easily determine whether the regions should be the parts of the target object. Also, CISA provides a more efficient and effective way to summarize information among multiple support images. Those QPA vectors conditioned on the same query region would represent relevant information, and therefore taking the mean feature across them is more effective than previous manners. By better utilizing support images, our method achieves the most significant improvement as the number of support images increases (see Tab. II).</p><p>In this paper, we evaluate models across various settings, including the multi-shot, multi-way, cross-domain, and episodebased evaluations. In Tab. I, we show our method significantly outperforms the strongest baseline [17] by 6.9 AP under the 30-shot setting. Furthermore, we are the first to test FSOD models on novel domains without fine-tuning (Tab. II and Tab. III) to further evaluate the generalization ability of each method. We also offer a comprehensive ablation study to demonstrate the impact of each proposed component.</p><p>Our main contributions can be summarized as follows:</p><p>? We point out the critical issues in previous FSOD methods that lead to the limited performance and the lack of robustness. ? We present novel dual-awareness attention to precisely capture object-wise correlations. <ref type="bibr">?</ref> We conduct comprehensive experiments to fairly assess each approach. Extensive experiments manifest the effectiveness and robustness of the proposed methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Few-Shot Classification</head><p>Few-shot classification (FSC) has multiple branches, including the optimization-based and metric-based approaches. The optimization-based methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> aim to learn a good initialization parameter set that can swiftly be adapted to new tasks within few gradient steps. The metricbased methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, on the other hand, compute the distance between learned representations in the embedding space. The concept of prototypical representation <ref type="bibr" target="#b8">[9]</ref> is widely adopted in FSC, which takes the mean feature over different support images as a class-specific embedding. Such a strategy is intuitive yet the data scarcity in few-shot scenarios might lead to biased prototypes and consequently hinder the performance <ref type="bibr" target="#b23">[24]</ref>. To enhance the reliability of class representations, Tian et al. <ref type="bibr" target="#b24">[25]</ref> leveraged pre-trained embedding and showed that using good representations is more effective than applying sophisticated metalearning algorithms. Although the attempts have successfully boosted the performance of FSC, the progress of few-shot object detection (FSOD) is still in a very early stage. In this work, we explore novel attention mechanisms and significantly enhance the performance of FSOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanism</head><p>The attention modules were first developed in natural language processing (NLP) to facilitate machine translation <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Recently, inspired by the great success of Transformer <ref type="bibr" target="#b28">[29]</ref>, researchers start to explore the self-attention  mechanism on computer vision (CV) problems <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b33">[34]</ref>, attempting to break the physical restrictions of CNN. Wang et al. <ref type="bibr" target="#b29">[30]</ref> presented a pioneering approach, Non-local (NL) Neural Networks, leveraging selfattention to capture long-range dependencies in an image. Hu et al. <ref type="bibr" target="#b34">[35]</ref> adopted a self-attention function on channels, reweighting features along the channel dimension. Following NL, <ref type="bibr" target="#b30">[31]</ref> described the features as information flows that can be bidirectionally propagated; <ref type="bibr" target="#b32">[33]</ref> showed that simplifying NL block does not deteriorate its ability but rather enhance the performance; recently, Yin et al. <ref type="bibr" target="#b35">[36]</ref> succeeded in capturing better visual clues by proposing a disentangled NL block. Emami et al. <ref type="bibr" target="#b36">[37]</ref> combined spatial attention with GAN to handle image-to-image translation tasks. Li et al. <ref type="bibr" target="#b37">[38]</ref> explored attention in both spatial and temporal dimensions, improving the performance of video action detection. The main differentiating factor between the proposed DAnA and aforementioned attention mechanisms is that DAnA can capture cross-image dependencies and interpret support images adaptively according to the given query. It would be plausible that the idea of DAnA can be applied to other research topics aiming to capture shared attributes among images with diverse backgrounds, viewpoints and illumination conditions <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, yet in this work we will emphasize the application in FSOD only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Few-Shot Object Detection</head><p>Deep-learning-based object detectors have shown remarkable performance in many applications. Two-stage detectors <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b43">[44]</ref> are usually dominant in performance, and one-stage detectors <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> are superior in runtime efficiency. Most modern object detectors are categoryspecific, which means they are incapable of recognizing objects of unseen categories. To explore generalized object detectors, previous attempts exploited transfer learning <ref type="bibr" target="#b47">[48]</ref> and distance metric learning <ref type="bibr" target="#b9">[10]</ref> to achieve quick adaption to novel domains. As recasting object detection problem into the few-shot learning paradigm, the idea of current methods could be somewhat similar to multiple-instance learning (MIL) <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. For FSOD, we can also perceive a query image as a bag, aiming to identify the positive image patches in it by capturing the contexts relevant to given support images <ref type="bibr" target="#b48">[49]</ref>.</p><p>Recently, there is an important line of works encoding support images into global vectors, measuring the similarity between feature vectors and the RoI proposals inferred by detection networks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Following the spirit, <ref type="bibr" target="#b12">[13]</ref> perceived the task as a guided process where support features are used to guide the object detection networks. Inspired by <ref type="bibr" target="#b15">[16]</ref>, Fan et al. <ref type="bibr" target="#b2">[3]</ref> measured the correlations by regarding support images as kernels and performing a convolution-based operation over queries. Current works have a tendency to take mean features as class representations and measure cross-image correlations by either feature concatenation or element-wise product <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b2">[3]</ref>. We argue that these techniques will engender serious issues (illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>) in FSOD, degenerating the performance even on the seen (training) categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>Let s be a support image and S = {s i } M i=1 be a support set that represents a specific category. An individual FSOD task can be formulated as T = {(S 1 , ..., S N ), I}, where I is a query image comprised of multiple instances and backgrounds. Given T , the model should detect all the objects in I belonging to the target categories {S 1 , ..., S N }. The object categories in a training dataset are divided into two disjoint parts: base classes C base and novel classes C novel . To train a FSOD model, a meta-training set</p><formula xml:id="formula_0">H train = {T i } |H| i=1</formula><p>should be constructed, where all the bounding box annotations belong to C base . Similarly, a meta-testing set H test is constructed where all the target objects belong to C novel . It is allowed to use a finetuning set H f inetune to fine-tune the mdoel before evaluating on H test . However, in an N-way K-shot setting, only K box annotations of each novel category can be used to fine-tune the model <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b0">[1]</ref>.</p><p>To summarize, the primary goal of FSOD is to leverage rich source-domain knowledge in H train to learn a model that can swiftly generalize to target domains where only few annotated data are available. Instead of only considering the features of I, the model f (I|S) is trained to recognize objects conditioned on the given support information.</p><p>B. Dual-Awareness Attention 1) Overview: FSOD relies on limited support information to detect novel objects, and therefore we consider two important aspects: (1) The quality of support features, and (2) how to better construct correlations between support and query images. In this work, we propose an attention mechanism comprised of two novel modules to undermine the influence of noise and precisely measure object-wise correlations. In the BA block, the extracted feature is superimposed on the feature map to undermine irrelevant background features while maintain distinguishing information. In the CISA block, support feature maps are adaptively transformed into query-position-aware (QPA) vectors, which are customized support information for each query region.</p><p>2) Background Attenuation Block: It is infeasible to always ensure high-quality support images in real-world scenarios. Those noise in support images will inhibit models from reaching robustness (see <ref type="figure">Fig 2)</ref>. We propose a novel mechanism, Background Attenuation (BA), to undermine the irrelevant support information. The detailed structure of BA block is illustrated in <ref type="figure">Fig. 4</ref>, where a support image s and a query image I are encoded into a support feature map Y ? R C?H S ?W S and a query feature map X ? R C?H Q ?W Q by a shared CNN backbone. In BA block, the feature map Y will be reshaped and transformed by a linear learnable matrix W e ? R C?1 . The process can be formulated as</p><formula xml:id="formula_1">A BA (y i ) = ? (W e y i ) = exp(W e y i ) ? j?? exp(W e y j )<label>(1)</label></formula><p>where y i ? R 1?C denotes the feature vector at i th pixel of Y ; ? is the set of all pixel indices and ? is the softmax function applied along the spatial dimension. Thus, the learned aggregation of Y can be obtained by</p><formula xml:id="formula_2">G = ? i?? A BA (y i ) ? y i .<label>(2)</label></formula><p>Intuitively, G ? R 1?C should represent the most important feature of Y according to the resulting attention maps. However, we empirically discover that using the learned attention maps to filter out noise would harm the performance (see Tab. VI). By visualization, we observe that only few narrow regions of support image contribute to the aggregated feature G. Such a naive attention process leads to a considerable loss of support information and therefore deteriorate the ability of models.  Y and X are support and query feature maps respectively; Z 1 denotes the 1 st feature map in the support set processed after the BA block; p i denotes the query-position-aware support vector based on the i th pixel of X.</p><formula xml:id="formula_3">Y Z Z 1 Z 2 X p i 1 p i 2 p i !??$ % &amp; % !??$ ' &amp; ' $ ' &amp; ' ?$ ( &amp; % #$'&amp;' !?$ % ?&amp; % 1?$ % &amp; % !?$ % ?&amp; % 1?! 1?$ % ?&amp; % QPA</formula><p>Consequently, we propose a much softer attention strategy to remove noise. In physics, interference is a term indicating two signals superpose to form a resultant signal of greater or lower amplitudes. Inspired by that, we view each feature vector of Y as a C-dimensional signal and superimpose the extracted feature G on those signals</p><formula xml:id="formula_4">Z = Y + ? ? LeakyReLU(G)<label>(3)</label></formula><p>where ? is a constant hyper-parameter, and the nonlinear function is used to rectify the results. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, if the original signals (e.g., regions belonging to the dog) are aligned with G along the channel dimension, the features can be enhanced or maintained. On the contrary, those signals associated with unrelated regions (e.g., the Frisbee) will be blurred after addition due to the significant difference with G. Thus, the BA block can benefit detection networks by providing more discriminative support features. It is worth mentioning the idea of BA is different from the channelwise attention <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b31">[32]</ref> which re-weights feature maps along the channel dimension. In addition, unlike <ref type="bibr" target="#b32">[33]</ref> constructing heavy linear transformation matrices to rew-eight semantic dependencies, BA block is more efficient since W e ? R C?1 is the only learnable weights in it.</p><p>3) Cross-Image Spatial Attention Block: Since even the intra-class objects would have obvious deviation in appearance, the model should learn to focus on those most representative parts of the objects to determine the similarity among them. The core idea of Cross-Image Spatial Attention (CISA) is to adaptively transform each support feature map into queryposition-aware (QPA) support vectors that represent specific information of a support image. The first step of CISA is similar to QKV attention <ref type="bibr" target="#b28">[29]</ref>, transforming X and Z into the query and key embeddings Q = W q X, K = W k Z by learned weight matrices W q ,W k ? R C?C respectively. The similarity scores between the query and support can then be measured by</p><formula xml:id="formula_5">? (X, Z) = ? ((Q ? ? Q ) (K ? ? K ))<label>(4)</label></formula><p>where ? Q and ? K are the averaged embedding values over all pixels; the softmax function ? is performed over the spatial dimension. Furthermore, we add a simplified self-attention term <ref type="bibr" target="#b32">[33]</ref> in CISA because we assume the attention should be based on not only query-support correlations but also the support image itself. Thus, the CISA attention function is formulated as</p><formula xml:id="formula_6">A CISA (X, Z) = ? (X, Z) + ? ?W r Z<label>(5)</label></formula><p>where ? is a constant coefficient; W r ? R C?1 . Note that the output of A CISA (X, Z) has the shape of H Q W Q ? H S W S , which indicates the fact that we have obtained multiple support attention maps (H S W S ) conditioned on each spatial location of the query feature map (H Q W Q ). The QPA vectors can therefore be obtained by</p><formula xml:id="formula_7">p i = ? j?? A CISA (X, Z) i j ? z j<label>(6)</label></formula><p>where ? denotes all pixel indices of the support feature map.</p><p>The equation shows that all the vectors of Z are adaptively weighted and aggregated into a vector p i ? R 1?C according to each query position i (resolve <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>). Additionally, if there are K-shot images available in a support set {s} K , we can perform average pooling across the resulting QPA vectors</p><formula xml:id="formula_8">p i = 1 K K ? k=1 p i,k<label>(7)</label></formula><p>without the concern of uncertainty, because those QPA vectors p i,k conditioned on the same entry i have been refined and  All the models are trained on base categories and then finetuned on a small set of novel samples (e.g., 10 shots of each category). After fine-tuning, models will be evaluated on the novel classes. "-": no reported results.</p><p>will carry relevant features (resolve <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>). Furthermore, it breaks the physical restriction of CNN since all the query features are aligned with their customized QPA vectors (resolve <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Architecture</head><p>To apply the DAnA component to existing object detection frameworks, we can simply combine the output P ? R C?H Q ?W Q with X into PX ? R 2C?H Q ?W Q and send it to the modules such as the region proposal network (RPN) to propose regions having high responses with the given supports (see <ref type="figure" target="#fig_4">Fig. 5</ref>). In our experiments, we choose Faster R-CNN <ref type="bibr" target="#b5">[6]</ref> and RetinaNet <ref type="bibr" target="#b6">[7]</ref> as the backbones to verify the effectiveness of the DAnA mechanism. For DAnA-FasterRCNN, one CISA block is employed before RPN and the other is applied in the second stage taking cropped RoI features as inputs. For DAnA-RetinaNet, we apply both BA and CISA modules to each level of the feature pyramids.</p><p>In addition, we follow the modification applied in <ref type="bibr" target="#b2">[3]</ref>, replacing the multi-class classification output with the binary one. Since the pipeline of FSOD can be regarded as a matching process based on given supports, we suggest the binary output could better fit the problem scenarios where an instance will be either positively or negatively labeled. The rest details of the proposed models remain the same as the original Faster R-CNN and RetinaNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>By default, we employ the pretrained ResNet50 as the feature extractor, and the batch size is set to 32 for all the experiments. All the models including baselines were implemented in Pytorch [52] on a workstation with 4 NVIDIA Tesla V100. The shorter side of query images is resized to 600 pixels, while the longer side is cropped at 1000. Each support image will be zero-padded and then resized to a square image of 320 ? 320. The embedding dimension C in the weight matrices W q and W k is a quarter of the number of original feature channels C. The constant coefficients ? and ? of the proposed modules are 0.5 and 0.1 individually. For those <ref type="bibr">Method</ref> Novel    models based on Faster R-CNN <ref type="bibr" target="#b5">[6]</ref>, we adopt SGD with an initial learning rate of 0.001, which decays into 0.0001 after 12 epochs; the four anchor scales are [60 2 , 120 2 , 240 2 , 480 2 ] and the three aspect ratios are [0.5, 1.0, 2.0]; the momentum and weight decay coefficients are set to 0.9 and 0.0005. For DAnA-RetinaNet, we construct a pyramid with levels P3 through P7 (P l has resolution 2 l lower than the input) and adopt Adam with a learning rate of 0.00001. We adopt the two-way contrastive training strategy, which is first proposed in <ref type="bibr" target="#b2">[3]</ref>, to train the baselines and our models.</p><note type="other">Categories Base Categories # parameters FPS AP AP50 AP75 AP AP50 AP75 Faster</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Dataset:</head><p>In the experiments, we leverage the challenging Microsoft COCO 2017 <ref type="bibr" target="#b52">[53]</ref> benchmark for evaluation. COCO has 80 object classes, consisting of a training set with 118,287 images and a validation set with 4,952 images. Generally, the validation split of COCO will serve as the testing data due to the fact that the testing split is not released to the public.</p><p>We define the 20 COCO categories intersecting with PASCAL VOC <ref type="bibr" target="#b53">[54]</ref> as the novel classes, while the rest 60 categories covered by COCO but not VOC to be the base classes.</p><p>2) FSOD Training: Following the paradigm of metalearning, the training data are organized into episodes. Each episode contains one or more support sets, a query image and corresponding ground truth annotations. Unlike the queries in FSC, a query image in FSOD might contain multiple objects of different categories. Therefore, in each training step we will choose one of the object categories as the target class. A support set of the target class will be offered and the model should learn to detect the target objects conditioned on the given support information. Note that all the bounding box annotations belonging to the novel classes have been removed in order the prevent models from peeking at the novel task.</p><p>3) FSOD Evaluation: Generally, N-way K-shot in the fewshot paradigm indicates that we can use K images of each novel category to adapt the models before testing. However,  we observe previous works adopted different evaluation settings and fine-tuning protocols, leading to the ambiguity in evaluation. For instance, since a query in FSOD might consist of multiple objects of different categories, Kang et al. <ref type="bibr" target="#b0">[1]</ref> defined that there should be only K box annotations of each category in the fine-tuning set, while Chen et al. <ref type="bibr" target="#b47">[48]</ref> adopted the protocol of collecting K query images of each category to fine-tune their models. To thoroughly evaluate the proposed method, in Tab. I we follow the widely-used protocol in <ref type="bibr" target="#b0">[1]</ref> and compare our results with those works adopting the same setting. Furthermore, we carry out additional experiments under our proposed zero-shot protocol (Tab. II, III, IV, V), where we do NOT leverage any annotated data of the novel classes to fine-tune models. We expect a well-developed fewshot object detector can deal with novel objects as long as few support images are given, reducing the costly data re-collection and annotations processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generic FSOD Protocol</head><p>The performance of each method under the general FSOD protocol is reported in Tab. I. In this table, we comply with the evaluation protocol widely used in previous works and compare our results with the numbers reported in previous papers. As we have discussed in Sec. IV-B1, the models will not be exposed to the annotations of the 20 novel classes during training. After training, there will be K = 10, 30 bounding box annotations per novel class can be used to fine-tune the models. Under such a challenging setting, Tab. I shows our method significantly outperforms the other baselines. By equipping with DAnA, Faster R-CNN conspicuously outperforms the strongest baseline [17] by 6.1 and 6.9 AP under the 10-shot and 30-shot settings respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Zero-shot FSOD Protocol</head><p>In this section, we evaluate the models under a more challenging setting without fine-tuning. Note that the word "zero-shot" here is not referred to the zero-shot learning paradigm <ref type="bibr" target="#b54">[55]</ref> but referred to the fact that we do not take any novel image to fine-tuned the models. In order to evaluate each method under an unified protocol, we re-implement three previous methods: Meta R-CNN <ref type="bibr" target="#b1">[2]</ref>, FGN <ref type="bibr" target="#b12">[13]</ref> and Attention RPN <ref type="bibr" target="#b2">[3]</ref>. These baselines and our model are all based on  Faster R-CNN, and therefore we can fairly analyze the impact brought by different attention mechanisms. Note that we have slight modifications on these methods: 1) For the model architectures, we replace the multi-class classification output with the binary one as explained in III-C. 2) For training, we apply the two-way contrastive strategy <ref type="bibr" target="#b2">[3]</ref> to each method since we empirically observe it can improve the performance.</p><p>In addition, as we discussed in Sec. IV-B3, all the methods are evaluated without fine-tuning under such a protocol. Tab. II shows the results predicted with different numbers of support images. With 5-shot support images given at inference, DAnA-FasterRCNN outperforms FGN and Attention RPN by 3.5/6.4/4.0 and 3.8/6.0/4.7 mAP respectively on AP/AP 50 /AP 75 metrics. Moreover, the proposed model achieves the most significant improvement as the shot increases, supporting our argument that DAnA can better retrieve the information carried within support images by leveraging QPA features.</p><p>Intuitively, without being fine-tuned on the novel domain, a well-developed FSOD model should perform well on base classes. In order to evaluate to what extent the ability of detection networks has been undermined by the modifications, we also provide the performance of Faster R-CNN <ref type="bibr" target="#b5">[6]</ref>, which is the backbone of these few-shot object detectors. The Faster R-CNN is trained on the 60 base classes and has not been exposed to the novel categories. The performance of it can be regarded as the upper bound of performance on base classes. By precisely capturing object-wise correlations, our model has a much smaller gap with the upper bound comparing with baselines. Additionally, we report the model sizes and inference speed in Tab. II as well. The proposed DAnA component only causes little additional cost in comparison to prior approaches.</p><p>In Tab. III, we further evaluate each method under the multiway evaluation setting. Under the multi-way setting, models must detect more than one object category conditioned on the given support set. Suppose there are M categories in a query image, we will randomly sample N classes from them to form an N-way support set. Sometimes the number of object classes in a query image will be less than N, and we just select other  N ? M classes to form an N-way support set.</p><p>Fan et al. <ref type="bibr" target="#b12">[13]</ref> evaluated models under the COCO2VOC setting, where models are trained on the 60 COCO categories disjoint with VOC, and then tested on PASCAL VOC <ref type="bibr" target="#b53">[54]</ref>. In this work, we consider a much more challenging setting termed PASCAL2COCO, where models are trained on PASCAL VOC 2007 and tested on the COCO benchmark. The 60 categories disjoint with VOC will serve as the novel categories, which is opposite to the setting in <ref type="bibr" target="#b12">[13]</ref>. In Tab. IV, even the models are applied to a domain that is more complicated than the training domain, the models equipped with DAnA still demonstrate remarkable ability.</p><p>In Tab. V, we apply the same zero-shot protocol but follow the tradition in FSC to construct episodes. Instead of directly testing by COCO validation split, we follow the episode-based evaluation protocol defined in RepMet <ref type="bibr" target="#b9">[10]</ref> to prepare 500 random evaluation episodes in advance. In each episode, take N-way K-shot evaluation for example, there will be K support images and 10 query images for each of the N categories. Consequently, there will be K ? N support images and 10 ? N query images in each episode.</p><p>In Tab. IV and Tab. V, we have compared DAnA-FasterRCNN with another proposed framework, DAnA-RetinaNet, to evaluate the performance as adopting different detection networks. Generally, the two-stage few-shot object detectors <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref> with region proposal network (RPN) have higher performance than the one-stage <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref> methods (see Tab. I). Nonetheless, though our DAnA-RetinaNet is based on the one-stage object detector, it still outperforms previous methods using two-stage networks, which suffices to verify the effectiveness and dexterity of our DAnA. On the other hand, it seems the use of feature pyramids does not help DAnA-RetinaNet reach better results. The result could be caused by the lack of RPN. Without RPN, networks must tackle a nearly exhaustive list of potential object locations. Furthermore, in FSOD, those foregrounds irrelevant to the presented support set should be classified as backgrounds as well. Therefore, the huge amount of anchors and the extremely imbalanced foreground-background ratio would seriously undermine the ability of DAnA-RetinaNet. To conclude, we suggest RPN plays a deep role in fully presenting the advantages of DAnA, which is verified by the aforementioned experiments and the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablations</head><p>In Tab. VI, we show the results of the ablation study to analyze the impact brought by each component. a) Whether CISA is effective at utilizing support images: In Sec. III-B3, we argue that the issue of feature uncertainty can be addressed by performing average pooling across queryposition-aware (QPA) vectors. In the ablations, we investigate the improvement brought by our CISA component. For setting (a), we directly take the mean feature over multiple images as the class representations. The ablation (a, b) suffices to show that transforming ordinary features into QPA vectors indeed brings improvement on the performance. b) Why not just denoise by soft attention: To attenuate noise in support images, it is intuitive to directly learn a soft attention mask and reweights the importance of each pixel. In Tab. VI, "Mask" means we leverage CNN to learn a soft attention mask and perform element-wise product between the mask and the support feature map. As it can be observed, the result suggests that using the BA module is more effective than using general soft attention by preserving much more information.</p><p>c) How to combine query features and QPA vectors: Since the resulting QPA features will have the same size as query feature maps, they can be combined by either concatenation or element-wise product. According to (b, c), we conclude that concatenation is a better strategy for DAnA.</p><p>d) The importance of RPN: In the experiments, we have compared the two proposed detection networks, DAnA-FasterRCNN (with RPN) and DAnA-RetinaNet (without RPN). Based on the results in Tab. IV and V, we conclude that RPN is a critical component in our problem. Nonetheless, there could be other factors influencing the results, such as the use of feature pyramids and Focal Loss <ref type="bibr" target="#b6">[7]</ref>. Therefore, in the ablations (e), we re-train a DAnA-RetinaNet but remove the feature pyramids from it. However, we still adopt the Focal Loss in (e) because either RPN or Focal Loss is necessary to prevent the model from being severely deteriorated by numerous negative anchors. The result (e, f) shows that, without the influence of feature pyramids, the model including RPN has significantly better performance. To conclude, we suggest RPN is an indispensable component to fully represent the effectiveness of DAnA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visualizations</head><p>The examples of few-shot object detection are demonstrated in <ref type="figure" target="#fig_6">Fig. 7</ref>. In FSOD, the prediction is dependent on the object category of the given support set. Notably, all the target instances in <ref type="figure" target="#fig_6">Fig. 7</ref> belong to the novel classes, so the model has never been trained to recognize these objects. However, given few support images, the proposed model is capable of recognizing and locating the instances. The last row of <ref type="figure" target="#fig_6">Fig. 7</ref> presents a failure case where a motorcycle is given yet the bicycle in the query is detected.</p><p>We also visualize the cross-image spatial attention (CISA) in <ref type="figure">Fig. 8</ref>. The CISA module is capable of capturing the semantic correspondence between the query and support. Take (e) for example, given the head (colored in red) or the feet of  <ref type="figure">Fig. 8</ref>: For each query image, we consider two different regions (colored in red and blue) and visualize their corresponding support attention maps. The CISA module is capable of capturing the semantic correspondence between the query and support images (e.g., the head or legs of an animal). (f) represents the case where neither of the support categories exists in the query image.</p><p>a human (colored in blue), the attention maps will highlight the head or the feet areas of the support images. The result (f) represents the case that neither of the support categories exists in the query image, which shows that if there is no corresponding contextual information, CISA has a tendency to highlight the most distinguishing components that can best describe an object.</p><p>V. CONCLUSION In this work, we observed the problem of spatial misalignment and feature uncertainty in the challenging few-shot object detection (FSOD) task. We propose a novel and effective Dual-Awareness Attention (DAnA) mechanism to tackle the problem. Our method is adaptable to both one-stage and twostage object detection networks. DAnA remarkably boosts the FSOD performance on the COCO benchmark and reaches state-of-the-art results. We are excited to point out a new direction to solve FSOD tasks. We encourage future works to extend our method to other challenging tasks such as fewshot instance segmentation and co-salient object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of few-shot object detection and the concerns in previous methods (a-c). (a) Previous works seek computational efficiency by encoding support images into global feature vectors, causing the loss of spatial and local information. (b) The convolution-based attention has difficulty in modeling relationships across objects with varying spatial distributions. (c) Performing average pooling across multiple high-level features might cause uncertainty in the resulting feature representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of our proposed methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The model architectures of DAnA-FasterRCNN and DAnA-RetinaNet. Our proposed components are flexible and can be easily combined with existing object detection frameworks which are not originally designed for FSOD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization of the AP on each base (training) category of COCO. A well-developed few-shot object detector should be comparable to the traditional object detector on the training categories if it has not been fine-tuned. This figure demonstrates our model indeed achieves competitive performance on training categories and even surpasses the traditional one on particular classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>At training, the model has never seen the novel categories, including bird, car, dog, person, motorcycle, bicycle, etc. However, given different support examples, our model is capable of recognizing unseen target objects in the query image. The last row represents the failure case where the model confuses a bicycle with a motorcycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Test #1 Test #2 Test #N Image Pool Meta R-CNN FGN Attention RPN Ours</head><label></label><figDesc></figDesc><table><row><cell>AP</cell></row><row><cell>Fig. 2: Illustration of the pilot study. We carry out an experi-</cell></row><row><cell>ment to test whether the noise and spatial variability of support</cell></row><row><cell>images would seriously influence FSOD results. The standard</cell></row><row><cell>deviations of AP of the four methods are [0.72, 0.81, 0.68, 0.46]</cell></row><row><cell>respectively. Our method is effective at capturing object-wise</cell></row><row><cell>correlations irrespective of the choice of support images.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>The performance on novel categories of COCO.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>The 1-way, zero-shot evaluation on COCO. All the few-shot models are trained on base categories and then tested on novel domains without fine-tuning. Note that the performance of Faster R-CNN<ref type="bibr" target="#b5">[6]</ref> on base classes can serve as the upper bound because all the methods in this table leverage<ref type="bibr" target="#b5">[6]</ref> as their backbone detector. In comparison with baselines, the relative mAP improvement of our method is up to 49% on novel categories, and the performance gap with the traditional object detector has been reduced. The model size and inference speed are reported as well. ? : re-implemented results.</figDesc><table><row><cell>Method</cell><cell></cell><cell>AP</cell><cell></cell><cell cols="3">Novel Categories AP 50</cell><cell></cell><cell>AP 75</cell><cell></cell><cell></cell><cell>AP</cell><cell></cell><cell cols="3">Base Categories AP 50</cell><cell></cell><cell>AP 75</cell><cell></cell></row><row><cell># Way</cell><cell>1way</cell><cell>3way</cell><cell>5way</cell><cell>1way</cell><cell>3way</cell><cell>5way</cell><cell>1way</cell><cell>3way</cell><cell>5way</cell><cell>1way</cell><cell>3way</cell><cell>5way</cell><cell>1way</cell><cell>3way</cell><cell>5way</cell><cell>1way</cell><cell>3way</cell><cell>5way</cell></row><row><cell># Given Supports</cell><cell></cell><cell>5shot</cell><cell></cell><cell></cell><cell>5shot</cell><cell></cell><cell></cell><cell>5shot</cell><cell></cell><cell></cell><cell>5shot</cell><cell></cell><cell></cell><cell>5shot</cell><cell></cell><cell></cell><cell>5shot</cell><cell></cell></row><row><cell>Meta R-CNN  ? [2]</cell><cell>11.2</cell><cell>11.0</cell><cell>10.2</cell><cell>25.9</cell><cell>25.2</cell><cell>23.4</cell><cell>8.6</cell><cell>8.6</cell><cell>8.0</cell><cell>28.5</cell><cell>27.4</cell><cell>26.2</cell><cell>52.3</cell><cell>50.8</cell><cell>48.7</cell><cell>28.2</cell><cell>26.8</cell><cell>25.7</cell></row><row><cell>FGN  ? [13]</cell><cell>10.9</cell><cell>10.8</cell><cell>9.6</cell><cell>24.0</cell><cell>23.4</cell><cell>21.2</cell><cell>9.0</cell><cell>9.1</cell><cell>8.1</cell><cell>26.9</cell><cell>25.1</cell><cell>23.6</cell><cell>47.6</cell><cell>45.4</cell><cell>42.4</cell><cell>27.4</cell><cell>25.3</cell><cell>23.8</cell></row><row><cell>Attention RPN  ? [3]</cell><cell>10.6</cell><cell>9.8</cell><cell>9.0</cell><cell>24.4</cell><cell>22.8</cell><cell>20.8</cell><cell>8.3</cell><cell>7.9</cell><cell>7.3</cell><cell>23.0</cell><cell>21.3</cell><cell>20.2</cell><cell>42.0</cell><cell>39.8</cell><cell>37.7</cell><cell>22.4</cell><cell>20.6</cell><cell>19.7</cell></row><row><cell>DAnA-FasterRCNN</cell><cell>14.4</cell><cell>13.7</cell><cell>12.6</cell><cell>30.4</cell><cell>28.2</cell><cell>25.9</cell><cell>13.0</cell><cell>12.4</cell><cell>11.3</cell><cell>32.0</cell><cell>31.0</cell><cell>29.5</cell><cell>54.1</cell><cell>52.2</cell><cell>49.9</cell><cell>32.9</cell><cell>31.7</cell><cell>30.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc></figDesc><table /><note>The multi-way, zero-shot evaluation on COCO. Under the N-way setting, we introduce a support set comprised of N categories to the model and therefore the model should detect the objects belonging to any of these N categories.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>The results of PASCAL2COCO evaluation. All the models are trained on PASCAL VOC 2007 and tested on COCO 2014. In this setting, the base domain denotes the 20 classes shared between PASCAL VOC and COCO, while the novel domain denotes the other 60 classes in the COCO dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell>: Following [10], we construct class-balanced</cell></row><row><cell>episodes from COCO for evaluation. In addition, we report the</cell></row><row><cell>performance of two different detection networks adopting the</cell></row><row><cell>proposed DAnA component. Even though the performance of</cell></row><row><cell>DAnA-RetinaNet is less remarkable than DAnA-FasterRCNN,</cell></row><row><cell>it still significantly outperforms the baselines which are based</cell></row><row><cell>on Faster R-CNN, showing the effectiveness of our proposed</cell></row><row><cell>attention mechanism.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VI :</head><label>VI</label><figDesc>The results of ablations. (a,b) shows the ef-</figDesc><table><row><cell>fectiveness of using QPA features. (b,f) and (d,f) show the</cell></row><row><cell>effectiveness of the proposed BA mechanism. (e,f) shows RPN</cell></row><row><cell>is a critical component to fully represent the advantages of</cell></row><row><cell>DAnA.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported in part by the Ministry of Science and Technology, Taiwan, under Grant MOST 110-2634-F-002-026. We benefit from NVIDIA DGX-1 AI Supercomputer and are grateful to the National Center for High-performance Computing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8420" to="8429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meta rcnn: Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9577" to="9586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Few-shot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-scale positive sample refinement for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09384</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06957</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Repmet: Representative-based metric learning for classification and few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fgn: Fully guided network for few-shot instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9172" to="9181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crnet: Cross-reference networks for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4165" to="4173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Few-shot object detection and viewpoint estimation for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12107</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Prototype rectification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10713</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11539</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An empirical study of spatial attention mechanisms in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6688" to="6697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06668</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spa-gan: Spatial attention gan for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Aliabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Chinnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="391" to="401" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatiotemporal attention networks for action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2990" to="3001" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A selfpaced multiple-instance learning framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A unified metric learning-based framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2473" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Group collaborative learning for co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Lstd: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01529</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Concurrent multiple instance learning for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiple component learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust object tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1619" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3666</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<title level="m">2019, and the M.S. degree in the Department of Computer Science &amp; Information Engineering</title>
		<imprint/>
		<respStmt>
			<orgName>National Cheng Kung University ; NCKU ; National Taiwan University ; NTU</orgName>
		</respStmt>
	</monogr>
	<note>2021. His research interests include computer vision. object detection and machine learning theory</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">degree from computer science department in National Taiwan University (NTU) in 2020. He is currently a research assistant in vision science lab in EE department at National Tsing Hua University. His research interests include computer vision, robotic learning, and 3D scene understanding</title>
		<editor>Yueh-Cheng Liu received M.S.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Su is currently pursuing the Ph.D. degree with the Graduated Institute of Networking and Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hung-Ting</surname></persName>
		</author>
		<imprint>
			<pubPlace>Taipei, Taiwan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National Taiwan University</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include multimodal comprehension and unsupervised learning</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">His research interests include machine learning and medical image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu-Cheng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<pubPlace>Taipei, Taiwan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Chang received his M.S. degree from the Department of Computer Science and Information Engineering, National Taiwan University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">He is currently a research assistant at National Tsing Hua University, Taiwan. His research interests include object detection, natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu-Hsiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Hsinchu, Taiwan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Lin received the M.S. degree in statistics from National Yang Ming Chiao Tung University</orgName>
		</respStmt>
	</monogr>
	<note>sentiment analysis, and deep learning</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">He is currently pursuing his Ph.d. degree in Computer Science and Information Engineering at Natioal Taiwan University (NTU), Taiwan. His research interests include machine learning (ML), evolutionary algorithms (EAs), and computer games (CG). Recently, He is devoted to the study on fewshot learning</title>
	</analytic>
	<monogr>
		<title level="m">Jia-Fong Yeh received the B.S. degree and the M.S. degree in</title>
		<meeting><address><addrLine>Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Computer Science and Information Engineering from National Taiwan Normal University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
