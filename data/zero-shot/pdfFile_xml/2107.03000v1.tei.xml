<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POSERN: A 2D POSE REFINEMENT NETWORK FOR BIAS-FREE MULTI-VIEW 3D HUMAN POSE ESTIMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Sayo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Thomas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kawasaki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Osaka University</orgName>
								<address>
									<settlement>Japan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsushi</forename><surname>Ikeuchi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Corp</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Kyushu University</orgName>
								<address>
									<settlement>Japan</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">POSERN: A 2D POSE REFINEMENT NETWORK FOR BIAS-FREE MULTI-VIEW 3D HUMAN POSE ESTIMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Pose refinement</term>
					<term>3D human pose</term>
					<term>multi- view reconstruction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new 2D pose refinement network that learns to predict the human bias in the estimated 2D pose. There are biases in 2D pose estimations that are due to differences between annotations of 2D joint locations based on annotators' perception and those defined by motion capture (MoCap) systems. These biases are crafted into publicly available 2D pose datasets and cannot be removed with existing error reduction approaches. Our proposed pose refinement network allows us to efficiently remove the human bias in the estimated 2D poses and achieve highly accurate multi-view 3D human pose estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Early works on 3D human pose estimation have using convolutional neural networks (CNN) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> have focused on using a single image as input. This is an ill-posed problem and as a consequence, such CNN-based methods rely much on the knowledge from the dataset. However, unlike 2D human pose datasets, publicly available MoCap-based 3D human pose datasets only contain images of a limited number of persons taken in a controlled environment and with tight clothes.</p><p>To take advantage of the publicly available large 2D datasets, methods were proposed to estimate 3D human poses from multi-view 2D images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. The core idea is to first estimate 2D poses in each view by using a 2D pose estimation network and then triangulate them to generate a 3D pose. To cope with noise in the 2D pose estimate two strategies exist: (1) defining a robust 3D triangulation algorithm <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> or (2) refining the 2D pose estimates prior to doing the 3D triangulation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>In this work, we take the second approach. We observed in the commonly used datasets like Human3.6M <ref type="bibr" target="#b12">[13]</ref> that there are consistent errors between projections of 3D poses and the corresponding 2D estimates (or even annotations). We reason that errors in 2D pose estimates do not come solely from computational errors but also come from consistent displacements due to human perception bias that depends on the viewpoint and the pose. Unlike computational errors, biases cannot be removed by using averaging strategies. We propose a new 2D pose refinement network that leverages both 2D and 3D information to un-bias the initial 2D pose estimates and achieve accurate 3D pose estimation. Our core idea is to build a network, dubbed PoseRN (1), that learns to predict the 2D bias between the initial multi-view 2D poses and the anatomical ones defined by the (few) available MoCap-based 3D datasets. The 2D human biases depend on the camera viewpoint and the human pose. We reason that the camera viewpoint information is contained into the estimated 2D pose, while the human pose information is contained in the initial 3D pose estimate.</p><p>In summary, our main contributions are (1) a marker-less multi-view 3D pose estimation method, which outperforms previous methods; (2) a pose-refining network, PoseRN, to rectify human perceptional 2D annotations to anatomical ones, which is defined by MoCap systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BIASES IN 2D POSE ANNOTATIONS</head><p>The 3D poses obtained by triangulation based on estimated 2D poses in multi-view images, in general, are not accurate. One of the major factors that introduce such errors is that humans cannot make pixel-accurate annotations. This type of error behaves more like variance in annotations and can be alleviated by averaging over multiple annotations. Aside from this variance, there are inevitable errors between 2D human arXiv:2107.03000v1 [cs.CV] 7 Jul 2021 S9 S11 S1 S6 S5 "Posing" "Directions 1" "Discussion 1" "Phoning 1"  pose annotations that come from the difference of joint definitions for 3D and 2D joint annotations.</p><p>The 3D poses in the datasets, such as Human3.6M <ref type="bibr" target="#b12">[13]</ref>, used to train and evaluate 3D human pose estimation methods are obtained with a MoCap system. As a consequence, each dataset has its own marker-based definition of joints. Such joint positions may be computed by averaging multiple marker positions associated with a certain joint; therefore, the obtained joint positions can be inside the human body.</p><p>Meanwhile, the dataset used for training 2D pose estimation employed human annotators. When manually annotating a 2D image, an annotator is asked to click on the pixels that should be the 2D re-projection of the 3D joint. However, the 3D joints are inside the body, so the annotator can only guess the 2D re-projection location by looking at the surface of the body. This means that a 3D joint position is less likely to be on the ray formed by the camera center and the corresponding annotated 2D joint position.</p><p>We reason that humans have a similar perception of the human body and that the errors made when guessing 2D joint locations tend to be similar for different persons. As a consequence, there is a bias in 2D pose annotation datasets that depends on both camera pose and human body pose. This can be seen in the top row of <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">3D POSE ESTIMATION WITH 2D POSE DE-BIASING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">De-biasing 2D poses</head><p>As discussed in the previous section, the displacements in 2D pose estimates may be described by 1) the discrepancy in the definitions of joints for 3D and 2D pose annotations, 2) human poses, and 3) camera positions. More specifically, lettin? J 3D and? 2D denote a 3D joint position annotation and its corresponding 2D joint position annotation in a certain camera image, we assume that the relationship between? 3D and? 2D can be described by:</p><formula xml:id="formula_0">?(? 3D |M ) =? 2D + ?(? 3D , M ) + .<label>(1)</label></formula><p>where M is the camera parameter for the image, ?(?|M ) the projection and is some noise. ?(? 3D , M ) is the bias between the projection of? 3D and? 2D . Our PoseRN ideally learns this ? to de-bias the estimated 2D poses. We use 3D pose estimates J 3D = {J 3D j |j ? J } and 2D pose estimates J 2D = {J 2D cj |c ? C, j ? J } as input to PoseRN, where J 3D j is the 3D position of joint j of the 3D pose estimate; J 2D cj the 2D position of joint j in camera c of the 2D pose estimates; J and C are the index sets for joints and cameras.</p><p>Similarly to <ref type="bibr" target="#b3">[4]</ref>, our network consists of two linear layers followed by batch normalization, ReLU, and dropout ( <ref type="figure" target="#fig_0">Fig. 1)</ref>. Before training, all 3D pose annotations and 2D poses estimated from the input images are translated so that the pelvis joint coincides with the origin of the local coordinate system.</p><p>We employ the MSE loss and minimize the error between real biases and predicted biases, given by:</p><formula xml:id="formula_1">= E[ ? cj ? PoseRN(J 3D , J 2D c ) 2 ],<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">? cj = J 2D cj ? ?(? 3D j |M c ) based on Eq. (1), PoseRN is the output of the network, J 2D</formula><p>c is the set of 2D joint positions in camera c. Both 3D joint positions and 2D joint positions are concatenated into respective vectors to form the input to the network. At test time, we de-bias the 2D pose by subtracting the predicted bias from 2D pose estimate J 2D c . <ref type="figure" target="#fig_2">Figure 3</ref> shows the overview of our method. Specifically, the pipeline consists of 1) 2D poses estimation from multi-view images, 2) initial 3D pose estimation by multi-view triangulation, 3) 2D poses de-biasing by PoseRN, and 4) final 3D pose estimation from de-biased 2D poses. 2D pose estimation: We firstly estimate 2D human pose J 2D in multi-view images independently. We use OpenPose <ref type="bibr" target="#b13">[14]</ref> as our 2D pose estimator, in which each joint position has the probability of the joint being at the pixel position. This probability, w cj , corresponding to J 2D cj can be in turn viewed as the confidence of the joint position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">multi-view 3D pose reconstruction</head><p>Initial 3D pose estimation: On multi-view system, the cameras are calibrated and thus the camera parameter M c of camera c is known. We employ multi-view geometry to compute 3D pose J 3D from multiple 2D pose estimates J 2D . Specifically, the initial 3D pose is obtained by</p><formula xml:id="formula_3">J 3D = argmin J 3D c?C j?J ?(w cj ?(J 3D j |M c ) ? J 2D cj ),<label>(3)</label></formula><p>where ?(?) is the Huber loss. Note that we use confidence w cj as weight because a joint with a low confidence may imply a significant error in the 2D pose estimation. When the 2D positions of joint j are not estimated in images of all camera or are estimated only in a single image, we exclude joint index j from J to ignore the joint. 2D pose de-biasing by PoseRN: PoseRN takes the initial 3D pose estimate J 3D and 2D pose estimates J 2D as input and predicts the bias between 3D pose and 2D poses. As mentioned in Section 3.1, we de-bias the 2D pose estimates b?</p><formula xml:id="formula_4">J 2D cj = J 2D cj + PoseRN cj (J 3D , J 2D ),<label>(4)</label></formula><p>where PoseRN cj is predicted bias for joint j in camera c. Final 3D pose estimation: In order to compute the final 3D pose estimateJ 3D , we use Eq. (3) but the de-biased 2D estimatesJ 2D are used instead of J 2D . The output of our method is thusJ 3D .</p><p>For PoseRN, we preprocessed the 2D poses from Open-Pose <ref type="bibr" target="#b13">[14]</ref>. They are re-scaled by the smaller length of the width or height of the input image and a third of the length of the spine. This scaling parameter is determined empirically so that the distributions of the joints coordinates in the estimated 2D poses and 3D poses become similar. We trained PoseRN using the Adam optimizer with a learning rate of 0.001 for 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We evaluate the performance of our proposed method with three experiments. First, we evaluate the ability of PoseRN <ref type="figure">Fig. 4</ref>: Qualitative results on MPI-INF-3DHP <ref type="bibr" target="#b14">[15]</ref> and Total-Capture <ref type="bibr" target="#b15">[16]</ref>. From left to right: input images, Iskakov et al. <ref type="bibr" target="#b6">[7]</ref>, ours, and ground-truth 3D pose annotations.</p><p>to reduce the bias using Human3.6M dataset. The second experiment evaluates the generalization performance in comparison with state-of-the-art Learnable Triangulation <ref type="bibr" target="#b6">[7]</ref>. We trained both methods on the Human3.6M dataset <ref type="bibr" target="#b12">[13]</ref> and tested them on other 3D human pose datasets, i.e., MPI-INF-3DHP <ref type="bibr" target="#b14">[15]</ref> and TotalCapture <ref type="bibr" target="#b15">[16]</ref>. In the third experiment, we compare our proposed multi-view 3D pose estimation method with other multi-view 3D human pose estimation methods quantitatively. We employ the mean-per-joint position error (MPJPE) as our evaluation criterion, which is the average Euclidean distance between an estimated joint position to the ground-truth joint position in millimeters.</p><p>Note that for all the methods evaluated in our experiments, we discarded the input data for which OpenPose totally fails. We applied this elimination to all methods. Also, we retrained all previous methods with the same datasets used for our proposed method for a fair comparison. cj ? ?(J 3D j |M c ) for PoseRN. We can see that some Open-Pose's joint positions are clearly off from the origin, indicating the presence of the bias. In contrast, PoseRN successfully reduces this bias. The fact that the bias is predictable based on J 2D c and J 3D may inductively support our assumption that the bias comes from the discrepancy in the joint definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Qualitative evaluation of PoseRN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalization to other dataset</head><p>To evaluate the generalization performance of our method, we compare it with Iskakov et al. <ref type="bibr" target="#b6">[7]</ref> and vanilla multi-view triangulation in Eq. 6M. All methods are tested on some excerpts from TotalCapture and the training split of MPI-INF-3DHP. Specifically, from MPI-INF-3DHP, we used Seq1 of S1 and Seq2 of S2. From TotalCapture, we used 2 subjects (s1 and s4) and 3 actions for each subject (acting3, freestyle3, and walk-ing2). During the test on MPI-INF-3DHP, we measured the MPJPE on 12 joints: Neck, L/R shoulder, L/R elbow, L/R wrist, Pelvis, L/R hip, L/R knee. We used the same joints  Note that <ref type="bibr" target="#b6">[7]</ref> takes the 2D bounding box of the person as input. We created the bounding boxes from the ground-truth foreground masks provided in both datasets and use them to crop the input images before estimating the 2D pose. Our proposed method has the advantage that it works directly on the 2D pose and does not require image input. As a consequence, we do not need the bounding box as input and our proposed method is insensitive to changes in the appearance of the person and its background.</p><p>The quantitative results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Our method outperformed <ref type="bibr" target="#b6">[7]</ref> for all the sequences. We carefully checked each frame and found that <ref type="bibr" target="#b6">[7]</ref> has severe errors when the subjects are wearing loose clothes with rich textures or when the sequence was captured by cameras that are placed far apart from the subject compared to Human 3.6M as shown in <ref type="figure">Fig. 4</ref>. These results show the generalizability of our method compared to SOTA (i.e., <ref type="bibr" target="#b6">[7]</ref>) in terms of changes in the appearance and viewpoint. In some sequences, the baseline multi-view method slightly outperformed our proposed method because some poses in these sequences were rarely seen in the training data.</p><p>In addition, PoseRN has the advantage that it successfully removed the annotation bias even when the subject wears totally different clothes and when the viewing directions are totally different from those of the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Human3.6M</head><p>We quantitatively evaluated our proposed method with two recent methods <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref> on Human3.6M. <ref type="table" target="#tab_1">Table 2</ref> shows the results when all methods are tested on Human3.6M and trained on other datasets. We trained our proposed network on the MPI-INF-3DHP dataset <ref type="bibr" target="#b14">[15]</ref> (Seq1 and Seq2 of S1 and S2). MV optimization means that we just use the 2D pose estimator pre-trained on the 2D human pose dataset and conduct multi-view optimization as described in Sec. 3.2.</p><p>Here we assume that the joints positioned inside the body may have more bias than that of near the surface (wrist, ankle, etc...). To test this assumption, we have measured MPJPE for all estimated joints and subset of them. When we use all joints to measure, the MPJPE is 38.4mm and when use a subset joints (neck, L/R elbow, spine), that is 29.6mm. These results confirmed our hypothesis and demonstrated the ability of PoseRN to reduce not only the gaussian errors but also the errors that come from the human perceptual bias.</p><p>Qiu et al. <ref type="bibr" target="#b7">[8]</ref> first estimates the 3D human pose with a network pre-trained on MPII Human Pose Dataset <ref type="bibr" target="#b16">[17]</ref> and then fine-tune the pre-trained network on the estimated 3D poses. Iskakov et al. <ref type="bibr" target="#b6">[7]</ref> train their network on CMU Panoptic dataset <ref type="bibr" target="#b8">[9]</ref>, using 27 of the 31 HD cameras. They test their network on Human3.6M and measure MPJPE for the subset of all joints (L/R elbows, L/R wrists, and L/R knees). Although the Panoptic dataset has more variety in terms of camera viewpoints and subjects' appearance than the MPI-INF-3DHP, the results of our method are more accurate than that of <ref type="bibr" target="#b6">[7]</ref>. This confirms that our method (or PoseRN) requires only a few camera viewpoints and small variation on subjects during training to achieve accurate 3D human pose estimation. In addition, unlike Qiu et al.'s <ref type="bibr" target="#b7">[8]</ref>, our method can be applied to any camera setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We introduced a simple yet effective method for multi-view 3D pose estimation that does not require any marker. We proposed PoseRN that allows us to learn the bias due to the difference of the joint definitions for 3D pose annotations and 2D pose annotations. Our experimental results demonstrated that our method has a clear advantage when the situations of training data and test data are totally different.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Network structure of pose refinement network (PoseRN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Analysis of the bias between 3D pose annotations Human3.6M and 2D pose estimates by OpenPose (top) and PoseRN (bottom). Each scatter plot shows the displacements from GT 2D joint position. Joint positions in the same color represents those in the same camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of our proposed two-pass 3D pose estimation method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>shows displacements of 2D pose estimates by Open-Pose and de-biased 2D pose estimates by PoseRN in some example multi-view videos. More specifically, each dot in the figure represents J 2D cj ? ?(J 3D j |M c ) for OpenPose and J 2D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(3). Ours and Iskakov et al.'s are trained on Human3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MPJPE in mm on MPI-INF-3DHP<ref type="bibr" target="#b14">[15]</ref> (left) and TotalCapture<ref type="bibr" target="#b15">[16]</ref> (right).</figDesc><table><row><cell></cell><cell>S1</cell><cell>S2</cell><cell></cell><cell>S1</cell><cell></cell><cell></cell><cell>S4</cell></row><row><cell></cell><cell>Seq1</cell><cell>Seq2</cell><cell>A3</cell><cell>FS3</cell><cell>W2</cell><cell>A3</cell><cell>FS3</cell><cell>W2</cell></row><row><cell>Iskakov et al. [7]</cell><cell cols="8">129.47 101.39 116.43 97.87 110.45 83.01 69.06 67.45</cell></row><row><cell cols="2">Multi-view reconstruction 64.09</cell><cell>53.13</cell><cell cols="6">50.02 59.92 58.69 50.52 63.41 45.07</cell></row><row><cell>Ours(PoseRN)</cell><cell>51.97</cell><cell>58.97</cell><cell cols="6">45.84 56.76 53.29 49.62 65.53 45.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation of the methods that do not use ground-truth 3D pose annotations in the training. All values except ours are adopted from the original papers. The number of the joints following each method name is used to measure MPJPE in each method.</figDesc><table><row><cell>Method</cell><cell>Avg. MPJPE (mm)</cell></row><row><cell>Qiu et al. [8] (17 joints)</cell><cell>43.0</cell></row><row><cell>Iskakov et al. [7] (algebraic) (6 joints)</cell><cell>36.0</cell></row><row><cell>Iskakov et al. [7] (volumetric) (6 joints)</cell><cell>34.0</cell></row><row><cell>Ours(MV optimization) (17 joints)</cell><cell>40.0</cell></row><row><cell>Ours(PoseRN) (17 joints)</cell><cell>38.4</cell></row><row><cell>Ours(PoseRN) (4 joints)</cell><cell>29.6</cell></row><row><cell cols="2">except for Pelvis, L/R hip, and L/R knee, during the test on</cell></row><row><cell>TotalCapture.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by JSPS/KAKENHI 20H00611, 18K19824, 18H04119 in Japan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d human pose estimation with 2d marginal heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision, WACV 2019</title>
		<meeting><address><addrLine>Waikoloa Village, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1477" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2344" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6988" to="6997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Toso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Fifth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
