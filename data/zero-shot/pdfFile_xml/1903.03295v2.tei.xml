<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romero</forename><surname>Morais</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Applied Artificial Intelligence Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Applied Artificial Intelligence Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Applied Artificial Intelligence Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Applied Artificial Intelligence Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moussa</forename><surname>Mansour</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">iCetana, Inc. |</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Western Australia</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
							<email>svetha.venkatesh@deakin.edu.au2moussa@icetana.com.au</email>
							<affiliation key="aff0">
								<orgName type="department">Applied Artificial Intelligence Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appearance features have been widely used in video anomaly detection even though they contain complex entangled factors. We propose a new method to model the normal patterns of human movements in surveillance video for anomaly detection using dynamic skeleton features. We decompose the skeletal movements into two sub-components: global body movement and local body posture. We model the dynamics and interaction of the coupled features in our novel Message-Passing Encoder-Decoder Recurrent Network. We observed that the decoupled features collaboratively interact in our spatio-temporal model to accurately identify human-related irregular events from surveillance video sequences. Compared to traditional appearancebased models, our method achieves superior outlier detection performance. Our model also offers "open-box" examination and decision explanation made possible by the semantically understandable features and a network architecture supporting interpretability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video anomaly detection is a core problem of unsupervised video modeling. An effective solution is learning the regular patterns in normal training video sequences in an unsupervised setting, based on which irregular events in test videos can be detected as outliers. The problem is challenging due to the lack of human supervision and the ambiguous definition of human-perceivable abnormality in video events. Most current approaches operate on pixelbased appearance and motion features. These features are usually extracted from whole frames <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>, localized on a grid of image patches <ref type="bibr" target="#b23">[24]</ref>, or concentrated on pre-identified regions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>. Unfortunately, pixel-based features are high-dimensional unstructured signals sensitive to noise, that mask important information about the scene <ref type="bibr" target="#b27">[28]</ref>. Furthermore, the redundant information present in these features increases the burden on the models trained <ref type="bibr">Figure 1</ref>. We detect human-related anomalies in video by learning regular spatio-temporal patterns of skeleton features. In this example, we detect the anomalous event of a person catching a backpack. This anomaly is detected by using his unusual skeleton pose and motion compared to those of normal activities. Skeletons in red denote high anomaly scores, while skeletons in green denote low anomaly scores. The order of the frames is specified by the blue arrows. on them to discriminate between signal and noise.</p><p>Another key limitation of current methods is the lack of interpretability due to the semantic gap between visual features and the real meaning of the events. This limitation can be amplified through processing in deep neural networks <ref type="bibr" target="#b2">[3]</ref>. This lack of understanding prevents practitioners from using domain knowledge to customize model architectures and obstructs error analysis.</p><p>In this paper, we propose to leverage 2D human skeleton trajectories for detecting abnormal events related to human behavior in surveillance videos. The skeleton trajectories contain the locations of a collection of body joints in the spatio-temporal domain of video sequences, as illustrated in <ref type="figure">Figure 1</ref>. By using skeleton features we explicitly exploit the common structure of surveillance videos, which consists of humans and objects attached to them moving on top of a static background. Compared to appearance-based rep-resentations, skeleton features are compact, strongly structured, semantically rich, and highly descriptive about human action and movement, which are keys to anomaly detection.</p><p>By studying the human skeleton dynamics in a large collection of surveillance videos, we observed that human behavioral irregularity can be factorized into a few factors regarding body motion and posture, such as location, velocity, direction, pose and action. Motivated by this natural factorization, we propose to decompose the dynamic skeleton motions into two sub-processes, one describing global body movement and the other local body posture. The global movement tracks the dynamics of the whole body in the scene, while the local posture describes the skeleton configuration in the canonical coordinate frame of the body's bounding box, where the global movement has been factored out.</p><p>We jointly model the two sub-processes in a novel model called Message-Passing Encoder-Decoder Recurrent Neural Network (MPED-RNN). The network consists of two RNN branches dedicated to the global and local feature components. The branches process their data separately and interact via cross-branch message-passing at each time step. The model is trained end-to-end and regularized so that it distills the most compact profile of the normal patterns of training data and effectively detects abnormal events. In addition to anomaly detection, MPED-RNN supports openbox interpretation of its internal reasoning by providing the weights of the contributing factors to the decision and the visualization of these factors. We trial our method on two of the most challenging video anomaly datasets and compare our results with the state-of-the-art on the field. The results show that our proposed method is competitive in detection performance and easier to analyze the failure modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video anomaly detection</head><p>Unsupervised video anomaly detection methods have been an old-timer topic in the video processing and computer vision communities. Traditional approaches consider video frames as separate data samples and model them using one-class classification methods, such as one-class SVM <ref type="bibr" target="#b24">[25]</ref> and mixture of probabilistic PCA <ref type="bibr" target="#b14">[15]</ref>. These methods usually attain suboptimal performance when processing large scale data with a wide variety of anomaly types.</p><p>Recent approaches rejuvenate the field by using convolutional neural networks (CNN) to extract high-level features from video frame intensity and achieve improved results. Some of these methods include Convolutional autoencoder <ref type="bibr" target="#b12">[13]</ref>, spatio-temporal autoencoder <ref type="bibr" target="#b4">[5]</ref>, 3D Convnet AE <ref type="bibr" target="#b26">[27]</ref>, and Temporally-coherent Sparse Coding Stacked-RNN <ref type="bibr" target="#b19">[20]</ref>. Acknowledging the limitation of the intensity based features such as sensitivity to appearance noise, Liu et al. <ref type="bibr" target="#b17">[18]</ref> proposed to use the prediction of optical flow in their temporal coherent loss, effectively filtering out parts of the noise in pixel appearance. However, optical flow is costly to extract and still far from the semantic nature of the events.</p><p>Structured representations have recently attracted increased attention for its potential to get closer to the semantic concepts present in the anomalies. In <ref type="bibr" target="#b25">[26]</ref>, object trajectories were used to guide the pooling of the visual features so that interesting areas are paid more attention to. Towards model interpretability, Hinami et al. <ref type="bibr" target="#b13">[14]</ref> proposed to use object, attribute, and action detection labels to understand the reason of abnormality scores. Although it works well for a number of events, their method fails in many cases due to the incompleteness of the label sets and the distraction from unrelated information in the labels.</p><p>Our method of using skeleton features is another step towards using low-dimensional semantic-rich features for anomaly detection. We also advance the research efforts toward interpretability of the anomaly detection models by providing the ability to explain every abnormal event in our factorized semantic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Human trajectory modeling</head><p>Human motion in video scenes is an important factor for studying social behavior. It has been applied in multiple computer vision applications, mostly with supervised learning tasks such as action recognition <ref type="bibr" target="#b6">[7]</ref> and person reidentification <ref type="bibr" target="#b7">[8]</ref>. Recently, more effort has been invested into unsupervised learning of human motion in social settings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref> and single pose configuration <ref type="bibr" target="#b9">[10]</ref>. In this work, we propose to expand the application of skeleton motion features to the task of video anomaly detection. In MPED-RNN, we share the encoder-decoder structure with most unsupervised prediction models. However, instead of perfectly generating the expected features, we aim at distilling only the principal feature patterns so that anomalies are left out. This involves building a highly regulated autoencoder.</p><p>Regarding feature representation and modeling, apart from traditional methods that rely on hand-crafted local features and state machines, several recent works proposed to use interacting recurrent networks for motion modeling in social settings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>. In these approaches, the input data are in the form of whole body xy-location sequences while local postures are ignored. To bridge this gap, in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> the input of the RNN is extended to the skeleton joint locations. Recently, Du et al. <ref type="bibr" target="#b6">[7]</ref> proposed to divide the skeleton joints into five parts, which are jointly modeled in a five-branch bidirectional neural network. Different to previous approaches, we factorize skeleton motion based on natural decomposition of human motion into global movement/local deformation and model them jointly in an inter-active recurrent network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The anomalous human-related events in a surveillance video scene can be identified by the irregular human movement patterns observed in the video. Our method detects those anomalies by learning a regularity model of the dynamic skeleton features found in training videos. We assume the skeleton trajectories have already been extracted from the videos. At each time step t, a skeleton is represented by a set of joint locations in image coordinates</p><formula xml:id="formula_0">f t = x i t , y i t i=1..k ,</formula><p>where k is the number of skeleton joints. This set of temporal sequences is the input to our anomaly detection algorithm. Naturally, human motion consists of two factors: rigid movement of the whole body and non-rigid deformation of the skeleton joints. The simplest way to model human motion using a recurrent network is by feeding it the raw sequence of skeleton trajectories in image coordinates f t = x i t , y i t <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>, implicitly merging the global and local factors together. This solution performs well in videos with uniform skeleton scales and types of activities where the contribution of the two factors is consistent. On realistic surveillance videos, however, the scales of human skeletons vary largely depending on their location and actions. For skeletons in the near field, the observed motion is mainly influenced by the local factor. Meanwhile, for skeletons in the far field, the motion is dominated by the global movement while local deformation is mostly ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Skeleton Motion Decomposition</head><p>Inspired by the natural composition of human skeleton motion and motivated by the factorization models widely used in statistical modeling, we propose to decompose the skeletal motion into "global" and "local" components. The global component carries information about the shape, size and rigid movement of the human bounding box. The local component models the internal deformation of the skeleton and ignores the skeleton's absolute position in relation to the environment.</p><p>Geometrically, we set a canonical reference frame attached to the human body (called local frame), which is rooted at the center of the skeleton's bounding box. The global component is defined as the absolute location of the local frame center within the original image frame. On the other hand, the local component is defined as the residue after subtracting the global component from the original motion. It represents the relative position of skeleton joints with respect to the bounding box. This decomposition is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> and can be written in 2D vector space as:</p><formula xml:id="formula_1">f i t = f g t + f l,i t<label>(1)</label></formula><p>In 2D image space, xy-coordinates alone poorly represent the real location in the scene because the depth is missing. However, the size of a skeleton's bounding box is correlated with the skeleton's depth in the scene. To bridge this gap, we augment the global component with the width and height of the skeleton's bounding box f g = (x g , y g , w, h) and use them to normalize the local component f l,i = (x l,i , y l,i ). These features can be calculated from the input features as:</p><formula xml:id="formula_2">x g = max(x i ) + min(x i ) 2 ; y g = max(y i ) + min(y i ) 2 w = max(x i ) ? min(x i ); h = max(y i ) ? min(y i ) (2) x l,i = x i ? x g w ; y l,i = y i ? y g h<label>(3)</label></formula><p>The global and local dynamics can be modeled separately as two concurrent sub-processes. In generic videos, these two processes can even manifest independently. For example, a person can move her limbs around while keeping her global location relatively still. Similarly, a person riding a motorbike can move around while having a relatively fixed pose. However, given a specific context, regular human activities contain a strong correlation between these two components. Therefore breaking the cross-component correlation is also a sign of abnormality. In the previous examples, if those actions occurred in the scene where people were normally walking, they would be valid anomaly events. In the next section, we present how both individual dynamic patterns and the relationship between these two components are modeled in our MPED-RNN model. <ref type="figure">Figure 3</ref>. MPED-RNN consists of two interacting branches for two skeleton feature components. The local branch is drawn in green with shaded GRU blocks and the global branch is drawn in blue with transparent GRU blocks. The two components interact through messages (purple dashed) exchanged between the branches. The outputs are generated by a set of MLPs, represented by black rectangles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MPED-RNN Architecture</head><p>MPED-RNN models the global and local components as two interacting sub-processes where the internal state of one process is used as extra features to the input of the other process. More specifically, the model consists of two recurrent encoder-decoder network branches, each of them dedicated to one of the components. Each branch of the model has the single-encoder-dual-decoder architecture with three RNNs: Encoder, Reconstructing Decoder and Predicting Decoder. This structure is similar to the composite LSTM autoencoder (LSTM AE) of Srivastava et al. <ref type="bibr" target="#b21">[22]</ref>. However, unlike LSTM AE, MPED-RNN does not only model the dynamics of each individual component, but also the interdependencies between them through a cross-branch message-passing mechanism. We use Gated Recurrent Units (GRU) <ref type="bibr" target="#b3">[4]</ref> in every segment of MPED-RNN for its simplicity and similar performance to LSTM <ref type="bibr" target="#b10">[11]</ref>. At each time step, the GRU unit of one branch receives a message from the other branch informing its internal state at the previous time step. This message is incorporated into the GRU structure by treating it as an additional input. The same procedure is applied to the other branch. MPED-RNN's architecture is depicted in <ref type="figure">Figure 3</ref>.</p><p>Given an input skeleton segment of length T , we first initialize the hidden states of all the GRUs to null. Then, for each time step t, the skeleton f t is decomposed into f g t and f l t (using Eqs. <ref type="formula" target="#formula_1">(1)</ref>, <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>), which are input to the global encoder (E g ) and to the local encoder (E l ), respectively. The messages to be exchanged between the global and local branches are computed as specified by Eqs. 4 and 5 below.</p><formula xml:id="formula_3">m l?g t = ? W l?g h l t?1 + b l?g (4) m g?l t = ? W g?l h g t?1 + b g?l<label>(5)</label></formula><p>For t = 1, 2, . . . , T , the global and local segments are encoded using Eqs. 6 and 7:</p><formula xml:id="formula_4">E ge : h ge t = GRU f g t , m le?ge t , h ge t?1 (6) E le : h le t = GRU f l t , m ge?le t , h le t?1<label>(7)</label></formula><p>After encoding the input segments, the global and local Reconstructing Decoders initialize their hidden states as h gr T = h ge T and h lr T = h le T , respectively, and for t = T, T ? 1, ..., 1, we have:</p><formula xml:id="formula_5">D g r : h gr t?1 = GRU(m lr?gr t , h gr t ) (8) D l r : h lr t?1 = GRU(m gr?lr t , h lr t )<label>(9)</label></formula><p>Similarly, the global and local Predicting Decoders initialize their hidden states as h gp T = h ge T and h lp T = h le T , respectively, and for t = T + 1, T + 2, . . . , T + P , we have:</p><formula xml:id="formula_6">D g p : h gp t = GRU(m lp?gp t , h gp t?1 )<label>(10)</label></formula><formula xml:id="formula_7">D l p : h lp t = GRU(m gp?lp t , h lp t?1 )<label>(11)</label></formula><p>In training, the dual decoders in the MPED-RNN's architecture jointly enforce the encoder to learn a compact representation rich enough to reconstruct its own input and predict the unseen future. Meanwhile, in testing, the abnormal patterns cannot be properly predicted because they were neither seen before nor follow the normal dynamics.</p><p>In each decoder network, the projected features of the corresponding decoders,f g t andf l t , are independently generated from the hidden states h g t and h l t by fully-connected layers. These two projected features are concatenated and input to another fully-connected layer, which generates the projected perceptual featuref t in the original image space. Ideally,f t can be calculated fromf g t andf l t by inverting Eqs. <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>. However, by being projections into lowdimensional subspaces, a direct computation is unlikely to be optimal. Thus, using a fully-connected layer to learn the inverse mapping allows the computation to be robust to noise. These projected features are used to evaluate the conformity of an input sequence of skeletons to the learned normal behavior and hence are used to build the loss function for training and score function for testing. These procedures are detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training MPED-RNN</head><p>Training setup The trajectory of a person can span many frames in a video. However, recurrent networks are trained on fixed-size sequences. To cope with this issue, we extract fixed-size segments from every skeleton's trajectory using a sliding-window strategy. Therefore, each segment is computed as:</p><formula xml:id="formula_8">seg i = {f t } t=bi..ei<label>(12)</label></formula><p>where b i and e i are beginning and ending indices of the ith segment calculated from the chosen sliding stride s and segment length T :</p><formula xml:id="formula_9">b i = s ? i; e i = s ? i + T<label>(13)</label></formula><p>During training, batches of training segments are decomposed into global and local features, which are input to MPED-RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss functions</head><p>We consider three loss functions defined in three related coordinate frames. The Perceptual loss L p constrains MPED-RNN to produce the normal sequences in the image coordinate system. The Global loss L g and the Local loss L p act as regularization terms that enforce that each encoder-decoder branch of MPED-RNN work as designed. Each of the losses includes the mean squared error made by the reconstructing and predicting decoders:</p><formula xml:id="formula_10">L * (seg i ) = 1 2 1 T ei t=bi f * t ? f * t 2 2 + 1 P ei+P t=ei+1 f * t ? f * t 2 2<label>(14)</label></formula><p>where P denotes the prediction length and * represents one of l, g or p. In case of p notice that it makes f p t equal to f t of Section 3.1. The prediction loss is truncated if the end of trajectory is reached within the prediction length.</p><p>The three losses contribute to the combined loss by a weighted sum:</p><formula xml:id="formula_11">L(seg i ) = ? g L g (seg i ) + ? l L l (seg i ) + ? p L p (seg i ) (15)</formula><p>where {? g , ? l , ? p } ? 0 are corresponding weights to the losses.</p><p>In training, we minimize the combined loss in Eq. (15) by optimizing the parameters of GRU cells of the RNN networks, message building transformations in Eqs. 4 and 5, and the output MLPs.</p><p>Model regularization When training autoencoder style models for anomaly detection, a major challenge is that even if the model learns to generate normal data perfectly, there is still no guarantee that the model will produce high errors for abnormal sequences <ref type="bibr" target="#b17">[18]</ref>. In training MPED-RNN, we address this challenge by empirically searching for the smallest latent space that still adequately covers the normal patterns so that outliers fall outside the manifold represented by this subspace.</p><p>We implement this intuition by splitting the normal trajectories into training and validation subsets, and use them to regularize the network's hyperparameters that govern the capacity of the model (e.g. number of hidden units). More specifically, we train a high capacity network and record the lowest loss on the validation set. The validation set is also used for early stopping. Then, we train a network with lower capacity and record the lowest loss on the validation set again. We repeat this procedure until we find the network with the smallest capacity that is still within 5% of the initial validation loss attained by the high capacity network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Detecting Video Anomalies</head><p>To estimate the anomaly score of each frame in a video, we follow a four-step algorithm:</p><p>1. Extract segments: With each trajectory, we select the overlapping skeleton segments by using a sliding window of size T and stride s on the trajectory, similar to Eqs. <ref type="bibr" target="#b11">(12)</ref> and (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Estimate segment losses:</head><p>We decompose the segment using Eq. (1) and feed all segment features to the trained MPED-RNN, which outputs the normality loss as in Eq. (15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Gather skeleton anomaly score: To measure the conformity of a sequence to the model given both the past and future context, we propose a voting scheme to gather the losses of related segments into an anomaly score for each skeleton instance:</p><formula xml:id="formula_12">? ft = u?St L p (u) |S t | (16)</formula><p>where S t denotes the set of decoded segments that contain f t from both reconstruction and prediction. For each of those segments u, the corresponding perceptual loss, L p (u), is calculated by Eq. <ref type="figure" target="#fig_1">(14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Calculate frame anomaly score: The anomaly score of a video frame v t is calculated from the score of all skeleton instances appearing in that frame by a max pooling operator:</p><formula xml:id="formula_13">? vt = max (? ft ) ft?Skel(vt)<label>(17)</label></formula><p>where Skel(v t ) stands for the set of skeleton instances appearing in the frame. The choice of max pooling over other aggregation functions is to suppress the influence of normal trajectories present in the scene, since the number of normal trajectories can vary largely in real surveillance videos. We then use ? vt as the frame-level anomaly score of v t and use it to calculate all accuracy measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>To detect skeletons in the videos, we utilized Alpha Pose <ref type="bibr" target="#b8">[9]</ref> to independently detect skeletons in each video frame. To track the skeletons across a video, we combined sparse optical flow with the detected skeletons to assign similarity scores between pairs of skeletons in neighboring frames, and solved the assignment problem using the Hungarian algorithm <ref type="bibr" target="#b15">[16]</ref>. The global and local components of the skeleton trajectories are standardized by subtracting the median of each feature, and scaling each feature relative to the 10%-90% quantile range. All recurrent encoder-decoder networks have similar architectures but are trained with independent weights. The regularization of MPED-RNN's hyperparameters is done for each data set, following the method described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on two datasets for video anomaly detection: ShanghaiTech Campus <ref type="bibr" target="#b19">[20]</ref> and CUHK Avenue <ref type="bibr" target="#b18">[19]</ref>. Each of these datasets has specific characteristics in terms of data source, video quality and types of anomaly. Therefore, we setup customized experiments for each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ShanghaiTech Campus Dataset</head><p>The ShanghaiTech Campus dataset <ref type="bibr" target="#b19">[20]</ref> is considered one of the most comprehensive and realistic datasets for video anomaly detection currently available. It combines footage of 13 different cameras around the ShanghaiTech <ref type="table">Table 1</ref>. Frame-level ROC AUC performance of MPED-RNN and other state-of-the-art methods on the ShanghaiTech dataset and its human-related subset. We use the reported results of the referenced methods on ShanghaiTech and carry out their identical experiments on HR-ShanghaiTech whenever possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HR-ShanghaiTech ShanghaiTech</head><p>Conv-AE <ref type="bibr" target="#b12">[13]</ref> 0.698 0.704 TSC sRNN <ref type="bibr" target="#b19">[20]</ref> N/A 0.680</p><p>Liu et al. <ref type="bibr" target="#b17">[18]</ref> 0.727 0.728 MPED-RNN 0.754 0.734</p><p>University campus with a wide spectrum of anomaly types.</p><p>Because of the sophistication of the anomaly semantics, current methods struggle to get adequate performance on it.</p><p>Most of the anomaly events in the ShanghaiTech dataset are related to humans, which are the target of our method. We left out 6/107 test videos whose abnormal events were not related to humans and kept the other 101 videos as a subset called Human-related (HR) ShanghaiTech. Most of the experiments discussed in this section are conducted on the HR-ShanghaiTech dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparison with Appearance-based Methods</head><p>We train MPED-RNN on all training videos, which is the practice adopted in previous works. <ref type="table">Table 1</ref> compares the frame-level ROC AUC of MPED-RNN against three stateof-the-art methods. We observe that on HR-ShanghaiTech, MPED-RNN outperforms all the compared methods. For completeness, we also evaluate MPED-RNN on the original dataset where non-human related anomalies are present and MPED-RNN still attains the highest frame-level ROC AUC.</p><p>To understand how the detection of anomalies is made by all models, we visually compare in <ref type="figure" target="#fig_1">Figure 4</ref> the map of anomaly scores produced by MPED-RNN to those produced by Conv-AE <ref type="bibr" target="#b12">[13]</ref> and Liu et al. <ref type="bibr" target="#b17">[18]</ref>. As we can observe, our method avoids many irrelevant aspects of the scene since we focus on skeletons. On the other hand, the other two methods try to predict the whole scene and are more susceptible to noise in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Interpreting Open-box MPED-RNN</head><p>For a deeper understanding on how MPED-RNN works under the hood, we visualize the features generated from the global and local predicting decoders, and the predicted skeleton in image space. For comparison, we also draw the corresponding features of the input sequence. <ref type="figure" target="#fig_3">Figure  5</ref> shows two example sequences from the same scene, a and MPED-RNN in jet color map. Higher scores are represented closer to red while lower scores are represented closer to blue. The first row shows the original input frames and subsequent rows show the score map of each method. Since MPED-RNN focuses on skeletons it does not produce any score on background pixels. normal example and an anomalous example. This scene is of the walking area in the campus, where regular activities include people standing and walking casually. In the normal sequence, the predictions follow the input closely in all three domains, which shows that MPED-RNN encodes enough information to predict the normal sequence. On the other hand, the abnormal event contains a person running. Its predicted global bounding box lags behind the input bounding box, indicating that the expected movement is slower than the observed one. The local prediction also struggles to reproduce the running pose and ends up predicting a walking gait remotely mimicking the original poses. <ref type="table">Table 2</ref> reports the results of simplified variants of MPED-RNN. It confirms that RNN is needed for this problem, and when both global and local sub-processes are modeled, message passing between the sub-processes is necessary. It also shows that the dual decoders are valuable for regularizing the model and detecting anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Error Mode Analysis</head><p>Even though MPED-RNN outperforms related methods, it still makes incorrect decisions. To understand the weaknesses of MPED-RNN, we sorted the test sequences by decreasing level of error made by MPED-RNN and looked for <ref type="table">Table 2</ref>. Ablation study about the components of MPED-RNN. We show the frame-level ROC AUC of simpler models that compose MPED-RNN on the HR-ShanghaiTech dataset. AE: Frame-level Autoencoder, ED: Encoder-Decoder, G+L: Global and Local features without message passing. The columns stand for different ways the loss is calculated; Rec: reconstruction only, Pred: prediction ony, Rec+Pred: reconstruction and prediction combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HR-ShanghaiTech</head><p>Rec.</p><p>Pred the root causes of the major mistakes.</p><p>The most prominent source of errors is from the inaccuracy of the skeleton detection and tracking. All of the skeleton detection methods we tried produced inaccurate skeletons in several common difficult cases such as low resolution of human area or unwanted lighting, contrast or shadow. Moreover, when there is occlusion or multiple people crossing each other, the tracking IDs can get lost or swapped and confuse MPED-RNN. <ref type="figure" target="#fig_4">Figure 6</ref>.a shows an example frame containing a badly detected skeleton.</p><p>Apart from the input noise, a small portion of error comes from a more interesting phenomenon when the abnormal action of subjects produce similar skeletons to normal ones. <ref type="figure" target="#fig_4">Figure 6</ref>.b shows the case of a person slowly riding a bicycle with motion and posture similar to walking, which tricks our model into a false negative. This issue is a predicted downside of geometrical skeleton features, where all appearance features have been filtered out. Augmenting the skeleton structure with visual features is a future work towards solving this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CUHK Avenue dataset</head><p>We also tested MPED-RNN on the CUHK Avenue dataset, which is another representative dataset for video anomaly detection. It contains 16 training videos and 21 testing videos captured from a single camera. Based on earlier error analysis on the ShanghaiTech dataset, we understand that the unstable skeleton inputs are the most important source of inaccuracy. To avoid this issue, we manually leave out a set of video frames where the main anomalous event is non-human related, or the person involved is nonvisible (e.g. person out of view throwing an object into the scene), or the main subject cannot be detected and tracked. This selection is detailed in the Appendix A. We called  the remaining dataset HR-Avenue. On HR-Avenue, we achieved a frame-level ROC AUC of 0.863, against 0.862 and 0.848 achieved by Liu et al. <ref type="bibr" target="#b17">[18]</ref> and Conv-AE <ref type="bibr" target="#b12">[13]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>With less than a hundred dimensions per frame on average, equal to a small fraction of the popular visual features for anomaly detection (ResNet features of 2048 <ref type="bibr" target="#b12">[13]</ref>, AlexNet fc7 of 4096 <ref type="bibr" target="#b13">[14]</ref>), skeleton features still provide equal or better performance than current state-of-the-art methods. This revives the hope for using semantic guided stage-by-stage approaches for anomaly detection amid the trend of end-to-end image processing deep networks. It also reflects the current trend of architectures being modular, with multiple independent modules <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>. Apparently, MPED-RNN's performance still depends on the quality of skeleton detection and tracking. This problem becomes more significant in the case of low quality videos. It prevents us from trying our method on UCSD Ped1/Ped2 <ref type="bibr" target="#b20">[21]</ref>, another popular dataset whose video quality is too low to detect skeletons. Furthermore, for the cases where skeletons are unavailable, appearance based features can provide complementary information to help. This opens a promising direction of combining these features in a cascaded model, where they can cover the weaknesses of each other. Our message-passing scheme can naturally be extended to incorporate sub-processes with non-skeleton features.</p><p>Although dynamic movement and posture of single person can reflect the anomalies in most cases, they do not contain information about the interactions between multiple people in the events, and between humans and objects. The global-local decomposition used in our method can be extended to objects by exploring the part-based configuration for each type of them. Towards multi-person/object anomalies, the message passing framework in MPED-RNN is ready to extend support to them, by expanding to interentity messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Through our experiments, we learned that skeleton motion sequences are effective to identify human-related video anomalous events. We observed that the decomposition of the skeleton sequence into global movement and local deformation -combined with our novel message-passing encoder-decoder RNN architecture -appropriately separates anomalous sequences from normal sequences. MPED-RNN is simple, achieves competitive performance and is highly interpretable. Future work includes examining the regularity of inter-human interactions, combining skeleton features with appearance counterparts, and expanding the component based model to non-human objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. HR-ShanghaiTech and HR-Avenue Datasets</head><p>The ShanghaiTech dataset contains anomalies that are not related to humans in 6 out of its 107 test videos. These 6 videos are:</p><p>? Camera 01: Videos 0130, 0135 and 0136;</p><p>? Camera 06: Videos 0144 and 0145;</p><p>? Camera 12: Video 0152.</p><p>HR-ShanghaiTech was assembled by removing those videos from the ShanghaiTech dataset.</p><p>As for the HR-Avenue dataset, since the original Avenue dataset contains only 21 testing videos, we ignored segments of the videos where the anomalies were not detectable by the pose detector we employed or where the anomaly was not related to a human. The ignored segments were:</p><p>? Video 01: Frames 75 to 120, 390 to 436, 864 to 910 and 931 to 1000;</p><p>? Video 02: Frames 272 to 319 and 723 to 763;</p><p>? Video 03: Frames 293 to 340;</p><p>? Video 06: Frames 561 to 624 and 814 to 1006;</p><p>? Video 16: Frames 728 to 739.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Code</head><p>The source code for the models described in this paper is available in the following link: https://github.com/RomeroBarata/skeleton based anomaly detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Global and local decomposition of a skeleton in a frame. Based on the canonical local reference frame defined by the green bounding box, the location vector of the left knee jointft (dashed red) is decomposed into global f g t (dashed blue) and local f l t (dashed green) components. The bounding box's width and height are included as global features and used to normalize local features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Anomaly score map of Conv-AE<ref type="bibr" target="#b12">[13]</ref>, Liu et al.<ref type="bibr" target="#b17">[18]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of the predicted features (red) compared to input features (black) in a sample case for a normal trajectory and an anomalous trajectory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>: Error mode examples. (a) Skeleton detection inaccuracy: bad detection of the person reflected in the glass lead to bad predictions by MPED-RNN. (b) Confusion in feature space: The person riding the bicycle (red) has a moving pattern "similar" to a person walking.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social LSTM: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3319" to="3327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN Encoder-Decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using spatiotemporal autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Shean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Haur</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Symposium on Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward abnormal trajectory and event detection in video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serhan</forename><surname>Co?ar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Donatiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vania</forename><surname>Bogorny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Garate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Otavio Alvares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Br?mond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="683" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation learning of temporal dynamics for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3010" to="3022" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis of skeletal shape trajectories for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amani</forename><surname>Elaoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Barhoumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassen</forename><surname>Drira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ezzeddine</forename><surname>Zagrouba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="138" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2353" to="2362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
	<note>2016. 1, 2.1, 1, 4.1.1, 4, 4.2, 5</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: a space-time mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
		<idno>1955. 3.5</idno>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal modular networks for retrieving complex compositional activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="569" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection -a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
	<note>2018. 1, 2.1, 3.3, 1, 4.1.1, 4, 4.2</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 FPS in MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked RNN framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Viral Bhalodia, and Nuno Vasconcelos. Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to generate longterm future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungryull</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="3560" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Energy-based localized anomaly detection in video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Travers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="641" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Detecting anomalous events in videos by learning deep representations of appearance and motion. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online anomaly detection in crowd scenes via structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="548" to="561" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatio-temporal autoencoder for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on unsupervised outlier detection in high-dimensional numerical data. Statistical Analysis and Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The ASA Data Science Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="363" to="387" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
