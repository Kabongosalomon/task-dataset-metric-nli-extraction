<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Human Pose Estimation via 3D Event Point Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozu</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
							<email>kailun.yang@kit.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
							<email>wangkaiwei@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Human Pose Estimation via 3D Event Point Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human Pose Estimation (HPE) based on RGB images has experienced a rapid development benefiting from deep learning. However, event-based HPE has not been fully studied, which remains great potential for applications in extreme scenes and efficiency-critical conditions. In this paper, we are the first to estimate 2D human pose directly from 3D event point cloud. We propose a novel representation of events, the rasterized event point cloud, aggregating events on the same position of a small time slice. It maintains the 3D features from multiple statistical cues and significantly reduces memory consumption and computation complexity, proved to be efficient in our work. We then leverage the rasterized event point cloud as input to three different backbones, PointNet, DGCNN, and Point Transformer, with two linear layer decoders to predict the location of human keypoints. We find that based on our method, PointNet achieves promising results with much faster speed, whereas Point Transfomer reaches much higher accuracy, even close to previous event-frame-based methods. A comprehensive set of results demonstrates that our proposed method is consistently effective for these 3D backbone models in eventdriven human pose estimation. Our method based on Point-Net with 2048 points input achieves 82.46mm in MPJPE 3D on the DHP19 dataset, while only has a latency of 12.29ms on an NVIDIA Jetson Xavier NX edge computing platform, which is ideally suitable for real-time detection with event cameras. Code is available at https://github.com/ MasterHow/EventPointPose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human Pose Estimation (HPE) aims to predict the keypoints of each person from perceived signals. The predicted results could be used for understanding people in images and videos, with wide potential applications such as action recognition <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b57">58]</ref> and animations <ref type="bibr" target="#b53">[54]</ref>. Predicting 2D human poses <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b55">56]</ref> from RGB images and recovering 3D poses <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref> have developed rapidly in the last years.</p><p>Yet, HPE still meets challenges with the drawbacks of frame-based cameras. The prediction of keypoints in sce-* The first two authors contribute equally to this work. narios with motion blur or high dynamic range will be inaccurate. Event cameras, such as the Dynamic Vision Sensor (DVS) <ref type="bibr" target="#b16">[17]</ref>, with higher dynamic range and larger temporal resolution, can tackle these disadvantages, which could maintain stable output in such extreme scenes. Enrico et al. <ref type="bibr" target="#b7">[8]</ref> first addressed the application of DVS cameras for HPE to promote the development of efficient HPE, and introduced the first DVS dataset for 3D human pose estimation. Due to the high performance of 2D heatmap representation in 2D HPE, a direct way to explore HPE with DVS is to process the events as an event frame or event voxels with Convolutional Neural Networks (CNNs) and predict 2D heatmaps to yield pose estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b43">44]</ref>. Still, the CNN-based methods do not sufficiently consider the asynchronous sparse characteristics of this data format, and the high amount of computation brings high latency, which is limited in real-time processing for DVS (see <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>However, owing to the distinctiveness of events for its multidimensional nature, we aim to explore a new estimation paradigm to unleash the potential of the device, with the characteristic of sparse and low-latency output, for HPE. In 3D point cloud processing, classification <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> and segmentation <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b61">62]</ref> of point clouds have been explored via deep learning. Further, 3D HPE through 3D point clouds has also been studied <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b62">63]</ref>. Nevertheless, predicting human keypoints directly through event streams is scarcely addressed in the state of the art. To fully unfold the potential of efficient HPE with DVS by processing events from event point sets, we treat events as multidimensional data and predict the 2D joints for a single event camera via backbones established for point cloud processing. To our best knowledge, we are the first to explore 2D HPE directly from 3D event point clouds ( <ref type="figure" target="#fig_0">Fig. 1</ref>). Instead of utilizing the raw event data, we propose a rasterized format of event point cloud, aggregating events on the same position of a small time slice to maintain the multidimensional features of events. We then leverage the rasterized event point cloud as input to a 3D learning backbone and decode the output of the backbone. Three different point cloud learning backbones, PointNet <ref type="bibr" target="#b39">[40]</ref>, DGCNN <ref type="bibr" target="#b50">[51]</ref>, and Point Transformer <ref type="bibr" target="#b61">[62]</ref> are employed in our framework, with two linear layer decoders to predict the location of human points. Finally, the 2D predictions are transformed to 3D via triangulation.</p><p>Following this strategy, a comprehensive variety of experiments is performed on the DHP19 dataset <ref type="bibr" target="#b7">[8]</ref>. We further test the HPE models with our own DAVIS camera, which achieve robust results on new, previously unseen domains. Our approach based on PointNet exceeds the method of event-frame-based DHP19 <ref type="bibr" target="#b7">[8]</ref> in MPJPE 3D and a lower latency is reserved, 9.43ms for 1024 points as input.</p><p>On a glance, we deliver the following contributions:</p><p>? We explore the feasibility of estimating human pose from 3D event point clouds directly, which is the first work from this perspective to our best knowledge. ? We demonstrate the effectiveness of well-known Li-DAR point cloud learning backbones for event point cloud based human pose estimation. ? We propose a new event representation-rasterized event point cloud, which maintains the 3D features from multiple statistical cues and significantly reduces memory consumption and computational overhead with the same precision. ? We explore labeling methods for the event point cloud dataset and show that the Mean Label is more effective than the Last Label, considering that the temporal resolution of labels is usually smaller than that of events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Human pose estimation</head><p>In recent years, human pose estimation, which usually includes 2D and 3D HPE, has been an important part of computer vision community, and most of the previous researches are investigated based on RGB frames. In the field of 2D single-person pose estimation, recent mainstream CNN-based methods usually predict heatmaps for joints and select the positions with maximum confidence as the final prediction <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56]</ref> and these kinds of solutions usually outperform counterparts which predict keypoints coordinates directly <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref>. Another group of efforts attempts to investigate HPE with integral regression methods and learnable operations <ref type="bibr" target="#b47">[48]</ref>. Top-down <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b55">56]</ref> and bottomup <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref> are two study perspectives for multi-person pose estimation. 2D HPE could be developed on several datasets, such as MPII <ref type="bibr" target="#b4">[5]</ref>, MSCOCO <ref type="bibr" target="#b33">[34]</ref>, PoseTrack <ref type="bibr" target="#b3">[4]</ref>, CrowdPose <ref type="bibr" target="#b29">[30]</ref>, FLIC <ref type="bibr" target="#b42">[43]</ref>, and LSP <ref type="bibr" target="#b26">[27]</ref>, etc.</p><p>Existing approaches reconstruct the 3D pose from single <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref> or multiple <ref type="bibr" target="#b25">[26]</ref> camera views. Multi-view approaches ease the problem of occlusion and ambiguities and achieve higher accuracy than single-view methods. Datasets such as HumanEva <ref type="bibr" target="#b45">[46]</ref>, Human3.6M <ref type="bibr" target="#b24">[25]</ref>, CMU Panoptic <ref type="bibr" target="#b27">[28]</ref>, and MPI-INF-3DHP <ref type="bibr" target="#b36">[37]</ref> are usually used for 3D researches. For real-time applications, knowledge distillation could be used to obtain small models <ref type="bibr" target="#b58">[59]</ref> and lightweight frameworks for mobile devices could achieve fast speeds <ref type="bibr" target="#b14">[15]</ref>. We do not apply any model compression techniques, and a fair comparison is kept when exploring the new event point cloud based paradigm. Our method falls into the category of multi-view approaches for singleperson HPE, while predicting 2D results directly from 3D event point cloud, which is different from existing works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Event based vision</head><p>Event camera <ref type="bibr" target="#b16">[17]</ref> is a kind of bio-inspired asynchronous sensor responding to changes in brightness on each pixel independently. With high temporal resolution (in the order of ?s) and high dynamic range (over 100dB), it has huge potential in many extreme scenes, and has been proved effective in many applications such as deblurring <ref type="bibr" target="#b46">[47]</ref>, optical flow estimation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39]</ref>, corner tracking <ref type="bibr" target="#b1">[2]</ref>, scene segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>, visual odometry <ref type="bibr" target="#b63">[64]</ref>, human motion capture and shape estimation <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b67">68]</ref>. To leverage the advantages of asynchronous events, different types of events representation are explored for event-based vision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b66">67]</ref>. Most algorithms manage events to event frames to adapt CNN-based methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b65">66]</ref>.</p><p>Responding only to dynamic objects brings fixedposition event camera native advantages in human action recognition, gesture recognition, and gaits identification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref>. Event-based HPE also needs to be explored to deal with extreme scenes and efficiency-critical conditions. Enrico et al. <ref type="bibr" target="#b7">[8]</ref> introduce the first DVS dataset for 3D human pose estimation with event streams, and it is the only event-based human pose dataset recorded in the real world till now. They propose a lightweight but effective CNN model to predict human pose from constant count event frames and obtain 3D results via triangulation. Because of the limited quantity of real event datasets, generating events in simulators becomes an alternative <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>. Alex et al. <ref type="bibr" target="#b64">[65]</ref> and Scarpellini et al. <ref type="bibr" target="#b43">[44]</ref> test their methods on the simulated datasets and Scarpellini et al. <ref type="bibr" target="#b43">[44]</ref> propose a monocular method to predict 3D coordinates. Baldwin et al. <ref type="bibr" target="#b6">[7]</ref> attain a competitive 3D result on the DHP19 dataset via a new representation of events and a process of a 3D estimation model which enhances temporal consistency and enables multi-camera feature fusion. <ref type="bibr" target="#b43">[44]</ref> introduces a monocular method and <ref type="bibr" target="#b6">[7]</ref> relies on a temporal process, and both do not fit in a fair comparison with our method.</p><p>Further, most of these works are mainly conducted with 2D CNNs, while the natural characteristic of events as asynchronous signals with high temporal resolution is undermined. Exploiting the advantage of events through raw event points is also studied in human action recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b49">50]</ref>. They use LiDAR point clouds processing neural networks to process events and achieve great action classification results. Different from the action classifica-tion, human pose estimation via regressing keypoints coordinates, has still not been explored through event point clouds, which is the main contribution of our work. We explore and prove the efficiency of event-based HPE through processing the event point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The pipeline of our proposed method is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The raw 3D event point cloud is first rasterized, and then processed by the point cloud backbone, connected with two linear layers to predict 2D positions of human keypoints. We then describe the methods in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We mainly explore human pose estimation based on the DHP19 dataset <ref type="bibr" target="#b7">[8]</ref>, which is the only dataset with real events for event-based HPE, and it is recorded by four event cameras. In the previous event-frame-based work <ref type="bibr" target="#b7">[8]</ref>, they generated event frames for the four cameras, which are time synchronized, by accumulating 30k events for the four views in total. To obtain the raw event point sets used for our approach, we follow the process of denoising and filtering in DHP19, but we do not accumulate constant count frames, instead we save the raw event point sets, in order to process through point-wise methods. For each event point, it is represented by e = (x, y, t, p), where (x, y) is the pixel location of the event, t represents the timestamp in microsecond resolution, and p is the polarity of the brightness change, 0 for decrease and 1 for increase. However, when we accumulate the events together, some cameras may not be calculated, leading to an empty point cloud or have too few points. It also occurs when generating event frames which correspond to an empty image. We remove the data with points fewer than 1024 when training our model. Same as the raw DHP19, we handle its four views together to generate the shared Mean Label. In DHP19, the streams of events from the four cameras are merged and have an ordered monotonic timestamp, which is convenient for processing data together. When N = 30k events from the four views are accumulated in total, given by E in Eq. 1, one 3D label, gt mean is produced. The label is the mean value of the 3D coordinates for each joint generated in the window of N events, followed by Eq. 2, and is shared by the four cameras.</p><formula xml:id="formula_0">E = {E i | i = 1, 2, . . . N } ,<label>(1)</label></formula><formula xml:id="formula_1">gt mean = Mean (gt Tmin , gt Tmin+dt , . . . gt Tmax ) , (2) T min = arg min T (T ? E 1 (t)) | T ? E 1 (t) , (3) T max = arg min T (E N (t) ? T ) | T ? E N (t) . (4)</formula><p>After obtaining the 3D joints labels, we project them to 2D labels for a single camera view with the projection matrices. We train the human pose estimation model under the dataset with 2D labels for each camera. Main experiments are designed in the Mean Label setting. We also explore another label generation method, naming Last Label, by assigning labels nearest to the last timestamp, and the results of the two kinds of dataset setting will be discussed in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data preprocess</head><p>Rasterized point cloud format of the event. The event camera generates events with microsecond temporal resolution once a pixel brightness change exceeds a threshold. Once all events are fed into the network, the forward propagation will be quite slow, and the memory consumption is also an issue to be considered. We expect a way to retain as much information as possible while significantly reducing the number of events. Here, we propose an event rasterization method. For events between T i and T i+1 , first, we split them into K time slices. Within each small time slice, we condense all events occurring on the same pixel into a new event. Specifically, given all M events on position (x, y) in time slice k, where p i is converted to ?1 for brightness decrease:</p><formula xml:id="formula_2">E k (x, y) = (x, y, t i , p i ), i = 1, .., M,<label>(5)</label></formula><p>we use the following equations to obtain the rasterized event E k (x, y):</p><formula xml:id="formula_3">E k (x, y) = (x, y, t avg , p acc , e cnt ) (6) t avg = 1 M M i t i , p acc = M i p i , e cnt = M,<label>(7)</label></formula><p>With this rasterization, we can reduce the number of events while retaining a considerable amount of event information, and this operation can be also seen as an aggregation of events in the time dimension. We select K=4 in this work, which is appropriate to maintain the time resolution. And t avg in all K slices are normalized to the range of [0, 1]. An example of the proposed event point cloud rasterization is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref> and the result is visualized in <ref type="figure" target="#fig_1">Fig. 2</ref>, where the color represents the value of p acc and the point size for e cnt . Sample points. As mentioned above, event points after being rasterized, are reduced via a kind of sampling, but retain enough event information. Yet, the number of rasterized points is not equal for each event point cloud. Therefore, we need to sample the data to the same numbers. As a preprocess in this task, we consider the random point sampling method better than the furthest point sampling method, mainly considering the speed, where random point sampling is much faster than the furthest point sampling, retaining the advantage of real-time prediction. The number of points input to the model is the main reason impacting the speed of the model, while also affecting the accuracy. In Sec. 4.2, we further explore the effect of the numbers after sampling on the trade-off between efficiency and accuracy in human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Label representation</head><p>By projecting 3D labels to 2D labels for a single camera view with the projection matrices, we have 2D labels shape as (J, 2), where J is the number of joints, 13 for the DHP19 dataset, and (x , y ) corresponds to pixels in the sensor. In previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56]</ref> based on RGB images with a CNN model, it is proved that converting the labels to a 2D heatmap and predicting the heatmap is a better way to locate the joints. To adapt to the task based on event point cloud, we flexibly adopt a new coordinate representation, SimDR <ref type="bibr" target="#b32">[33]</ref>. We convert the 2D labels, (x , y ) into two 1D heat-vectors, p v , correspond to the size of sensor, which are one-hot and further blurred by a Gaussian core, shaped as (H, 1) and (W, 1), respectively:  where v ? {x, y}, S| x = W , and S| y = H. We set ? = 8 in the experiments, as the best choice for the task shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. Then we convert the value in the vectors with Min-Max Normalization, to make the largest value remain 1. The predicted joint position (x,?) is determined by decoding the predicted vectorsp v through an argmax operation to find the position of the maximum value in vectors for the x axis and the y axis respectively:</p><formula xml:id="formula_4">? ? ? ? ? p v = [v 0 , v 1 , . . . , v S ] ? R S , v i = 1 ? 2?? exp ? (i ? v ) 2 2? 2 ,<label>(8)</label></formula><formula xml:id="formula_5">( , , ) E x y t ( , ) k E x y (b) (a) (c) ' 0 ( , ) ( , ,</formula><formula xml:id="formula_6">pred v = arg max j (p v (j)) .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">3D learning model</head><p>Our pipeline is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The event point cloud is first preprocessed into the rasterized point cloud format, and then sampled to a fixed number. The sampled rasterized event point clouds are then encoded by the backbone. We study with three variants of the famous LiDAR point cloud processing backbones, precisely, PointNet <ref type="bibr" target="#b39">[40]</ref>, DGCNN <ref type="bibr" target="#b50">[51]</ref>, and Point Transformer <ref type="bibr" target="#b61">[62]</ref>, to adapt to our mission of event point cloud based human pose estimation. The features output from the backbone are processed by two linear layers to generate the 1D vectors. Then, we decode the 1D vectors for x-axis and y-axis, separately, where we can obtain the predicted result of joints location. All the three backbones can be applied to classification and segmentation in LiDAR point cloud processing, here we only modify and apply the encoder part to our task. And the decoder is replaced by our linear layers. For the Point-Net backbone, we use a 4-layer MLP structure and cascade event point features at multiple scales to aggregate nonlocal information, global features are then obtained by two symmetry functions: max pooling and average pooling. We also remove the joint alignment network <ref type="bibr" target="#b39">[40]</ref> to keep the backbone efficient, considering the natural timing order of event point cloud. As for DGCNN and Point Transformer, we keep its original structure to verify the effectiveness of the method for event point cloud processing. Our methods can be easily deployed and integrated to work with different 3D learning architectures, adapted to event-based HPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment setups</head><p>The DHP19 dataset <ref type="bibr" target="#b7">[8]</ref> contains 17 subjects. We follow the original split setting of the dataset, which uses S1-S12 for training and test on S13-S17. Also, we only use data from the two front camera views, same as DHP19. We train the model using the Adam optimizer <ref type="bibr" target="#b28">[29]</ref> with the Kullback-Leibler divergence loss for 30 epochs with an initial learning rate of 1e ?4 and 1e ?5 for epochs 15 to 20, 1e ?6 for epochs 20 to 30 on a single RTX 3090 GPU. The speed of inference is tested on an NVIDIA Jetson Xavier NX edge computing platform with a batch size of 1, to simulate real time applications. We evaluate the results with the Mean Per Joint Position Error (MPJPE), commonly used in human pose estimation:</p><formula xml:id="formula_7">MPJPE = 1 J J i pred i ? gt i 2<label>(10)</label></formula><p>where pred i and gt i are respectively the prediction and ground truth of the i-th joint among J joints in total for a person. The metric space of 2D error is pixels, and millimeters for 3D error. Our model predicts 2D locations of joints directly. To acquire the 3D locations, we project each of the 2D predictions from the pixel space to the physical space through triangulation <ref type="bibr" target="#b7">[8]</ref>, knowing two projection matrices as well as the positions of the two cameras. We omit the optimization on time continuity with a confidence threshold and only compare the results which are directly output. Adapting the strategy of updating results with the output confidence may further improve the accuracy, which is not discussed in detail in this paper and we leave it for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation studies</head><p>Event point cloud rasterization. To analyze the contribution of the proposed rasterized point cloud format of the event, we conduct ablation studies based on PointNet with 2048 points as input. The results are shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>With the raw data of events, it performs unsatisfactorily because the timestamps are in microseconds, which leads to a large scale difference between timestamps and x-y axes. We then normalize the timestamps into range [0, 1], and the polarity where is 0, is changed to ?1, in which the model could achieve better results. After processing the events data into the rasterized format, the MPJPE can be further decreased, also with more proposed channels, which indicates the effectiveness of our rasterization method. We take all the five-channel representation (x, y, t avg , p acc , e cnt ) as the best choice. In particular, the event point cloud rasterization is used as a preprocess step in this work, while it can be easily employed in the real-time processing, making use of a buffer to refresh the events information in all channels with high speed, where a preset time window length is needed. In this way, it also has the advantage of the high temporal resolution of DVS and can perform efficiently in real-time applications.</p><p>Number of sampling points. The number of points input to the model is the main reason impacting the speed of the model, while also affecting the accuracy. We test different point numbers after sampling. We experiment based on PointNet with the rasterized event point format. The results are shown in <ref type="table">Table 2</ref>. In general, more points lead to higher accuracy and cause a decrease in speed. As a result of adopting the sampling strategy with replacement when the number of points is not enough, the accuracy starts to decline when it reaches 7500 points, with lots of duplicate points in the set. To hold a good balance between speed and accuracy, we chose 2048 points for other ablation studies.</p><p>Label of event point cloud. We study the parameter of ? in Eq. 8 with Mean Label setting, and the results are shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. To this end, we set ? = 8 as the appropriate value. As mentioned in Sec. 3.1, we introduced two kinds of label setting for event point cloud based HPE. Event point data is different from RGB frames as well as the LiDAR point clouds. The latter two can be easily unified in time dimension and assigned an instantaneous label. The high time resolution of event cameras leads them produce too many events in a period, while the 3D joints labels produced by Vicon motion capture system have a much lower time resolution. We process the events in a tiny time window instead of an instant, so how to set the label properly for the task of HPE based on event streams should be considered. Another label setting method, naming Last Label, is explored. We process the four views separately and count 7500 events as a window for each camera. In this way, we assign labels nearest to the last timestamp in the window separately for each camera. We consider this method is closer to real-time applications, that is to say, the model is predicting the results closest to the current state. Yet, this also means that the data we use from four cameras is out of sync, and could not project to 3D space with two views. A schematic diagram of the two label settings is shown in the supplementary material.</p><p>Here, we test the Last Label setting and Mean Label setting through all three backbones, with 2048 points and rasterized event point format and compare MPJPE 2D for them as shown in <ref type="table">Table 3</ref>. The Mean Label performs better for the addressed task. We consider two possible reasons. Although the Last Label setting seems to be more reasonable for real-time perception, the early events are too far from the label in the time dimension and we do not explicitly introduce any modules to capture the long-distance dependency, in order to maintain the simplicity of the overall structure. And the Mean Label could take advantage of the early-and later events. On the other hand, the Mean Label reduces single measurement error in instantaneous label recording, which is more robust for the human pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison on the DHP19 dataset</head><p>Our methods can be easily integrated with and consistently effective for different 3D learning architectures to attain robust event-based human pose estimation. <ref type="table" target="#tab_3">Table 4</ref> shows the results of the proposed method with 2048 points as input versus the 2D event-frame-based methods ( ? indicates our reimplementation). We follow the Simple Baseline <ref type="bibr" target="#b55">[56]</ref> to train the models of Pose-ResNet18 and Pose-ResNet50 with constant count event frames. In addition, we implement a model based on LeViT <ref type="bibr" target="#b22">[23]</ref>, an advanced backbone with faster inference. MobileHumanPose <ref type="bibr" target="#b14">[15]</ref> is also tested as another backbone for 2D prediction with the same framework as ours. CNN models with much more parameters and computation overheads perform better in this scene, which is reasonable. Our PointNet is superior to DHP19 in terms of both accuracy and speed, which is demonstrated to be very effective for the dedicated task. DGCNN and Point Transformer achieve higher accuracy, even close to the results of Pose-ResNet18 and Pose-ResNet50.</p><p>For this movement-driven task demonstrated in DHP19 <ref type="bibr" target="#b7">[8]</ref>, we count every piece of data in the test dataset and set the minimum time span value of about 36ms as the inference latency that the model should achieve to meet the real-time prediction requirement for event cameras in this scene. The latency vs. MPJPE 3D for different models are illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, where our method achieves fast inference with good performance. Our PointNet with 2048  points only has a latency of 12.29ms, which is ideally suitable for efficiency-critical mobile applications. <ref type="figure" target="#fig_5">Fig. 5</ref> visualizes the results of different models. We plot the 2D joints together with skeletons over the constant count event frames, while the results for event point cloud based methods are predicted from point sets. To explore the generalizability of the proposed method, we also test different models in real-world scenarios by capturing event data with a DAVIS camera, which has a resolution of 262?320. Although DHP19 is also recorded by the DAVIS camera, a kind of event camera that transmits DVS events in addition to APS (Active Pixel Sensor) frames, only the DVS output was recorded and the APS output was discarded due to bandwidth constraints. To make use of the point-wise methods, events are scaled to the DHP19 resolutions (260?346), and the results are finally re-scaled to the resolution of the device itself. Results are shown over the APS frames of our DAVIS, with no ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results visualization and real-world estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DHP19 ResNet18</head><p>PointNet1024 PointNet4096  Our method achieves a more successful prediction for our PointNet even with many static limbs. The second row shows the results in 3D (red for ground truth, blue for prediction). The last two rows show the results on our own device, where the DHP19 model fails.</p><p>It can be clearly seen that for the new data with different devices, our approach still delivers impressive results, which reveals that our human pose estimation models generalize well to such unseen domains. However, the real-time CNN method DHP19 fails on the entire sequence, where we use the original open source pre-training model <ref type="bibr" target="#b7">[8]</ref> (see supplementary material for details). Moreover, we find that the DHP19 model fails on the new data despite the conducted denoising and filtering, which may be caused by the small amount of parameters leading to poor generalization to a new device as well as different views. Note that the input to our method is the raw event point cloud, which we believe is important for efficient event HPE, considering that the denoising process will introduce additional computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have explored the potential of efficient Human Pose Estimation (HPE) by point-wise net-works through directly processing the event points. We proposed the rasterized event point cloud, a new event representation, which could maintain the 3D features from multiple statistical cues. Experiments based on three famous point-wise backbones are carried out, attaining different trade-offs between speed and accuracy. In particular, Point Transformer reaches the best performance, while PointNet is the fastest. We have also discussed the label setting for the new perspective of the mission, and the Mean Label is more productive. We expect this work to pave the way for subsequent event point cloud HPE, and we will continue to explore the potential of event point cloud based efficient pipelines in other fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Label setting</head><p>In DHP19 <ref type="bibr" target="#b7">[8]</ref>, the event streams from the four event cameras are merged and have an ordered monotonic timestamp. In <ref type="figure" target="#fig_6">Fig. 6(a)</ref>, an amount of 60 events, 15 for each camera with various colors, in an ordered timestamp are shown schematically. And the time resolution of the output label from Vicon system is much lower than that of the event camera, which is shown as the red star marker on the timestamp axis for illustration. The Mean Label setting is conducted under a fixed number of events for all cameras in total, e.g., 40 events. By counting all events, a shared Mean Label is generated, which is the mean value of all labels in the window, shown in <ref type="figure" target="#fig_6">Fig. 6(b)</ref>. For the Last Label setting, when a fixed number of events is counted for each camera, e.g., 10 events, the label nearest the last event is set as the label for each camera separately, shown in <ref type="figure" target="#fig_6">Fig. 6(c)</ref>.</p><p>As it can be seen, the number of events for different cameras may differ in Mean Label setting. Also, there may be a difference with the time span of events in Last Label setting for different cameras. The Last Label setting leads the model to predict the results closest to the current state, while the Mean Label taking advantage of the early-and later events to predict results corresponding to the interval. We have studied the performance of the two different label settings in the experiment section and the Mean Label performs better for the addressed task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Event point numbers in the dataset</head><p>When generating event point cloud data for the Mean Label setting, events are accumulated together for the four views, and some cameras may not be calculated, leading to an empty point cloud or have too few points. It also occurs when generating event frames which correspond to an empty image. We consider that this is a meaningful finding when using the DHP19 dataset with the Mean Label setting, as it is ignored in frame-based methods where empty images exist in the dataset. Compared with the previous eventframe-based DHP19 <ref type="bibr" target="#b7">[8]</ref>, we remove the data with points fewer than 1024 when training our model, and the testing set is the same for both with all data. Also, we experiment a training with all data for PointNet-2048. It causes a bit accuracy decrease, but still better than DHP19. the result is shown in <ref type="table">Tab</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Time slice K</head><p>We conduct ablations on time slice K (see Tab. A). When K is smaller, the information density of the rasterized event point cloud is higher, but the temporal resolution will be reduced, thus the choice needs to be weighed. We note the HPE task is not sensitive to K. Our model achieves satisfactory performance at K=4, consistent with our setting.</p><p>While the event point cloud rasterization setting works well in our task of HPE, it does not have to be the optimal choice when transferring to other event-driven vision applications. We will further explore in this direction like the setting of K in more event-based learning problems such as optical flow estimation and human pose tracking.  <ref type="figure">Fig. 7</ref> and <ref type="figure">Fig. 8</ref> display more results on the DHP19 test dataset <ref type="bibr" target="#b7">[8]</ref> for different models, which are selected from movements "Punch straight forward left", "Kick forwards left", and " <ref type="figure">Figure-8</ref> left hand". The 2D results show two front views, which are used to project the predictions from the pixel space to the physical space through triangulation. We find that our event point cloud based method is more robust when facing static limbs than the DHP19 model. Static limbs during the movement, such as legs in movement " <ref type="figure">Figure-8</ref> left hand", generate few events which lead to invisible parts when accumulating events to a constant count event frame. However, such few events are retained in the event point cloud, and they could be processed by the point-wise backbone more effectively, which we think is the main reason for the superiority of our method in predicting keypoints for static limbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More visualizations</head><p>Also, in the unseen domains, such as different views and noise with our own device, the event point cloud with a multidimensional feature outperforms the event-framebased DHP19 and is robust to the noise brought by the background as well as the device itself. A video is submitted as a supplementary with more continuous visualizations, which further demonstrates the high efficiency of our event point cloud based human pose estimation framework.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) 2D event frame based human pose estimation paradigm vs. the proposed novel 3D event point cloud based paradigm, (b) Latency vs. Mean Per Joint Position Error with a logarithmic x-axis of 2D CNN backbones (square markers) and our 3D proposed pipeline (circular markers). *The real-time criterion is statistically obtained on the DHP19 test dataset<ref type="bibr" target="#b7">[8]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The proposed pipeline. The raw 3D event point cloud is first rasterized, and then processed by the point cloud backbone. The features output from the backbone are then connected to linear layers to predict two vectors. 2D positions of human keypoints are proposed via decoding the two vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Schematic diagram of event point cloud rasterization. (a) Raw 3D event point cloud input, (b) Time slice of event point cloud, (c) Rasterized event point cloud at (x, y) position. Note that the rasterization process preserves the discrete nature of the point cloud, rather than the 2D image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Parameter analysis of ? values of label Gaussian distribution on human pose estimation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Results visualization for different models. The first row shows the results in 2D (yellow for ground truth, blue for prediction).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Schematic diagram of label setting for event point cloud based human pose estimation. (a) The event point cloud output frequency of all four event cameras and the acquisition frequency of the motion capture device Vicon. (b) Determine the shared mean label for event camera 2 and event camera 3. (c) Determine the last label for event camera 2 and event camera 3, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>2D results visualization with two front views on the DHP19 test dataset for different models (yellow for ground truth, blue for prediction). The three movements from top to bottom are "Punch straight forward left", "Kick forwards left", and "Figure-8 left hand". 3D results visualization on the DHP19 test dataset for different models (red for ground truth, blue for prediction). The three movements from top to bottom are "Punch straight forward left", "Kick forwards left", and "Figure-8 left hand".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Event point cloud rasterization ablations.</figDesc><table><row><cell>Input</cell><cell>Channel</cell><cell cols="2">MPJPE 2D MPJPE 3D</cell></row><row><cell>Raw</cell><cell>x, y, t</cell><cell>24.75</cell><cell>310.65</cell></row><row><cell></cell><cell>x, y, t, p</cell><cell>24.74</cell><cell>310.64</cell></row><row><cell>Normalized</cell><cell>x, y, t norm</cell><cell>7.92</cell><cell>89.62</cell></row><row><cell></cell><cell>x, y, t norm , p ?1</cell><cell>7.61</cell><cell>86.07</cell></row><row><cell></cell><cell>x, y, t avg</cell><cell>7.77</cell><cell>87.59</cell></row><row><cell>Rasterized</cell><cell>x, y, t avg , p acc</cell><cell>7.40</cell><cell>84.58</cell></row><row><cell></cell><cell>x, y, t avg , p acc , e cnt</cell><cell>7.29</cell><cell>82.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Ablations on the number of sampling points. Comparison of different dataset labels.</figDesc><table><row><cell cols="4">Sampling Number MPJPE 2D MPJPE 3D Latency (ms)</cell></row><row><cell>1024</cell><cell>7.49</cell><cell>85.14</cell><cell>9.43</cell></row><row><cell>2048</cell><cell>7.29</cell><cell>82.46</cell><cell>12.29</cell></row><row><cell>4096</cell><cell>7.21</cell><cell>81.42</cell><cell>18.80</cell></row><row><cell>7500</cell><cell>7.24</cell><cell>81.72</cell><cell>29.18</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">MPJPE 2D</cell></row><row><cell></cell><cell></cell><cell>Last Label</cell><cell>Mean Label</cell></row><row><cell>PointNet [40]</cell><cell></cell><cell>7.50</cell><cell>7.29</cell></row><row><cell>DGCNN [51]</cell><cell></cell><cell>6.96</cell><cell>6.83</cell></row><row><cell cols="2">Point Transformer [62]</cell><cell>6.74</cell><cell>6.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>3D human pose estimation on the DHP19 dataset.</figDesc><table><row><cell>Input</cell><cell>Method</cell><cell cols="5">MPJPE 2D MPJPE 3D #Params (M) #MACs (G) Latency (ms)</cell></row><row><cell></cell><cell>Pose-ResNet18 ? [56]</cell><cell>5.37</cell><cell>61.03</cell><cell>15.4</cell><cell>8.30</cell><cell>68.07</cell></row><row><cell></cell><cell>Pose-ResNet50 ? [56]</cell><cell>5.28</cell><cell>59.83</cell><cell>34.0</cell><cell>12.91</cell><cell>93.56</cell></row><row><cell>2D Event Frames</cell><cell>MobileHP-S ? [15]</cell><cell>5.65</cell><cell>64.14</cell><cell>1.83</cell><cell>0.69</cell><cell>48.09</cell></row><row><cell></cell><cell>LeViT-128S ? [23]</cell><cell>7.68</cell><cell>87.79</cell><cell>7.87</cell><cell>0.20</cell><cell>14.40</cell></row><row><cell></cell><cell>DHP19 [8]</cell><cell>7.67</cell><cell>87.90</cell><cell>0.22</cell><cell>3.51</cell><cell>27.55</cell></row><row><cell></cell><cell>PointNet [40]</cell><cell>7.29</cell><cell>82.46</cell><cell>4.46</cell><cell>1.19</cell><cell>12.29</cell></row><row><cell>3D Event Point Cloud</cell><cell>DGCNN [51]</cell><cell>6.83</cell><cell>77.32</cell><cell>4.51</cell><cell>4.91</cell><cell>127.96</cell></row><row><cell></cell><cell>Point Transformer [62]</cell><cell>6.46</cell><cell>73.37</cell><cell>3.65</cell><cell>5.03</cell><cell>497.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>last label &amp; mean label</figDesc><table><row><cell></cell><cell></cell><cell>Cam 0 events</cell><cell>Cam 1 events</cell></row><row><cell></cell><cell>Cam 3 events</cell><cell>Cam 2 events</cell><cell>Vicon stamp DVS stamp</cell></row><row><cell>(a)</cell><cell>All</cell><cell></cell><cell>timestamp</cell><cell>Cam 0 events</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cam 1 events</cell></row><row><cell></cell><cell>Cam 2</cell><cell></cell><cell>timestamp</cell><cell>Cam 2 events</cell></row><row><cell>(b)</cell><cell>Cam 3</cell><cell></cell><cell>timestamp</cell><cell>Cam 3 events</cell></row><row><cell></cell><cell cols="2">Shared Mean Label</cell><cell></cell><cell>Vicon stamp</cell></row><row><cell>(c)</cell><cell>Cam 2 Cam 3</cell><cell></cell><cell>Last Label for Cam 2 timestamp timestamp Last Label for Cam 3</cell><cell>DVS stamp Final label</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic segmentation for event-based cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ev-Segnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Asynchronous multihypothesis tracking of features with event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Alzugaray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margarita</forename><surname>Chli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A low power, fully event-based gesture recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnon</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Melano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmelo</forename><surname>Di Nolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapan</forename><forename type="middle">K</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Andreopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Kusnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Debole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbr?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myron</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmendra</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic vision sensors for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><forename type="middle">Anna</forename><surname>Baby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bimal</forename><surname>Vinod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Chinni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Time-ordered recent event (TORE) volumes for event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Mutlaq Almatrafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keigo</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirakawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DHP19: Dynamic vision sensor 3D human pose dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Taverni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Awai Easthope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Skriabine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Corradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Longinotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kynan</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A 3-D-point-cloud system for human-pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Chi</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Kok</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>George Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D human pose estimation = 2D pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for event-camera based gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCAS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HigherHRNet: Scaleaware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mobile-HumanPose: Toward real-time 3D human pose estimation in mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangbum</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokeon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbr?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<title level="m">J?rg Conradt, Kostas Daniilidis, and Davide Scaramuzza. Event-based vision: A survey. TPAMI, 2022</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A unifying contrast maximization framework for event cameras, with applications to motion, depth, and optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video to events: Recycling video datasets for event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Hidalgo-Carri?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end learning of representations for asynchronous event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Derpanis, and Davide Scaramuzza</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">E-RAFT: Dense optical flow from event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Millh?usler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bottom-up human pose estimation via disentangled keypoint regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zigang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LeViT: a vision transformer in ConvNet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">v2e: From video frames to realistic DVS events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">Scott</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CrowdPose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pose recognition with cascade transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03332,2021.4</idno>
		<title level="m">Is 2D heatmap representation even necessary for human pose estimation? arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Event-based vision meets deep learning on steering prediction for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">I</forename><surname>Maqueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narciso</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Single image optical flow estimation with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ESIM: an open event camera simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MODEC: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lifting monocular events to 3D human poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Scarpellini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio Del</forename><surname>Bue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Event-based fusion for motion deblurring with cross-modal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2022</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Space-time event clouds for gesture recognition: From RGB cameras to event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yexin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Event-stream representation for human gaits identification using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrong</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<editor>Lizhen Cui Cui Lizhen, and Hongkai Wen</editor>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pose2Pose: Pose selection and transfer for 2D character animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hijung</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Valentina Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilmot</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IUI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multipath event-based network for low-power human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WF-IoT</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">EventCap: Monocular 3D capture of high-speed human motions using an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fast human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">IS-SAFE: Improving semantic segmentation in accidents by fusing event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Exploring event-driven dynamic context for accident scene segmentation. T-ITS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Point transformer. In ICCV, 2021. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulmotaleb</forename><forename type="middle">El</forename><surname>Saddik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Event-based stereo visual odometry. T-RO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">EventGAN: Leveraging large scale image datasets for event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaung</forename><surname>Khant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno>ICCP, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">EV-FlowNet: Self-supervised optical flow estimation for event-based cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In RSS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unsupervised event-based learning of optical flow, depth, and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">EventHPE: Event-based 3D human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
