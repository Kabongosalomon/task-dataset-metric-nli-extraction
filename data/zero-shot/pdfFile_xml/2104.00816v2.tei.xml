<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Partition-Guided GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Armandpour</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
							<email>asadeghian@ufl.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
							<email>mzhou@utexas.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Partition-Guided GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the success of Generative Adversarial Networks (GANs), their training suffers from several well-known problems, including mode collapse and difficulties learning a disconnected set of manifolds. In this paper, we break down learning complex high dimensional distributions to simpler sub-tasks, supporting diverse data samples. Our solution relies on designing a partitioner that breaks the space into smaller regions, each having a simpler distribution, and training a different generator for each partition. This is done in an unsupervised manner without requiring any labels.</p><p>We formulate two desired criteria for the space partitioner that aid the training of our mixture of generators: 1) to produce connected partitions and 2) provide a proxy of distance between partitions and data samples, along with a direction for reducing that distance. These criteria are developed to avoid producing samples from places with nonexistent data density, and also facilitate training by providing additional direction to the generators. We develop theoretical constraints for a space partitioner to satisfy the above criteria. Guided by our theoretical analysis, we design an effective neural architecture for the space partitioner that empirically assures these conditions. Experimental results on various standard benchmarks show that the proposed unsupervised model outperforms several recent methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b20">[20]</ref> have gained remarkable success in learning the underlying distribution of observed samples. However, their training is still unstable and challenging, especially when the data distribution of interest is multimodal. This is particularly important due to both empirical and theoretical evidence that suggests real data also conforms to such distributions <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b72">72]</ref>.</p><p>Improving the vanilla GAN, both in terms of training stability and generating high fidelity images, has been the * Authors contributed equally.</p><p>Real Generated subject of great interest in the machine learning literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b80">80]</ref>. One of the main problems is mode collapse, where the generator fails to capture the full diversity of the data. Another problem, which hasn't been fully explored, is the mode connecting problem <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b71">71]</ref>. As we explain in detail in Section 2, this phenomenon occurs when the GAN generates samples from parts of the space where the true data is non-existent, caused by using a continuous generator to approximate a distribution with disconnected support. Moreover, GANs are also known to be hard to train due to the unreliable gradient provided by the discriminator. Our solution to alleviate the aforementioned problems is introducing an unsupervised space partitioner and training a different generator for each partition. <ref type="figure">Figure 1</ref> illustrates real and generated samples from several inferred partitions.</p><p>Having multiple generators, which are focused on dif-ferent parts/modes of the distribution, reduces the chances of missing a mode. This also mitigates mode connecting because the mixture of generators is no longer restricted to be a continuous function responsible for generating from a data distribution with potentially disconnected manifolds. In this context, an effective space partitioner should place disconnected data manifolds in different partitions. Therefore, assuming semantically similar images are in the same connected manifold, we use contrastive learning methods to learn semantic representations of images and partition the space using these embeddings. We show that the space partitioner can be utilized to define a distance between points in the data space and partitions. The gradient of this distance can be used to encourage each generator to focus on its corresponding region by providing a direction to guide it there. In other words, by penalizing a generator when its samples are far from its partition, the space partitioner can guide the generator to its designated region. Our partitioner's guide is particularly useful where the discriminator does not provide a reliable gradient, as it can steer the generator in the right direction.</p><p>However, for a reliable guide, the distance function must follow certain characteristics, which are challenging to achieve. For example, to avoid misleading the GANs' training, the distance should have no local optima outside the partition. In Section 4.2, we formulate sufficient theoretical conditions for a desirable metric and attain them by enforcing constraints on the architecture of the space partitioner. This also guarantees connected partitions in the data space, which further mitigates mode connecting as a by-product.</p><p>We perform comprehensive experiments on StackedM-NIST <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b70">70]</ref>, CIFAR-10 <ref type="bibr" target="#b39">[39]</ref>, STL-10 <ref type="bibr" target="#b11">[12]</ref> and Ima-geNet <ref type="bibr" target="#b63">[63]</ref> without revealing the class labels to our model. We show that our method, Partition-Guided Mixture of GAN (PGMGAN), successfully recovers all the modes and achieves higher Inception Score (IS) <ref type="bibr" target="#b67">[67]</ref> and Frechet Inception Distance (FID) <ref type="bibr" target="#b26">[26]</ref> than a wide range of supervised and unsupervised methods.</p><p>Our contributions can be summarized as:</p><p>? Providing a theoretical lower bound on the total variational distance of the true and estimated densities using a single generator.</p><p>? Introducing a novel differentiable space partitioner and demonstrating that simply training a mixture of generators on its partitions alleviates mode collapse/connecting.</p><p>? Providing a practical way (with theoretical guarantees) to guide each generator to produce samples from its designated region, further improving mode collapse/connecting. Our experiments show significant improvement over relevant baselines in terms of FID and IS, confirming the efficacy of our model.</p><p>? Elaborating on the design of our loss and architecture by making connection to supervised GANs that employ a classifier. We explain how PGMGAN avoids their shortcomings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mode connecting problem</head><p>Suppose the data distribution is supported on a set of disconnected manifolds embedded within a higher-dimensional space. Since continuous functions preserve the space connectivity <ref type="bibr" target="#b35">[35]</ref>, one can never expect to have an exact approximation of this distribution by applying a continuous function (G ? ) to a random variable with a connected support. Furthermore, if we restrict G ? to the class of c-Lipschitz functions, the distance between the true density and approximated will always remain more than a certain positive value. In fact, the generator would either have to discard some of the data manifolds or connect the manifolds. The former can be considered a form of mode collapse, and we refer to the latter as the mode connecting problem.</p><p>The following theorem formally describes the above statement and provides a lower bound for the total variation distance between the true and estimated densities. Theorem 1. Let p data be a distribution supported on a set of disjoint manifolds M 1 , . . . , M k in R d , and [? 1 , . . . , ? k ] be the probabilities of being from each manifold. Let G ? be a c-Lipschitz function, and p model be the distribution of G ? (z), where z ? N (0, I n ), then:</p><formula xml:id="formula_0">d T V (p data , p model ) ? |? i ? p i |? ?</formula><p>where d T V is the total variation distance and:</p><formula xml:id="formula_1">? * i := min(? i , 1 ? ? i ) p i := p model (M i ) ? := max i {? * i ? ?(? ?1 (? * i ) ? d i /c)} d i := inf{||x ? y|| | x ? M i , y ? M j , j = i}</formula><p>d i is the distance of manifold M i from the rest, and ? is the CDF of the univariate standard normal distribution. Note ? is strictly larger than zero iff ?i : d i , ? * i = 0.</p><p>According to Theorem 1, the distance between the estimated density and the data distribution can not converge to zero when G ? is a Lipschitz function. It is worth noting that this assumption holds in practice for most neural architectures as they are a composition of simple Lipschitz functions. Furthermore, most of the state-of-the-art GAN architectures (e.g., BigGAN <ref type="bibr" target="#b4">[5]</ref> or SAGAN <ref type="bibr" target="#b82">[82]</ref>) use spectral normalization in their generator to stabilize their training, which promotes Lipschitzness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related work</head><p>Apart from their application in computer vision <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b83">83,</ref><ref type="bibr" target="#b86">86]</ref>, GANs have also been been employed in natural language processing <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b81">81]</ref>, medicine <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b37">37]</ref> and several other fields <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b60">60]</ref>. Many of recent research have accordingly focused on providing ways to avoid the problems discussed in Section 2 <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b53">53]</ref>.</p><p>Mode collapse For instance, Metz et al. <ref type="bibr" target="#b53">[53]</ref> unroll the optimization of the discriminator to obtain a better estimate of the optimal discriminator at each step, which remedies mode collapse. However, due to high computational complexity, it is not scalable to large datasets. VEEGAN <ref type="bibr" target="#b70">[70]</ref> adds a reconstruction term to bi-directional GANs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref> objective, which does not depend on the discriminator. This term can provide a training signal to the generator even when the discriminator does not. PacGAN <ref type="bibr" target="#b45">[45]</ref> changes the discriminator to make decisions based on a pack of samples. This change mitigates mode collapse by making it easier for the discriminator to detect lack of diversity and naturally penalize the generator when mode collapse happens. Lucic et al. <ref type="bibr" target="#b49">[49]</ref>, motivated by the better performance of supervised-GANs, propose using a small set of labels and a semi-supervised method to infer the labels for the entire data. They further improve the performance by utilizing an auxiliary rotation loss similar to that of RotNet <ref type="bibr" target="#b17">[17]</ref>.</p><p>Mode connecting Based on Theorem 1, to avoid mode connecting one has to either use a latent variable z with a disconnected support, or allow G ? to be a discontinuous function <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b65">65]</ref>.</p><p>To obtain a disconnected latent space, DeLiGAN <ref type="bibr" target="#b22">[22]</ref> samples z from a mixture of Gaussian, while Odena et al. <ref type="bibr" target="#b59">[59]</ref> add a discreet dimension to the latent variable. Other methods dissect the latent space post-training using some variant of rejection sampling, for example, Azadi et al. <ref type="bibr" target="#b1">[2]</ref> perform rejection sampling based on the discriminator's score, and Tanielian et al. <ref type="bibr" target="#b71">[71]</ref> reject the samples where the generator's Jacobian is higher than a certain threshold.</p><p>The discontinuous generator method is mostly achieved by learning multiple generators, with the primary motivation being to remedy mode-collapse, which also reduces mode connecting. Both MGAN <ref type="bibr" target="#b27">[27]</ref> and DMWGAN <ref type="bibr" target="#b36">[36]</ref> employ K different generators while penalizing them from overlapping with each other. However, these works do not explicitly address the issue when some of the data modes are not being captured. Also, as shown in Liu et al. <ref type="bibr" target="#b46">[46]</ref>, MGAN is quite sensitive to the choice of K. By contrast, Self-Conditioned GAN <ref type="bibr" target="#b46">[46]</ref> clusters the space using the discriminator's final layer and uses the labels as self-supervised conditions. However, in practice, their clustering does not seem to be reliable (e.g., in terms of NMI for labeled datasets), and the features highly depend on the choice of the discriminator's architecture. In addition, there is no guarantee that the generators will be guided to generate from their assigned clusters. GAN-Tree <ref type="bibr" target="#b40">[40]</ref> uses hierarchical clustering to address continuous multi-modal data, with the number of parameters increasing linearly with the number of clusters. Thus it is limited to very few cluster numbers (e.g., 5) and can only capture a few modes.</p><p>Another recently expanding direction explores the benefit of using image augmentation techniques for generative modeling. Some works simply augment the data using various perturbations (e.g., random crop, horizontal flipping) <ref type="bibr" target="#b34">[34]</ref>. Others <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b85">85]</ref> incorporated regularization on top of the augmentations, for example CRGAN <ref type="bibr" target="#b84">[84]</ref> enforces consistency for different image perturbations. ADA <ref type="bibr" target="#b32">[32]</ref> processes each image using non-leaking augmentations and adaptively tunes the augmentation strength while training. These works are orthogonal to ours and can be combined with our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>This section first describes how GANs are trained on a partitioned space using a mixture of generators/discriminators and the unified objective function required for this goal. We then explain our differentiable space partitioner and how we guide the generators towards the right region. We conclude the section by making connections to supervised GANs, which use an auxiliary classifier <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b59">59]</ref>.</p><p>Multi-generator/discriminator objective: Given a partitioning of the space, we train a generator (G i ) and a discriminator (D i ) for each region. To avoid over-parameterization and allow information sharing across different regions, we employ parameter sharing across different G i (D i ) by tying their parameters except the input (last) layer. The mixture of these generators serves as our main generator G. We use the following objective function to train our GANs:</p><formula xml:id="formula_2">k i ? i min Gi max Di V (D i , G i , A i )<label>(1)</label></formula><p>where A 1 , A 2 , ..., A k is a partitioning of the space, ? i := p data (x ? A i ) and:</p><formula xml:id="formula_3">V (D, G, A) = E x?pdata(x|x?A) [log D(x)] + E z?pz(z|G(z)?A) [log(1 ? D(G(z)))] (2)</formula><p>We motivate this objective by making connection to the Jensen-Shannon distance (JSD) between the distribution of our mixture of generators and the data distribution in the following Theorem.</p><formula xml:id="formula_4">Theorem 2. Let P = k i ? i p i , Q = k i ? i q i , and A 1 , A 2 , .</formula><p>.., A K be a partitioning of the space, such that the support of each distribution p i and q i is A i . Then:</p><formula xml:id="formula_5">JSD(P Q) = i ? i JSD(p i q i )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Partition GAN</head><p>Space Partitioner: Based on Theorem 1, an ideal space partitioner should place disjoint data manifolds in different partitions to avoid mode connecting (and consequently mode collapse). It is also reasonable to assume that semantically similar data points lay on the same manifold. Hence, we train our space partitioner using semantic embeddings.</p><p>We achieve this goal in two steps: 1) Learning an unsupervised representation for each data point, which is invariant to transformations that do not change the semantic meaning. 2) Training a partitioner based on these features, where data points with similar embeddings are placed in the same partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning representations:</head><p>We follow the self-supervised literature <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">24]</ref> to construct image representations.</p><p>These methods generally train the networks via maximizing the agreement between augmented views (e.g., random crop, color distortion, rotation, etc.) of the same scene, while minimizing the agreement of views from different scenes. To that end, they optimize the following contrastive loss:</p><formula xml:id="formula_6">(i,j)?P log exp(sim(h i , h j )/? ) 2N k=1 1 k =i exp(sim(h i , h k )/? )<label>(4)</label></formula><p>where h is the embedding for image x, (i, j) is a positive pair (i.e., two views of the same image) and (i, k) refers to negative pairs related to two different images. We refer to this network as pretext, implying the task being solved is not of real interest but is solved only for the true purpose of learning a suitable data representation.</p><p>Learning partitions: To perform the partitioning step, one can directly apply K-means on these semantic representations. However, this may result in degenerated clusters where one partition contains most of the data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b75">75]</ref>. Inspired by Van Gansbek et al. <ref type="bibr" target="#b75">[75]</ref>, to mitigate this challenge, we first make a k-nearest neighbor graph G = (V, E) based on the representations h of the data points. We then train an unsupervised model that motivates connected points to reside in the same cluster and disconnected points to reside in distinct clusters. More specifically, we train a space partitioner S : R d ? [0, 1] k to maximize:</p><formula xml:id="formula_7">(i,j)?E log S(x i ) T ? S(x j ) ? ? i?V H C (S(x i ))) + ?H C ( i?V S(x i ) N )<label>(5)</label></formula><p>where H C (.) is the entropy function of the categorical distribution based on its probability vector. The first term in <ref type="figure" target="#fig_4">Equation 5</ref> motivates the neighbors in G to have similar class probability vectors, with the log function used to significantly penalize the classifier if it assigns dissimilar probability vector to two neighboring points. The last term is designed to avoid placing all the data points in the same category by motivating the average cluster probability vector to be similar to the uniform distribution. The middle term is intended to promote the probability vector for each data point to significantly favor one class over the other. This way, we can be more confident about the cluster id of each data point. Furthermore, if the average probability of classes has a homogeneous mean (because of the last term), we can expect the number of data points in each class to not degenerate.</p><p>To train S efficiently, both in term of accuracy and computational complexity, we initialize S using the already trained network of unsupervised features. More specifically, for:</p><formula xml:id="formula_8">h = W pretext 2 ?(W pretext 1 ? 0 (x))</formula><p>we initialize S as follows:</p><formula xml:id="formula_9">S init (x) = softmax(W partitioner 0 ? 0 (x))</formula><p>where ? is an activation function, and W partitioner 0 is a randomly initialized matrix; we ignore the bias term here for brevity. We drop the sub index 0, from W partitioner 0 and ? 0 to refer to their post-training versions. Given a fully trained S, each point x is assigned to partition A i , based on the argmax of the probability vector of S(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Partition-Guided GAN</head><p>In this section we describe the design of guide and its properties. As stated previously, we want to guide each generator G i to its designated region A i by penalizing it the farther its current generated samples are from A i .</p><p>A simple, yet effective proxy of measuring this distance can be attained using the already trained space partitioner. Let f i s denote the partitioner's last layer logits, expressed as</p><formula xml:id="formula_10">[f 1 (x), . . . , f k (x)] T := W partitioner ?(x).</formula><p>and define the desired distance as:</p><formula xml:id="formula_11">R i (x) := c (f c (x) ? f i (x)) +<label>(6)</label></formula><p>Property 1. It is easy to show that for any generated sample x, R i (x) achieves a larger value, the less likely S believes x to be from partition A i . This is clear from how we defined R i , the more probability mass S(x) assigns any class c = i, the larger the value of R i (x).</p><p>Property 2. It is also straightforward to see that R i (x) is always non-negative and obtains its minimum (zero) only on the A i th partition:</p><formula xml:id="formula_12">x ? A i ?? f i (x) ? f c (x); ?c ? [1 : k] ?? R i (x) = c (f c (x) ? f i (x)) + = 0 (7)</formula><p>Therefore, we guide each generator G i to produce samples from its region by adding a penalization term to its canonical objective function:</p><formula xml:id="formula_13">min Gi n j=1 log(1 ? D i (G i (z (j) )))) +? j R i (G i (z (j) ))/n.<label>(8)</label></formula><p>Intuitionally, G i needs to move its samples towards partition A i in order to minimize the newly added term. Fortunately, given the differentiability of R i (.) with respect to its inputs and Property 1, R i can provide the direction for G i to achieve that goal.</p><p>It is also worth noting that R i should not interfere with the generator/discriminator as long as G i 's samples are within A i . Otherwise, this may lead to the second term favoring parts of A i over others and conflicting with the discriminator. Property 2 assures that learning the distribution of p data over A i remains the responsibility of D i . We also use this metric to ensure each trained generator G i only draws samples from within its region by only accepting samples with R i (x) being equal to zero.</p><p>A critical point left to consider is the possibility of G i getting fooled to generate samples from outside A i , by falling in local optima of R i (x). In the remaining part of this section, we will explain how the architecture design of the space partitioner S avoids this issue. In addition, it will also guarantee the norm of the gradient provided by R i to always be above a certain threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avoiding local optima:</head><p>We can easily obtain a guide R i with no local optima, if achieving a good performance for the partitioner was not important. For instance, a simple singlelinear-layer neural network as S would do the trick. The main challenge comes from the fact that we need to perform well on partitioning 1 , which usually requires deep neural networks while avoiding local optima. We fulfill this goal by first finding a sufficient condition to have no local optima and then trying to enforce that condition by modifying the ResNet <ref type="bibr" target="#b25">[25]</ref> architecture.</p><p>The following theorem states the sufficient condition:</p><formula xml:id="formula_14">Theorem 3. Let ?(x) : R d ? R d be a C 1 (differentiable with continuous derivative) function, W partitioner ? R k?d ,</formula><p>and R i as defined in Eq 6. If there exists c 0 &gt; 0, such that:</p><formula xml:id="formula_15">? x, y ? R d , c 0 ||x ? y||? ||?(x) ? ?(y)||,</formula><p>then for every i ? [1 : k], every local optima of R i is a global optima, and there exists a positive constant b 0 &gt; 0 such that: where</p><formula xml:id="formula_16">?x ? R d \ A i , b 0 ? ||?R i (x)||</formula><formula xml:id="formula_17">A i = {x|x ? R d , R i (x) = 0}. Furthermore A i is a connected set for all i's.</formula><p>The proof is provided in the Appendix. Next we describe how to satisfy this constraint in practice.</p><p>Motivated by the work of Behrmann et al. <ref type="bibr" target="#b2">[3]</ref> who design an invertible network without significantly sacrificing their classification accuracy, we implement ? by stacking several residual blocks,</p><formula xml:id="formula_18">?(x) = B T ? B T ?1 ? ? ? ? ? B 1 (x), where: B t+1 (x (t) ) = x (t+1) := x (t) + ? t (x (t) )</formula><p>and x (t) refers to the out of the t th residual block. <ref type="figure" target="#fig_1">Figure 2</ref>, gives an overview of the proposed architecture.</p><p>We model each ? t as a series of m convolutional layers, each having spectral norm L &lt; 1 intertwined by 1-Lipschitz activation functions (e.g., ELU, ReLU, LeakyReLU). Thus it can be easily shown for all x (t) , y (t) ? R d :</p><formula xml:id="formula_19">(1 ? L m )||x (t) ? y (t) ||? ||B t (x (t) ) ? B t (y (t) )||.</formula><p>This immediately results in the condition required in Theorem 3 by letting c 0 := (1 ? L m ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Connection to supervised GANs</head><p>In this section, we make a connection between our unsupervised GAN and some important work in the supervised regime. This will help provide better insight into why the mentioned properties of guide are important. Auxiliary Classifier GAN <ref type="bibr" target="#b59">[59]</ref> has been one of the well-known supervised GAN methods which uses the following objective function:</p><formula xml:id="formula_20">min G,C max D L AC (G, D, C) = E X?PX [log D(X)] + E Z?PZ ,Y ?PY [log(1 ? D(G(Z, Y )))] a ? ? c E (X,Y )?PXY [log C(X, Y )] b ?? c E Z?PZ ,Y ?PY [log(C(G(Z, Y ), Y ))]</formula><p>c It simultaneously learns an auxiliary classifier C as well as D/G. Other works have also tried fine-tuning the generator using a pre-trained classifier <ref type="bibr" target="#b56">[56]</ref>. The term a is related to the typical supervised conditional GAN, term b motivates the classifier to better classify the real data. The term c encourages G to generate images for each class such that the classifier considers them to belong to that class with a high probability.</p><p>The authors motivate adding this term as it can provide further gradient to the generator G(?|Y ) to generate samples from the correct region P X (?|Y ). However, recent works <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b69">69]</ref> show this tends to motivate G to down-sample data points near the decision boundary of the classifier. It has also been shown to reduce sample diversity and does not behave well when the classes share overlapping regions <ref type="bibr" target="#b18">[18]</ref>.</p><p>Our space partitioner acts similar to the classifier in these GANs, with the term c sharing some similarity with our proposed guide. In contrast, our novel design of R i (?) enjoys the benefits of the classifier based methods (providing gradient for the generator) but alleviates its problems. Mainly because 1) It provides gradient to the generator to generate samples from its region. At the same time, due to having no local optima (only global optima), it does not risk the generator getting stuck where it is not supposed to. 2) Within regions, our guide does not mislead the generator to favor some samples over others. 3) Since the space partitioner uses the partition labels as "class" id, it does not suffer from the overlapping classes problem, and naturally, it does not require supervised labels.</p><p>We believe our construction of the modified loss can also be applied to the supervised regime to avoid putting the data samples far from the boundary. In addition, combining our method with the supervised one, each label itself can be partitioned into several segments. We leave the investigation of this modification to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section provides an empirical analysis of our method on various datasets 2 . We adopt the architecture of SN-GAN <ref type="bibr" target="#b55">[55]</ref> for our generators and discriminators. We use a Lipschitz constant of 0.9 for our space partitioner that consists of 20 residual blocks, resulting in 60 convolutional layers. We use Adam optimizer <ref type="bibr" target="#b38">[38]</ref> to train the proposed generators, discriminators, and space partitioner, and use SGD for training the pretext model. Please refer to the Appendix for complete details of hyper-parameters and architectures used for each component of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and evaluation metrics</head><p>We conduct extensive experiments on CIFAR-10 <ref type="bibr" target="#b39">[39]</ref> and STL-10 <ref type="bibr" target="#b11">[12]</ref> (48?48),  two real image datasets widely used as benchmarks for image generation. To see how our method fares against large dataset, we also applied our method on ILSVRC2012 dataset (ImageNet) <ref type="bibr" target="#b63">[63]</ref> which were downsampled to 128?128?3. To evaluate and compare our results, we use Inception Score (IS) <ref type="bibr" target="#b67">[67]</ref> and Frechet Inception Distance (FID) <ref type="bibr" target="#b26">[26]</ref>. It has been shown that IS may have many shortcomings, especially on non-ImageNet datasets. FID can detect mode collapse to some extent for larger datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b66">66]</ref>. However, since FID is not still a perfect metric, we also evaluate our models using reverse-KL which reflects both mode dropping and spurious modes <ref type="bibr" target="#b48">[48]</ref>. All FIDs and Inception Scores (IS) are reported using 50k samples. No truncation trick is used to sample from the generator.</p><p>We also conduct experiments on three synthetic datasets: Stacked-MNIST <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b70">70]</ref> that has up to 1000 modes, produced by stacking three randomly sampled MNIST <ref type="bibr" target="#b41">[41]</ref> digits into the three channels of an RGB image, and the 2D-grid dataset described in Section 5.1 as well as 2D-ring dataset. The empirical results of the two later datasets are presented in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Toy dataset</head><p>This section aims to illustrate the importance of having a proper guide with no local optima. We also provide intuition about how our method helps GAN training. To that end, we use the canonical 2D-grid dataset, a mixture of 25 bivariate Gaussian with identical variance, and means covering the vertices of a square lattice. <ref type="bibr" target="#b20">[20]</ref> 133.4 ? 17.70 2.97 ? 0.216 28.08 ? 0.47 6.98 ? 0.062 0.0150 ? 0.0026 PacGAN2 <ref type="bibr" target="#b45">[45]</ref> 1000.0 ? 0.00 0.06 ? 0.003 27.97 ? 0.63 7.12 ? 0.062 0.0126 ? 0.0012 PacGAN3 <ref type="bibr" target="#b45">[45]</ref> 1000.0 ? 0.00 0.06 ? 0.003 32.55 ? 0.92 6.77 ? 0.064 0.0109 ? 0.0011 PacGAN4 <ref type="bibr" target="#b45">[45]</ref> 1000.0 ? 0.00 0.07 ? 0.005 34.16 ? 0.94 6.77 ? 0.079 0.0150 ? 0.0005 Logo-GAN-AE <ref type="bibr" target="#b65">[65]</ref> 1000.0 ? 0.00 0.09 ? 0.005 32.49 ? 1.37 7.05 ? 0.073 0.0106 ? 0.0005 Self-Cond-GAN <ref type="bibr" target="#b46">[46]</ref> 1000.0 ? 0.00 0.08 ? 0.009 18.03 ? 0. <ref type="bibr" target="#b55">55</ref>  Logo-GAN-RC <ref type="bibr" target="#b65">[65]</ref> 1000.0 ? 0.00 0.08 ? 0.006 28.83 ? 0.43 7.12 ? 0.047 0.0091 ? 0.0001 Class-conditional GAN <ref type="bibr" target="#b54">[54]</ref> 1000.0 ? 0.00 0.08 ? 0.003 23.56 ? 2.24 7.44 ? 0.080 0.0019 ? 0.0001 <ref type="table">Table 1</ref>: Performance comparison of the unsupervised (above midline)/supervised (below midline) image generation methods on the Stacked MNIST and CIFAR-10 datasets. The number of recovered modes, reverse KL, FID, and IS are used as the evaluation metrics. We report the means and standard deviations over five random initializations. For CIFAR-10, all methods recover all 10 modes. Results of the compared models are quoted from Liu et al. <ref type="bibr" target="#b46">[46]</ref> For this toy example the data points are low dimensional, thus we skip the feature learning step and directly train the space partitioner S. We train our space partitioner using two different architectures: one sets its neural network architecture to a multi-layer fully connected network with ReLU activations, while the other follows the properties of architecture construction in Section 4.2. Once the two networks are trained, both successfully learn to put each Gaussian component in a different cluster, i.e., both get perfect clustering accuracy. Nonetheless, the guide functions obtained from each architecture behave significantly different. <ref type="figure" target="#fig_2">Figure 3</ref> provides the graph of ?R i (x) for the two space partitioners, where i is the partition ID for the red Gaussian data samples in the corner. The right plot shows that ?R i (x) can have undesired local optima when the conditions of Section 4.2 are not enforced. Therefore, a universal reliable gradient is not provided to move the data samples toward the partition of interest. On the other hand, when the guide's architecture follows these conditions (left plot), taking the direction of ? ? R i (x) guarantees reaching to partition i. <ref type="figure" target="#fig_3">Figure 4</ref> shows the effect of both these guides in the training of our mixture of generators using Equation <ref type="bibr" target="#b7">8</ref>. As shown, when R i has local optima, the generator of that region may get stuck in those local optima and miss the desired mode. As shown in Liu et al. <ref type="bibr" target="#b46">[46]</ref> and the Appendix, GANs trained with no guide also fail to generate all the modes in this dataset. Furthermore, in contrast to standard GANs, we don't generate samples from the space between different modes due to our partitioning and mixture of generators, mitigating the mode connecting problem. We provide empirical results for this dataset in the Appendix.</p><formula xml:id="formula_21">Modes (Max 1000) ? Reverse KL ? FID ? IS ? Reverse KL ? GAN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Stacked-MNIST, CIFAR-10 and STL-10</head><p>In this section, we conduct extensive experiments to evaluate the proposed Partition-Guided Mixture of Generators (PGMGAN) model. We also quantify the performance gains for the different parts of our method through an ablation study. We randomly generated the partition labels in one baseline to isolate the effect of proper partitioning from the architecture choice of G i /D i 's. We also ablate the benefits of the guide function by making a baseline where ? = 0. For all experiments, we use k = 200 unless specified otherwise.</p><p>Tables 1 and 2 presents our results on Stacked MNIST, CIFAR-10 and STL-10 respectively. From these tables, it is evident how training multiple generators using the space partitioner allows us to significantly outperform the other benchmark algorithms in terms of all metrics. Comparing Random Partition ID to Partition+GAN clearly shows the importance of having an effective partitioning in terms of performance and mode covering. Furthermore, the substantial gap between PGMGAN and Partition+GAN empirically demonstrates the value of utilizing the guide term. We first perform overall comparisons against other recent GANs. As shown in <ref type="table" target="#tab_1">Table 2</ref>, PGMGAN achieves state-ofthe-art FID and IS on the STL-10 dataset. Furthermore, PGMGAN outperforms several other baseline models, even over supervised class-conditional GANs, on both CIFAR-10 and Stacked-MNIST, as shown in <ref type="table">Table 1</ref>. The significant improvements of FID and IS reflect the large gains in diversity and image quality on these datasets.</p><p>Following Liu et al. <ref type="bibr" target="#b46">[46]</ref>, we calculate the reverse KL metric using pre-trained classifiers to classify and count the occurrences of each mode for both Stacked-MNIST and CIFAR-10. These experiments and comparisons demonstrate that our proposed guide model effectively improves the performance of GANs in terms of mode collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Image generation on unsupervised ImageNet</head><p>To show that our method remains effective on a larger more complex dataset, we also evaluated our model on unsupervised ILRSVRC2012 (ImageNet) dataset which contains roughly 1.2 million images with 1000 distinct categories; we down-sample the images to 128?128 resolution for the experiment. We use k = 1000 and adopt the architecture of BigGAN <ref type="bibr" target="#b4">[5]</ref> for our generators and discriminators. Please see the Appendix for the details of experimental settings.</p><p>Our results are presented in <ref type="table" target="#tab_2">Table 3</ref>. To the best of our knowledge, we achieve a new state of the art (SOTA) in unsupervised generation on ImagNet apart from augmentation based methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Parameter sensitivity</head><p>Additionally, we study the sensitivity of our overall PGMGAN method to the choice of guide's weight ? (Eq. 8), and number of clusters k. <ref type="figure" target="#fig_4">Figure 5</ref> shows the results with varying ? over CIFAR-10, demonstrating that our method is relatively robust to the choice of this hyperparameter. Next, we change k for a fixed ? = 6.0 and report the results in <ref type="table" target="#tab_3">Table 4</ref>. we observe that our method performs well for a wide range of k.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduce a differentiable space partitioner to alleviate the GAN training problems, including mode connecting and mode collapse. The intuition behind how this works is twofold. The first reason is that an efficient partitioning makes the distribution on each region simpler, making its approximation easier. Thus, we can have a better approximation as a whole, which can alleviate both mode collapse and connecting problems. The second intuition is that the space partitioner can provide extra gradient, assisting the discriminator in training the mixture of generators. This is especially helpful when the discriminator's gradient is unreliable. However, it is crucial to have theoretical guarantees that this extra gradient does not deteriorate the GAN training convergence in any way. We identify a sufficient theoretical condition for the space partitioner (in the functional space), and we realize that condition empirically by an architecture design for the space partitioner. Our experiments on natural images show the proposed method improves existing ones in terms of both FID and IS. For future work, we would like to investigate using the space partitioner for the supervised regime, where each data label has its own partitioning. Designing a more flexible architecture for the space partitioner such that its guide function does not have local optima is another direction we hope to explore.</p><formula xml:id="formula_22">d i := inf{||x ? y|| | x ? M i , y ? M j , j = i}</formula><p>d i is the distance of manifold M i from the rest, and ? is the CDF of the univariate standard normal distribution. Note ? is strictly larger than zero iff ?i : d i , ? * i = 0. Proof. We begin by re-stating the definition of Minkowski sum and proceed by proving the theorem for the case where the number of manifold is 2. To extend the theorem form k = 2 to the general case, one only needs to consider manifold M i as M 1 , and j?[1:k]\i M j as M 2 .</p><p>Definition 1 (Minkowski sum). The Minkowski sum of two sets U, V ? R d defined as</p><formula xml:id="formula_23">U + V := {u + v|u ? U, v ? V }</formula><p>and when V is a d dimensional ball with radius r and centered at zero, we use the notation U r to refer to their Minkowski sum.</p><formula xml:id="formula_24">If we let U (1) := G ?1 ? (M 1 ), U (2) := G ?1 ? (M 2 )</formula><p>, then:</p><formula xml:id="formula_25">?r 1 , r 2 ? R + , if r 1 + r 2 &lt; d 1 /c =? U (1) r1 ? U (2) r2 = ? that is because if there exists an x ? U (1) r1 ? U (2)</formula><p>r2 , there would be u 1 ? U (1) , u 2 ? U <ref type="bibr" target="#b1">(2)</ref> such that:</p><formula xml:id="formula_26">||x ? u 1 ||? r 1 , ||x ? u 2 ||? r 2 =? ||u 1 ? u 2 ||&lt; r 1 + r 2 &lt; d 1 /c (9)</formula><p>However, due to lipsitz condition of G ? :</p><formula xml:id="formula_27">||G ? (u 1 ) ? G ? (u 2 )||&lt; c||u 1 ? u 2 ||&lt; c ? d 1 /c = d 1</formula><p>which contradicts with our assumption that the distance between M 1 , M 2 is d 1 . Therefore there is no point in the intersection of U <ref type="bibr" target="#b0">(1)</ref> r1 and U <ref type="bibr" target="#b1">(2)</ref> r2 . The disjointness of this two sets provides us:</p><formula xml:id="formula_28">? n (U (1) r1 ) + ? n (U (2)</formula><p>r2 ) ? ? n (R n ) = 1 where ? n (.) of any set is the probability of a random draw of N (0, I n ) being from that set. We proceed by using a remark from theorem 1.3 of <ref type="bibr" target="#b42">[42]</ref> which restated below: Lemma 1. If U is a Borel set in R n , then:</p><formula xml:id="formula_29">p ? ? n (U ) =? ?(? ?1 (p) + r) ? ? n (U r ).</formula><p>Based on above lemma if we let:</p><formula xml:id="formula_30">p 1 := ? n (U (1) ), p 2 := ? n (U (2) )</formula><p>then for ?r 1 , r 2 ? R + such that r 1 + r 2 &lt; d 1 /c, we have:</p><formula xml:id="formula_31">?(? ?1 (p 1 ) + r 1 ) + ?(? ?1 (p 2 ) + r 2 ) ? 1<label>(10)</label></formula><p>We can now calculate the total variational distance of the marginal distributions of p data , p model on the set</p><formula xml:id="formula_32">{G ? (U (1) ), G ? (U (2) ), G ? R n \ (U (1) ? U (2) ) } as: d T V (p (marginal) data , p (marginal) model ) = |? 1 ? p 1 |+|? 2 ? p 2 |+|1 ? (p 1 + p 2 )|<label>(11)</label></formula><p>and since total variational distance takes a smaller value on marginal distributions than the full distribution, we only need to show that for any i the expression in the equation 11 is larger than g(? i , d i , c) to prove the theorem 1 for k = 2.</p><formula xml:id="formula_33">Here g(? i , d i , c) = ? * i ? ?(? ?1 (? * i ) ? d i /c).</formula><p>Assume p 1 ? ? 1 , and define ? 1 := ? 1 ? p 1 ? 0, based on equation 10 if r 1 = d 1 /c, r 2 = 0, we have:</p><formula xml:id="formula_34">?(? ?1 (? 1 ? ? 1 ) + d 1 /c) + p 2 ? 1</formula><p>which based on equation 10 implies:</p><formula xml:id="formula_35">r(? 1 ) := ?(? ?1 (? 1 ? ? 1 ) + d 1 /c) ? (? 1 ? ? 1 ) ? D T V</formula><p>which D T V refers to total variational distance between the marginal distributions of the data and model. We also know from the equation 10, that ? 1 ? D T V , therefore:</p><formula xml:id="formula_36">max(r(? 1 ), ? 1 ) ? D T V for a ? 1 ? [0, ? 1 ]. Therefore min ?1?[0,?1] {max(r(? 1 ), ? 1 )} ? D T V</formula><p>To find the ? 1 which minimize the above equation, we need to check endpoints of the interval [0, ? 1 ], points where the curve of two functions r(? 1 ), ? 1 intersects with each other, and points that are the local minimaum of each of them. It can be shown the function r(? 1 ) does not have any local minima when 0 &lt; ? 1 &lt; 1 because:</p><formula xml:id="formula_37">r(? 1 ) = P (z ? [? ?1 (? 1 ?? 1 ), ? ?1 (? 1 ?? 1 )+d 1 /c]) (12)</formula><p>where z is univarite standard normal random variable. Therefore r(? 1 ) is the probablity of a univarite normal being in a fixed length interval d 1 /c, and ? 1 only changes the starting point of the interval. By using this fact, it can be easily shown this function does not have any local optima in the open interval (0, ? 1 ). Also the identity function ? 1 also has no local optima inside the interval. The endpoints values are:</p><formula xml:id="formula_38">max(r(0), 0) = ?(? ?1 (? 1 ) + d 1 /c) ? ? 1 max(r(? 1 ), ? 1 ) = max(?(? ?1 (0) + d 1 /c), ? 1 ) = ? 1</formula><p>The function curves of r and identity also intersects only when:</p><formula xml:id="formula_39">?(? ?1 (? 1 ? ? * 1 ) + d 1 /c) = ? 1</formula><p>which only happens when:</p><formula xml:id="formula_40">? 1 ? ?(? ?1 (? 1 ) ? d 1 /c) = ? * 1</formula><p>where for this point, max(r(? * 1 ), ? * 1 ) = ? * 1 . Therefore based on the above calculations:</p><formula xml:id="formula_41">min ? ? ? ?(? ?1 (? 1 ) + d 1 /c) ? ? 1 I , ? 1 , ? 1 ? ?(? ?1 (? 1 ) ? d 1 /c) II ? ? ? ? D T V</formula><p>Note, ? 1 is always smaller than term II, and term I (II) is equal to probability of a univariate standard normal random variable being inside the interval [? ?1 (? 1 ), ? ?1 (? 1 )+d 1 /c] ([? ?1 (? 1 ) ? d 1 /c, ? ?1 (? 1 )]). This observation implies that term II is smaller than term I, if and only if ? 1 ? ? 2 . Based on this fact and symmetry of ? with respect to zero, it can be easily shown that:</p><formula xml:id="formula_42">g(? 1 , d 1 , c) ? D T V<label>(13)</label></formula><p>which proves the theorem. However, we made an assumption that p 1 ? ? 1 , this does not harm the argument because otherwise we would have p 2 ? ? 2 , and we can restate all the above arguments for ? 2 instead of ? 1 . And, since ? 2 = 1 ? ? 1 and d 1 = d 2 , therefore we can have g(? 1 , d 1 , c) = g(? 2 , d 2 , c), which proves equation 13.</p><p>Theorem 2. Let P = k i ? i p i , Q = k i ? i q i , and A 1 , A 2 , ..., A K be a partitioning of the space, such that the support of each distribution p i and q i is A i . Then:</p><formula xml:id="formula_43">JSD(P Q) = i ? i JSD(p i q i )<label>(3)</label></formula><p>Proof. Based on the definition of the JSD, we have:</p><formula xml:id="formula_44">JSD(P Q) = 1 2 KL(P P + Q 2 ) + 1 2 KL(Q P + Q 2 )</formula><p>We also have:</p><formula xml:id="formula_45">KL(P P + Q 2 ) = R d P (x) log P (x) P (x) + Q(x) dx + log 2 = i Ai P (x) log P (x) P (x) + Q(x) dx + log 2 = i Ai ? i p i (x) log ? i p i (x) ? i p i (x) + ? i q i (x) dx + log 2 = i ? i ( Ai p i (x) log p i (x) p i (x) + q i (x) dx + log 2) = i ? i KL(p i p i + q i 2 ).</formula><p>Therefore:</p><formula xml:id="formula_46">KL(P P + Q 2 ) = i ? i KL(p i p i + q i 2 ),</formula><p>and similarly:</p><formula xml:id="formula_47">KL(Q P + Q 2 ) = i ? i KL(q i p i + q i 2 )</formula><p>Adding these two terms completes the proof. </p><formula xml:id="formula_48">?x ? R d \ A i , b 0 ? ||?R i (x)||</formula><p>where A i = {x|x ? R d , R i (x) = 0}. Furthermore A i is a connected set for all i's.</p><p>Proof. We start by proving that the Jacobian matrix of function ? is invertible for any x ? R d . Since ? ? C 1 , based on Taylor's expansion theorem for multi-variable vector-valued function ?, we can write:</p><formula xml:id="formula_49">?(y) ? ?(x) = J ? (x) (y ? x) + o (||y ? x||)</formula><p>Were o(?) is the Little-o notation. By taking norm from both sides and using triangle inequality, we have: Taking the gradient of the new formulation of R i , we have:</p><formula xml:id="formula_50">?R i (x) = ? ? c?I(x) (w c ? w i ) ? ? ?(?(x)) = vJ ? (x)</formula><p>but since we showed earlier that all of the singular values of the Jacobian matrix is larger than c 0 /2, the Jacobian matrix is d ? d, and v is not equal to zero, it can be easily shown:</p><formula xml:id="formula_51">||?R i (x)||&gt; ||v|| c 0 2 := b 0</formula><p>Now, we also need to show A i is connected for any i to complete the proof. To that end, we first show ? is a surjective function, which means its image is R d . To show the ? is surjective, we prove its image is both an open and closed set, then since the only sets which are both open and closed (in R d ) are R d , ?, we can conclude the surejective property. The image of ? is an open set due to Inverse Function Theorem <ref type="bibr" target="#b62">[62]</ref> for ?. We are allowed to use Inverse Function Theorem, since ? satisfies both C 1 condition and non zero determinant for all the points in the domain. We will also show that the image of ? is a closed set by showing it contains all of its limit points. Let y be a limit point in the image of ?, that is there exists {x 1 , x 2 , ? ? ?} such that ?(x r ) ? y. Since R d is complete and we have c 0 ||x r ? x s ||? ||?(x r ) ? ?(x s )||, then {x 1 , x 2 , ? ? ?} is a Cauchy sequence. Finally since ? is a continues function ?(x * ) = y, completing the proof.</p><p>The function ? is also an invertible function because if ?(x) = ?(y) then c 0 ||x ? y||? ||?(x) ? ?(y)||= 0 which implies x = y. Therefore ? is in fact a continuous bijecitve function, which means it has a continuous inverse defined on all the space R d . Furthermore, it can be easily shown R i for a datapoint is zero iff its transformation by ? lies in a polytope (where each of its facets is a hyperplance perpendicular to a w c ? w i ). Since convex polytope is a connected set, and by applying ? ?1 (it is well defined everywhere because of bijective property of ?) to it, we would have a connected set. That is because a continuous function does not change the connectivity and ? ?1 is continuous.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>85 Figure 1 :</head><label>851</label><figDesc>Examples of unsupervised partitioning and their corresponding real/generated samples on the CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1Figure 2 :</head><label>2</label><figDesc>Accurately put different manifolds in different partitions. Diagram of proposed partitioner and guide. We employ spectral normalization for each convolutional layer to make each layer (as a function) have Lipschitz constant of less than one. The details of our architecture is provided in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Left/right: the graph of ?Ri(x) with/without assumption on the architecture, where the data points of the i th partition are shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Left/right: visual comparison of generated samples on the 2D-grid dataset using PGMGAN, with/without architecture restriction for the space partitioner. The red/blue points illustrate the real/generated data samples. In the right plot, some modes are missed and their corresponding generators focus on the wrong area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Effect of changing the guide's weight ? in equation 8 on PGMGAN performance. ? = 0 corresponds to the partition+GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Theorem 3 .</head><label>3</label><figDesc>Let ?(x) : R d ? R d be a C 1 (differentiable with continuous derivative) function, W partitioner ? R k?d , and R i as defined in Eq 6. If there exists c 0 &gt; 0, such that: ? x, y ? R d , c 0 ||x ? y||? ||?(x) ? ?(y)||, then for every i ? [1 : k], every local optima of R i is a global optima, and there exists a positive constant b 0 &gt; 0 such that:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 ?</head><label>2</label><figDesc>||?(y) ? ?(x)||? ||J ? (x) (y ? x) ||+||o (||y ? x||) || Also because:c 0 ||y ? x||? ||?(y) ? ?(x)|| =? c 0 ||y ? x||? ||J ? (x) (y ? x) ||+||o (||y ? x||) ||(14) thus for any fixed x: ? &gt; 0 such that ? y ? R d where ||y ? x||? then: ||o (||y ? x||) ||? c 0 2 ||y ? x|| which combined with the inequality 14, results in:c 0 2 ||y ? x||? ||J ? (x) (y ? x) ||For y = x, let u := (y ? x)/||y ? x||, then by dividing both sides of the above inequality to ||y ? x|| we have:?u ? R d , ||u||= 1 =? c 0 ||J ? (x)u||which shows the Jacobian matrix of ? is invertiable for any x and all of its singular values are larger than c 0 /2. If there is no x ? R d \ A i the proof is complete. Otherwise, consider any x ? R d \ A i , for this x we have:0 &lt; R i (x) = c (f c (x)?f i (x)) + = c ((w c ?w i )?(x)) +where w j is the j'th row of the matrix W partitioner . Let:I(x) := {c|f c (x) &gt; f i (x), c ? [1 : k]}which is an non-empty set, because 0 &lt; R i (x) and we have0 &lt; R i (x) = ? ? c?I(x) (w c ? w i ) ? ? ?(x) =? v := c?I(x) (w c ? w i ) = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Unsupervised image generation results on STL-10. The results of all compared method are taken from Tian et al.<ref type="bibr" target="#b73">[73]</ref> </figDesc><table><row><cell></cell><cell cols="2">STL-10</cell></row><row><cell></cell><cell>FID ?</cell><cell>IS ?</cell></row><row><cell>D2GAN [58]</cell><cell>-</cell><cell>7.98</cell></row><row><cell>DFM [79]</cell><cell>-</cell><cell>8.51</cell></row><row><cell>ProbGAN [23]</cell><cell>46.74</cell><cell>8.87</cell></row><row><cell>SN-GAN [55]</cell><cell>40.15</cell><cell>9.10</cell></row><row><cell>Dist-GAN [74]</cell><cell>36.19</cell><cell>-</cell></row><row><cell>MGAN [27]</cell><cell>-</cell><cell>9.22</cell></row><row><cell cols="2">Improved MMD [77] 37.63</cell><cell>9.34</cell></row><row><cell>AGAN [76]</cell><cell>52.75</cell><cell>9.23</cell></row><row><cell>AutoGAN [19]</cell><cell>31.01</cell><cell>9.16</cell></row><row><cell>E 2 GAN [73]</cell><cell>25.35</cell><cell>9.51</cell></row><row><cell cols="3">Partition GAN (Ours) 26.28 10.35</cell></row><row><cell>PGMGAN (Ours)</cell><cell cols="2">19.52 11.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>FID and Inception Score (IS) metrics for unsupervised image generation on ImageNet at resolution 128?128. The results of all compared methods are taken from Liu et al.<ref type="bibr" target="#b46">[46]</ref> </figDesc><table><row><cell></cell><cell cols="2">ImageNet</cell></row><row><cell></cell><cell>FID ?</cell><cell>IS ?</cell></row><row><cell>GAN [20]</cell><cell cols="2">54.17 14.01</cell></row><row><cell>PacGAN2 [45]</cell><cell cols="2">57.51 13.50</cell></row><row><cell>PacGAN3 [45]</cell><cell cols="2">66.97 12.34</cell></row><row><cell>MGAN [27]</cell><cell cols="2">58.88 13.22</cell></row><row><cell cols="3">RotNet Feature Clustering 53.75 13.76</cell></row><row><cell>Logo-GAN-AE [65]</cell><cell cols="2">50.90 14.44</cell></row><row><cell>Self-Cond-GAN [46]</cell><cell cols="2">40.30 15.82</cell></row><row><cell>PGMGAN (Ours)</cell><cell cols="2">21.73 23.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Effect of number of partitions (k) on PGMGAN perfor-</cell></row><row><cell cols="3">mance. Results are averaged over five random trials, with standard</cell></row><row><cell>error reported.</cell><cell></cell></row><row><cell></cell><cell>CIFAR-10</cell></row><row><cell></cell><cell>FID ?</cell><cell>IS ?</cell></row><row><cell>GAN [20]</cell><cell cols="2">28.08 ? 0.47 6.98 ? 0.06</cell></row><row><cell>PGMGAN (k = 50)</cell><cell cols="2">9.27 ? 0.45 8.69 ? 0.07</cell></row><row><cell>PGMGAN (k = 100)</cell><cell cols="2">8.97 ? 0.33 8.75 ? 0.05</cell></row><row><cell>PGMGAN (k = 200)</cell><cell cols="2">8.93 ? 0.38 8.81 ? 0.10</cell></row></table><note>Class Conditional GAN [54] 23.56 ? 2.24 7.44 ? 0.08</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The code to reproduce experiments is available at https:// github.com/alisadeghian/PGMGAN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This work was supported in part by NSF IIS-1812699.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A. Proofs Theorem 1. Let p data be a distribution supported on a set of disjoint manifolds M 1 , . . . , M k in R d , and [? 1 , . . . , ? k ] be the probabilities of being from each manifold. Let G ? be a c-Lipschitz function, and p model be the distribution of G ? (z), where z ? N (0, I n ), then:</p><p>where d T V is the total variation distance and:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Additional qualitative results</head><p>We present more samples of our method showing both the partitioner and generative model's performance. <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref> visualize the sample diversity and quality of our method on CIFAR-10 and STL-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. CIFAR-10</head><p>Real Generated (a) Partition 1</p><p>Real Generated</p><p>Real Generated (c) Partition 12 Real Generated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(d) Partition 13</head><p>Real Generated (e) Partition 28</p><p>Real Generated (f) Partition 37 Real Generated (g) Partition 42</p><p>Real Generated </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. STL-10</head><p>Real Generated </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Implementation details</head><p>We use two RTX 2080 Ti GPUs for experiments on STL-10, eight V-100 GPUs for ImageNet and a single GPU for all other experiments.</p><p>Space partitioner. For all experiments we use the same architecture for our space partitioner S. We use preactivation Residual-Nets with 20 convolutional bottleneck blocks with 3 convolution layers each and kernel sizes of 3 ? 3, 1 ? 1, 3 ? 3 respectively and the ELU <ref type="bibr" target="#b10">[11]</ref> nonlinearity. The network has 4 down-sampling stages, every 4 blocks where a dimension squeezing operation is used to decrease the spatial resolution. We use 160 channels for all the blocks. We do not use any initial padding due to our theoretical requirements. The negative slope of LeakyReLU is set as 0.2. In fact we can use a soft version of LeakyReLU if it is critical to guarantee the C 1 constraint of ?. We train our pretext network for 500 epochs with momentum SGD and a weight decay of 3e-5, learning rate of 0.4 with cosine scheduling, momentum of 0.9, and batch size of 400 for CIFAR-10 and 200 for STL-10. The final space partitioner is trained for 100 epochs using Adam <ref type="bibr" target="#b38">[38]</ref> with a learning rate of 1e-4 and batch size of 128. The weights in equation 5 are set to ? = 5 and ? = 1e-3.</p><p>Generative model. Following SN-GANs <ref type="bibr" target="#b55">[55]</ref> for image generation at resolution 32 or 48, we use the architectures described in <ref type="table">Tables 6 and 7</ref>. Generators/discriminators are different from each other in first-layer/last-layer by having different partition ID embeddings, (which in fact acts as the condition). We use Adam optimizer with a batch size of 100. For the coefficient of guide ? we utilized linear annealing during training, decreasing form 6.0 to 0.0001. Both G's and D networks are initialized with a normal N (0, 0.02I). For all GAN's experiments, we use Adam optimizer <ref type="bibr" target="#b38">[38]</ref> with ? 1 = 0, ? 2 = 0.999 and a constant learning rate 2 for both G and D. The number of D steps per G step training is 4.</p><p>For ImageNet experiment, we adopt the full version of BigGAN model architecture <ref type="bibr" target="#b4">[5]</ref> described in <ref type="table">Table 8</ref>. In this experiment, we apply the shared class embedding for each CBN layer in G, and feed noise z to multiple layers of G by concatenating with the partition ID embedding vector. Moreover, we add Self-Attention layer with the resolution of 64, and we employ orthogonal initialization for network parameters <ref type="bibr" target="#b67">[67]</ref>. We use batch size of 256 and set the number of gradient accumulations to 8.</p><p>Evaluation. It has been shown that <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">48]</ref> when the sample size is not large enough, both FID and IS are biased, therefor we use N=50,000 samples for computing both IS and FID metrics. We also use the official TensorFlow scripts for computing FID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Additional Experiments:</head><p>Our quantitative results on the (2D-ring, 2D-grid) toy datasets <ref type="bibr" target="#b45">[45]</ref> are: recovered modes: <ref type="bibr">(8 , 25)</ref>, high quality samples: (99.8 , 99.8), reverse KL: (0.0006, 0.0034).</p><p>Given the strong performances of recent models (and ours) on these datasets we suffice to these stats. For visualizations of our generated samples related to these two datasets please see <ref type="figure">Figure 4</ref>-left and refer to the appendix of <ref type="bibr" target="#b46">[46]</ref> for other methods.</p><p>Additional architecture dependent experiment: Since SelfCondGAN <ref type="bibr" target="#b46">[46]</ref> uses certain features from the discriminator, it is not trivial to adopt to other architectures. Thus, we trained PGMGAN with the same G/D architecture on CIFAR10 yielding an FID of 10.65.</p><p>Partitioning method One way to assess the quality of the space partitioner is by measuring its performance on placing semantically similar images in the same partition.</p><p>To that end, we use the well-accepted clustering metric Normalized Mutual Information (NMI). NMI is a normalization of the Mutual Information (MI) between the true and inferred labels. This metric is invariant to permutation of the class/partition labels and is always between 0 and 1, with a higher value suggesting a higher quality of partitioning. <ref type="table">Table 5</ref> compares the clustering performance of our method to the-stat-of-the-art partition-based GAN method Liu et. <ref type="bibr" target="#b46">[46]</ref>, which clearly shows superiority of our method.     </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ian Goodfellow, and Augustus Odena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discriminator rejection sampling. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pros and cons of gan evaluation measures. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="41" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12154" to="12163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effectively unbiased fid and inception score and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Analysis of Single Layer Networks in Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://cs.stanford.edu/acoates/papers/coatesleeng_aistats_2011.pdf.2" />
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic goal generation for reinforcement learning agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Florensa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Twin auxilary classifiers gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1330" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autogan: Neural architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3224" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deligan: Generative adversarial networks for diverse and limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminathan</forename><surname>Gurumurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">Kiran</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Babu</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Probgan: Towards probabilistic gan with theoretical guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Guang-He Lee, and Yonglong Tian</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Bernhard Nessler, and Sepp Hochreiter</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mgan: Training generative adversarial nets with multiple generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Tu Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kelley</surname></persName>
		</author>
		<title level="m">General topology. Courier</title>
		<imprint>
			<publisher>Dover Publications</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Disconnected manifold learning for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Khayatkhoei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00880</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Generating and designing dna with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Killoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gan-tree: An incrementally learned hierarchical generative framework for multi-modal data distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maharshi</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakshit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8191" to="8200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>ATT Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Isoperimetry and gaussian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Ledoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lectures on probability theory and statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06547</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial ranking for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ting</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pacgan: The power of two samples in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Diverse image generation via selfconditioned gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Normalized diversification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiao</forename><surname>Wangni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10306" to="10315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Are gans created equal? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">High-fidelity image generation with fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Realistic in silico generation and augmentation of single-cell rna-seq data using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Marouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Machart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bonn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3478" to="3487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">cgans with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sample complexity of testing the manifold hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hariharan</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Mitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dual discriminator generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2670" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Principles of mathematical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1349" to="1358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Logo synthesis and manipulation with clustered generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5879" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5228" to="5237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Ac-gan learns a biased distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Veegan: Reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazar</forename><surname>Valkoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Michael U Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Learning disconnected manifolds: a no gans land</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Tanielian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Issenhuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elvis</forename><surname>Dohmatob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Mary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">A global geometric framework for nonlinear dimensionality reduction. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Joshua B Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Off-policy reinforcement learning for efficient and effective gan architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Fink</surname></persName>
		</author>
		<idno>2020. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Dist-gan: An improved gan using distance constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Anh</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Agan: Towards automated design of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11080</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Improving MMD-GAN training with repulsive loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saman</forename><surname>Halgamuge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Improving generative adversarial networks with denoising feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Bourgan: Generative networks with metric embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Consistency regularization for generative adversarial networks. International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Image augmentations for gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02595,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
