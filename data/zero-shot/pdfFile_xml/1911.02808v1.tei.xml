<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transition-Based Deep Input Linearization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">?Kohli Center on Intelligent Systems (KCIS)</orgName>
								<orgName type="department" key="dep2">International Institute of Information Technology</orgName>
								<orgName type="institution">?Singapore University of Technology and Design</orgName>
								<address>
									<settlement>Hyderabad (IIIT Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">?Kohli Center on Intelligent Systems (KCIS)</orgName>
								<orgName type="department" key="dep2">International Institute of Information Technology</orgName>
								<orgName type="institution">?Singapore University of Technology and Design</orgName>
								<address>
									<settlement>Hyderabad (IIIT Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">?Kohli Center on Intelligent Systems (KCIS)</orgName>
								<orgName type="department" key="dep2">International Institute of Information Technology</orgName>
								<orgName type="institution">?Singapore University of Technology and Design</orgName>
								<address>
									<settlement>Hyderabad (IIIT Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transition-Based Deep Input Linearization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional methods for deep NLG adopt pipeline approaches comprising stages such as constructing syntactic input, predicting function words, linearizing the syntactic input and generating the surface forms. Though easier to visualize, pipeline approaches suffer from error propagation. In addition, information available across modules cannot be leveraged by all modules. We construct a transition-based model to jointly perform linearization, function word prediction and morphological generation, which considerably improves upon the accuracy compared to a pipelined baseline system. On a standard deep input linearization shared task, our system achieves the best results reported so far.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language generation (NLG) <ref type="bibr" target="#b26">(Reiter and Dale, 1997;</ref><ref type="bibr" target="#b33">White, 2004)</ref> aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation <ref type="bibr">(Chang and Toutanova, 2007;</ref>, abstractive summarization <ref type="bibr" target="#b6">(Barzilay and McKeown, 2005)</ref> and grammatical error correction <ref type="bibr" target="#b13">(Lee and Seneff, 2006)</ref>.</p><p>A line of traditional methods treat the problem as a pipeline of several independent steps <ref type="bibr">(Bohnet et al., 2010;</ref><ref type="bibr" target="#b31">Wan et al., 2009;</ref><ref type="bibr" target="#b5">Bangalore et al., 2000;</ref><ref type="bibr" target="#b11">H. Oh and I. Rudnicky, 2000;</ref><ref type="bibr" target="#b12">Langkilde and Knight, 1998)</ref>. For example, shown in <ref type="figure">Figure</ref> 1b, a pipeline based on the meaning text theory (MTT) <ref type="bibr" target="#b17">(Mel?uk, 1988)</ref> splits NLG into three * Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. independent steps 1. syntactic generation: generating an unordered and lemma-formed syntactic tree from a semantic graph, introducing function words; 2. syntactic linearization: linearizing the unordered syntactic tree; 3. morphological generation: generating the inflection for each lemma in the string.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep graph</head><p>In this paper we focus on deep graph as input. Exemplified in <ref type="figure">Figure 2</ref>, the deep input type is intended to be an abstract representation of the meaning of a sentence. Unlike semantic input, where the nodes are semantic representations of input, deep input is more surface centric, with lem-mas for each word being connected by semantic labels <ref type="bibr" target="#b4">(Banarescu et al., 2013;</ref><ref type="bibr" target="#b18">Mel?uk, 2015)</ref>. In contrast to shallow syntactic trees, function words in surface forms are not included in deep graphs <ref type="bibr">(Belz et al., 2011)</ref>. Deep inputs can more commonly occur as input of NLG systems where entities and content words are available, and one has to generate a grammatical sentence using them with only provision for inflections of words and introduction of function words. Such usecases include summarization, dialog generation etc.</p><p>A pipeline of deep input linearization is shown in <ref type="figure">Figure 1a</ref>. Generation involves predicting the correct word order, deciding inflections and also filling in function words at the appropriate positions. The worst-case complexity is n! for permuting n words, 2 n for function word prediction (assuming that a function word can be inserted after each content word), and 2 n for inflection generation (assuming two morphological forms for each lemma). On the dataset from the First Surface Realisation Shared Task, <ref type="bibr">Bohnet et al. (2011)</ref> achieved the best reported results on linearizing deep input representation, following the pipeline of <ref type="figure">Figure 1b</ref> (with input as deep graph instead of semantic graph). They construct a syntactic tree from deep input graph followed by function word prediction, linearization and morphological generation. A rich set of features are used at each stage of the pipeline and for each adjacent pair of stages, an SVM decoder is defined.</p><p>Pipelined systems suffer from the problem of error propagation. In addition, because the steps are independent of each other, information available in a later stage is not made use of in the earlier stages. We introduce a transition-based <ref type="bibr" target="#b21">(Nivre, 2008)</ref> method for joint deep input surface realisation integrating linearization, function word prediction and morphological generation. The model is shown in <ref type="figure">Fig 1c,</ref> as compared with the pipelined baseline in <ref type="figure">Fig 1a.</ref> For a directly comparable baseline, we construct a pipeline system of function words prediction, linearization and morphological generation similar to the pipeline of <ref type="bibr">Bohnet et al. (2011)</ref>, but with the following difference. Our baseline pipeline system makes function word prediction for a deep input graph, whereas <ref type="bibr">Bohnet et al. (2011)</ref> have a preprocessing step to construct a syntactic tree from the deep input graph, which is given as input to the function word prediction module. Our pipeline is directly comparable to the joint system with regard to the use of information.</p><p>Standard evaluations show that: 1. Our joint model for deep input surface realisation achieves significantly better scores over its pipeline counterpart. 2. We achieve the best results reported on the task. Our system scores 1 BLEU point better over <ref type="bibr">Bohnet et al. (2011)</ref> without using any external resources. We make the source code available at https://github.com/SUTDNLP/ ZGen/releases/tag/v0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Related work can be broadly summarized into three areas: abstract word ordering, applications of meaning-text theory and joint modelling of NLP tasks. In abstract word ordering <ref type="bibr" target="#b31">(Wan et al., 2009;</ref><ref type="bibr" target="#b39">Zhang, 2013;</ref><ref type="bibr" target="#b39">Zhang and</ref><ref type="bibr">Clark, 2015), De Gispert et al. (2014)</ref> compose phrases over individual words and permute the phrases to achieve linearization. <ref type="bibr" target="#b27">Schmaltz et al. (2016)</ref> show that strong surface-level language models are more effective than models trained with syntactic information for the task of linearization. Transitionbased techniques have also been explored <ref type="bibr" target="#b24">Puduppully et al., 2016)</ref>. To our knowledge, we are the first to use transition-based techniques for deep input linearization.</p><p>There has been work done in the area of sentence linearization using meaning-text theory <ref type="bibr" target="#b17">(Mel?uk, 1988)</ref>. Belz et al. (2011) organized a shared task on both shallow and deep linearization according to meaning-text theory, which provides a standard benchmark for system comparison.  achieved the best results for the task of shallow-syntactic linearization. Using SVM models with rich features, Bohnet et al.</p><p>(2011) achieved state-of-art results on the task of deep realization. While they built a pipeline system, we show that joint models can be used to overcome limitations of the pipeline approach giving the best results.</p><p>Joint models for NLP have shown effectiveness in recent years. Though having to tackle increased search space, they overcome issues with error propagation in pipelined models. Joint models have been explored for grammar-based approaches to surface realisation using <ref type="bibr">HPSG and CCG (Carroll and Oepen, 2005;</ref><ref type="bibr" target="#b30">Velldal and Oepen, 2006;</ref><ref type="bibr" target="#b9">Espinosa et al., 2008;</ref><ref type="bibr" target="#b32">White and Rajkumar, 2009;</ref><ref type="bibr" target="#b34">White, 2006;</ref><ref type="bibr">Carroll et al., 1999)</ref>. Joint models have been proposed for word segmentation and POS-tagging <ref type="bibr" target="#b35">(Zhang and Clark, 2010)</ref>, POS-tagging and syntactic chunking <ref type="bibr" target="#b29">(Sutton et al., 2007)</ref>, segmentation and normalization <ref type="bibr" target="#b25">(Qian et al., 2015)</ref>, syntactic linearization and morphologization <ref type="bibr">(Song et al., 2014), parsing and</ref><ref type="bibr">NER (Finkel and</ref><ref type="bibr" target="#b10">Manning, 2009</ref>), entity and relation extraction <ref type="bibr" target="#b14">(Li and Ji, 2014)</ref> and so on. We propose a first joint model for deep realization, integrating linearization, function word prediction and morphological generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline</head><p>We build a baseline following the pipeline in <ref type="figure">Figure</ref> 1a. Three stages are involved: 1. prediction of function words, inserting the predicted function words in the deep graph, resulting in a shallow graph; 2. linearizing the shallow graph; 3. generating the inflection for each lemma in the string.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Function Word Prediction</head><p>In the First Surface Realisation Shared Task dataset <ref type="bibr">(Belz et al., 2011)</ref>, there are three classes of function words to predict: to infinitive, that complementizer and comma. We implement classifiers to predict these classes of function words locally at respective positions in the deep graph resulting in a shallow graph <ref type="figure" target="#fig_1">(Figure 3</ref>). At each location the input is a node and output is a class indicating if to or that need to inserted under the node or the count of comma to be introduced under the node. <ref type="table">Table 1</ref> shows the feature templates for classification of to infinitives and that complementizers and <ref type="table">Table 2</ref> shows the feature templates for predicting the count of comma child nodes for each non-leaf node in the graph. These feature templates are a subset of features used in the joint model (Section 4) with the exceptions being word order features, which are not available here for the pipeline system, since earlier stages cannot leverage features in subsequent outcomes. We use av-Features for predicting function words including to infinitive, that complementizer WORD(n); POS(n); WORD(c) <ref type="table">Table 1</ref>: Feature templates for the prediction of function words-to infinitive and that complementizer. Indices on the surface string: n -word index; c -child of n; Functions: WORD -word at index; POS -part-of-speech at index.</p><p>Features for predicting count of comma WORD(n); POS(n) BAG(WORD-MOD(n)) BAG(LABEL-MOD(n)) <ref type="table">Table 2</ref>: Feature templates for the comma prediction system. Indices on the surface string: nword index; Functions: WORD -word at index; POS -part-of-speech at index; WORD-MODmodifiers of index; LABEL-MOD -dependency labels of modifiers; BAG -set. eraged perceptron classifier <ref type="bibr" target="#b7">(Collins, 2002)</ref> to predict function words, which is consistent with the joint model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Linearization</head><p>The next step is linearizing the graph, which we solve using a novel transition-based algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Transition-Based Tree Linearization</head><p>Liu et al. <ref type="formula">(2015)</ref> introduce a transition-based model for tree linearization. The approach extends from transition-based parsers <ref type="bibr" target="#b20">(Nivre and Scholz, 2004;</ref><ref type="bibr">Chen and Manning, 2014)</ref>, where state consists of stack to hold partially built outputs and a queue to hold input sequence of words. In case of linearization, the input is a set of words. Liu et al. therefore use a set to hold the input instead of a queue. State is represented by a tuple (?, ?, A), where ? is stack to store partial derivations, ? is set of input words and A is the set of dependency relations that have been built. There are three transition actions:</p><p>? SHIFT-Word-POS -shifts Word from ?, assigns POS to it and pushes it to top of stack as S 0 ;   <ref type="table">Table 3</ref>: Transition action sequence for linearizing the graph in <ref type="figure" target="#fig_1">Figure 3</ref>. SH -SHIFT, RA -RIGHTARC, LA -LEFTARC. POS is not shown in SHIFT actions.</p><formula xml:id="formula_0">] {3} A ? {6 ? 4} 10 RA [7 8 2 5 1 9] {3} A ? {9 ? 6} 11 RA [7 8 2 5 1] {3} A ? {1 ? 9} 12 RA [7 8 2 5] {3} A ? {5 ? 1} 13 SH-. [7 8 2 5 3] {} 14 RA [7 8 2 5] {} A ? {5 ? 3} 15 LA [7 8 5] {} A ? {2 ? 5} 16 LA [7 5] {} A ? {8 ? 5} 17 LA [5] {} A ? {7 ? 5}</formula><p>goes, home} is SHIFT-he, SHIFT-goes, SHIFThome, RIGHTARC-OBJ, LEFTARC-SBJ. The full set of feature templates are shown in <ref type="table">Table 2</ref> of , partly shown in <ref type="table" target="#tab_2">Table  4</ref>. The features include word(w), POS(p) and dependency label (l) of elements on stack and their descendants S 0 , S 1 , S 0,l , S 0,r etc. For example, word on top of stack is S 0 w and word on first left child of S 0 is S 0,l w. These are called configuration features. They are combined with all possible actions to score the action. <ref type="bibr" target="#b24">Puduppully et al. (2016)</ref> extend  by redefining features to address feature sparsity and introduce lookahead features, thereby achieving highest accuracies on task of abstract word ordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Shallow Graph Linearization</head><p>Our transition based graph linearization system extends from <ref type="bibr" target="#b24">Puduppully et al. (2016)</ref>. In our case, the input is a shallow graph instead of a syntactic tree, and hence the search space is larger. On the other hand, the same set of actions can still be applied, with additional constraints on valid actions given each configuration (Section 3.2.3). Table 3 shows the sequence of transition actions to linearize shallow graph in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Obtaining Possible Transition Actions Given a Configuration</head><p>The purpose of a GETPOSSIBLEACTIONS function is to find out the set of transition actions that can lead to a valid output given a certain state. This is because not all sequences of actions corre-  Algorithm 3: SHIFTSUBTREE</p><formula xml:id="formula_1">1 T ? ? 2 if s.? == ? then 3 for k ? s.? do 4 T ? T ? (SHIFT, P OS, k) 5 else 6 if ?k, k ? (DIRECTCHILDREN(i) ? s.?) then 7 SHIFTSUBTREE(i, ?) 8 else 9 if A.LEFTCHILD(i) is NIL then 10 SHIFTSUBTREE(i, ?) 11 if {j ? i} ? C? A.LEFTCHILD(j) is NIL then 12 T ? T ? (RIGHTARC) 13 if i ? DESCENDANT(j) then 14 PROCESSDESCENDANT(i, j) 15 if i ? SIBLING(j) then 16 PROCESSSIBLING(i, j) 17 else if {j ? i} ? C then 18 T ? T ? (LEFTARC) 19 if i ? SIBLING(j) then 20 PROCESSSIBLING(i, j) 21 else 22 if size(s.?) == 1 then 23 SHIFTPARENTANDSIBLINGS(i) 24 else 25 if i ? DESCENDANT(j) then 26 PROCESSDESCENDANT(i, j) 27 if i ? SIBLING(j) then 28 PROCESSSIBLING(i, j) 29 return T Algorithm 2: DIRECTCHILDREN Input: A state s=([?|j i], ?, A), input node and graph C. Output: DC direct child nodes of input node 1 DC ? ? 2 for k ? (C.CHILDREN(input node)) do 3 Parents ? C.PARENTS(k) 4 if Parents.size == 1 then 5 DC ? DC ? k 6 else 7 for m ? Parents do 8 if A.LEFTCHILD(m) is not NIL ? m == input</formula><formula xml:id="formula_2">Input: A state s = ([?|j i], ?, A), graph C, head k Output: a set of possible Transition actions T 1 T ? ? 2 T ? T ? (SHIFT, POS, k) 3 queue q 4 q.push(k) 5 while q is not empty do 6 front = q.pop() 7 for m ? (C.CHILDREN(front) ? s.?) do 8 q.push(m) 9 T ? T ? (SHIFT, POS, m)</formula><p>spond to a well-formed output. Essentially, given a state s = ([?|j i], ?, A) and an input graph C, the Decoder extracts syntactic tree from the graph (cf. <ref type="figure" target="#fig_3">Figure 4</ref>  In particular, if node i has direct child nodes in C, the descendants of i are shifted (line 6-7) (see Algorithm 3). Here direct child nodes (see Algorithm 2) include those child nodes of i for which i is the only parent or if there is more than one parent then every other parent is shifted on to the stack without possibility to reduce the child node. If no direct child node is in the buffer, then all graph descendants of i are shifted. Now, there are three configurations possible between i and j: 1. i and j are directly connected in C. This results in RIGHTARC or LEFTARC action; 2. i is descendant of j. In this case the parents of i (such that they are descendants of j) and siblings of i through such parents are shifted. 3. i is sibling of j. In this case, parents of i and their descendants are shifted such that A remains consistent. Because the input is a graph, more than one of the above configuration can occur simultaneously. More detailed discussion related to GETPOSSIBLEACTIONS is given in Appendix A. Unigrams S0w; S0p; S 0,l w; S 0,l p; S 0,l l; S0,rw; S0,rp; S0,rl; Bigram S0wS 0,l w; S0wS 0,l p; S0wS 0,l l; S0pS 0,l w; Linearization w0; p0; w?1w0; p?1p0; w?2w?1w0; p?2p?1p0  .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Feature Templates</head><p>There are three sets of features. The first is the set of baseline linearization feature templates from <ref type="table">Table 2</ref> in , partly shown in <ref type="table" target="#tab_2">Table  4</ref>. The second is a set of lookahead features similar to that of <ref type="bibr" target="#b24">Puduppully et al. (2016)</ref>, shown in Table 5. 1 Parent lookahead feature in <ref type="bibr" target="#b24">Puduppully et al. (2016)</ref> is defined for the only parent. For graph linearization, however, the parent lookahead feature need to be defined for set of parents. The third set of features in <ref type="table">Table 6</ref> are newly introduced for Graph Linearization. Arc left is a binary feature indicating if there is left arc between S 0 and S 1 , whereas Arc right indicates if there is a right arc. L is descendant is a binary feature indicating if L is descendant of S 0 , and L is parent or sibling indicates if it is a parent or sibling of S 0 . S 0descendants shifted is binary feature indicating if all the descendants of S 0 are shifted.</p><p>Not having POS in the input dataset, we compute the feature templates for POS making use of the most frequent POS of the lemma in the gold training data. For the features with dependency labels, we use the input graph labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Search and Learning</head><p>We follow <ref type="bibr" target="#b24">Puduppully et al. (2016)</ref> and , applying the learning and search framework of <ref type="bibr" target="#b36">Zhang and Clark (2011)</ref>. Pseudocode is shown in Algorithm 4. It performs beam search holding k best states in an agenda at each incremental step. At the start of decoding, agenda holds the initial state. At a step, for each state in the set of label and POS of child nodes of L L cls ; L clns ; Lcps; Lcpns; S0wL cls ; S0pL cls ; S1wL cls ; S1pL cls ; set of label and POS of first-level siblings of L L sls ; L slns ; Lsps; Lspns; S0wL sls ; S0pL sls ; S1wL sls ; S1pL sls ; set of label and POS of parents of L L pls ; L plns ; Lpps; Lppns; S0wL pls ; S0pL pls ; S1wL pls ; S1pL pls ; <ref type="table">Table 5</ref>: Lookahead linearization feature templates for the word L to shift. A subset is shown here. For the full feature set, refer to <ref type="table">Table 2</ref> of <ref type="bibr" target="#b24">Puduppully et al. (2016)</ref>. An identical set of feature templates are defined for S 0 . arc features between S0 and S1 Arc left ; Arc right ; lookahead features for L L is descendant ; L is parent or sibling ; are all descendants of S0 shifted S 0descendants shifted ; feature combination S 0descendants shifted Arc left ; S 0descendants shifted Arc right ; S 0descendants shifted L is descendant ; S 0descendants shifted L is parent or sibling ; <ref type="table">Table 6</ref>: Graph linearization feature templates agenda, each of transition actions in GETPOSSI-BLEACTIONS is applied. The top-k states are updated in the agenda for the next step. The process repeats for 2n steps as each word needs to be shifted once on to the stack and reduced once. After 2n steps, the highest scoring state in agenda is taken as the output. The complexity of algorithm is n 2 , as it takes 2n steps to complete and during each step, the number of transition actions is proportional to ?. Given a configuration C, the score of a possible action a is calculated as:</p><formula xml:id="formula_3">Score(a) = ? ? ?(C, a),</formula><p>where ? is the model parameter vector and ?(C, a) denotes a feature vector consisting of configuration and action components. Given a set of labeled training examples, the averaged perceptron with early update <ref type="bibr">(Collins and Roark, 2004</ref>) is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Morphological Generation</head><p>The last step is to inflate the lemmas in the sentence. There are three POS categories, including nouns, verbs and articles, for which we need to generate morphological forms. We use Wiktionary 2 as a basis and write a small set of rules 2 https://en.wiktionary.org/  similar to that used in , listed in <ref type="table" target="#tab_4">Table 7</ref>, to generate a candidate set of inflections. An averaged perceptron classifier <ref type="bibr" target="#b7">(Collins, 2002)</ref> is trained for each lemma. For distinguishing between singular and plural candidate verb forms, the feature templates in <ref type="table" target="#tab_5">Table 8</ref> are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Joint Method</head><p>We design a joint method for function word prediction (Section 3.1), linearization (Section 3.2) and morphological generation (Section 3.3) by further extending the transition-based system of Section 3.2, integrating actions for function word prediction and morphological generation. n -word index; Functions: WORD -word at index n; COUNT -word at n is singular or plural form; SUBJ -word at subject of n; COUNT SUBJ -word at subject of n is singular or plural form.</p><p>think to have C-A1 think have C-A1 INF <ref type="figure">Figure 5</ref>: Example for SPLITARC-to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transition Actions</head><p>In addition to SHIFT, LEFTARC and RIGHTARC in Section 3.2.1, we use the following new transition actions for inserting function words:</p><p>? INSERT, inserts comma at the present position; ? SPLITARC-Word, splits an arc in the input graph C, inserting a function word between the words connected by the arc. Here Word specifies the function word being inserted ( <ref type="figure">Figure 5</ref>). We generate a candidate set of inflections for each lemma following the approach in Section 3.3. For each candidate inflection, we generate a corresponding SHIFT transition action. The rules in <ref type="table" target="#tab_4">Table 7</ref> are used to prune impossible inflections. 3 <ref type="table" target="#tab_7">Table 9</ref> shows the transition actions to linearize the graph in <ref type="figure">Figure 2</ref>. These newly introduced transition actions result in variability in the number of transition actions. With function word prediction, the number of transition actions for a bag of n words is not necessarily 2n-1. For example, considering an INSERT, SPLITARC-to or SPLITARC-that action post each SHIFT action, the maximum number of possible actions is 5n-1. This variance in the number of actions can impact the linear separability of state items. Following <ref type="bibr" target="#b40">Zhu et al. (2013)</ref>, we use IDLE actions as a form of padding method, which results in completed state items being further expanded up to 5n-1 steps. The joint model uses the same perceptron training al-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Obtaining Possible Transition Actions Given a Configuration</head><p>Given a state s = ([?|j i], ?, A) and an input graph C, the possible transition actions include as a subset the transition actions in Algorithm 1 for shallow graph linearization. In addition, for each lemma being shifted, we enumerate its inflections and create SHIFT transition actions for each inflection. Further, we predict SPLITARC, INSERT and IDLE actions to handle function words. If node i has a child node in C, which is not shifted, we predict SPLITARC and INSERT. If i is sibling to j, we predict INSERT. If both the stack and buffer are empty, we predict IDLE. Pseudocode for GET-POSSIBLEACTIONS for the joint method is shown in Algorithm 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>We work on the deep dataset from the Surface Realisation Shared Task <ref type="bibr">(Belz et al., 2011) 4</ref> . Sentences are represented as sets of unordered nodes with labeled semantic edges between them. Semantic representation is obtained by merging Nombank <ref type="bibr" target="#b19">(Meyers et al., 2004)</ref>, Propbank <ref type="bibr" target="#b22">(Palmer et al., 2005)</ref>  that complementizer, to infinitive and commas are omitted from the input. There are two punctuation features for information about brackets and quotes. <ref type="table">Table 10</ref> shows a sample training instance.</p><p>Out of 39k total training instances, 2.8k are non-projective, which we discard. We exclude instances which result in non-projective dependencies mainly because our transition actions predict only projective dependencies. It has been derived from the arc-standard system <ref type="bibr" target="#b21">(Nivre, 2008)</ref>. There are 1.8k training instances with a mismatch be-Input (unordered lemma-formed graph) <ref type="table" target="#tab_2">:  Sem  ID  PID  Lemma  Attr  Lexeme  SROOT  1  0  be  tense=pres  are  ADV  2  1  meanwhile  meanwhile  P  3  1  .  .  SBJ  4  1  start.02  num=pl  starts  A1  5  4  housing  num=sg  housing  AM-TMP  6  4  september  num=sg  september  VC  9  1  think.01  partic=past  thought  A1  4  9  C-A1  10  9  have  have  VC  11  10  inch.01  partic=past  inched  A1  4  11  A5  12  11  upward  upward   Table 10</ref>: Deep type training instance from Surface Realisation Shared Task 2011. Sem -semantic label, ID -unique ID of node within graph, PID -the ID of the parent, Attr -Attributes such as partic (participle), tense or number, Lexeme -lexeme which is resolved using wiktionary and rules in <ref type="table" target="#tab_4">Table 7</ref>.</p><p>tween edges in the input deep graph and gold output tree. The gold output tree is the corresponding shallow tree from the shared task. We approach the task of linearization as extracting a linearized tree from the input semantic graph. So we exclude those instances which do not have edges corresponding to gold tree i.e mismatch between edges of gold tree and input graph. After excluding these instances, we have 34.3k training instances. We also exclude 800 training instances where the function words to and that have more than one child, and around 100 training instances where function words' parent and child nodes are not connected by an arc in the deep graph. The above cases are deemed annotation mistakes. We thus train on a final subset of 33.4k training instances. The development set comprises 1034 instances and the test set comprises 2398 instances. Evaluation is done using the BLEU metric <ref type="bibr" target="#b23">(Papineni et al., 2002)</ref>.</p><p>6 Development Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Influence of Beam Size</head><p>We study the effect of beam size on the accuracies of joint model in <ref type="figure">Figure 6,</ref>   <ref type="table">Table 11</ref>: Average F-measure for function word prediction for development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Pipeline vs Joint Model</head><p>We compare the results of the joint model with the pipeline baseline system. <ref type="table">Table 11</ref> shows the development results of function word prediction, and <ref type="table" target="#tab_10">Table 12</ref> shows the overall development results. Our joint model of Transition-Based Deep Input Linearization (TBDIL) achieves an improvement of 5 BLEU points over the pipeline using the same feature source and training algorithm. Thanks to the sharing of word order information, the joint model improves function word prediction compared to the pipeline, which forbids such feature integration because function word prediction is the first step, taken before order becomes available. <ref type="table" target="#tab_11">Table 13</ref> shows the final results. The best performing system for the Shared Task was STUMABA-D by <ref type="bibr">Bohnet et al. (2011)</ref>, which leverages a largescale n-gram language model. The joint model TBDIL significantly outperforms the pipeline system and achieves an improvement of 1 BLEU point over STUMABA-D, obtaining 80.49 BLEU without making use of external resources. <ref type="table" target="#tab_2">Table 14</ref> shows sample outputs from the Pipeline system and the corresponding output from TBDIL. In the first instance, the function word to is incorrectly predicted in the arc between nodes does and yield in the pipeline system. In case of TBDIL, the n-gram feature helps avoid incorrect insertion of to which demonstrates the advantage of integrating information across stages. In the second   if it does n't yield on these matters and eventually begin talking directly to the anc Pipeline if it does not to yield on these matters and eventually begin talking directly to the anc TBDIL if it does n't yield on these matters and eventually begin talking directly to the anc ref.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Final Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Analysis</head><p>economists who read september 's low level of factory job growth as a sign of a slowdown Pipeline september 's low level of factory job growth who as a sign of a slowdown reads economists TBDIL economists who read september 's low level of factory job growth as a sign of a slowdown instance, because of incorrect linearization, there is error propagation to morphological generation in the pipeline system. In particular, economists is linearized to the object part of the sentence and the subject is singular. This, in turn, results in the incorrect prediction of morphological form of verb read as its singular variant. In TBDIL, in contrast, the joint modelling of linearization and morphology helps ordering the sentence correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We showed the usefulness of a joint model for the task of Deep Linearization, by taking <ref type="bibr" target="#b24">(Puduppully et al., 2016)</ref> as the baseline and extending it to perform joint graph linearization, function word prediction and morphological generation. To our knowledge, this is the first work to use Transition-Based method for joint NLG from semantic structure. Our system gave the highest scores reported for the NLG 2011 shared task on Deep Input Linearization <ref type="bibr">(Belz et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Obtaining possible transition actions given a configuration for Shallow Graph</head><p>During shallow linearization, a state is represented by s = ([?|j i], ?, A) and C is the input graph. Given C, the Decoder outputs actions which extract syntactic tree from the graph. Thus the Decoder outputs RIGHTARC or LEFTARC only if corresponding arc exists in C. The detailed pseudocode is given in Algorithm 1. If i has direct child nodes in C, the descendants of i are shifted (line 6-7) (see Algorithm 3). Here, direct child nodes (see Algorithm 2) include those child nodes of i for which i is the only parent or if there is more than one parent then every other parent is shifted on to the stack without possibility to reduce the child node. If no direct child node is in buffer, then descendants of i are shifted (line 9-10). Now, there are three configurations possible between i and j: 1. i and j are connected by arc in C. This results in RIGHTARC or LEFTARC action; 2. i is descendant of j. In this case the parents of i (such that they are descendants of j) and siblings of i through such parents are shifted. 3. i is sibling of j. In this case, the parents of i and their descendants are shifted such that A remains consistent. Additionally, because the input is a graph structure, more than one of the above configuration can occur simultaneously. We analyse the three configurations in detail below.</p><p>Since the direct child nodes of i are shifted, {j ? i} results in a LEFTARC action (line 18). Also because the input is a graph, i can be a sibling node of j. In this case, the valid parents and siblings of i are shifted. We iterate through the other elements in stack to identify the valid parents and siblings. These conditions are encapsulated in PROCESSSIBLING (line 20). Conditions for RIGHTARC are similar to that of LEFTARC with the following differences. We ensure that there is no left arc relationship for j in A (line 11). If there is a left arc relationship for j in A, it means that in an arc-standard setting, the RIGHTARC actions for j have already been made. If i is a descendant of j, valid parents and siblings of i are shifted. We iterate through the parents of i and those parents which are in turn descendants of j and not shifted on to the stack are valid parents. We shift the parent and the subtree through each such parent. These conditions are denoted by PROCESS-DESCENDANT (line 14). If there is no arc between j and i and there is only one element on the stack, then the parents and siblings of i are shifted (line 22-23). If there is more than one element on the stack, and if i is descendant of j, then we use PROCESSDESCEN-DANT (line 25-26). If i is sibling to j we use PRO-CESSSIBLING (line 27-28).</p><p>Consider an example to see the working of PROCESSSIBLING in detail. In PROCESSSIB-LING, we need to ensure that i is in stack because of sibling relation with j and we need to shift the valid parent nodes of i and their descendants. We call these valid nodes inflection points. Consider the following stack entries [D, A, B, C] with C as stack top. Assume that the input graph is as in <ref type="figure" target="#fig_6">Figure 7</ref>. C is sibling of B through B's parents X 11 , X 12 , X 13 . Out of these, only X 11 and X 12 are valid parents. X 13 is sibling to A through A's parent X 23 . But X 23 is in turn neither descendant of D nor sibling of D. Thus X 13 is not a valid inflection point for C. Now, X 12 is sibling of A through A's parent X 22 . X 22 is in turn sibling of D through X 32 . Thus there is a path to the stack bottom through a path of siblings/ descendant. In case of X 11 , X 11 is descendant of stack element A and is thus valid. X 11 and X 12 are called valid inflection points. If inflection point is a common parent to both S 0 and S 1 then inflection point and its descendants are shifted. Instead, if inflection point is ancestor to S 0 , then parents of S 0 (say P 0 ) which are descendants of inflection point are shifted. Additionally, descendants of P 0 are shifted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Linearization pipelines (a) NLG pipeline with deep input graph, (b) pipeline based on the meaning text theory, (c) this paper. Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by their lemmas. The function word to and comma are absent in graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Equivalent shallow graph for Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>?? ? S 0 and pops out second element from top of stack S 1 ; ? RIGHTARC-LABEL -constructs dependency arc S 1 LABEL ? ???? ? S 0 and pops out top of stack S 0 . The sequence of actions to linearize the set {he,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Equivalent syntactic tree for Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>extracted from Figure 3), outputting RIGHTARC, LEFTARC only if the corresponding arc exists in C. The corresponding pseudocode is shown in Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>and syntactic dependencies. Edge labeling follows PropBank annotation scheme such as {A0, A1, ... An}. The nodes are annotated with lemma and where appropriate number, tense and participle features. Function words including Algorithm 5: GETPOSSIBLEACTIONS for deep graph linearization, where C is a input graph Input: A state s = ([?|j i], ?, A) and graph C Output: A set of possible transition actions T 1 T ? ? 2 if s.? == ? then 3 for k ? s.? do 4 T ? T ? (SHIFT, P OS, k) C.Children(i) ?s.? = ? then 30 T ? T ? (SPLITARC ? to) 31 T ? T ? (SPLITARC ? that) 32 if C.Children(i) ?s.? = ? ? i ? SIBLING(j) then 33 T ? T ? (INSERT) 34 if s.? == ? ? s.? == ? then 35 T ? T ? (IDLE) 36 return T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Sample graph to illustrate PROCESSSI-BLING</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1: GETPOSSIBLEACTIONS for shallow graph linearizationInput: A state s = ([?|j i], ?, A) and input graph C Output: A set of possible transition actions T</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Baseline linearization feature templates. A subset is shown here. For the full feature set, refer toTable 2 of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 4: transition-based linearization Input: C, a set of input syntactic constraints Output: The highest-scored final state 1 candidates ? ([ ], set(1..n), ?) 2 agenda ? ? 3 for i ? 1..2n do</figDesc><table><row><cell>4</cell><cell>for s in candidates do</cell></row><row><cell>5</cell><cell>for action in GETPOSSIBLEACTIONS(s, C)</cell></row><row><cell></cell><cell>do</cell></row><row><cell>6</cell><cell>agenda ? APPLY(s, action)</cell></row><row><cell>7</cell><cell>candidates ? TOP-K(agenda)</cell></row><row><cell>8</cell><cell>agenda ? ?</cell></row><row><cell cols="2">9 best ? BEST(candidates)</cell></row><row><cell cols="2">10 return best</cell></row><row><cell></cell><cell>Rules for be</cell></row><row><cell></cell><cell>attr['partic'] == 'pres' ? being</cell></row><row><cell></cell><cell>attr['partic'] == 'past' ? been</cell></row><row><cell></cell><cell>attr['tense'] == 'past'</cell></row><row><cell></cell><cell>sbj.attr['num'] == 'sg' ? was</cell></row><row><cell></cell><cell>sbj.attr['num'] == 'pl' ? were</cell></row><row><cell></cell><cell>other ? [was,were]</cell></row><row><cell></cell><cell>attr['tense'] == 'pres'</cell></row></table><note>sbj.attr['num'] == 'sg' ? is sbj.attr['num'] == 'pl' ? are other ? [am,is,are] Rules for other verbs attr['partic'] == 'pres' ? wik.get(lemma, VBG) attr['partic'] == 'past' ? wik.get(lemma, VBN ) attr['tense'] == 'past' ? wik.get(lemma, VBD) attr['tense'] == 'pres' sbj.attr['num'] == 'sg' ? wik.get(lemma, VBZ ) other ? wik.getall(lemma) Rules for other types lemma==a ? [a,an] lemma==not ? [not,n't] attr['num'] == 'sg' ? wik.get(lemma,NNP/NN) attr['num'] == 'pl' ? wik.get(lemma,NNPS/NNS)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Lemma rules. All rules are in the format: conditions ? candidate inflections. Nested condi- tions are listed in multi-lines with indentation. wik denotes english wiktionary.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Feature templates for predicting singular/ plural verb forms. Indices on the surface string:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: Transition action sequence for lineariz-</cell></row><row><cell>ing the sentence in Figure 2. SH -SHIFT, SP -</cell></row><row><cell>SPLITARC, RA -RIGHTARC, LA -LEFTARC, IN</cell></row><row><cell>-INSERT. POS is not shown in SHIFT actions.</cell></row><row><cell>gorithm and similar features compared to the base-</cell></row><row><cell>line model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 :</head><label>12</label><figDesc>Development results.</figDesc><table><row><cell>System</cell><cell>BLEU Score</cell></row><row><cell>STUMABA-D</cell><cell>79.43</cell></row><row><cell>Pipeline</cell><cell>70.99</cell></row><row><cell>TBDIL</cell><cell>80.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 :</head><label>13</label><figDesc>Test results.</figDesc><table /><note>output ref.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 :</head><label>14</label><figDesc>Example outputs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here L cls represents set of arc labels of child nodes (of word to shift L) shifted on the stack, L clns represents set of arc labels of child nodes not shifted on the stack, Lcps the POS set of shifted child nodes, Lcpns the POS set of unshifted child nodes, L sls the set of arc labels of shifted siblings, L slns the set of arc labels of unshifted siblings, Lsps the POS set of shifted siblings, Lcpns the POS set of unshifted siblings, L pls the set of arc labels of shifted parents, L plns the set of arc labels of unshifted parents, Lpps the POS set of shifted parents, Lppns the POS set of unshifted parents.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For example inFigure 2, price is the subject of be and if be is in present tense and price is in plural form, the inflections {am, is, was, were} are impossible and are is the correct inflection for be. We therefore generate transition action as SHIFT-are.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.nltg.brighton.ac.uk/research/sr-task/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Litton Kurisinkel for helpful discussions and the anonymous reviewers for their detailed and constructive comments. Yue Zhang is supported by the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Features for predicting singular/ plural verb forms WORD(n-1)WORD(n-2)WORD(n-3)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Count</forename><surname>Subj</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">SUBJ(n)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">WORD(n+1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneider</surname></persName>
		</author>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, chapter Abstract Meaning Representation for Sembanking</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse, chapter Abstract Meaning Representation for Sembanking</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Whittaker</surname></persName>
		</author>
		<title level="m">Proceedings of the First International Conference on Natural Language Generation, chapter Evaluation Metrics for Generation</title>
		<meeting>the First International Conference on Natural Language Generation, chapter Evaluation Metrics for Generation</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentence fusion for multidocument news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="328" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<title level="m">chapter Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2002 Conference on EMNLP</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Word ordering with phrase-based grammars. 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tomalin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypertagging: Supertagging for surface realization with ccg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Mehay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="183" to="191" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL-08: HLT</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint parsing and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Jenny Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="326" to="334" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Rudnicky</surname></persName>
		</author>
		<title level="m">Workshop: Conversational Systems, chapter Stochastic Language Generation for Spoken Dialogue Systems</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generation that exploits corpus-based statistical knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Langkilde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 17th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic grammar correction for second-language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Seneff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1978" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An empirical comparison between n-gram and syntactic language models for word ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on EMNLP</title>
		<meeting>the 2015 Conference on EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transition-based syntactic linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-31" />
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dependency Syntax: theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mel?uk</forename><surname>Igor Aleksandrovi?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>SUNY press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semantics: From meaning to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mel?uk</forename><surname>Igor Aleksandrovi?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>John Benjamins Publishing Company</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Annotating noun argument structure for nombank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deterministic dependency parsing of english text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Scholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics, page 64. Association for Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics, page 64. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The proposition bank: An annotated corpus of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transition-based syntactic linearization with lookahead features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="488" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A transition-based model for joint segmentation, pos-tagging and normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on EMNLP</title>
		<meeting>the 2015 Conference on EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1837" to="1846" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Schmaltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shieber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.08633</idno>
		<title level="m">Word ordering without syntax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint morphological generation and syntactic linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1522" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Rohanimanesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="693" to="723" />
			<date type="published" when="2007-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Velldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<title level="m">Proceedings of the 2006 Conference on EMNLP, chapter Statistical Ranking in Tactical Generation</title>
		<meeting>the 2006 Conference on EMNLP, chapter Statistical Ranking in Tactical Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="517" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?cile</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009)</title>
		<meeting>the 12th Conference of the European Chapter of the ACL (EACL 2009)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="852" to="860" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Perceptron reranking for ccg realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on EMNLP</title>
		<meeting>the 2009 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reining in ccg chart realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Generation</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="182" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient realization of coordinate structures in combinatory categorial grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="75" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A fast decoder for joint word segmentation and pos-tagging using a single discriminative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on EMNLP</title>
		<meeting>the 2010 Conference on EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discriminative syntax-based word ordering for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="503" to="538" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Syntactic smt using a discriminative text generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on EMNLP</title>
		<meeting>the 2014 Conference on EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="177" to="182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Partial-tree linearization: generalized word ordering for text synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third international joint conference on Artificial Intelligence</title>
		<meeting>the Twenty-Third international joint conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2232" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
