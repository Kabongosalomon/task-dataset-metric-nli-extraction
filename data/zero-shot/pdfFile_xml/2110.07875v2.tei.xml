<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPH NEURAL NETWORKS WITH LEARNABLE STRUCTURAL AND POSITIONAL REPRESENTATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Prakash</forename><surname>Dwivedi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
							<email>tlaurent@lmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Loyola Marymount University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<email>yoshua.bengio@mila.quebec</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Montr?al</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">CIFAR</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
							<email>xavier@nus.edu.sg</email>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRAPH NEURAL NETWORKS WITH LEARNABLE STRUCTURAL AND POSITIONAL REPRESENTATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>GNNs have recently emerged as a powerful class of deep learning architectures to analyze datasets where information is present in the form of heteregeneous graphs that encode complex data connectivity. Experimentally, these architectures have shown great promises to be impactful in diverse domains such as drug design <ref type="bibr" target="#b57">(Stokes et al., 2020;</ref><ref type="bibr" target="#b20">Gaudelet et al., 2020)</ref>, social networks <ref type="bibr" target="#b46">(Monti et al., 2019;</ref><ref type="bibr" target="#b51">Pal et al., 2020)</ref>, traffic networks <ref type="bibr" target="#b15">(Derrow-Pinion et al., 2021)</ref>, physics <ref type="bibr" target="#b12">(Cranmer et al., 2019;</ref><ref type="bibr" target="#b2">Bapst et al., 2020)</ref>, combinatorial optimization <ref type="bibr" target="#b5">(Bengio et al., 2021;</ref><ref type="bibr" target="#b9">Cappart et al., 2021)</ref> and medical diagnosis <ref type="bibr" target="#b40">(Li et al., 2020c)</ref>.</p><p>Most GNNs (such as <ref type="bibr" target="#b14">Defferrard et al. (2016)</ref>; <ref type="bibr" target="#b58">Sukhbaatar et al. (2016)</ref>; <ref type="bibr" target="#b31">Kipf &amp; Welling (2017)</ref>; <ref type="bibr" target="#b22">Hamilton et al. (2017)</ref>; <ref type="bibr" target="#b45">Monti et al. (2017)</ref>; <ref type="bibr" target="#b8">Bresson &amp; Laurent (2017)</ref>; <ref type="bibr" target="#b61">Veli?kovi? et al. (2018)</ref>; ) are designed with a message-passing mechanism <ref type="bibr" target="#b21">(Gilmer et al., 2017</ref>) that builds node representation by aggregating local neighborhood information. It means that this class of GNNs is fundamentally structural, i.e. the node representation only depends on the local structure of the graph. As such, two atoms in a molecule with the same neighborhood are expected to have similar representation. However, it can be limiting to have the same representation for these two atoms as their positions in the molecule are distinct, and their role may be specifically separate <ref type="bibr" target="#b49">(Murphy et al., 2019)</ref>. As a consequence, the popular message-passing GNNs (MP-GNNs) fail to differentiate two nodes with the same 1-hop local structure. This restriction is now properly understood in the context of the equivalence of MP-GNNs with Weisfeiler-Leman (WL) test <ref type="bibr" target="#b64">(Weisfeiler &amp; Leman, 1968)</ref> for graph isomorphism <ref type="bibr" target="#b47">Morris et al., 2019)</ref>. The said limitation can be alleviated, to certain extents, by (i) stacking multiple layers, (ii) applying higher-order GNNs, or (iii) considering positional encoding (PE) of nodes (and edges). Let us assume two structurally identical nodes in a graph with the same 1-hop neighborhood, but different with respect to 2-hop or higher-order neighborhoods. Then, stacking several layers <ref type="bibr" target="#b8">(Bresson &amp; Laurent, 2017;</ref> can propagate the information from a node to multiple hops, and thus differentiate the representation of two far-away nodes. However, this solution can be deficient for long-distance nodes because of the over-squashing phenomenon <ref type="bibr" target="#b0">(Alon &amp; Yahav, 2020)</ref>. Another approach is to compute higher-order node-tuple aggregations such as in WL-based GNNs <ref type="bibr" target="#b43">(Maron et al., 2019;</ref><ref type="bibr" target="#b10">Chen et al., 2019)</ref>; though these models are computationally more expensive to scale than MP-GNNs, even for medium-sized graphs <ref type="bibr" target="#b18">(Dwivedi et al., 2020)</ref>. An alternative technique is to consider a global positioning of the nodes in the graph that can encode a graph-based distance between the nodes <ref type="bibr" target="#b68">(You et al., 2019;</ref><ref type="bibr" target="#b18">Dwivedi et al., 2020;</ref><ref type="bibr" target="#b39">Li et al., 2020b;</ref><ref type="bibr" target="#b17">Dwivedi &amp; Bresson, 2021)</ref>, or can inform about specific sub-structures <ref type="bibr" target="#b7">(Bouritsas et al., 2020;</ref><ref type="bibr" target="#b6">Bodnar et al., 2021)</ref>.</p><p>Contribution. In this work, we turn to the idea of learning positional representation that can be combined with structural GNNs to generate more expressive node embedding. Our main intent is to alleviate the lack of canonical positioning of nodes in arbitrary graphs to improve the representation power of MP-GNNs, while keeping their linear complexity for large-scale applications. For this objective, we propose a novel framework, illustrated with <ref type="figure" target="#fig_0">Figure 1</ref>, that enables GNNs to learn both structural and positional representations at the same time (thus named MPGNNs-LSPE). Alongside, we present a random-walk diffusion based positional encoding scheme to initialize the positional representations of the nodes. We show that the proposed architecture with learnable PE can be used with any graph network that fits to the MP-GNNs framework, and improves its performance (1.79% to 64.14%). In our demonstrations, we formulate LSPE instances of both sparse GNNs, such as GatedGCNs <ref type="bibr" target="#b8">(Bresson &amp; Laurent, 2017)</ref> and PNA <ref type="bibr" target="#b11">(Corso et al., 2020)</ref> and fully-connected Transformers-based GNNs <ref type="bibr" target="#b32">(Kreuzer et al., 2021;</ref><ref type="bibr" target="#b44">Mialon et al., 2021)</ref>. Our numerical experiments on three standard molecular benchmarks show that different instantiations of MP-GNNs with LSPE surpass the previous state-of-the-art (SOTA) on one dataset by a considerable margin (26.23%), while achieving SOTA-comparable score on the other two datasets. The architecture also shows consistent improvements on three non-molecular benchmarks. In addition, our evaluations find the sparse MP-GNNs to be outperforming fully-connected GNNs, hence suggesting greater potential towards the development of highly efficient, yet powerful architectures for graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review briefly the three research directions theoretical expressivity of GNNs, graph positional encoding, and Transformer-based GNNs.</p><p>Theoretical expressivity and Weisfeiler-Leman GNNs. As the theoretical expressiveness of MP-GNNs is bounded by the 1-WL test <ref type="bibr" target="#b47">Morris et al., 2019)</ref>, they may perform poorly on graphs that exhibit several symmetries <ref type="bibr" target="#b49">(Murphy et al., 2019)</ref>, and additionally some message-passing functions may not be discriminative enough <ref type="bibr" target="#b11">(Corso et al., 2020)</ref>. To this end, k-order Equivariant-GNNs were introduced in Maron et al. <ref type="bibr">(2018)</ref> requiring O(n k ) memory and speed complexities. Although the complexity was improved to O(n 2 ) memory and O(n 3 ) respectively <ref type="bibr" target="#b43">(Maron et al., 2019;</ref><ref type="bibr" target="#b10">Chen et al., 2019;</ref><ref type="bibr" target="#b1">Azizian &amp; Lelarge, 2020)</ref>, it is still inefficient compared with the linear complexity of MP-GNNs.</p><p>Graph Positional Encoding. The idea of positional encoding, i.e. the notion of global position of pixels in images, words in texts and nodes in graphs, plays a central role in the effectiveness of the most prominent neural networks with <ref type="bibr">ConvNets (LeCun et al., 1998)</ref>, RNNs <ref type="bibr" target="#b23">(Hochreiter &amp; Schmidhuber, 1997)</ref>, and Transformers <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref>. For GNNs, the position of nodes is more challenging due to the fact that there does not exist a canonical positioning of nodes in arbitrary graphs. Despite these issues, graph positional encoding are as much critical for GNNs as they are for ConvNets, RNNs and Transformers, as demonstrated for prediction tasks on graphs <ref type="bibr" target="#b13">Cui et al., 2021)</ref>. Nodes in a graph can be assigned index positional encoding (PE). However, such a model must be trained with the n! possible index permutations or else sampling needs to be done <ref type="bibr" target="#b49">(Murphy et al., 2019)</ref>. Another PE candidate for graphs can be Laplacian Eigenvectors <ref type="bibr" target="#b18">(Dwivedi et al., 2020;</ref><ref type="bibr" target="#b17">Dwivedi &amp; Bresson, 2021)</ref> as they form a meaningful local coordinate system, while preserving the global graph structure. However, there exists sign ambiguity in such PE as eigenvectors are defined up to ?1, leading to 2 k number of possible sign values when selecting k eigenvectors which a network needs to learn. Similarly, the eigenvectors may be unstable due to eigenvalue multiplicities. <ref type="bibr" target="#b68">You et al. (2019)</ref> proposed learnable position-aware embeddings based on random anchor sets of nodes, where the random selection of anchors has its limitations, which makes their approach less generalizable on inductive tasks. There also exists methods that encode prior information about a class of graphs of interest such as rings for molecules <ref type="bibr" target="#b7">(Bouritsas et al., 2020;</ref><ref type="bibr" target="#b6">Bodnar et al., 2021)</ref> which make MP-GNNs more expressive. But the prior information regarding graph sub-structures needs to be pre-computed, and sub-graph matching and counting require O(n k ) for k-tuple sub-structure.</p><p>Transformer-based GNNs. Although sparse MP-GNNs are very efficient, they are susceptible to the information bottleneck limitation <ref type="bibr" target="#b0">(Alon &amp; Yahav, 2020)</ref> in addition to vanishing gradient (similar to RNNs) on tasks when long-range interactions between far away nodes are critical. To overcome these limitations, there have been recent works that generalize Transformers to graphs <ref type="bibr" target="#b17">(Dwivedi &amp; Bresson, 2021;</ref><ref type="bibr" target="#b32">Kreuzer et al., 2021;</ref><ref type="bibr" target="#b67">Ying et al., 2021;</ref><ref type="bibr" target="#b44">Mialon et al., 2021)</ref> which alleviates the long-range issue as 'everything is connected to everything'. However, these methods either use non-learnable PEs to encode graph structure information <ref type="bibr" target="#b17">(Dwivedi &amp; Bresson, 2021;</ref><ref type="bibr" target="#b67">Ying et al., 2021;</ref><ref type="bibr" target="#b44">Mialon et al., 2021)</ref>, or inject learned PEs to the Transformer network that relies on Laplacian eigenvectors <ref type="bibr" target="#b32">(Kreuzer et al., 2021)</ref>, thus inheriting the sign ambiguity limitation.</p><p>A detailed review of the above research directions is available in the supplementary Section B. We attempt to address some of the major limitations of GNNs by proposing a novel architecture with consistent performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED ARCHITECTURE</head><p>In this work, we decouple structural and positional representations to make it easy for the network to learn these two critical characteristics. This is in contrast with most existing architectures s.a. <ref type="bibr" target="#b17">Dwivedi &amp; Bresson (2021)</ref>; <ref type="bibr" target="#b3">Beani et al. (2021)</ref>; <ref type="bibr" target="#b32">Kreuzer et al. (2021)</ref> that inject the positional information into the input layer of the GNNs, and <ref type="bibr" target="#b68">You et al. (2019)</ref> that rely on distance-measured anchor sets of nodes limiting general, inductive usage. Given the recent theoretical results on the importance of informative graph PE for expressive GNNs <ref type="bibr" target="#b49">(Murphy et al., 2019;</ref><ref type="bibr" target="#b41">Loukas, 2020)</ref>, we are interested in a generic framework that can enable GNNs to separate positional and structural representations to increase their expressivity. Section 3.1 will introduce our approach to augment GNNs with learnable graph PE. Our framework can be used with different GNN architectures. We illustrate this flexibility in Sections C.1 and C.2 where the decoupling of structural and positional information is applied to both sparse MP-GNNs and fully-connected GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GENERIC FORMULATION: MP-GNNS-LSPE</head><p>Notation. Let G = (V, E) be a graph with V being the set of nodes and E the set of edges. The graph has n = |V| nodes and E = |E| edges. The connectivity of the graph is represented by the adjacency matrix A ? R n?n where A ij = 1 if there exists an edge between the nodes i and j; otherwise A ij = 0. The degree matrix is denoted D ? R n?n . The node features and positional features for node i is denoted by h i and p i respectively, while the features for an edge between nodes i and j is indicated by e ij . A GNN model is composed of three main components; an embedding layer for the input features, a stack of convolutional layers, and a final task-based layer, as in <ref type="figure" target="#fig_0">Figure 1</ref>. The layers are indexed by and = 0 denotes the input layer.</p><p>Standard MP-GNNs. Considering a graph which has available node and edge features, and these are transformed at each layer, the update equations for a conventional MP-GNN layer are defined as:</p><formula xml:id="formula_0">MP-GNNs : h +1 i = f h h i , h j j?Ni , e ij , h +1 i , h i ? R d ,<label>(1)</label></formula><formula xml:id="formula_1">e +1 ij = f e h i , h j , e ij , e +1 ij , e ij ? R d ,<label>(2)</label></formula><p>where f h and f e are functions with learnable parameters, and N i is the neighborhood of the node i.</p><p>The design of functions f h and f e depends on the GNN architecture used, see  for a review. As Transformer neural networks <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref> are a special case of MP-GNNs <ref type="bibr" target="#b29">(Joshi, 2020)</ref>, Eq.</p><p>(1) can be simplified to encompass the original Transformers by dropping the edge features and making the graph fully connected.</p><p>Input features and initialization. The node and edge features at layer = 0 are produced by a linear embedding of available input node and edge features denoted respectively by h in Positional Encoding. Existing MP-GNNs that integrate positional information usually propose to concatenate the PE with the input node features, similarly to Transformers <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref>:</p><formula xml:id="formula_2">i ? R dv , e in ij ? R de : h =0 i = LL h (h in i ) = A 0 h in i + a 0 ? R d , e =0 ij = LL e (e in ij ) = B 0 e in ij + b 0 ? R d , where A 0 ? R d?dv , B 0 ? R d?de and a 0 , b 0 ? R d</formula><formula xml:id="formula_3">MP-GNNs-PE : h +1 i = f h h i , h j j?Ni , e ij , h +1 i , h i ? R d ,<label>(3)</label></formula><formula xml:id="formula_4">e +1 ij = f e h i , h j , e ij , e +1 ij , e ij ? R d ,<label>(4)</label></formula><formula xml:id="formula_5">with initial h =0 i = LL h h in i p in i = D 0 h in i p in i + d 0 ? R d ,<label>(5)</label></formula><formula xml:id="formula_6">and e =0 ij = LL e (e in ij ) = B 0 e in ij + b 0 ? R d ,<label>(6)</label></formula><p>where p in i ? R k is the input PE of node i, D 0 ? R d?(dv+k) , d 0 ? R d are parameters for the linear transformation. Such architecture merges the positional and structural representations together. It has the advantage to keep the same linear complexity for learning, but it does not allow the positional representation to be changed and better adjusted to the task at hand.</p><p>Decoupling position and structure in MP-GNNs. We decouple the positional information from the structural information such that both representations are learned separately resulting in an architecture with Learnable Structural and Positional Encodings, which we call MP-GNNs-LSPE. The layer update equations are defined as:</p><formula xml:id="formula_7">MP-GNNs-LSPE : h +1 i = f h h i p i , h j p j j?Ni , e ij , h +1 i , h i ? R d ,<label>(7)</label></formula><formula xml:id="formula_8">e +1 ij = f e h i , h j , e ij , e +1 ij , e ij ? R d ,<label>(8)</label></formula><formula xml:id="formula_9">p +1 i = f p p i , p j j?Ni , e ij , p +1 i , p i ? R d ,<label>(9)</label></formula><p>The difference of this architecture with the standard MP-GNNs is the addition of the positional representation update Eq. (9), along with the concatenation of these learnable PEs with the node structural features, Eq. (7). As we will see in the next section, the design of the message-passing function f p follows the same analytical form of f h but with the use of the tanh activation function to allow positive and negative values for the positional coordinates. It should be noted that the inclusion of the edge features, e ij in the h or p update is optional as several MP-GNNs do not include edge features in their h updates. Nevertheless, the architecture we present is made as generic so as to be used for future extensions in a convenient way.</p><p>Definition of initial PE. The choice of the initial PE is critical. In this work, we consider two PEs: Laplacian PE (LapPE) and Random Walk PE (RWPE). LapPE are defined in Section B.2 as</p><formula xml:id="formula_10">p LapPE i = [ U i1 , U i2 , ? ? ? , U ik ] ? R k .</formula><p>LapPE provide a unique node representation and are distancesensitive w.r.t. the Euclidean norm. However, they are limited by the sign ambiguity, which requires random sign flipping during training for the network to learn this invariance <ref type="bibr" target="#b18">(Dwivedi et al., 2020)</ref>. <ref type="bibr" target="#b39">Li et al. (2020b)</ref>, we propose RWPE, a PE based on the random walk (RW) diffusion process (although other graph diffusions can be considered s.a. PageRank <ref type="bibr" target="#b44">(Mialon et al., 2021)</ref>). Formally, RWPE are defined with k-steps of random walk as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inspired by</head><formula xml:id="formula_11">p RWPE i = RW ii , RW 2 ii , ? ? ? , RW k ii ? R k ,<label>(10)</label></formula><p>where RW = AD ?1 is the random walk operator. In contrast of <ref type="bibr" target="#b39">Li et al. (2020b)</ref> which uses the full matrix RW ij for all pairwise nodes, we adopt a low-complexity usage of the random walk matrix by considering only the landing probability of a node i to itself, i.e. RW ii . Note that these PE do not suffer from the sign ambiguity of LapPE, so the network is not required to learn additional invariance. RWPE provide a unique node representation under the condition that each node has a unique k-hop topological neighborhood for a sufficient large k. This assumption can be discussed. If we consider synthetic strongly regular graphs like the CSL graphs <ref type="bibr" target="#b49">(Murphy et al., 2019)</ref>, then all nodes in a graph have the same RWPE for any k value, since they are isomorphic by construction. However, despite RWPE being the same for all nodes in a graph, these PE are unique for each class of isomorphic graphs, resulting in a perfect classification of the CSL dataset, see Section A.1. For graphs such as Decalin and Bicyclopentyl <ref type="bibr" target="#b54">(Sato, 2020)</ref>, nodes which are not isomorphic receive different RWPE for k ? 5, also in Section A.1. Finally, for real-world graphs like ZINC molecules, most nodes receive a unique node representation for k ? 24, see <ref type="figure" target="#fig_2">Figure 2</ref> for an illustration, where the two molecules have 100% and 71.43% unique RWPEs respectively. Section A.3 presents a detailed study.</p><p>Experimentally, we will show that RWPE outperform LapPE, suggesting that learning the sign invariance is more difficult (as there exist 2 k possible sign flips for each graph) than not exactly having unique node representation for each node. As mentioned above for CSL, RWPE are related to the problem of graph isomorphism and higher-order node interactions. Precisely, iterating the random walk operator for a suitable number of steps allows coloring non-isomorphic nodes, thus distinguishing several cases of non-isomorphic graphs on which the 1-WL test, and equivalently MP-GNNs, fail s.a. the CSL, Decalin and Bicyclopentyl graphs. We refer to Section A.2 for a formal presentation of the iterative algorithm. Finally, the initial PE of the network is obtained by embedding the LapPE or RWPE into a d-dimensional feature vector:</p><formula xml:id="formula_12">p =0 i = LL p (p PE i ) = C 0 p PE i + c 0 ? R d , where C 0 ? R d?k , c 0 ? R d .<label>(11)</label></formula><p>Positional loss. As we separate the learning of the structual and positional representations, it is possible to consider a specific positional encoding loss along with the task loss. A natural candidate is the Laplacian eigenvector loss <ref type="bibr" target="#b4">(Belkin &amp; Niyogi, 2003;</ref><ref type="bibr" target="#b34">Lai &amp; Osher, 2014</ref>) that enforces the PE to form a coordinate system constrained by the graph topology. As such, the final loss function of MP-GNNs-LSPE is composed of two terms:</p><formula xml:id="formula_13">Loss = Loss Task h =L p =L + ? Loss LapEig (p =L ),<label>(12)</label></formula><p>where h =L ? R n?d , p =L ? R n?k , k is the dimension of learned PE, = L is the final GNN layer, and ? &gt; 0 an hyper-parameter. Observe also that we enforce the final positional vectors p =L to have centered and unit norm with mean(p =L ?,k ) = 0, p =L ?,k = 1, ?k to better approximate the Laplacian eigenvector loss defined by Loss</p><formula xml:id="formula_14">LapEig (p) = 1 k trace p T ?p + ? k p T p ? I k 2 F with ? &gt; 0 and ? 2</formula><p>F being the Frobenius norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">INSTANCES OF LSPE WITH MP-GNNS AND TRANSFORMER GNNS</head><p>We instantiate two classes of GNN architectures, both sparse MP-GNNs and fully-connected Transformer GNNs using our proposed LSPE framework. For sparse MP-GNNs, we consider GatedGCN <ref type="bibr" target="#b8">(Bresson &amp; Laurent, 2017)</ref> and PNA <ref type="bibr" target="#b11">(Corso et al., 2020)</ref>, while we extend the recently developed SAN <ref type="bibr" target="#b32">(Kreuzer et al., 2021)</ref> and GraphiT <ref type="bibr" target="#b44">(Mialon et al., 2021)</ref> with LSPE to develop Transformer-LSPE architectures. We briefly demonstrate here how a GNN can be instantiated using LSPE (Eqs. (7-9)) by developing GatedGCN-LSPE (Eqs. (14-16)), while the complete equations for the four models are defined in Section C of the supplementary material, given the space constraint.</p><p>GatedGCN-LSPE: Originally, GatedGCNs are sparse MP-GNNs equipped with a soft-attention mechanism that is able to learn adaptive edge gates to improve the message aggregation step of GCN networks <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref>. Our proposed extension of this model with LSPE is defined as:</p><formula xml:id="formula_15">h +1 , e +1 , p +1 = GatedGCN-LSPE h , e , p , h ? R n?d , e ? R E?d , p ? R n?d ,<label>(13)</label></formula><formula xml:id="formula_16">with h +1 i = h i + ReLU BN A 1 h i p i + j?N (i) ? ij A 2 h j p j ,<label>(14)</label></formula><formula xml:id="formula_17">e +1 ij = e ij + ReLU BN ? ij ,<label>(15)</label></formula><formula xml:id="formula_18">p +1 i = p i + tanh C 1 p i + j?N (i) ? ij C 2 p j ,<label>(16)</label></formula><p>where</p><formula xml:id="formula_19">? ij = ? ? ij / j ?N (i) ? ? ij + ,? ij = B 1 h i + B 2 h j + B 3 e ij , h i , e ij , p i , ? ij ,? ij ? R d , A 1 , A 2 ? R d?2d and B 1 , B 2 , B 3 , C 1 , C 2 ? R d?d .</formula><p>Notice the p-update in Eq. (16) follows the same analytical form as the h-update in Eq. (14) except for the difference in activation function, and omission of BN, which was not needed in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NUMERICAL EXPERIMENTS</head><p>We evaluate the proposed MPGNNs-LSPE architecture on the instances of sparse GNNs and Transformer GNNs defined in Section 3.2 (all models are presented in Section C), using PyTorch <ref type="bibr" target="#b53">(Paszke et al., 2019)</ref> and DGL <ref type="bibr" target="#b62">(Wang et al., 2019)</ref> on standard molecular benchmarks -ZINC (Irwin et al., 2012), OGBG-MOLTOX21 and OGBG-MOLPCBA . ZINC and MOLTOX21 are of medium scale with 12K and 7.8K graphs respectively, whereas MOLPCBA is of large scale with 437.9K graphs. These datasets, each having a global graph-level property to be predicted, consist of molecules which are represented as graphs of atoms as nodes and bonds between the atoms as edges. Additionally, we evaluate our architecture on three non-molecular graph datasets to show the usefulness of LSPE on any graph domain in general, see Section D in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS AND EXPERIMENTAL SETTINGS</head><p>ZINC is a graph regression dataset where the property to be predicted for a graph is its constrained solubility which is a vital chemical property in molecular design <ref type="bibr" target="#b28">(Jin et al., 2018)</ref>. We use the 12,000 subset of the dataset with the same splitting defined in <ref type="bibr" target="#b18">Dwivedi et al. (2020)</ref>. Mean Absolute Error (MAE) of the property being regressed is the evaluation metric. OGBG-MOLTOX21 is a multi-task binary graph classification dataset where a qualitative (active/inactive) binary label is predicted against 12 different toxicity measurements for each molecular graph <ref type="bibr" target="#b59">(Tox21, 2014;</ref><ref type="bibr" target="#b65">Wu et al., 2018)</ref>. We use the scaffold-split version of the dataset included in OGB ) that consists of 7,831 graphs. ROC-AUC averaged across the tasks is the evaluation metric. OGBG-MOLPCBA is also a multi-task binary graph classification dataset from OGB where an active/inactive binary label is predicted for 128 bioassays <ref type="bibr" target="#b63">(Wang et al., 2012;</ref><ref type="bibr" target="#b65">Wu et al., 2018)</ref>. It has 437,929 graphs with scaffold-split and the evaluation metric is Average Precision (AP) averaged over the tasks.</p><p>To evaluate different instantiations of our proposed MPGNNs-LSPE, we follow the same benchmarking protocol in <ref type="bibr" target="#b18">Dwivedi et al. (2020)</ref> to fairly compare several models on a fixed number of 500k model parameters, for ZINC. We relax the model sizes to larger parameters for evaluation on the two OGB datasets as observed being practised on their leaderboards . The total size of parameters of each model, including the number of layers used, are indicated in the respective experiment tables, with the remaining implementation details included in supplementary Section E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS AND DISCUSSION</head><p>The results of all our experiments on different instances of LSPE along with performance without using PE are presented in <ref type="table" target="#tab_0">Table 1</ref> whereas the comparison of the best results from <ref type="table" target="#tab_0">Table 1</ref> with baseline models and SOTA is shown in <ref type="table" target="#tab_1">Table 2</ref>. We now summarize our observations and insights.   <ref type="table" target="#tab_1">Table 2a</ref> are the models with the ?500k parameters. The scores on OGBG-MOL* in <ref type="table" target="#tab_1">Tables 2b and 2c</ref> are taken from the OGB project and its leaderboards , where models have different number of parameters.</p><formula xml:id="formula_20">(a) ZINC Model Test MAE GCN 0.367?0.011 GAT 0.384?0.007 GatedGCN-LapPE 0.202?0.006 GT 0.226?0.014 SAN 0.139?0.006 Graphormer 0.122?0.006 GatedGCN-LSPE 0.090?0.001 (b) OGBG-MOLTOX21 Model Test ROC-AUC GCN 0.7529?0.0069 GCN-VN 0.7746?0.0086 GIN 0.7491?0.0051 GIN-VN 0.7757?0.0062 GatedGCN-LapPE 0.7743?0.0073 GatedGCN-LSPE 0.7754?0.0032 (c) OGBG-MOLPCBA Model Test AP GIN 0.2266?0.0028 GIN-VN 0.2703?0.0023 DeeperGCN-VN 0.2781?0.0038 PNA 0.2838?0.0035 DGN 0.2885?0.0030 PHC-GNN 0.2947?0.0026 PNA-LSPE 0.2840?0.0021</formula><p>LSPE boosts the capabilities of existing GNNs. Both sparse GNNs and Transformer GNNs are improved significantly when they are augmented with LSPE having RWPE as initial PE, see <ref type="table" target="#tab_0">Table 1</ref>. For instance, the best GNN without PE for ZINC, i.e. PNA, gives an improvement of 32.62% (0.095 vs. 0.141) when LSPE is used to learn the structural and positional representations in a decoupled manner. On other GNNs, this boost is even higher, see GatedGCN-LSPE which shows a gain of 64.14% (0.090 vs. 0.251). On MOLTOX21, PNA-LSPE improves 0.79% (0.761 vs. 0.755) over PNA while the remaining models show either minor gains or attain the same performance when not using PE. This consistent trend is also observed for MOLPCBA where LSPE boosts PNA by 1.79%.</p><p>Sparse vs. Transformer GNNs. When we compare the performance of sparse GNNs (GatedGCN, PNA) against Transformer GNNs (SAN, GraphiT) augmented with LSPE in <ref type="table" target="#tab_0">Table 1</ref>, the performance of the sparse GNNs is surprisingly better than the latter, despite Transformer GNNs being theoretically well-posed to counter the limitations of long-range interactions of the former. Notably, the evaluation of our proposed architecture, in this work, is on molecular graphs on which the information among local structures seems to be the most critical, diminishes the need of full attention. This also aligns with the insight put forward in <ref type="bibr" target="#b32">Kreuzer et al. (2021)</ref> where the SAN, a Transformer model, benefited less from full attention on molecules. Beyond molecular graphs, there may be other domains where Transformer GNNs could give better performance, but still these would not scale in view of the quadratic computational complexity. Indeed, it is important to notice the much lesser training times of sparse GNNs compared to Transformer GNNs in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>LSPE improves the state-of-the-art for domain-agnostic GNNs. When we compare the best performing instantiation of the LSPE from <ref type="table" target="#tab_0">Table 1</ref> with baseline GNN models from the literature on the three benchmark datasets, our proposed architecture improves the SOTA on ZINC, while achieving SOTA-comparable performance on remaining datasets, see <ref type="table" target="#tab_1">Table 2</ref>. On ZINC, GatedGCN-LSPE surpasses most baselines by a large margin to give a test MAE of 0.090 which is an improvement of 35.25% and 26.23% respectively over the two recent-most Transformer based GNNs, SAN and Graphormer. On MOLTOX21, GatedGCN-LSPE reports a test ROC-AUC score of 0.7754 which is similar to the best baseline GIN (0.7757) that uses virtual node (VN). Finally, LSPE enables PNA to achieve comparable performance to SOTA on MOLPCBA while boosting its performance when no PE was used. We note here that ZINC scores can even be boosted beyond LSPE's SOTA when domain expertise is used <ref type="bibr" target="#b7">(Bouritsas et al., 2020;</ref><ref type="bibr" target="#b6">Bodnar et al., 2021)</ref> while Graphormer <ref type="bibr" target="#b67">(Ying et al., 2021)</ref> achieved the top score on MOLPCBA when pre-trained on a very large (3.8M graphs) dataset. To ensure fair comparison with other scores, we did not use these two results in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>On Positional loss. It can be observed in <ref type="table" target="#tab_0">Table 1</ref> that the positional loss Eq. (12), further pushes the best LSPE score on ZINC slightly from 0.093 to 0.090, while on MOLTOX21 it only improves the train score though obtaining comparable test performance. We will investigate a more consistent positional loss in a future work.</p><p>Finally, we would like to highlight the generic nature of our proposed architecture which can be applied to any MP-GNN in practice as demonstrated by four diverse GNNs in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION STUDIES</head><p>Through ablation studies, we show -i) the usefulness of learning positional representation at every layer vs. simply injecting a pre-computed positional encoding in the input layer, and ii) the selection of the number of k for the steps in RWPE in the proposed LSPE architecture. Learning PE at every layer provides the best performance. In <ref type="table" target="#tab_2">Table 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This work presents a novel approach to learn structural and positional representations separately in a graph neural network. The resultant architecture, LSPE enables a principled and effective learning of these two key properties that make GNN representation even more expressive. Main design components of LSPE are -i) higher-order position informative random walk features as PE initialization, ii) decoupling positional representations at every GNN layer, and iii) the fusion of the structural and positional features finally to generate hybrid features for the learning task. We observe a consistent increase of performance across several instances of our model on the benchmark datasets used for evaluation. Our architecture is simple and universal to be used with any sparse GNNs or Transformer GNNs as demonstrated by two sparse GNNs and two fully connected Transformer based GNNs in our numerical experiments. Given the importance of incorporating expressive positional encodings to theoretically improve GNNs as seen in the recent literature, we believe this paper provides a useful architectural framework that can be considered when developing future models which improve graph positional encodings, for both GNNs and Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>In this work, we present an approach to improve neural network methods for graphs by considering efficient learnable positional encoding while keeping the linear complexity of the model w.r.t to the number of nodes. This improves the cost of training such models, as contrast to some previous works that improved GNNs at the cost of higher-order tensor computation. We discover another insight that the linear complexity models (sparse GNNs) can outperform quadratic complexity models (Transformers). Consequently, one beneficial impact of our work is that its use can reduce GPU and computational resources, eventually contributing to minimizing the adverse effect of deep learning training on environment. However, the method we propose belongs to a class of architectures that can be used on malicious applications since the internet and several of the processes in its ecosystem can be represented in form of graphs. To prevent such applications, ethical guidelines can be set and enforced which constraint the usage of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>XB is supported by NRF Fellowship NRFF2017-10 and NUS-R-252-000-B97-133. This research is supported by Nanyang Technological University, under SUG Grant (020724-00001). VPD would like to thank Andreea Deac for her helpful feedback, Quan Gan for his support on the DGL library, Gabriele Corso for answering questions related to the PNA model, and Chaitanya K. Joshi for useful comments. Finally, the authors would like to thank the anonymous reviewers for their helpful suggestions and feedbacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SUPPLEMENTARY A.1 DISTINGUISHING NON-ISOMORPHIC GRAPHS USING RANDOM WALK FEATURES</head><p>The choice of the initial PE in our proposed architecture can be several based on graph diffusion or other related techniques. In this section, we study RWPE (Eqn. 10) which we initialize with k-steps of random walk. Precisely we use a k-dim vector that encodes the landing probabilities of a node i to itself in 1 to k steps. This initial PE vector for a node i is given by [RW ii , RW 2 ii , . . . , RW k ii ] ? R k which is pre-computed before the model training. Here, we demonstrate that such PE vector can help distinguish i) structurally dissimilar nodes and ii) non-isomorphic graphs on which 1-WL, and equivalently MP-GNNs, fail, thus illustrating the empirically powerful nature of MPGNNs-LSPE that relies on this choice of positional features initialization.   <ref type="figure" target="#fig_6">Figure 4</ref> where the graphs either do not have any node attributes ( <ref type="figure" target="#fig_5">Figure 3)</ref>, or have the same node attributes ( <ref type="figure" target="#fig_6">Figure 4</ref> where each node denotes a Carbon atom). When we apply MPGNNs on the graph in 3 (Left), each node will have the same feature representation as it is a regular graph without any node attributes. However, there are structurally 3 different kinds of nodes denoted by the same number of different colors. If we initialize the PE for these nodes for k = 4 random walk steps, we can observe that the nodes are being assigned the 4-dim feature vectors that is consistent to their initial structural roles in the graph, thus being distinguishable.</p><p>Similarly, <ref type="figure" target="#fig_5">Figure 3</ref> (Right) is a pair of non-isomorphic graphs from the theoretically challenging and highly symmetric Circulant Skip Link (CSL) dataset from <ref type="bibr" target="#b49">Murphy et al. (2019)</ref>. It can be noticed that every node in a graph here has the same structural role as the each node has edges with other nodes at same hops. However, in G skip <ref type="figure" target="#fig_0">(11, 2)</ref>, the edges are between nodes at 1, 2 hops whereas in G skip <ref type="figure" target="#fig_0">(11, 3)</ref>, the edges are between the nodes at 1, 3 hops, with 2 and 3 being the skip-links of the two graphs, respectively. In such a scenario, the node in the G skip (11, 2) gets a different 4-dim initial PE than a node in G skip <ref type="figure" target="#fig_0">(11, 3)</ref>, thus helping eventually to distinguish the two graphs when these node features are pooled to generate the graph feature vector.</p><p>Finally, in <ref type="figure" target="#fig_6">Figure 4</ref>, a pair of non-isomorphic and non-regular graphs is shown from <ref type="bibr" target="#b54">Sato (2020)</ref> that MPGNNs fail to distinguish. If we use 5 steps of Random Walk to initialize the node's PE vector, we can observe that the two graphs can easily be distinguished. We note here that the random walk based PE initialization (RWPE) is close to one of the Distance Encoding instantiations used in <ref type="bibr" target="#b39">Li et al. (2020b)</ref>. However, we do not require to consider pairwise scores RW k ij between nodes i and j and any sub-set of nodes from the original graph, thus making our method less computationally demanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 RANDOM WALK PE FEATURE AND GRAPH ISOMORPHISM TEST</head><p>Similar to the 1-WL test for graph isomorphism <ref type="bibr" target="#b64">(Weisfeiler &amp; Leman, 1968;</ref><ref type="bibr" target="#b47">Morris et al., 2019;</ref><ref type="bibr" target="#b54">Sato, 2020)</ref>, the RWPE can be used as a node coloring algorithm to test if two graphs are non-isomorphic, as described in Algorithm 1. Note that this algorithm cannot guarantee that two graphs are isomorphic, like the WL test. However, our analysis in Section A.1 shows this algorithm to be strictly powerful than the 1-WL test as the pairs of graphs in <ref type="figure" target="#fig_5">Figure 3</ref> (Right) and in <ref type="figure" target="#fig_6">Figure 4</ref> are not distinguishable by 1-WL. Although this increase in power is being achieved without the need of maintaining colors for tuple of nodes to encode higher order interactions (as in k-WL), the algorithm's complexity is of O(k * n 3 ) due to the matrix multiplication in Step 5 (b) and</p><p>Step 5 (c), compared to O(k * n 2 ) of 1-WL, with k being the number of iterations until convergence.</p><p>Algorithm 1 Algorithm to decide whether a pair of graphs are not isomorphic based on random walk landing probabilities of each node to itself. Input: A pair of graphs G 1 = (V 1 , E 1 ), G 2 = (V 2 , E 2 ) with n nodes and e edges in each graph. A 1 ? R n?n and A 2 ? R n?n denote the adjacency matrices, D 1 ? R n?n and D 2 ? R n?n denote the degree matrices of graphs G 1 and G 2 respectively. Output: Return "non-isomorphic" if G 1 and G 2 are not isomorphic else "possibly isomorphic".</p><formula xml:id="formula_21">1. M (0) ? A 1 D ?1 1 ? R n?n 2. N (0) ? A 2 D ?1 2 ? R n?n 3. c (0) u ? M (0) u,u ?u ? V 1 4. d (0) v ? N (0) v,v ?v ? V 2 5. for k = 1, 2, ? ? ? (until convergence to stationary distribution) (a) if HASH {{c (k?1) u ? R k | u ? V 1 }} = HASH {{d (k?1) v ? R k | v ? V 2 }} then return "non-isomorphic" (b) M (k) ? M (k?1) M (0) ? R n?n (c) N (k) ? N (k?1) N (0) ? R n?n (d) c (k) u ? append M (k) u,u to c (k?1) u ?u ? V 1 (e) d (k) v ? append N (k) v,v to d (k?1) v ?v ? V 2 6. return "possibly isomorphic"</formula><p>where HASH is an injective hash function and {{. . .}} denotes a multiset.  <ref type="figure" target="#fig_0">1000 graphs)</ref> where the x-axis is the number of nodes, the y-axis is the number of unique PEs and the point intensity is the number of graphs with the same pair (x, y). Besides, <ref type="figure" target="#fig_7">Fig. 5a</ref> has 36-dim LapPE (trailing dims padded with zero for a graph with n &lt; 36), and <ref type="figure" target="#fig_7">Fig. 5b</ref> has 24-dim RWPE.  <ref type="figure" target="#fig_7">Figure 5</ref> visualizes the uniqueness of the node representation with LapPE and RWPE (which serve as initial PE of our network) using the ZINC validation set of 1000 real-world molecular graphs. If the initial PE is unique for each node in a graph, then the graph lies on the straight diagonal line. <ref type="figure" target="#fig_7">Figure  5a</ref> shows the result for LapPE, all graphs lie on the diagonal line as Laplacian eigenvectors guarantee unique node coordinates in the Euclidean transformed space. <ref type="figure" target="#fig_7">Figure 5b</ref> presents the result for RWPE. We observe that not all, but a large amount of ZINC molecular graphs stay close to the straight line, showing that most graphs have a large amount of nodes with unique RWPE. For example, there are 30 graphs with 24 nodes having 21 unique RWPE, equivalent to 87.5% of nodes with unique PE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 STUDY OF LAPPE AND RWPE AS INITIAL PE</head><p>Additionally, we visualize four sample graph plots from the ZINC validation set in <ref type="figure" target="#fig_8">Figure 6</ref> where the first two graphs have completely unique RWPE features, while the next two graphs have partially unique RWPEs (71.43% and 72.22% respectively). The visualization assigns a unique node color for each unique RWPE representation. Therefore, graphs in <ref type="figure" target="#fig_8">Figures 6a and 6b</ref> are plotted with each node assigned to a unique color based on their RWPE features, and graphs in <ref type="figure" target="#fig_8">Figures 6c and 6d</ref> are represented with 10 and 13 unique colors respectively corresponding to their number of unique RWPE representations. In particular, observe the green-shade colored nodes in <ref type="figure" target="#fig_8">Figure 6c</ref> (top and bottom-right) as well as blue-shade (mid-left) and orange-shade (bottom-right) colored nodes in <ref type="figure" target="#fig_8">Figure 6d</ref>. We can easily see that the nodes with the same color are isomorphic in the graph, i.e. their k-hop structural neighborhoods are the same for values k ? 11.</p><p>We remind that RWPE provides a unique node representation under the condition that each node have a unique k-hop topological neighborhood for a sufficient large k. While this condition is experimentally true for most nodes, it is not always satisfied. But despite this approximation, for a sufficiently large number k of random walk iterations, RWPE is still able to capture global higher-order positioning of nodes that is used as initial PE, and is beneficial to the proposed LSPE architecture as demonstrated by the gain of performance in several experiments. <ref type="table" target="#tab_1">IN TABLE 2</ref> As a complete reference, the different GNN baselines and SOTA models that are used for the comparison in <ref type="table" target="#tab_1">Table 2</ref> are Graph Convolutional Networks (GCN) (Kipf &amp; Welling, 2017), Graph Attention Networks (GAT) <ref type="bibr" target="#b61">(Veli?kovi? et al., 2018)</ref>, GatedGCN-LapPE <ref type="bibr" target="#b8">(Bresson &amp; Laurent, 2017;</ref><ref type="bibr" target="#b18">Dwivedi et al., 2020)</ref>, Graph Transformer (GT) <ref type="bibr" target="#b17">(Dwivedi &amp; Bresson, 2021)</ref>, Spectral Attention Networks (SAN) <ref type="bibr" target="#b32">(Kreuzer et al., 2021)</ref>, Graphormer <ref type="bibr" target="#b67">(Ying et al., 2021)</ref>, Graph Isomorphism Networks (GIN) , DeeperGCN <ref type="bibr" target="#b38">(Li et al., 2020a)</ref>, Principle Neighborhood Aggregation (PNA) <ref type="bibr" target="#b11">(Corso et al., 2020)</ref>, Directional Graph Networks (DGN) <ref type="bibr" target="#b3">(Beani et al., 2021)</ref> and Parameterized Hypercomplex GNNs (PHC-GNN) <ref type="bibr" target="#b35">(Le et al., 2021)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 MODELS USED FOR COMPARISON</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 FIGURE FOR THE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RELATED WORK IN DETAIL</head><p>In this detailed section on related work, we first review the limitations of existing MP-GNN architectures in terms of their theoretical expressiveness, suggesting possible improvements to make GNNs more powerful. Then, we introduce a number of non-learned and learning techniques that can be studied under the umbrella of graph positional encoding. Finally, we highlight the recent developments for generalizing Transformers to graphs. Our aim is to connect meaningful innovations through the detailed background on these three research directions, the unification of which spearheaded the development of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 THEORETICAL EXPRESSIVITY AND WEISFEILER-LEMAN GNNS</head><p>Weisfeiler-Leman test. The limitation of MP-GNNs in failing to distinguish non-isomorphic graphs was first carefully studied in  and <ref type="bibr" target="#b47">Morris et al. (2019)</ref>, based on the equivalence of MP-GNNs and the 1-WL isomorphism test <ref type="bibr" target="#b64">(Weisfeiler &amp; Leman, 1968)</ref>. As such, MP-GNNs may perform poorly on graphs that exhibit several symmetries in their original structure, such as node and edge isomorphisms <ref type="bibr" target="#b49">(Murphy et al., 2019;</ref>. Besides, some message-passing functions may not be discriminative enough <ref type="bibr" target="#b11">Corso et al., 2020)</ref>.</p><p>Equivariant GNNs. Graph Isomorphism Networks (GINs)  were designed to be as maximally expressive as the original 1-WL test <ref type="bibr" target="#b64">(Weisfeiler &amp; Leman, 1968</ref>). However, the 1-WL test can fail to distinguish (simple) non-isomorphic graphs, thus requiring novel GNNs with more expressivity power. As the original 1-WL test only considers 2-tuple of nodes, i.e. the standard edges in a graph, a natural approach to improve the expressivity power of the 1-WL test is to examine higher-order interactions between nodes with k-tuple of nodes with k ? 3. To this end, k-order Equivariant-GNNs were introduced in Maron et al. <ref type="bibr">(2018)</ref>. But these architectures require O(n k ) memory and speed complexities. This is an important practical limitation as k = 3 is at least needed to design more powerful GNNs than GINs. Along this line, the most efficient WL-GNNs that have been  <ref type="bibr" target="#b23">(Hochreiter &amp; Schmidhuber, 1997)</ref>, and Transformers <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref>. These architectures integrate structural and positional attributes of data when building abstract feature representations. For instances, ConvNets intrinsically consider regular spatial structure for the position of pixels <ref type="bibr" target="#b27">(Islam et al., 2020)</ref>, RNNs also build on the sequential structure of the word positions, and Transformers employ positional encoding of words (see <ref type="bibr" target="#b16">Dufter et al. (2021)</ref> for a review). For GNNs, the position of nodes is more challenging due to the fact that there does not exist a canonical positioning of nodes in arbitrary graphs. This implies that there is no obvious notion of global and relative position of nodes, and consequently no specific directions on graphs (like the top, down, left and right directions in images). Despite these issues, graph positional encoding are as much critical for GNNs as they are for ConvNets, RNNs and Transformers, as demonstrated for prediction tasks on graphs <ref type="bibr" target="#b13">Cui et al., 2021)</ref>.</p><p>Index Positional Encoding. Loukas (2020) identified another cause of the limited expressivity of the standard MP-GNNs. Such GNNs do not have the capacity to handle anonymous nodes, i.e. nodes which do not have unique node features. This property turns out to be critical to show that MP-GNNs can be universal approximators if each node in the graph can be assigned to a unique or discriminating feature. The theorem results from an alignment between MP-GNNs and distributed local algorithms <ref type="bibr" target="#b50">(Naor &amp; Stockmeyer, 1995;</ref><ref type="bibr" target="#b55">Sato et al., 2019)</ref>. In order to address the issue of anonymous MP-GNNs and improve their theoretical expressiveness w.r.t the WL test, <ref type="bibr" target="#b49">Murphy et al. (2019)</ref> introduced Graph Relational Pooling. Their model assigns a unique identifier to each node, defined by an indexing of the nodes. However, such a model must be trained with the n! possible index permutations to guarantee higher expressivity, which is not computationally feasible. As a consequence, during training, node indexing is uniformly sampled from the n! possible choices in order for their network to learn to be independent to the choice of the index PE at test time. Similarly, random node identifier could be used for breaking the node anonymity. Yet, this PE also suffers from the lack of generalization for unseen graphs <ref type="bibr" target="#b41">(Loukas, 2020)</ref>.</p><p>Laplacian Positional Encoding. Besides providing a unique representation for each node, meaningful graph PE should also be permutation-invariant and distance-sensitive, meaning that the difference between the PEs of two nodes far apart on the graph must be large, and small for two nodes nearby. Laplacian eigenvectors <ref type="bibr" target="#b4">(Belkin &amp; Niyogi, 2003)</ref> appear to be good candidates for graph PE, belonging to the class of unsupervised manifold learning techniques. Precisely, they are spectral techniques that embed graphs into an Euclidean space, and are defined via the factorization of the graph Laplacian ? = I n ? D ?1/2 AD ?1/2 = U T ?U , where I n is the n ? n identity matrix, A the n ? n adjacency matrix, D the n ? n degree matrix, and n ? n matrices ? and U correspond to the eigenvalues and eigenvectors respectively. The complexity for computing this full factorization is O(E 3/2 ) and O(n) with approximate Nystrom method <ref type="bibr" target="#b19">(Fowlkes et al., 2004)</ref>. Laplacian eigenvectors form a meaningful local coordinate system, while preserving the global graph structure. As these eigenvectors hold the key properties of permutation-invariant, uniqueness, computational efficiency and distance-aware w.r.t. the graph topology, they were proposed as graph <ref type="bibr">PE (Dwivedi et al., 2020;</ref><ref type="bibr" target="#b17">Dwivedi &amp; Bresson, 2021)</ref>. They also naturally generalize the positional encoding used in Transformers <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref> to arbitrary graphs. The main limitation of this graph PE is the existence of a sign ambiguity as eigenvectors are defined up to ?1. This leads to 2 k number of possible sign values when selecting k number of eigenvectors. In practice, we choose k ? n eigenvectors given the manifold assumption, and therefore 2 k is much smaller n! (the number of possible ordering of the nodes), and therefore smaller amount of ambiguities to be resolved by the network. During the training, eigenvectors are uniformly sampled at random between the 2 k possibilities <ref type="bibr" target="#b18">(Dwivedi et al., 2020;</ref><ref type="bibr" target="#b32">Kreuzer et al., 2021)</ref> in order for the network to learn to be invariant w.r.t the sign of the eigenvectors.</p><p>Other graph PE. <ref type="bibr" target="#b39">Li et al. (2020b)</ref> proposed the use of distance encoding (DE) as node attributes, and additionally as controller of message aggregation. DE captures relative distances between nodes in a graph using powers of the random walk matrix. The resulting GNN was shown to have better expressivity than the 1-WL test. However, the limitation on regular graphs, and the cost and memory requirement of using power matrices may prevent the use of this technique to larger graphs. <ref type="bibr" target="#b30">Khasahmadi et al. (2020)</ref> used random walk with restart <ref type="bibr" target="#b52">(Pan et al., 2004)</ref> as topological embeddings with the initial node features. <ref type="bibr" target="#b68">You et al. (2019)</ref> proposed learnable position-aware embeddings based on random anchor sets of nodes for pairwise nodes (or link) tasks. This work also seeks to develop positional encoding that can be learned along with the structural representation within the GNN. However, the random selection of anchors has its limitations, which makes their approach less generalizable on inductive tasks. <ref type="bibr" target="#b7">Bouritsas et al. (2020)</ref>; <ref type="bibr" target="#b6">Bodnar et al. (2021)</ref> introduced hybrid GNNs based on the WL-test and the message-passing aggregation mechanism. These networks use prior knowledge about a class of graphs of interest such as rings for molecules and cliques for social networks. The prior information is then encoded into MP-GNNs to obtain more expressive models by showing that the such GNNs are not less powerful than the 3-WL test. They obtained top performance on molecular datasets but the prior information regarding graph sub-structures needs to be pre-computed, and sub-graph matching and counting require O(n k ) for k-tuple sub-structure. Besides, complexity of the message passing process depends linearly w.r.t. the size of the sub-graph structure. Note that the core idea of substructure counting with e.g. the number of rings associated to an atom provides a powerful higher-order structural information to the network and can improve significantly the tasks related to substructure counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 TRANSFORMER-BASED GNNS</head><p>MP-GNNs are GNNs that leverage the sparse graph structure as computational graph, allowing training and inference with linear complexity and making them scalable to medium and large-scale graphs. However, besides their low expressivity, these GNNs hold two important and well-identified limitations. Firstly, MP-GNNs are susceptible to the information bottleneck limitation a.k.a. oversquashing <ref type="bibr" target="#b0">(Alon &amp; Yahav, 2020)</ref> when messages from across distant nodes are aggregated to a node. Secondly, long-range interactions between far away nodes can also be limited, and require multiple layers that can suffer from the vanishing gradient problem. These limitations are similar to the ones present in Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b23">(Hochreiter &amp; Schmidhuber, 1997)</ref>, and can lead MP-GNNs to perform poorly on tasks where long-range interactions are necessary.</p><p>To overcome these limitations, it seems natural to use Transformer networks <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref> which alleviates the long-range issue as 'everything is connected to everything'. However, it was found that the direct adoption of full-graph operable Transformers perform poorly compared to MP-GNNs on graph structured datasets <ref type="bibr" target="#b17">(Dwivedi &amp; Bresson, 2021</ref> GraphiT <ref type="bibr" target="#b44">(Mialon et al., 2021)</ref> and Graphormer <ref type="bibr" target="#b67">(Ying et al., 2021)</ref> were also very recently developed as full-graph operable Transformers for graphs with the idea to weigh (or, control) the attention mechanism based on the graph topology. Specifically, GraphiT employs diffusion geometry to capture short-range and long-range graph information, and Graphormer uses shortest paths. Altogether, these works exploit different relative positional encoding information to improve the expressivity of Transformers for graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C INSTANCES OF LSPE WITH SPARSE AND TRANSFORMER GNNS C.1 SPARSE GNNS WITH LSPE</head><p>In this section, we augment two MP-GNN architectures with learnable positional representation, namely GatedGCN <ref type="bibr" target="#b8">(Bresson &amp; Laurent, 2017)</ref> and PNA <ref type="bibr" target="#b11">(Corso et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 GATEDGCN-LSPE</head><p>GatedGCNs <ref type="bibr" target="#b8">(Bresson &amp; Laurent, 2017)</ref> are sparse MP-GNNs equipped with a soft-attention mechanism that is able to learn adaptive edge gates to improve the message aggregation step of GCN networks <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref>. We augment this model to develop GatedGCN-LSPE, defined as:</p><formula xml:id="formula_22">h +1 , e +1 , p +1 = GatedGCN-LSPE h , e , p , h ? R n?d , e ? R E?d , p ? R n?d ,<label>(17)</label></formula><formula xml:id="formula_23">with h +1 i = h i + ReLU BN A 1 h i p i + j?N (i) ? ij A 2 h j p j ,<label>(18)</label></formula><formula xml:id="formula_24">e +1 ij = e ij + ReLU BN ? ij ,<label>(19)</label></formula><formula xml:id="formula_25">p +1 i = p i + tanh C 1 p i + j?N (i) ? ij C 2 p j ,<label>(20)</label></formula><p>and</p><formula xml:id="formula_26">? ij = ? ? ij j ?N (i) ? ? ij + ,<label>(21)</label></formula><formula xml:id="formula_27">? ij = B 1 h i + B 2 h j + B 3 e ij ,<label>(22)</label></formula><formula xml:id="formula_28">where h i , e ij , p i , ? ij ,? ij ? R d , A 1 , A 2 ? R d?2d and B 1 , B 2 , B 3 , C 1 , C 2 ? R d?d .</formula><p>C.1.2 PNA-LSPE PNA <ref type="bibr" target="#b11">(Corso et al., 2020</ref>) is a sparse MP-GNN model which uses a combination of node aggregators to overcome the theoretical limitation of a single aggregator. We propose PNA-LSPE whose layer update equation is defined as:</p><formula xml:id="formula_29">h +1 , p +1 = PNA-LSPE h , e 0 , p , h ? R n?d , e 0 ? R E?d , p ? R n?d ,<label>(23)</label></formula><formula xml:id="formula_30">with h +1 i = h i + LReLU BN U h ? ? h i p i , j?N (i) M h h i p i , e 0 ij , h j p j ? ? ,<label>(24)</label></formula><formula xml:id="formula_31">p +1 i = p i + tanh U p ? ? p i , j?N (i) M p p i , e 0 ij , p j ? ? ,<label>(25)</label></formula><formula xml:id="formula_32">and = I S(D, ? = 1) S(D, ? = ?1) ? ? ? ? ? ? max min ? ? ?,<label>(26)</label></formula><p>where is the principal aggregator designed in <ref type="bibr" target="#b11">(Corso et al., 2020)</ref>, LReLU stands for LeakyReLU activation, amd U h , U p , M h and M p are linear layers (or multi-layer perceptrons) with learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 TRANSFORMER GNNS WITH LSPE</head><p>The recently developed SAN <ref type="bibr" target="#b32">(Kreuzer et al., 2021)</ref>, GraphiT <ref type="bibr" target="#b44">(Mialon et al., 2021)</ref> and Graphormer <ref type="bibr" target="#b67">(Ying et al., 2021)</ref> are promising full-graph operable Transformers incorporating several methods to encode positional and structural features into the network. In the next sections, we expand these Transformer-based networks with the proposed LSPE architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 SAN-LSPE</head><p>Like Transformers, Spectral Attention Networks (SAN) <ref type="bibr" target="#b32">(Kreuzer et al., 2021)</ref> operate on full graphs although the network separates the parameters coming from existing edges and non-existing edges in the graph. Furthemore, the contribution of attentions from existing and non-existing edges are weighted by an additive positive scalar ?, which can be tuned for different tasks. SAN also considers a Learnable Positional Encoding (LPE) module which takes in Laplacian eigenvectors and transforms them into a fixed size PE with a self-attention encoder. This PE is then used in the main architecture in a manner similar to MP-GNNs-PE as defined in Eq. (5). We propose to extend SAN by replacing the LPE module with the LSPE architecture proposed in Section 3.1 where positional representation is learned in line with structural embedding at each GNN layer: </p><formula xml:id="formula_33">h +1 , p +1 = SAN-LSPE h , e 0 , p , h ? R n?d , e 0 ? R n?n?d , p ? R n?d ,<label>(27)</label></formula><formula xml:id="formula_34">with h +1 i = BN h +1 i + W 2 ReLU W 1h +1 i ? R d (28) h +1 i = BN h i +? +1 i ? R d ,<label>(29)</label></formula><formula xml:id="formula_35">h +1 i = O H k=1 j?V w k, ij j ?V w k, ij v k, j ? R d ,<label>(30)</label></formula><formula xml:id="formula_36">w k, ij = 1 1+? ? exp(A k, ij ) if ij ? E ? 1+? ? exp(? k, ij ) if ij ? E ,<label>(31)</label></formula><formula xml:id="formula_37">A k, ij = q k, i T diag(c k, ij )k k, j / ? d k ? R if ij ? ? A k, ij =q i k, T diag(c k, ij )k k, j / ? d k ? R if ij ? E<label>(32</label></formula><formula xml:id="formula_38">? R d ,<label>(36)</label></formula><formula xml:id="formula_39">w k, p,ij = 1 1+? ? exp(A k, p,ij ) if ij ? E ? 1+? ? exp(? k, p,ij ) if ij ? E ,<label>(37)</label></formula><p>A k, p,ij = q k,</p><formula xml:id="formula_40">p,i T diag(c k, p,ij )k k, p,j / ? d k ? R if ij ? ? A k, p,ij =q p,i k, T diag(c k, p,ij )k k, p,j / ? d k ? R if ij ? E<label>(38)</label></formula><p>Q k, p = p W k, p,Q , K k, p = p W k, p,K , V k, p = p W k, p,V ? R n?d k (39) Q k, p = p W k, p,Q ,K k, p = p W k, p,K ,V k, p = p W k, p,V ? R n?d k</p><p>C k,0 p = e 0 W k p,e ,C k,0 p = e 0W k p,e ? R n?n?d k  <ref type="bibr" target="#b25">(Ioffe &amp; Szegedy, 2015)</ref>. Finally, we make the balance scalar parameter ? ? 0 learnable (also clipping its range in [0, 1]) differently from <ref type="bibr" target="#b32">(Kreuzer et al., 2021)</ref> where its optimal value is computed by grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 GRAPHIT-LSPE</head><p>Similarly to SAN, GraphiT <ref type="bibr" target="#b44">(Mialon et al., 2021</ref>) is a full-graph operable Transformer GNN which makes use of the diffusion distance to capture short-range and long-range interactions between nodes depending of the graph topology. This pairwise diffusion distance is used as a multiplicative weight to adapt the weight scores to the closeness or farness of the nodes. For example, if two nodes are close on the graph, them the diffusion distance K ij will have a value close to one, and when the two nodes are far away then the value of K ij will be small.</p><p>Unlike SAN, the GraphiT model does not consider separate parameters for existing and non-existing edges for a graph. However, following <ref type="bibr" target="#b32">Kreuzer et al. (2021)</ref> and our experiments, separating the parameters for each type of edges showed to improve the performance. Therefore, we augment the original GraphiT architecture with learnable positional features and use two sets of parameters for the edges and the complementary edges to define GraphiT-LSPE. The GraphiT-LSPE model uses the same update equation as SAN-LSPE except for the weight score which are re-defined to introduce the diffusion kernel:</p><formula xml:id="formula_43">w k, ij = K ij ? exp(A k, ij ) if ij ? E K ij ? exp(? k, ij ) if ij ? E .<label>(42)</label></formula><p>Following <ref type="bibr" target="#b44">(Mialon et al., 2021)</ref>, the diffusion distance is chosen to be the p-step random walk kernel defined as K = (I n ? ??) p ? R n?n where I n , ? ? R n?n is the identity matrix and the graph Laplacian matrix respectively. Hyper-parameter ? controls the amount of diffusion with value between [0.25, 0.50].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENTS ON NON-MOLECULAR GRAPHS</head><p>We conduct experiments on 3 non-molecular graph datasets to demonstrate the effectiveness of the proposed LSPE architecture on any graph domain in general. We select GatedGCN as the GNN instance here. The datasets used are from the domains of social network (IMDB-BINARY and <ref type="table">Table 4</ref>: Results on the IMDB-MULTI, IMDB-BINARY and CIFAR10 superpixels. All scores are averaged over 4 runs with 4 different seeds. On IMDB-each seed experiment is on 10-fold cross validation. Bold: GNN's best score. No PosLoss is used with LSPE. ? denotes the result is taken directly from <ref type="bibr" target="#b18">(Dwivedi et al., 2020)</ref>.</p><p>Published as a conference paper at ICLR 2022 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Block diagram illustration of the proposed MPGNNs-LSPE architecture along with the inputs, general framework of a layer, and the output and loss components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>are the learnable parameters of the linear layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Sample graph plots from the ZINC validation set with each node color in a graph representing a unique RWPE vector, when k = 24. The corresponding graph ids, the number of nodes in the graphs and the number of unique RWPEs are labelled against the figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, GatedGCN-RWPE corresponds to the model where LapPE are replaced with k-dim pre-computed random walk features at the first layer, and the PE are not updated in the subsequent layers. First, we observe a significant leap in performance (from 0.202 to 0.122) when the RWPE are injected in place of LapPE at the first layer, suggesting that RWPE could encode better positional information in GNNs as they are not limited by the sign ambiguity of LapPE. See Section A.1 in the supplementary material for an example of RWPE's representation power. Note that the injection of RWPE at the final layer instead of the input layer gives poor performance. The reason behind the better performance of concatenating RWPE at the input layer is to inform the GNN aggregation function of the node positions in order to distinguish them in the case of graph symmetries like isomorphic nodes. Now, if we observe the training performance, GatedGCN-RWPE leads to an overfit on ZINC. However, when the positional representations are also updated, the overfit is considerably alleviated improving the test score to 0.100. Finally, when we further fuse the learned positional features at the final layer with the structural features, Eq. (12), the model achieves the best MAE test of 0.093. This study justifies how the GNN model learns best when the positional representations can be tuned and better adjusted to the learning task being dealt with.The choice of k steps to initialize RWPE. InFigure 7(see Section A.5), we study the effect of choosing a suitable number of k steps for the random walk features that are used as initial positional encoding in Section 3.1. This value k is also used to set the final dimension of the learned positional representation in the last layer. Numerical experiments show the best values of k to be 20 and 16 for ZINC with GatedGCN-LSPE and OGBG-MOLTOX21 with PNA-LSPE respectively, which are larger values from what was used inLi et al. (2020b) (k = 3, 4)  where the RW features are treated as distance encoding. The difference of k value is due to two reasons. First, the proposed RWPE requires to use a large k value to possibly provide a unique node representation with different k-hop neighborhoods. Second,<ref type="bibr" target="#b39">Li et al. (2020b)</ref> not only uses RW k ii but also considers all pairwise RW k ij between nodes i and j in a target set of nodes, which increases the computational complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>authors support and advocate the principles of open science and reproducible research. The algorithms and architectures proposed in this work are open-sourced in a free and public code repository with easy-to-use scripts to reproduce different experiments and evaluations presented. The tables included in the paper mention critical details on the number of layers and the total number of model parameters that are trained. Similarly, the visualization and illustrations presented in the main paper as well as the supplementary material contain the exact details on the dataset examples (such as index) used. Finally, a detailed table consisting of several hyperparameters used for the experiments are included in the supplementary, ensuring the reproducibility of the results discussed in this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Left: Example 3-regular graph with 8 nodes from<ref type="bibr" target="#b39">Li et al. (2020b)</ref> where the nodes are structurally different and colored accordingly. The 4-dim initial RWPE vector is shown against the corresponding nodes with their respective colors. Right: Example pair of non-isomorphic graphs with 11 nodes and skip-links 2 and 3 from<ref type="bibr" target="#b49">Murphy et al. (2019)</ref>. Each node in a graph gets the same 4-dim RWPE vector, and shown above in colors are the respective graphs' RWPE vectors after averaging across all the nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>A pair of non-isomorphic and non-regular graphs (Left: Decalin, Right: Bicyclopentyl) from<ref type="bibr" target="#b54">Sato (2020)</ref>. The 5-dim initial PE vector is shown against the corresponding nodes with their respective colors.We show the simulation of the nodes' initial RWPE vectors on three examples inFigure 3(Left), Figure 3 (Right), and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Plot of the number of nodes in a graph vs. the number of unique PE for LapPE and RWPE. A point in the plots represents a graph in the ZINC validation set (composed of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Sample graph plots from the ZINC validation set with each node color in a graph representing a unique RWPE vector, when k = 24. The corresponding graph ids, the number of nodes in the graphs and the number of unique RWPEs are labelled against the figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Test scores on selecting different values of k which is used to determine the number of iterative steps of RW in RWPE as well as the dimension of the learned PE at the final layer, Eqn. 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>proposed are in Maron et al. (2019); Chen et al. (2019); Azizian &amp; Lelarge (2020), which have O(n 2 ) memory and O(n 3 ) speed complexities. B.2 GRAPH POSITIONAL ENCODING Importance of Positional Information. The idea of positional encoding, i.e. the notion of global position of pixels in images, words in texts and nodes in graphs, plays a central role in the effectiveness of the most prominent neural networks with ConvNets (LeCun et al., 1998), RNNs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the ZINC, OGBG-MOLTOX21 and OGBG-MOLPCBA datasets. All scores are averaged over 4 runs with 4 different seeds. Bold: GNN's best score, Red: Dataset's best score.No PE results in lowest performance. InTable 1, the GNNs which do not use PE tend to give the worse performance on all the three datasets. This finding is aligned to the recent literature (Sec.</figDesc><table><row><cell></cell><cell>GatedGCN</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>16 504309</cell><cell>0.251?0.009</cell><cell>0.025?0.005</cell><cell>440.25</cell><cell>8.76s/1.08hr</cell></row><row><cell></cell><cell cols="2">GatedGCN LapPE</cell><cell>x</cell><cell>x</cell><cell>16 505011</cell><cell>0.202?0.006</cell><cell>0.033?0.003</cell><cell>426.00</cell><cell>8.91s/1.22hr</cell></row><row><cell></cell><cell cols="2">GatedGCN RWPE</cell><cell></cell><cell>x</cell><cell>16 522870</cell><cell>0.093?0.003</cell><cell>0.014?0.003</cell><cell>440.75</cell><cell>15.17s/1.99hr</cell></row><row><cell></cell><cell cols="2">GatedGCN RWPE</cell><cell></cell><cell></cell><cell>16 522870</cell><cell>0.090?0.001</cell><cell>0.013?0.004</cell><cell>460.50</cell><cell>33.06s/4.39hr</cell></row><row><cell></cell><cell>PNA</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>16 369235</cell><cell>0.141?0.004</cell><cell>0.020?0.003</cell><cell>451.25</cell><cell>79.67s/10.03hr</cell></row><row><cell></cell><cell cols="2">PNA RWPE</cell><cell></cell><cell>x</cell><cell>16 503061</cell><cell>0.095?0.002</cell><cell>0.022?0.002</cell><cell cols="2">462.25 127.69s/16.61hr</cell></row><row><cell></cell><cell>SAN</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>10 501314</cell><cell>0.181?0.004</cell><cell>0.017?0.004</cell><cell>433.50</cell><cell>74.33s/9.23hr</cell></row><row><cell></cell><cell cols="2">SAN RWPE</cell><cell></cell><cell>x</cell><cell>10 588066</cell><cell>0.104?0.004</cell><cell>0.016?0.002</cell><cell cols="2">462.50 134.74s/17.23hr</cell></row><row><cell></cell><cell>GraphiT</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>10 501313</cell><cell>0.181?0.006</cell><cell>0.021?0.003</cell><cell>493.25</cell><cell>63.54s/9.37hr</cell></row><row><cell></cell><cell cols="2">GraphiT RWPE</cell><cell></cell><cell>x</cell><cell>10 588065</cell><cell>0.106?0.002</cell><cell>0.028?0.002</cell><cell cols="2">420.50 125.39s/14.84hr</cell></row><row><cell></cell><cell cols="8">Model Init PE LSPE PosLoss L #Param TestAUC?s.d. TrainAUC?s.d. Epochs</cell><cell>Epoch/Total</cell></row><row><cell></cell><cell>GatedGCN</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>8 1003739</cell><cell>0.772?0.006</cell><cell>0.933?0.010</cell><cell>304.25</cell><cell>5.12s/0.46hr</cell></row><row><cell></cell><cell cols="2">GatedGCN LapPE</cell><cell>x</cell><cell>x</cell><cell>8 1004355</cell><cell>0.774?0.007</cell><cell>0.921?0.006</cell><cell>275.50</cell><cell>5.23s/0.48hr</cell></row><row><cell></cell><cell cols="2">GatedGCN RWPE</cell><cell></cell><cell>x</cell><cell>8 1063821</cell><cell>0.775?0.003</cell><cell>0.906?0.003</cell><cell>246.50</cell><cell>5.99s/0.63hr</cell></row><row><cell>MOLTOX21</cell><cell cols="2">PNA PNA RWPE x PNA RWPE SAN x</cell><cell>x x</cell><cell>x x x</cell><cell>8 5244849 8 5357393 8 5357393 10 957871</cell><cell>0.755?0.008 0.761?0.007 0.758?0.003 0.744?0.007</cell><cell>0.876?0.014 0.871?0.009 0.875?0.012 0.915?0.015</cell><cell>214.75 215.50 194.25 279.75</cell><cell>6.25s/0.38hr 7.61s/0.56hr 18.09s/1.07hr 18.06s/1.44hr</cell></row><row><cell></cell><cell cols="2">SAN RWPE</cell><cell></cell><cell>x</cell><cell>10 1051017</cell><cell>0.744?0.008</cell><cell>0.918?0.018</cell><cell>281.75</cell><cell>30.82s/2.84hr</cell></row><row><cell></cell><cell>GraphiT</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>10 957870</cell><cell>0.743?0.003</cell><cell>0.919?0.023</cell><cell>276.50</cell><cell>16.73s/1.36hr</cell></row><row><cell></cell><cell cols="2">GraphiT RWPE</cell><cell></cell><cell>x</cell><cell>10 1051788</cell><cell>0.746?0.010</cell><cell>0.934?0.016</cell><cell>279.75</cell><cell>27.92s/2.57hr</cell></row><row><cell></cell><cell cols="5">Model Init PE LSPE PosLoss L #Param</cell><cell>TestAP?s.d.</cell><cell>TrainAP?s.d.</cell><cell>Epochs</cell><cell>Epoch/Total</cell></row><row><cell>MOLPCBA</cell><cell cols="2">GatedGCN GatedGCN LapPE x GatedGCN RWPE PNA x</cell><cell>x x x</cell><cell>x x x x</cell><cell>8 1008263 8 1008879 8 1068721 4 6550839</cell><cell>0.262?0.001 0.266?0.002 0.267?0.002 0.279?0.003</cell><cell>0.401?0.057 0.391?0.003 0.403?0.006 0.448?0.004</cell><cell cols="2">190.50 177.00 181.00 206.43s/11.64hr 149.10s/7.91hr 152.94s/8.29hr 129.25 174.75s/6.34hr</cell></row><row><cell></cell><cell cols="2">PNA RWPE</cell><cell></cell><cell>x</cell><cell>4 6521029</cell><cell>0.284?0.002</cell><cell>0.383?0.005</cell><cell cols="2">320.00 201.05s/22.99hr</cell></row></table><note>ZINC Model Init PE LSPE PosLoss L #Param TestMAE?s.d. TrainMAE?s.d. Epochs Epoch/TotalB.2) that has guided research towards powerful PE methods for expressive GNNs. Besides, it can be observed that the extent of poor performance of models without PE against using a PE (LapPE or LSPE) is greater for ZINC than the two OGBG-MOL* datasets used. This difference can be explained by the fact that ZINC features are purely atom and bond descriptors whereas OGB-MOL* features consist additional information that is informative of e.g. if an atom is in ring, among others.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our best LSPE results from Table 1 with baselines and state-of-the-art GNNs (Sec. A.4) on each dataset. For ZINC, all the scores in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparing the final LSPE architecture against simpler models which add pre-computed PE at input layer (or final layer) of a GNN, using GatedGCN model on ZINC. The column 'Final h' denotes whether only the node structural features are used as final node features (denoted by h L ), or are concatenated with (i) node positional features (denoted by [h L , p L ]) at the final layer, (ii) pre-computed RWPE (denoted by [h L , RWPE]).</figDesc><table><row><cell cols="3">Model Init PE LSPE</cell><cell>Final h</cell><cell cols="5">L #Param Test MAE?s.d. Train MAE?s.d. #Epochs Epoch/Total</cell></row><row><cell>GatedGCN</cell><cell>x</cell><cell>x</cell><cell>h L</cell><cell>16 504309</cell><cell>0.251?0.009</cell><cell>0.025?0.005</cell><cell>440.25</cell><cell>8.76s/1.08hr</cell></row><row><cell cols="2">GatedGCN LapPE</cell><cell>x</cell><cell>h L</cell><cell>16 505011</cell><cell>0.202?0.006</cell><cell>0.033?0.003</cell><cell>426.00</cell><cell>8.91s/1.22hr</cell></row><row><cell cols="2">GatedGCN RWPE</cell><cell>x</cell><cell>h L</cell><cell>16 505947</cell><cell>0.122?0.003</cell><cell>0.013?0.003</cell><cell>436.25</cell><cell>9.14s/1.28hr</cell></row><row><cell>GatedGCN</cell><cell>x</cell><cell>x</cell><cell cols="2">[h L , RWPE] 16 515249</cell><cell>0.249?0.012</cell><cell>0.024?0.002</cell><cell>437.50</cell><cell>10.05s/1.55hr</cell></row><row><cell cols="2">GatedGCN LapPE</cell><cell></cell><cell>h L</cell><cell>16 516722</cell><cell>0.202?0.008</cell><cell>0.032?0.005</cell><cell>405.25</cell><cell>15.10s/1.84hr</cell></row><row><cell cols="2">GatedGCN LapPE</cell><cell></cell><cell>[h L , p L ]</cell><cell>16 520734</cell><cell>0.196?0.008</cell><cell>0.023?0.004</cell><cell>454.00</cell><cell>15.22s/2.06hr</cell></row><row><cell cols="2">GatedGCN RWPE</cell><cell></cell><cell>h L</cell><cell>16 518150</cell><cell>0.100?0.006</cell><cell>0.018?0.012</cell><cell>395.00</cell><cell>15.09s/1.73hr</cell></row><row><cell cols="2">GatedGCN RWPE</cell><cell></cell><cell>[h L , p L ]</cell><cell>16 522870</cell><cell>0.093?0.003</cell><cell>0.014?0.003</cell><cell>440.75</cell><cell>15.17s/1.99hr</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>STUDY OF k STEPS IN RWPE (SECTION 4.3)</figDesc><table><row><cell cols="9">GatedGCN-LSPE on ZINC (lower is better)</cell><cell cols="8">0.78 PNA-LSPE on TOX21 (higher is better)</cell></row><row><cell>Test MAE</cell><cell>0.10 0.11 0.12 0.13 0.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test ROC-AUC</cell><cell>0.73 0.74 0.75 0.76 0.77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2 4</cell><cell>8</cell><cell>12</cell><cell>16 k</cell><cell>20</cell><cell>24</cell><cell>28</cell><cell>32</cell><cell>2 4</cell><cell>8</cell><cell>12</cell><cell>16 k</cell><cell>20</cell><cell>24</cell><cell>28</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>So these GNNs are limited to small graphs like molecules and cannot scale to larger ones like social graphs or knowledge graphs.<ref type="bibr" target="#b17">Dwivedi &amp; Bresson (2021)</ref> designed a sparsely-connected architecture called GraphTransformer that reduces the complexity to O(E) by considering the graph topology instead of connecting each node to all other nodes, similar to GATs<ref type="bibr" target="#b61">(Veli?kovi? et al., 2018)</ref>. Still, the GraphTransformer was unable to outperform SOTA GNNs on benchmark datasets. Along this line,<ref type="bibr" target="#b32">Kreuzer et al. (2021)</ref> recently proposed Spectral Attention Networks (SANs), a fully-graph operable Transformer model that improves GraphTransformer<ref type="bibr" target="#b17">(Dwivedi &amp; Bresson, 2021)</ref> with two contributions. First, the authors designed a learnable PE module based on self-attention applied to the Laplacian eigenvectors, and injected this resultant PE into the input layer of the network. Second, SANs separated the parameters for real edges and complementary (non-real) edges, enabling the model to process the available sparse graph structure and long-range node connections in a learnable manner. However, their learned PE, based on the Laplacian eigenvectors, inherently exhibits the limitation of sign ambiguity.<ref type="bibr" target="#b32">Kreuzer et al. (2021)</ref> attempted at alleviating the sign ambiguity through another architecture named Edge-Wise LPE. However, the architecture's complexity being O(n 4 ) makes it a practically infeasible model.</figDesc><table /><note>). Besides, Transformer-based GNNs require to replace O(n) complexity with O(n 2 ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>= e 0 W k e ,C k,0 = e 0W k e ? R n?n?d k</figDesc><table><row><cell></cell><cell></cell><cell cols="7">C k,0 (35)</cell></row><row><cell cols="2">and p +1 i</cell><cell cols="2">= p i + tanh O p</cell><cell>H k=1 j?V</cell><cell cols="2">w k, p,ij j ?V w k, p,ij</cell><cell>v k, p,j</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>)</cell></row><row><cell>Q k, =</cell><cell>h p</cell><cell>W k, Q , K k, =</cell><cell>h p</cell><cell cols="2">W k, K , V k, =</cell><cell>h p</cell><cell>W k, V ? R n?d k</cell><cell>(33)</cell></row><row><cell>Q k, =</cell><cell>h p</cell><cell>W k, Q ,K k, =</cell><cell>h p</cell><cell cols="2">W k, K ,V k, =</cell><cell>h p</cell><cell>W k, V ? R n?d k</cell><cell>(34)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>where W 1 , W 2 ? R d?d , O , O p ? R d?d , W ? R d?d k , and d k = d/His the dimension of the k th head for a total of H heads. BN denotes the standard Batch Normalization</figDesc><table><row><cell>k, Q , W k, K , W k, V ,W k, Q ,W k, K ,W k, V</cell><cell>? R 2d?d k ,</cell></row><row><cell>W k, p,Q , W k, p,K , W k, p,V ,W k, p,Q ,W k, p,K ,W k, p,V ? R d?d k , W k e ,W k e , W k p,e ,W k p,e</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Additional hyperparamters for the models used inTable 1. k is the dimension of PE, or the steps of random walk if the PE is RWPE. ? and p is applicable to GraphiT (Sec. C.2.2). Init lr and Min lr are the initial and final learning rates for the learning rate decay strategy where the lr decays with a reduce Factor if the validation score doesn't improve after the Patience number of epochs. ? and ? are applicable when PosLoss is used (Eqn. 12).</figDesc><table><row><cell></cell><cell cols="5">Model Init PE LSPE PosLoss k</cell><cell>?</cell><cell cols="5">p Init lr Patience Factor Min lr</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>GatedGCN</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1e-3</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GatedGCN LapPE</cell><cell>x</cell><cell>x</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>1e-3</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GatedGCN RWPE</cell><cell></cell><cell>x</cell><cell>20</cell><cell>-</cell><cell>-</cell><cell>1e-3</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GatedGCN RWPE</cell><cell></cell><cell></cell><cell>20</cell><cell>-</cell><cell>-</cell><cell>1e-3</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>1</cell><cell>1e-1</cell></row><row><cell>ZINC</cell><cell cols="2">PNA PNA RWPE x</cell><cell>x</cell><cell>x x</cell><cell>-16</cell><cell>--</cell><cell>--</cell><cell>1e-3 1e-3</cell><cell>25 25</cell><cell>0.5 0.5</cell><cell>1e-6 1e-6</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>SAN</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3e-4</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">SAN RWPE</cell><cell></cell><cell>x</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>7e-4</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>GraphiT</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>-</cell><cell cols="2">0.25 16</cell><cell>3e-4</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GraphiT RWPE</cell><cell></cell><cell>x</cell><cell cols="3">16 0.25 16</cell><cell>7e-4</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="5">Model Init PE LSPE PosLoss k</cell><cell>?</cell><cell cols="5">p Init lr Patience Factor Min lr</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>GatedGCN</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1e-3</cell><cell>25</cell><cell>0.5</cell><cell>1e-5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GatedGCN LapPE</cell><cell>x</cell><cell>x</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>1e-3</cell><cell>25</cell><cell>0.5</cell><cell>1e-5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GatedGCN RWPE</cell><cell></cell><cell>x</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>1e-3</cell><cell>25</cell><cell>0.5</cell><cell>1e-5</cell><cell>-</cell><cell>-</cell></row><row><cell>MOLTOX21</cell><cell cols="2">PNA PNA RWPE x PNA RWPE SAN x</cell><cell>x x</cell><cell>x x x</cell><cell>-16 16 -</cell><cell>----</cell><cell>----</cell><cell>5e-4 5e-4 5e-4 7e-4</cell><cell>10 10 10 25</cell><cell>0.8 0.8 0.8 0.5</cell><cell>2e-5 2e-5 2e-5 1e-6</cell><cell cols="2">--1e-1 100 ----</cell></row><row><cell></cell><cell cols="2">SAN RWPE</cell><cell></cell><cell>x</cell><cell>12</cell><cell>-</cell><cell>-</cell><cell>7e-4</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>GraphiT</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>-</cell><cell cols="2">0.25 16</cell><cell>7e-4</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GraphiT RWPE</cell><cell></cell><cell>x</cell><cell cols="3">16 0.25 16</cell><cell>7e-4</cell><cell>25</cell><cell>0.5</cell><cell>1e-6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="5">Model Init PE LSPE PosLoss k</cell><cell>?</cell><cell cols="5">p Init lr Patience Factor Min lr</cell><cell>?</cell><cell>?</cell></row><row><cell>MOLPCBA</cell><cell cols="2">GatedGCN GatedGCN LapPE x GatedGCN RWPE PNA x</cell><cell>x x x</cell><cell>x x x x</cell><cell>-3 16 -</cell><cell>----</cell><cell>----</cell><cell>1e-3 1e-3 1e-3 5e-4</cell><cell>25 25 25 4</cell><cell>0.5 0.5 0.5 0.8</cell><cell>1e-4 1e-4 1e-4 2e-5</cell><cell>----</cell><cell>----</cell></row><row><cell></cell><cell cols="2">PNA RWPE</cell><cell></cell><cell>x</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>5e-4</cell><cell>10</cell><cell>0.8</cell><cell>2e-5</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Nvidia 1080Ti GPUs, with each single GPU running 1 experiment which equals to 4 parallel experiments on the machine at a single time.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>IMDB-MULTI ) and image superpixels (CIFAR10 (Dwivedi et al., 2020)) with graph classification being the prediction task.</p><p>IDBM-BINARY and IMDB-MULTI contain 1,000 and 1,500 graphs respectively which are egonetworks extracted from actor collaboration graphs. There are 2 classes in IMDB-BINARY and 3 classes in IMDB-MULTI with the class denoting the genre of the graph. CIFAR10 is a superpixel dataset of 60,000 graphs where each graph represents a connectivity structure of the image superpixels as nodes. There are 10 classes to be predicted as with the original CIFAR10 image dataset <ref type="bibr" target="#b33">(Krizhevsky et al., 2009</ref>).</p><p>In <ref type="table">Table 4</ref>, we show the advantage of using LSPE by instantiating GatedGCN-LSPE to train and evaluate on these non-molecular graphs. For evaluation and reporting of results, we follow the respective protocols as specified in ; Dwivedi et al. (2020) while comparing two models on a dataset on the similar range of model parameters. In accordance with the molecular datasets (Section 4.2), we consistently observe performance gains on each of the three datasets in <ref type="table">Table 4</ref>. This result further justifies the usefulness of LSPE to be applicable in general for representation learning on any graph domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL MODEL CONFIGURATION DETAILS</head><p>In <ref type="table">Table 5</ref>, additional details on the hyperparameters of different models used in <ref type="table">Table 1</ref> are provided. As for hardware information, all models were trained on Intel Xeon CPU E5-2690 v4 server having</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05205</idno>
		<title level="m">On the bottleneck of graph neural networks and its practical implications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Expressive power of invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wa?ss</forename><surname>Azizian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lelarge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15646</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unveiling the predictive power of static structure in glassy systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Grabska-Barwi?ska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Physics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="448" to="454" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine learning for combinatorial optimization: a methodological tour d&apos;horizon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Prouvost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="405" to="421" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Weisfeiler and lehman go cellular: Cw networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Mont?far</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12575</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<title level="m">Stefanos Zafeiriou, and Michael M Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Cappart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Ch?telat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09544</idno>
		<title level="m">Combinatorial optimization and reasoning with graph neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning symbolic physics with graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Miles D Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirley</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hejie</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01495</idno>
		<title level="m">On positional and structural node features for graph neural networks on non-attributed graphs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Traffic Prediction with Graph Neural Networks in Google Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Derrow-Pinion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Nunkesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueying</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veli?kovi?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Position information in transformers: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11090</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Deep Learning on Graphs: Methods and Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spectral grouping using the nystrom method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="214" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Utilising graph machine learning within drug discovery and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gaudelet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyothish</forename><surname>Jamasb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertrude</forename><surname>Regep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Jeremy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hayter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05716</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zinc: a free tool to discover chemistry for biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teague</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><forename type="middle">S</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan G</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How much position information do convolutional neural networks encode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Md Amirul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2323" to="2332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Transformers are graph neural networks. The Gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Memorybased graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Amir Hosein Khasahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quaid</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1laNeBYPB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prudencio</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tossou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03893</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A splitting method for orthogonality constrained problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="431" to="449" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16584</idno>
		<title level="m">Parameterized hypercomplex graph neural networks for graph classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepergcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">All you need to train deeper gcns</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph neural network-based diagnosis prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyue</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="379" to="390" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-Hamu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09902</idno>
		<title level="m">Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11136</idno>
		<title level="m">Provably powerful graph networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margot</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05667</idno>
		<title level="m">Graphit: Encoding graph structure in transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fake news detection on social media using geometric deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06673</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franka</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">What can be computed locally?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moni</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Stockmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1259" to="1277" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pinnersage: Multi-modal user embedding framework for recommendations at pinterest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chantat</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2311" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Automatic multimedia crossmodal correlation discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung-Jeong</forename><surname>Jia-Yu Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="653" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Approximation ratios of graph neural networks for combinatorial problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4081" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the equivalence between positional node embeddings and structural graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A deep learning approach to antibiotic discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><forename type="middle">M</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Craig R Macnair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zohar</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bloom-Ackermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="688" to="702" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Tox21 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tox21</surname></persName>
		</author>
		<ptr target="https://tripod.nih.gov/tox21/challenge/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pubchem&apos;s bioassay database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jewen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tugba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Karapetyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dracheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shoemaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="400" to="412" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NTI Series</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>En Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">chem sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05234</idno>
		<title level="m">Do transformers really perform bad for graph representation? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
