<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
							<email>fabian.manhardt@tum.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
							<email>tombari@in.tum.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
							<email>xyji@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>6D pose estimation from a single RGB image is a fundamental task in computer vision. The current top-performing deep learning-based methods rely on an indirect strategy, i.e., first establishing 2D-3D correspondences between the coordinates in the image plane and object coordinate system, and then applying a variant of the PnP/RANSAC algorithm. However, this two-stage pipeline is not end-toend trainable, thus is hard to be employed for many tasks requiring differentiable poses. On the other hand, methods based on direct regression are currently inferior to geometry-based methods. In this work, we perform an indepth investigation on both direct and indirect methods, and propose a simple yet effective Geometry-guided Direct Regression Network (GDR-Net) to learn the 6D pose in an end-to-end manner from dense correspondence-based intermediate geometric representations. Extensive experiments show that our approach remarkably outperforms state-ofthe-art methods on LM, LM-O and YCB-V datasets. Code is available at https://git.io/GDR-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the 6D pose, i.e. the 3D rotation and 3D translation, of objects with respect to the camera is a fundamental problem in computer vision. It has wide applicability to many real-world tasks such as robotic manipulation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b52">53]</ref>, augmented reality <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b49">50]</ref> and autonomous driving <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b57">58]</ref>. Most traditional methods rely on depth data for this task <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b53">54]</ref>, while monocular methods lagged considerably behind <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>. Nonetheless, with the advent of deep learning and especially the rise of Convolutional Neural Networks (CNNs), accuracy and robustness of monocular 6D object pose estimation have been consistently improving, even at times surpassing methods relying on depth data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Different strategies for predicting 6D pose from monoc-RGB 2D-3D Corr.  <ref type="figure">Figure 1</ref>: Illustration of GDR-Net. We directly regress the 6D object pose from a single RGB using a CNN and the learnable Patch-PnP by leveraging the guidance of intermediate geometric features including 2D-3D dense correspondences and surface region attention. ular data have been proposed. For instance, learning of an embedding space for pose <ref type="bibr" target="#b48">[49]</ref> or direct regression of the 3D rotation and translation <ref type="bibr" target="#b30">[31]</ref>. While these methods generally perform well, they usually lack in accuracy when compared with approaches that instead rely on establishing 2D-3D correspondences prior to estimating the 6D pose <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surface Regions 6D Pose</head><p>Differently, this latter class of methods usually involves solving the 6D pose through a variant of the PnP/RANSAC algorithm. While such a paradigm provides good estimates, it also suffers from several drawbacks. First, these methods are usually trained with a surrogate objective for correspondence regression, which does not necessarily reflect the actual 6D pose error after optimization. In practice, two sets of correspondences can have the same average error while describing completely different poses. Second, these approaches are not differentiable with respect to the estimated 6D pose, which limits learning. For instance, these meth-ods cannot be coupled with self-supervised learning from unlabeled real data <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1]</ref>, as they require the computation of the pose to be fully differentiable in order to obtain a signal between data and pose. Finally, the RANSAC iterative process can be very time-consuming when dealing with dense correspondences.</p><p>To summarize, while methods grounded on 2D-3D correspondences are dominating the field, they still exhibit downsides due to the decoupling of the problem into two separate steps one of which not differentiable. Consequently, some efforts have been devoted to enabling backpropagation through the PnP/RANSAC stage. However, this either requires a complex training strategy in order to have good initialization of scene coordinates <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, or can only handle sparse correspondences of a predefined set of keypoints <ref type="bibr" target="#b7">[8]</ref>. Recently, the authors of <ref type="bibr" target="#b18">[19]</ref> proposed to leverage PointNet <ref type="bibr" target="#b40">[41]</ref> in order to approximate PnP for sparse correspondences. While this proves to work well, PointNet disregards the fact that the correspondences are organized with respect to the image pixels, which tends to strongly deteriorate the performance as shown in <ref type="bibr" target="#b29">[30]</ref>.</p><p>In this work, we propose to overcome these limitations by establishing 2D-3D correspondences whilst computing the final 6D pose estimate in a fully differentiable way <ref type="figure">(Fig. 1)</ref>. In its core, we propose to learn the PnP optimization, exploiting the fact that the correspondences are organized in image space, which gives a significant boost in performance, outperforming all prior works. To summarize, we make the following contributions:</p><p>? We revisit the key ingredients in direct 6D pose regression and observe that by choosing appropriate representations for the pose parameters, methods based on direct regression show competitive performance compared with state-of-the-art correspondence-based indirect methods.</p><p>? We further propose a simple yet effective Geometryguided Direct Regression Network (GDR-Net) to boost the performance of direct 6D pose regression via leveraging the geometric guidance from dense correspondence-based intermediate representations.</p><p>Extensive experiments on LM <ref type="bibr" target="#b12">[13]</ref>, LM-O <ref type="bibr" target="#b2">[3]</ref>, and YCB-V <ref type="bibr" target="#b59">[60]</ref> datasets show that our unified GDR-Net approach achieves accurate, yet real-time and robust, monocular 6D object pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>While methods based on depth data used to dominate the field of 6D pose estimation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b53">54]</ref>, Deep Learning-based methods have recently demonstrated promising results for the task at hand <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. In this section, we review several commonly employed strategies for monocular 6D pose estimation.</p><p>Indirect Methods. The most popular approach is to establish 2D-3D correspondences, which are then leveraged to solve for the 6D pose using a variant of the RANSACbased PnP algorithm. For instance, <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b50">[51]</ref> compute the 2D projections of a set of fixed control points (e.g. the 3D corners of the encapsulating bounding box). To enhance the robustness, <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b39">[40]</ref> additionally conduct segmentation coupled with voting for each correspondence. However, the recent trend goes towards predicting dense rather than sparse correspondences <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b27">28]</ref>. Moreover, while <ref type="bibr" target="#b37">[38]</ref> leverages a GAN on top of dense correspondences to increase stability, <ref type="bibr" target="#b14">[15]</ref> makes use of fragments in order to account for ambiguities in pose.</p><p>Another orthogonal line of works aims at learning a latent embedding of pose which can be utilized for retrieval during inference. These embeddings are commonly either grounded on metric learning employing a triplet loss <ref type="bibr" target="#b56">[57]</ref>, or via training of an Auto-Encoder <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48]</ref>. Direct Methods. Although indirect methods, leveraging 2D-3D correspondences, are currently performing better, they cannot be directly employed in many tasks, which require the pose estimation to be differentiable <ref type="bibr" target="#b54">[55]</ref>. Hence, some methods directly regress the 6D pose, either leveraging a point matching loss <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref> or employing separate loss terms for each component <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b10">11]</ref>. Other methods discretize the pose space and conduct classification rather than regression <ref type="bibr" target="#b21">[22]</ref>. A few methods also try to solve a proxy task during optimization. Thereby, <ref type="bibr" target="#b32">[33]</ref> proposes to employ an edge-alignment loss using the distance transform, while <ref type="bibr" target="#b54">[55]</ref> harnesses differentiable rendering to allow training on unlabeled samples. Differentiable Indirect Methods. Recently, a few works attempt to make PnP/RANSAC differentiable. In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> the authors introduce a novel differentiable way to apply RANSAC via sharing of hypotheses based on the predicted distribution. Nonetheless, these approaches require a complex training strategy, as they expect a good initialization for the scene coordinates. As for PnP, <ref type="bibr" target="#b7">[8]</ref> employs the Implicit Function Theorem <ref type="bibr" target="#b22">[23]</ref> to enable the computation of analytical gradients w.r.t. the pose loss. Yet, it is computationally expensive especially given too many correspondences since PnP/RANSAC is still needed for both training and inference. Instead, <ref type="bibr" target="#b18">[19]</ref> attempts to learn the PnP stage with a PointNet-based architecture <ref type="bibr" target="#b40">[41]</ref> which learns to infer the 6D pose from a fixed set of sparse 2D-3D correspondences. Beyond Instance-level 6D Pose Estimation. Noteworthy, a few methods are recently trying to go beyond the instancelevel scenario, even estimating the pose <ref type="bibr" target="#b55">[56]</ref>, sometimes paired with shape <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>, for previously unseen objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given an RGB image I and a set of N objects O = { O i | i = 1, ? ? ? , N } together with their corresponding 3D CAD models M = { M i | i = 1, ? ? ? , N }, our goal is to estimate the 6D object pose P = [R|t] w.r.t. the camera for each object O present in I. Notice that R describes the 3D rotation and t denotes the 3D translation of the detected object. <ref type="figure" target="#fig_1">Fig. 2</ref> presents a schematic overview of the proposed methodology. In the core, we first detect all objects of interest using an off-the-shelf object detector, such as <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52]</ref>. For each detection, we then zoom in to the corresponding Region of Interest (RoI) and feed it to our network to predict several intermediate geometric feature maps. Finally, we directly regress the associated 6D object pose from the dense correspondence-based intermediate geometric features.</p><p>In the following, we first (Sec. 3.1) revisit the key ingredients of direct 6D object pose estimation methods. Afterwards (Sec. 3.2), we illustrate a simple yet effective Geometry-Guided Direct Regression Network (GDR-Net) which unifies regression-based direct methods and geometry-based indirect methods, thus harnessing the best of both worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting Direct 6D Object Pose Estimation</head><p>Direct 6D pose estimation methods usually differ in one or more of the following components. Firstly, the parameterization of the rotation R and translation t, and secondly, the employed loss for pose. In this section, we investigate different commonly used parameterizations and demonstrate that appropriate choices have significant impact on the 6D pose estimates. Parameterization of 3D Rotation. Several different parameterization can be employed to describe 3D rotations. Since many representations exhibit ambiguities, i.e. R i and R j describe the same rotation with R i = R j , most works rely on parametrizations that are unique to help training. Therefore, common choices are unit quaternions <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27]</ref>, log quaternions <ref type="bibr" target="#b36">[37]</ref>, or Lie algebra-based vectors <ref type="bibr" target="#b10">[11]</ref>.</p><p>Nevertheless, it is well-known that all representations with four or fewer dimensions for 3D rotation have discontinuities in the Euclidean space. When regressing a rotation, this introduces an error close to the discontinuities which becomes often significantly large. To overcome this limitation, <ref type="bibr" target="#b64">[65]</ref> proposed a novel continuous 6-dimensional representation for R in SO(3), which has proven promising <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b24">25]</ref>. Specifically, the 6-dimensional representation R 6d is defined as the first two columns of R</p><formula xml:id="formula_0">R 6d = [R ?1 | R ?2 ] .<label>(1)</label></formula><p>Given a 6-dimensional vector R 6d = [r 1 |r 2 ], the rotation matrix R = [R ?1 |R ?2 |R ?3 ] can be computed according to</p><formula xml:id="formula_1">? ? ? ? ? R ?1 = ?(r 1 ) R ?3 = ?(R ?1 ? r 2 ) R ?2 = R ?3 ? R ?1 ,<label>(2)</label></formula><p>where ?(?) denotes the vector normalization operation. Given the advantages of this representation, in this work we employ R 6d to parameterize the 3D rotation. Nevertheless, in contrast to <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b24">25]</ref>, we propose to let the network predict the allocentric representation <ref type="bibr" target="#b23">[24]</ref> of rotation R a6d . This representation is favored as it is viewpoint-invariant under 3D translations of the object. Hence, it is more suitable to deal with zoomed-in RoIs. Note that the egocentric rotation can be easily converted from allocentric rotation given 3D translation and camera intrinsics K following <ref type="bibr" target="#b23">[24]</ref>. Parameterization of 3D Translation. Since directly regressing the translation t = [t x , t y , t z ] T ? R 3 in 3D space does not work well in practice, previous works usually decouple the translation into the 2D location (o x , o y ) of the projected 3D centroid and the object's distance t z towards the camera. Given the camera intrinsics K, the translation can be calculated via back-projection</p><formula xml:id="formula_2">t = K ?1 t z [o x , o y , 1] T .<label>(3)</label></formula><p>Exemplary, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b48">49]</ref> approximate (o x , o y ) as the bounding box center (c x , c y ) and estimate t z using a reference camera distance. PoseCNN <ref type="bibr" target="#b59">[60]</ref> directly regresses (o x , o y ) and t z . Nonetheless, this is not suitable for dealing with zoomed-in RoIs, since it is essential for the network to estimate position and scale invariant parameters. Therefore, in our work we utilize a Scale-Invariant representation for Translation Estimation (SITE) <ref type="bibr" target="#b27">[28]</ref>. Concretely, given the size s o = max(w, h) and center (c x , c y ) of the detected bounding box and the ratio r = s zoom /s o w.r.t. the zoom-in size s zoom , the network regresses the scale-invariant translation parameters</p><formula xml:id="formula_3">t SITE = [? x , ? y , ? z ] T , where ? ? ? ? ? ? x = (o x ? c x )/w ? y = (o y ? c y )/h ? z = t z /r .<label>(4)</label></formula><p>Finally, the 3D translation can be solved according to Eq. 3. Disentangled 6D Pose Loss. Apart from the parameterization of rotation and translation, the choice of loss function is also crucial for 6D pose optimization. Instead of directly utilizing distances based on rotation and translation (e.g., angular distance, L 1 or L 2 distances), most works employ a variant of Point-Matching loss <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b24">25]</ref> based on the ADD(-S) metric <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3]</ref> in an effort to couple the estimation of rotation and translation. Inspired by <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b24">25]</ref>, we employ a novel variant of disentangled 6D pose loss via individually supervising the rotation R, the scale-invariant 2D object center (? x , ? y ), and the distance ? z .</p><formula xml:id="formula_4">L Pose = L R + L center + L z .<label>(5)</label></formula><p>Thereby,</p><formula xml:id="formula_5">? ? ? ? ? ? ? L R = avg x?M R x ?Rx 1 L center = (? x ?? x ,? y ?? y ) 1 L z = ? z ?? z 1 ,<label>(6)</label></formula><p>where? and? denote prediction and ground truth, respectively. To account for symmetric objects, givenR, the set of all possible ground-truth rotations under symmetry, we further extend our loss to a symmetry-aware formulation L R,sym = min R?R L R (R,R).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Geometry-guided Direct Regression Network</head><p>In this section, we present our Geometry-guided Direct Regression Network, which we dub GDR-Net. Harnessing dense correspondence-based geometric features, we directly regress 6D object pose. Thereby, GDR-Net unifies approaches based on dense correspondences and direct regression. Network Architecture. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we feed the GDR-Net with a zoomed-in RoI of size 256 ? 256 and predict three intermediate geometric feature maps with spatial size of 64 ? 64, which are composed of the Dense Correspondences Map (M 2D-3D ), the Surface Region Attention Map (M SRA ) and the Visible Object Mask (M vis ).</p><p>Our network is inspired by CDPN <ref type="bibr" target="#b27">[28]</ref>, a state-of-the-art dense correspondence-based method for indirect pose estimation. In essence, we keep the layers for regressing M XYZ and M vis , while removing the disentangled translation head. Additionally, we append the channels required by M SRA to the output layer. Since these intermediate geometric feature maps are all organized 2D-3D correspondences w.r.t. the image, we employ a simple yet effective 2D convolutional Patch-PnP module to directly regress the 6D object pose from M 2D-3D and M SRA .</p><p>The Patch-PnP module consists of three convolutional layers with kernel size 3 ? 3 and stride = 2, each followed by Group Normalization <ref type="bibr" target="#b58">[59]</ref> and ReLU activation. Two Fully Connected (FC) layers are then applied to the flattened feature, reducing the dimension from 8192 to 256. Finally, two parallel FC layers output the 3D rotation R parameterized as R 6d (Eq. 1) and 3D translation t parameterized as t SITE (Eq. 4), respectively. Dense Correspondences Maps (M 2D-3D ). In order to compute the Dense Correspondences Maps M 2D-3D , we first estimate the underlying Dense Coordinates Maps (M XYZ ). M 2D-3D can then be derived by stacking M XYZ onto the corresponding 2D pixel coordinates. In particular, given the CAD model of an object, M XYZ can be obtained by rendering the model's 3D object coordinates given the associated pose. Similar to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b55">56]</ref>, we let the network predict a normalized representation of M XYZ . Concretely, each channel of M XYZ is normalized within [0, 1] by (l x , l y , l z ), which is the size of corresponding tight 3D bounding box of the CAD model.</p><p>Notice that M 2D-3D does not only encode the 2D-3D correspondences, but also explicitly reflect the geometric shape information of objects. Moreover, as previously mentioned, since M 2D-3D is regular w.r.t. the image, we are capable of learning the 6D object pose via a simple 2D convolutional neural network (Patch-PnP). Surface Region Attention Maps (M SRA ). Inspired by <ref type="bibr" target="#b14">[15]</ref>, we let the network predict the surface regions as additional ambiguity-aware supervision. However, instead of coupling them with RANSAC, we use them within our Patch-PnP framework.</p><p>Essentially, the ground-truth regions M SRA can be derived from M XYZ employing farthest points sampling.</p><p>For each pixel we classify the corresponding regions, thus the probabilities in the predicted M SRA implicitly rep-resent the symmetry of an object. For instance, if a pixel is assigned to two potential fragments due to a plane of symmetry, Minimizing this assignment will return a probability of 0.5 for each fragment. Moreover, leveraging M SRA not only mitigates the influence of ambiguities but also acts as an auxiliary task on top of M 3D . In other words, it eases the learning of M 3D by first locating coarse regions and then regressing finer coordinates. We utilize M SRA as a symmetryaware attention to guide the learning of Patch-PnP. Geometry-guided 6D Object Pose Regression. The presented image-based geometric feature patches, i.e., M 2D-3D and M SRA , are then utilized to guide our proposed Patch-PnP for direct 6D object pose regression as</p><formula xml:id="formula_6">P = Patch-PnP(M 2D-3D , M SRA ).<label>(7)</label></formula><p>We employ L 1 loss for normalized M XYZ and visible masks M vis , and cross-entropy loss (CE) for M SRA .</p><p>LGeom</p><formula xml:id="formula_7">= M vis (MXYZ ?MXYZ) 1 + M vis ?Mvis 1 + CE(Mvis M SRA,MSRA).<label>(8)</label></formula><p>Thereby, denotes element-wise multiplication and we only supervise M XYZ and M SRA using the visible region.</p><p>The overall loss for GDR-Net can be summarized as L GDR = L Pose + L Geom . Notice that our GDR-Net can be trained end-to-end, without requiring any three-stage training strategy as in <ref type="bibr" target="#b27">[28]</ref>. Decoupling Detection and 6D Object Pose Estimation. Similar to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b24">25]</ref>, we mainly focus on the network for 6D object pose estimation and make use of an existing 2D object detector to obtain the zoomed-in input RoIs. This allows us to directly make use of the advances in runtime <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b1">2]</ref> and accuracy <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52]</ref> within the rapidly growing field of 2D object detection, without having to change or re-train the pose network. Therefore, we adopt a simplified Dynamic Zoom-In (DZI) <ref type="bibr" target="#b27">[28]</ref> to decouple the training of our GDR-Net and object detectors. During training, we first uniformly shift the center and scale of the ground-truth bounding boxes by a ratio of 25%. We then zoom in the input RoIs with a ratio of r = 1.5 while maintaing the original aspect ratio. This ensures that the area containing the object is approximately half the RoI. DZI can also circumvent the need of dealing with varying object sizes.</p><p>Noteworthy, although we employ a two-stage approach, one could also implement GDR-Net on top of any object detector and train it in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we first introduce our experimental setup, and then present the evaluation results for several commonly employed benchmark datasets. Thereby, we first present experiments on a synthetic toy dataset, which clearly demonstrates the benefit of our Patch-PnP compared to the classic optimization-driven PnP. Additionally, we demonstrate the effectiveness of our individual components by performing an ablative study on LM <ref type="bibr" target="#b12">[13]</ref>. Finally, we compare GDR-Net with state-of-the-art methods on two challenging datasets, i.e. LM-O <ref type="bibr" target="#b2">[3]</ref> and YCB-V [60].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Implementation Details. All our experiments are implemented using PyTorch <ref type="bibr" target="#b38">[39]</ref>. We train all our networks endto-end using the Ranger optimizer [29, 64, 61] 1 with a batch size of 24 and a base learning rate of 1e-4, which we anneal at 72% of the training phase using a cosine schedule <ref type="bibr" target="#b20">[21]</ref>. Datasets. We conduct our experiments on four datasets: Synthetic Sphere <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19]</ref>, LM <ref type="bibr" target="#b12">[13]</ref>, LM-O <ref type="bibr" target="#b2">[3]</ref>, and YCB-V <ref type="bibr" target="#b59">[60]</ref>. The Synthetic Sphere dataset contains 20k samples for training and 2k for testing, created by randomly capturing a unit sphere model using a virtual calibrated camera with focal length 800, resolution 640?480, and the principal point located at the image center. The Rotations and translations are uniformly sampled in 3D space, and within an interval of [?2, 2] ? [?2, 2] ? <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, respectively. LM dataset consists of 13 sequences, each containing ? 1.2k images with ground-truth poses for a single object with clutter and mild occlusion. We follow <ref type="bibr" target="#b4">[5]</ref> and employ ?15% of the RGB images for training and 85% for testing. We additionally use 1k rendered RGB images for each object during training as in <ref type="bibr" target="#b27">[28]</ref>. LM-O consists of 1214 images from a LM sequence, where the ground-truth poses of 8 visible objects with more occlusion are provided for testing. Apart from the real data from LM, we also leverage synthetic data for training. Similarly, we render 10k synthetic images (syn) for each object as in <ref type="bibr" target="#b39">[40]</ref>. YCB-V is a very challenging dataset exhibiting strong occlusion, clutter and several symmetric objects. It comprises over 110k real images captured with 21 objects, both with and without texture. For both LM-O and YCB-V, we also leverage the publicly available synthetic data using physically-based rendering (pbr) <ref type="bibr" target="#b17">[18]</ref> for training. Evaluation Metrics. We use two common metrics for 6D object pose evaluation, i.e. ADD(-S) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>, and n?, n cm <ref type="bibr" target="#b44">[45]</ref>. The ADD metric <ref type="bibr" target="#b12">[13]</ref> measures whether the average deviation of the transformed model points is less than 10% of the object's diameter (0.1d). For symmetric objects, the ADD-S metric is employed to measure the error as the average distance to the closest model point <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>. When evaluating on YCB-V, we also compute the AUC (area under curve) of ADD(-S) by varying the distance threshold with a maximum of 10 cm <ref type="bibr" target="#b59">[60]</ref>. The n?, n cm metric <ref type="bibr" target="#b44">[45]</ref> measures whether the rotation error is less than n?and the translation error is below n cm. Notice that to ac- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Toy Experiment on Synthetic Sphere</head><p>We conduct a toy experiment comparing our approach with PnP/RANSAC and <ref type="bibr" target="#b18">[19]</ref> on the Synthetic Sphere dataset. We generate M XYZ from the provided poses and feed them to our Patch-PnP. For fairness, M SRA is excluded from the input. Following <ref type="bibr" target="#b18">[19]</ref>, during training, we randomly add Gaussian noise N (0, ? 2 ) with ? ? U[0, 0.03] to each point of the dense coordinates maps. Since the coordinates maps are normalized in [0, 1], we choose 0.03 as it reflects approximately the same level of noise as in <ref type="bibr" target="#b18">[19]</ref>. Additionally, we randomly generated 0% to 30% of outliers for M XYZ <ref type="figure" target="#fig_2">(Fig. 3c</ref>). During testing, we report the relative ADD error w.r.t. the sphere's diameter on the test set with different levels of noise and outliers. Comparison with PnP/RANSAC and <ref type="bibr" target="#b18">[19]</ref>. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we demonstrate the effectiveness and robustness of our approach by comparing Patch-PnP with the traditional RANSAC-based EPnP <ref type="bibr" target="#b25">[26]</ref> and the learning-based PnP from <ref type="bibr" target="#b18">[19]</ref>). As depicted in <ref type="figure" target="#fig_2">Fig. 3</ref>, while RANSAC-based EPnP 2 is more accurate when noise is unrealistically minimal, learning-based PnP methods are much more accurate and robust as the level of noise increases. Moreover, Patch-PnP is significantly more robust than Single-Stage <ref type="bibr" target="#b18">[19]</ref> w.r.t. to noise and outliers, thanks to our geometrically rich and dense correspondences maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study on LM</head><p>We present several ablation experiments for the widely used LM dataset <ref type="bibr" target="#b12">[13]</ref>. We train a single GDR-Net for all objects for 160 epochs without applying any color augmen-tation. For fairness in evaluation, we leverage the detection results from Faster-RCNN as provided by <ref type="bibr" target="#b27">[28]</ref>. Number of Regions in M SRA . In Tab. 1a, we show results for different numbers of regions in M SRA . Thereby, without our attention M SRA (number of regions = 0), the accuracy is deliberately good, which suggests the effectiveness and versatility of Patch-PnP. Nevertheless, the overall accuracy can be further improved with increasing number of regions in M SRA , despite starting to saturate around 64 regions. Thus, we use 64 regions for M SRA in all other experiments as trade-off between accuracy and memory. Effectiveness of Patch-PnP. We demonstrate the effectiveness of the image-like geometric features (M 2D-3D , M SRA ) by comparing our Patch-PnP with traditional PnP/RANSAC <ref type="bibr" target="#b27">[28]</ref>, the PointNet-like <ref type="bibr" target="#b40">[41]</ref> PnP from <ref type="bibr" target="#b18">[19]</ref>, and a differentiable PnP (BPnP <ref type="bibr" target="#b7">[8]</ref>). For PointNet-like PnP, we extend the PointNet in <ref type="bibr" target="#b18">[19]</ref> to account for dense correspondences. Specifically, we utilize PointNet to pointwisely transform the spatially flattened geometric features (M 2D-3D and M SRA ) and directly predict the 6D pose with global max pooling followed by two FC layers. Since the correspondences are explicitly encoded in M 2D-3D , no special attention is needed for the keypoint orders as in <ref type="bibr" target="#b18">[19]</ref>. For BPnP <ref type="bibr" target="#b7">[8]</ref>, we replace the Patch-PnP in our framework with their implementation of BPnP 3 . As BPnP was originally designed for sparse keypoints, we further adapt it appropriately to deal with dense coordinates.</p><p>As shown in Tab. 1b, Patch-PnP is more accurate than traditional PnP/RANSAC (B0 vs. A0), PointNet-like PnP (B0 vs. C0) and BPnP (B0 vs. C1) in estimating the 6D pose. Furthermore, in terms of rotation, our Patch-PnP outperforms PointNet-like PnP by a large margin, which proves the importance of exploiting the ordering within the correspondences. Noteworthy, Patch-PnP is much faster in inference and up to 4? faster in training than BPnP, since the latter relies on PnP/RANSAC for both phases. Parameterization of 6D Pose. In Tab. 1b, we illustrate the impact of our proposed 6D pose parameterization. In particular, the 6-dimensional R 6d (Eq. 1) achieves a much more accurate estimate of R than commonly used representations such as unit quaternions <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b26">27]</ref>, log quaternions <ref type="bibr" target="#b36">[37]</ref> and the Lie algebra-based vectors <ref type="bibr" target="#b10">[11]</ref> (c.f. B0 vs. D1-D3, and G0 vs. G2). Moreover, we can deduce that the allocentric representation is significantly stronger than the egocentric formulation (B0 vs. D0).</p><p>Similarly, the parameterization of the 3D translation is of high importance. Essentially, directly predicting t in 3D space leads to worse results than leveraging the scaleinvariant formulation t SITE (E0 vs. B0). Additionally, replacing the scale-invariant ? z in t SITE with the absolute distance t z or directly regressing the object center (o x , o z )    Hence, when dealing with zoomed-in RoIs, it is essential to parameterize the 3D translation in a scale-invariant fashion. Ablation on Pose Loss. As mentioned in Section 3.1, the loss function has an impact on direct 6D pose regression. In Tab. 1b, we compare our disentangled L Pose to a simple angular loss and the Point-Matching loss <ref type="bibr" target="#b26">[27]</ref> (F0). Furthermore, we present its disentangled versions following <ref type="bibr" target="#b45">[46]</ref>. As shown in (B0 and F0-F4), all variants of the PM loss are clearly better than the angular loss in terms of rotation estimation. In addition, disentangling the rotation R and distance t z in L PM largely enhances the rotation accuracy. Nonetheless, the overall performance is slightly inferior to our disentangled formulation L Pose , which disentangles t SITE rather than the 3D translation t. It is worth noting that L R,sym has a rather insignificant contribution compared with L R . This can be accounted to the lack of severe symmetries in LM and to our proposed surface region attention M SRA . Effectiveness of Geometry-Guided Direct Regression. Furthermore, we train GDR-Net leveraging only our pose loss L Pose by discarding the geometric supervision L Geom . Surprisingly, even the simple version outperforms CDPN <ref type="bibr" target="#b27">[28]</ref> w.r.t. ADD(-S) 0.1d, when employing R 6d for rotation (Tab. 1b G0 vs. A0). Yet, we clearly outperform our baseline using GDR-Net with explicit geometric guidance. If we predict the rotation as allocentric quaternions, the accuracy decreases (G2 vs. G0), which can partially ac-count for the weak performance of previous direct methods <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b10">11]</ref>. Moreover, when we remove the guidance of M 2D , the accuracy drops significantly (G0 vs. G1). Based on these results, we can see that an appropriate geometric guidance is essential for direct 6D pose regression. Direct pose regression also enhances the learning of geometric features as the error signal from pose can be backpropagated. Tab. 1b (B1, B2) shows that when evaluating GDR-Net with PnP/RANSAC from the predicted M 2D-3D , the overall performance exceeds CDPN <ref type="bibr" target="#b27">[28]</ref>. Similar to CDPN, we run tests using PnP/RANSAC for R and Patch-PnP for t, which achieves the overall best accuracy (B2). This demonstrates that our unified GDR-Net can leverage the best of both worlds, namely, geometry-based indirect methods and direct methods. Effectiveness of Detection and Pose Decoupling. Similar to CDPN <ref type="bibr" target="#b27">[28]</ref>, we decouple the detector and GDR-Net by means of Dynamic Zoom-In (DZI). When evaluating GDR-Net with the Yolov3 detections from <ref type="bibr" target="#b27">[28]</ref>, the overall accuracy only drops slightly while the accuracy for ADD(-S) 0.1d almost remains unchanged (Tab. 1b H0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State of the Art</head><p>We compare our approach with state-of-the-art methods on the LM-O and YCB-V datasets. <ref type="bibr" target="#b3">4</ref>    We report the results evaluated w.r.t. ADD(-S), and AUC of ADD-S and ADD(-S). As in <ref type="bibr" target="#b59">[60]</ref>, ADD-S uses the symmetric metric for all objects, while ADD(-S) only uses the symmetric metric for symmetric objects. P.E. means whether the method is trained with 1 pose estimator for the whole dataset or 1 per object (N objects in total). Ref. stands for Refinement and "-" denotes unavailable results.</p><p>apply similar color augmentation as in <ref type="bibr" target="#b48">[49]</ref> to prevent overfitting. For YCB-V, due to the large number of symmetric objects, the symmetric variant for the pose loss L R,sym is employed. During testing, for LM-O, we employ Faster-RCNN <ref type="bibr" target="#b43">[44]</ref> to obtain 2D detections from the RGB images; for YCB-V, we utilize the publicly available detections from FCOS <ref type="bibr" target="#b51">[52]</ref>  <ref type="bibr" target="#b4">5</ref> .</p><p>Results on LM-O. Tab. 2 presents the results of GDR-Net compared with state-of-the-art methods on LM-O. When trained with "real+syn", our single GDR-Net is comparable to <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b61">62]</ref>. Nevertheless, using one network per object, we easily surpass state of the art without refinement. Moreover, our GDR-Net trained with "real+pbr" even outperforms the refinement-based method DeepIM <ref type="bibr" target="#b26">[27]</ref>.</p><p>Results on YCB-V. We compare GDR-Net to state-of-theart approaches on YCB-V in Tab. 3 (see supplement for detailed results). Our GDR-Net trained one network per object exceeds again state of the art, even without leveraging any refinement. Our single model for all objects is also comparable to the refinement-based methods such as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25]</ref> w.r.t. AUC of ADD-S metric. Noteworthy, our approach runs much faster than the methods requiring refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Runtime Analysis</head><p>On a desktop with an Intel 3.40GHz CPU and an NVIDIA 2080Ti GPU, given a 640 ? 480 image, using the Yolov3 <ref type="bibr" target="#b42">[43]</ref> detector, our approach takes ? 22ms for a single object and ? 35ms for 8 objects, including 15ms for detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we revisited the ingredients of direct 6D pose regression and proposed a novel GDR-Net to unify direct and geometry-based indirect methods. The key idea is to exploit the intermediate geometric features regarding 2D-3D correspondences organized regularly as image-like 2D patches, which facilitates us to utilize a simple yet effective 2D convolutional Patch-PnP to directly regress 6D pose from geometric guidance. Our approach achieves real-time, accurate and robust monocular 6D object pose estimation. In the future, we want to extend our work to more challenging scenarios, such as the lack of annotated real data <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b33">34]</ref> and unseen object categories or instances <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>PoseCNN <ref type="bibr" target="#b59">[60]</ref> SegDriven <ref type="bibr" target="#b19">[20]</ref> Single-Stage <ref type="bibr" target="#b18">[19]</ref> GDR-Net (Ours) P.E.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of PnP/RANSAC</head><p>The implementation and hyper-parameters of PnP/RANSAC follow the state-of-the-art method CDPN <ref type="bibr" target="#b27">[28]</ref> for all our experiments. Specifically, we leverage EPnP <ref type="bibr" target="#b25">[26]</ref> together with 100 RANSAC iterations using a reprojection error threshold of 3 and confidence threshold of 0.99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detailed Results of YCB-V</head><p>We present detailed evaluation results on YCB-V <ref type="bibr" target="#b59">[60]</ref> for our GDR-Net in Tab. B.1 and Tab. B.2 and compare them to state-of-the-art approaches w.r.t. ADD(-S) and AUC of ADD-S/ADD(-S), respectively. As for methods trained simultaneously for all objects, our GDR-Net clearly outperforms all other state-of-the-art methods. Furthermore, when GDR-Net is trained separately for each individual object, we can even surpass refinement-based methods such as DeepIM <ref type="bibr" target="#b26">[27]</ref> w.r.t. AUC of ADD-S/ADD(-S) metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. BOP Results on LM-O and YCB-V</head><p>In the main paper, we have presented the results on LM-O and YCB-V following the most commonly used evaluation protocol following another learned PnP <ref type="bibr" target="#b18">[19]</ref> and many other works such as <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref>. Nevertheless, the evaluation protocol of BOP Challenge <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> has recently become more popular. Therefore, we also present the results of our GDR-Net on LM-O and YCB-V under the BOP setup.</p><p>The BOP evaluation protocol differs from the former in three main aspects as follows. i) No real data should be used for LM-O, thus we only employ the provided synthetic pbr data <ref type="bibr" target="#b17">[18]</ref> for training on LM-O; ii) The number of test images for both LM-O and YCB-V is smaller, i.e., they only contains a subset of the original test images; iii) The evaluation metric is different. Thereby, for each dataset, an Average Recall (AR) score is reported by calculating the mean Average Recall of three different metrics: AR = (AR MSPD + AR MSSD + AR VSD )/3. Please refer to <ref type="bibr" target="#b17">[18]</ref> for the detailed explanation of these metrics.</p><p>Tab. C.3 presents the results of our GDR-Net on LM-O and YCB-V compared with other state-of-the-art RGB-based methods under BOP setup. Since our method is built on top of CDPN <ref type="bibr" target="#b27">[28]</ref>, we follow <ref type="bibr" target="#b27">[28]</ref>   <ref type="table">Table B</ref>.2: Detailed results on YCB-V [60] w.r.t. AUC of ADD-S and ADD(-S). As in <ref type="bibr" target="#b59">[60]</ref>, ADD-S uses the symmetric metric for all objects, while ADD(-S) only uses the symmetric metric for symmetric objects. P.E. means whether the method is trained with 1 pose estimator for the whole dataset or 1 per object (N objects in total). ( * ) denotes symmetric objects and "-" denotes unavailable results. stands for refinement. For each column, we denote the best score in bold and the second best score in italics.</p><p>the sake of fairness. We utilize the publicly available detections from FCOS [52] 6 following CDPNv2 <ref type="bibr" target="#b27">[28]</ref>. We can see that our GDR-Net significantly outperforms all other state-of-the-art methods without refinement. It is worth noting that most of these top-performing methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28]</ref> rely on the indirect PnP/RANSAC solver, while ours directly regresses the 6D object pose leveraging geometric guidance, which again demonstrates the effectiveness of our proposed learning-based Patch-PnP. Our GDR-Net even outperforms the state-of-the-art refinement-based method CosyPose <ref type="bibr" target="#b24">[25]</ref> on LM-O. On YCB-V, ours is worse than CosyPose but far better than all other methods without refinement. Nevertheless, our method runs much faster than CosyPose as no refinement step is needed. Moreover, our method can be combined with an additional refiner such as CosyPose to achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>We demonstrated additional qualitative results for LM <ref type="bibr" target="#b12">[13]</ref>, LM-O <ref type="bibr" target="#b2">[3]</ref>, and YCB-V <ref type="bibr" target="#b59">[60]</ref> in <ref type="figure" target="#fig_1">Fig. D.1, Fig. D.2</ref> and <ref type="figure" target="#fig_2">Fig. D.3</ref>, respectively. Thereby, in <ref type="figure">Fig. D.1</ref>, we visualize the 6D pose by overlaying the image with the corresponding transformed 3D bounding box. In <ref type="figure" target="#fig_1">Fig. D.2 and Fig. D.3</ref>, we illustrate the estimated 6D poses by rendering the 3D models on top of the input image and highlighting the respective contours. Note that while Blue constitutes the ground-truth poses, we demonstrate in Green the predicted poses from GDR-Net. For better visualization we cropped the images and zoomed into the area of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure D.1:</head><p>Qualitative Results on LM <ref type="bibr" target="#b12">[13]</ref>. We visualize the 6D pose by overlaying the image with the corresponding transformed 3D bounding box. We demonstrate in Blue and Green the ground-truth pose and the predicted pose, respectively.  <ref type="bibr" target="#b2">[3]</ref>. For each image, we visualize the 6D poses by rendering the 3D models and overlaying the contours on the right. We demonstrate in Blue and Green the ground-truth pose and the predicted pose, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure D.3:</head><p>Qualitative Results on YCB-V <ref type="bibr" target="#b59">[60]</ref>. For each image, we visualize the 6D poses by rendering the 3D models and overlaying the contours on the right. We demonstrate in Blue and Green the ground-truth pose and the predicted pose, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Framework of GDR-Net. Given an RGB image I, our GDR-Net takes the zoomed-in RoI (Dynamic Zoom-In for training, off-the-shelf detections for testing) as input and predicts several intermediate geometric features. Then the Patch-PnP directly regresses the 6D object pose from Dense Correspondences (M 2D-3D ) and Surface Region Attention (M SRA ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Results of PnP variants on Synthetic Sphere. (a, b): We compare our Patch-PnP module with the traditional RANSAC EPnP [26] and another learning-based PnP [19]. The pose error is reported as relative ADD error w.r.t. the sphere's diameter (y-axis in log-scale). (c): Zoomed-In (64 ? 64) synthetic examples for Patch-PnP.count for symmetries, n?, n cm is computed w.r.t. the smallest error for all possible ground-truth poses<ref type="bibr" target="#b26">[27]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a): Ablation of number of regions in M SRA . (b): Ablation of PnP type, the parameterization of R and t, loss type and geometric guidance. leads to inferior poses w.r.t. translation (B0 vs. E1, E2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure D. 2 :</head><label>2</label><figDesc>Qualitative Results on LM-O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Net (Ours) 35.5 76.3 93.7 62.1 63.2 95.5 71.0 B1 B0: ? Test with PnP/RANSAC 31.0 72.1 92.2 67.1 68.9 94.5 71.0 B2 B0: ? Patch-PnP for t; PnP/RANSAC for R 35.6 76.0 93.6 67.1 69.0 95.5 72.8 C0 B0: Patch-PnP ? PointNet-like PnP 29.2 72.6 92.3 44.5 45.8 94.3 63.1</figDesc><table><row><cell>Row Method</cell><cell></cell><cell cols="2">ADD(-S) 0.02d 0.05d 0.1d</cell><cell cols="3">2?, 2 cm 2?2 cm MEAN</cell></row><row><cell>A0 CDPN [28]</cell><cell></cell><cell>-</cell><cell>-89.9</cell><cell>-</cell><cell>-92.8</cell><cell>-</cell></row><row><cell cols="2">B0 GDR-C1 B0: Patch-PnP ? BPnP [8]</cell><cell cols="5">34.3 72.6 92.0 64.3 66.0 94.4 70.6</cell></row><row><cell cols="2">D0 B0: R a6d ? Egocentric R 6d</cell><cell cols="5">36.1 75.7 93.2 60.4 61.5 95.3 70.4</cell></row><row><cell cols="2">D1 B0: R a6d ? Allocentric quaternion</cell><cell cols="5">24.8 67.4 90.5 35.5 36.9 92.2 57.9</cell></row><row><cell cols="2">D2 B0: R a6d ? Allocentric log quaternion</cell><cell cols="5">22.7 64.6 88.9 33.7 35.4 90.9 56.0</cell></row><row><cell cols="2">D3 B0: R a6d ? Allocentric Lie algebra vector</cell><cell cols="5">23.0 66.3 89.7 33.8 35.3 91.4 56.6</cell></row><row><cell>E0 B0: t SITE ? t</cell><cell></cell><cell cols="5">28.3 72.0 92.4 61.6 63.2 94.6 68.7</cell></row><row><cell>E1 B0: t SITE ? (ox, oy); tz</cell><cell></cell><cell cols="5">31.4 73.7 93.3 50.4 51.6 94.7 65.8</cell></row><row><cell>E2 B0: ?z ? tz</cell><cell></cell><cell cols="5">32.8 73.5 93.3 63.3 64.8 94.9 70.4</cell></row><row><cell>x?M F0 B0: L Pose ? LPM = avg</cell><cell cols="6">(Rx +t) ? (Rx +t) 1 33.7 76.5 94.1 47.4 48.2 95.8 65.9</cell></row><row><cell cols="2">F1 F0: L PM ? Disentangling R; t</cell><cell cols="5">30.8 71.1 91.8 64.6 66.8 93.5 69.8</cell></row><row><cell cols="2">F2 F0: L PM ? Disentangling R; (tx, ty); tz</cell><cell cols="5">32.2 73.9 93.6 63.8 65.3 94.8 70.6</cell></row><row><cell>F3 B0: L R ? Angular loss</cell><cell></cell><cell cols="5">32.4 75.5 93.8 40.2 40.9 95.7 63.1</cell></row><row><cell>F4 B0: L R ? L R,sym</cell><cell></cell><cell cols="5">35.5 75.8 93.9 61.6 62.7 95.4 70.8</cell></row><row><cell>G0 B0: L GDR ? w/o L Geom</cell><cell></cell><cell cols="5">30.8 72.7 92.2 45.9 46.8 94.1 63.7</cell></row><row><cell>G1 G0: ? w/o M 2D</cell><cell></cell><cell cols="5">18.6 60.1 85.6 26.0 27.8 87.6 51.0</cell></row><row><cell cols="2">G2 G0: R a6d ? Allocentric quaternion</cell><cell cols="2">6.7 40.6 73.2</cell><cell>6.2</cell><cell cols="2">7.4 75.6 34.9</cell></row><row><cell cols="2">H0 B0: Faster-RCNN [44] ? Yolov3 [43]</cell><cell cols="5">33.9 75.6 93.7 60.9 62.1 95.2 70.2</cell></row><row><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation Study on LM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with State of the Art on LM-O. We report the Average Recall (%) of ADD(-S). P.E. means whether the method is trained with 1 pose estimator for the whole dataset or 1 per object (N objects in total). ( * ) denotes symmetric objects and "-" denotes unavailable results.</figDesc><table><row><cell>Method</cell><cell cols="2">Ref. P.E. ADD(-S)</cell><cell>AUC of ADD-S</cell><cell>AUC of ADD(-S)</cell></row><row><cell>PoseCNN [60]</cell><cell>1</cell><cell>21.3</cell><cell>75.9</cell><cell>61.3</cell></row><row><cell>SegDriven [20]</cell><cell>1</cell><cell>39.0</cell><cell>-</cell><cell>-</cell></row><row><cell>PVNet [40]</cell><cell>N</cell><cell>-</cell><cell>-</cell><cell>73.4</cell></row><row><cell>Single-Stage [19]</cell><cell>N</cell><cell>53.9</cell><cell>-</cell><cell>-</cell></row><row><cell>GDR-Net (Ours)</cell><cell>1</cell><cell>49.1</cell><cell>89.1</cell><cell>80.2</cell></row><row><cell>GDR-Net (Ours)</cell><cell>N</cell><cell>60.1</cell><cell>91.6</cell><cell>84.4</cell></row><row><cell>DeepIM [27]</cell><cell>1</cell><cell>-</cell><cell>88.1</cell><cell>81.9</cell></row><row><cell>CosyPose [25]</cell><cell>1</cell><cell>-</cell><cell>89.8</cell><cell>84.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with State of the Art on YCB-V.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table B.1: Detailed results on YCB-V [60] w.r.t. ADD(-S). P.E. means whether the method is trained with 1 pose estimator for the whole dataset or 1 per object (N objects in total). (</figDesc><table /><note>* ) denotes symmetric objects and "-" denotes unavailable results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Results on LM-O and YCB-V under BOP [18] setup. The results for other methods are obtained from https://bop.felk.cvut.cz/leaderboards/. The time (s) is the average image processing time averaged over the datasets. Ref.</figDesc><table><row><cell>Method</cell><cell>Ref.</cell><cell>ARMSPD</cell><cell>LM-O [3] ARMSSD</cell><cell>ARVSD</cell><cell>AR</cell><cell>ARMSPD</cell><cell cols="2">YCB-V [60] ARMSSD ARVSD</cell><cell>AR</cell><cell>Time (s)</cell></row><row><cell>AAE [49]</cell><cell></cell><cell>25.4</cell><cell>9.5</cell><cell>9.0</cell><cell>14.6</cell><cell>41.0</cell><cell>41.3</cell><cell>30.7</cell><cell>37.7</cell><cell>0.190</cell></row><row><cell>Pix2Pose [38]</cell><cell></cell><cell>55.0</cell><cell>30.7</cell><cell>23.3</cell><cell>36.3</cell><cell>57.1</cell><cell>42.9</cell><cell>37.2</cell><cell>45.7</cell><cell>1.168</cell></row><row><cell>EPOS [15]</cell><cell></cell><cell>65.9</cell><cell>38.0</cell><cell>29.0</cell><cell>44.3</cell><cell>78.3</cell><cell>67.7</cell><cell>62.6</cell><cell>69.6</cell><cell>0.530</cell></row><row><cell>CDPNv2 [28]</cell><cell></cell><cell>81.5</cell><cell>61.2</cell><cell>44.5</cell><cell>62.4</cell><cell>63.1</cell><cell>57.0</cell><cell>39.6</cell><cell>53.2</cell><cell>0.153</cell></row><row><cell>GDR-Net (Ours)</cell><cell></cell><cell>86.4</cell><cell>65.2</cell><cell>50.2</cell><cell>67.2</cell><cell>84.2</cell><cell>75.6</cell><cell>66.8</cell><cell>75.5</cell><cell>0.065</cell></row><row><cell>CosyPose [15]</cell><cell></cell><cell>81.2</cell><cell>60.6</cell><cell>48.0</cell><cell>63.3</cell><cell>85.0</cell><cell>84.2</cell><cell>77.2</cell><cell>82.1</cell><cell>0.395</cell></row><row><cell>Table C.3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Ranger means the RAdam<ref type="bibr" target="#b28">[29]</ref> optimizer combined with Lookahead<ref type="bibr" target="#b63">[64]</ref> and Gradient Centralization<ref type="bibr" target="#b60">[61]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We follow the state-of-the-art method CDPN<ref type="bibr" target="#b27">[28]</ref> for the implementation and hyper-parameters of PnP/RANSAC in all our experiments, see supplement for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/BoChenYS/BPnP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We follow the most commonly used evaluation protocol for LM-O and YCB-V, which has also been employed by another learned PnP<ref type="bibr" target="#b18">[19]</ref> and many other works such as<ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref>. We kindly refer the readers to our supplement for the results under BOP<ref type="bibr" target="#b17">[18]</ref> setup.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/LZGMatrix/BOP19 CDPN 2019ICCV</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/LZGMatrix/BOP19 CDPN 2019ICCV</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement We thank Zhigang Li, Xingyu Liu at Tsinghua University for their helpful discussion, and Nikolas Brasch at Technical University of Munich for proofreading. We also thank anonymous reviewers for their constructive comments. This work was supported in part by China Scholarship Council (CSC) Grant #201906210393, and in part by the National Key R&amp;D Program of China under Grant 2018AAA0102801.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monocular Differentiable Rendering for Self-Supervised 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Beker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Mihai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">YOLOv4: Optimal Speed and Accuracy of Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6D Object Pose Estimation Using 3D Object Coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DSAC-Differentiable RANSAC for Camera Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6684" to="6692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3364" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Less is More-6D Camera Localization via 3D Surface Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4654" to="4662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4322" to="4331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Canonical Shape Space for Category-level 6D Object Pose and Size Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11973" to="11982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The MOPED Framework: Object Recognition and Pose Estimation for Manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1284" to="1306" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LieNet: Real-time Monocular Object Instance 6D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient Response Maps for Real-time Detection of Textureless Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="876" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model based Training, Detection and Pose Estimation of Texture-less 3D Objects in Heavily Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Going Further with Point Pair Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">EPOS: Estimating 6D Pose of Objects with Symmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On Evaluation of 6D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Hoda?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Obdr??lek</forename><surname>And?t?p?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops (ECCVW)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BOP: Benchmark for 6D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Glentbuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Ihrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenophon</forename><surname>Zabulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Labbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<title level="m">BOP Challenge 2020 on 6D Object Localization. European Conference on Computer Vision Workshops (EC-CVW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-Stage 6D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation-driven 6D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic Gradient Descent with Warm Restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loshchilov</surname></persName>
		</author>
		<idno>2017. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SSD-6D: Making RGB-based 3D Detection and 6D Pose Estimation Great Again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold R Parks</surname></persName>
		</author>
		<title level="m">The Implicit Function Theorem: History, Theory, and Applications</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CosyPose: Consistent Multi-view Multi-object 6D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EPnP: An Accurate O(n) Solution to the PnP Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DeepIM: Deep Iterative Matching for 6D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the Variance of the Adaptive Learning Rate and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking Pseudo-Lidar Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Explaining the Ambiguity of Object Detection and 6D Pose From Visual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6841" to="6850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2069" to="2078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Model-based 6D Pose Refinement in RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">CPS++: Improving Class-level 6D Pose and Shape Estimation From Monocular Images With Self-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05848</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pose Estimation for Augmented Reality: a Hands-on Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Spindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics (TVCG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2633" to="2651" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Global Hypothesis Generation for 6D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erix</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LatentFusion: End-to-End Differentiable Reconstruction and Rendering for Unseen Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10710" to="10719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An Incremental Improvement</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Disentangling Monocular 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">HybridPose: 6D Object Pose Estimation Under Hybrid Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-Path Learning for Object Pose Estimation Across Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltan-Csaba</forename><surname>Puang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narunas</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Vaskevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Implicit 3D Orientation Learning for 6D Object Detection from RGB Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fulin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<title level="m">3D Mapping and 6D Pose Computation for Real Time Augmented Reality on Cylindrical Objects. IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Real-Time Seamless Single Shot 6D Object Pose Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">FCOS: Fully Convolutional One-Stage Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakumar</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="306" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A Method for 6D Pose Estimation of Free-form Rigid Objects Using Point Pair Features on Range Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chyi-Yeu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Llad?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mart?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2678</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self6D: Self-Supervised Monocular 6D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhun</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning Descriptors for Object Recognition and 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3109" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">6D-VNet: End-To-End 6-DoF Vehicle Pose Estimation From Monocular RGB Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canqun</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Group Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gradient-Centralization: A New Optimization Technique for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Conputer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">DPOD: 6D Pose Object Detector and Refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1941" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1386" to="1383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Lookahead Optimizer: k Steps Forward, 1 Step Back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9593" to="9604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On the Continuity of Rotation Representations in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Single Image 3D Object Detection and Pose Estimation for Grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mabel</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3936" to="3943" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
