<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation Learn-ing for Heterogeneous Information Networks via Embedding Events</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="1997-07">1997. July 1997</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoji</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yuan</surname></persName>
							<email>yuanb@sustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
							<email>xiny@sustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoji</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Southern University of Science and Technology Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Representation Learn-ing for Heterogeneous Information Networks via Embedding Events</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACM Woodstock conference</title>
						<meeting>ACM Woodstock conference <address><addrLine>El Paso, Texas USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">10</biblScope>
							<date type="published" when="1997-07">1997. July 1997</date>
						</imprint>
					</monogr>
					<note>KEYWORDS Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WOODSTOCK&apos;97, El Paso, Texas USA 123-4567-24-567/08/06. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network representation learning (NRL) has been widely used to help analyze large-scale networks through mapping original networks into a low-dimensional vector space. However, existing NRL methods ignore the impact of properties of relations on the object relevance in heterogeneous information networks (HINs). To tackle this issue, this paper proposes a new NRL framework, called Event2vec, for HINs to consider both quantities and properties of relations during the representation learning process. Speci cally, an event (i.e., a complete semantic unit) is used to represent the relation among multiple objects, and both event-driven rst-order and second-order proximities are de ned to measure the object relevance according to the quantities and properties of relations. We theoretically prove how event-driven proximities can be preserved in the embedding space by Event2vec, which utilizes event embeddings to facilitate learning the object embeddings. Experimental studies demonstrate the advantages of Event2vec over state-ofthe-art algorithms on four real-world datasets and three network analysis tasks (including network reconstruction, link prediction, and node classi cation).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WOODSTOCK'97, El Paso, Texas USA </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Heterogeneous information networks (HINs), which contain multiple types of objects and links, are ubiquitous in a variety of realworld scenarios such as social networks <ref type="bibr" target="#b16">[17]</ref>, bibliographic networks <ref type="bibr" target="#b24">[25]</ref>, and user interest networks <ref type="bibr" target="#b4">[5]</ref>. Many real-world HINs are large-scale, e.g., social networks with millions of nodes <ref type="bibr" target="#b7">[8]</ref>. To analyze large-scale networks in many applications, such as link prediction <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref>, node classi cation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, e ective network analysis techniques are needed. However, most network analysis methods su er from high computation and space cost <ref type="bibr" target="#b2">[3]</ref>. To tackle this problem, a mainstream idea is network representation learning (NRL), which maps original networks into a low-dimensional vector space while preserving as much of the original network information as possible. Using the low-dimensional vector representations of objects as input features, the performance of downstream network analysis can be improved <ref type="bibr" target="#b2">[3]</ref>. Due to the heterogeneities of both objects and relations, the primary challenge of NRL for HINs is that the representation learning process should e ectively capture original network structural and semantic information. To this end, this paper aims to propose an e ective NRL framework to learn object embeddings for HINs.</p><p>In real-world networks, there may exist some relations among multiple objects. Taking <ref type="figure" target="#fig_0">Figure 1</ref> as an example, the relation among authors, a paper, and a venue is an indecomposable unit. Decomposing it into pairwise object relations will lose some semantic information <ref type="bibr" target="#b27">[27]</ref>. Recently, hyperedge was used to represent the relation among multiple objects <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">27]</ref>, which can be regarded as a complete semantic unit called event <ref type="bibr" target="#b12">[13]</ref>. ese hyperedge-based  <ref type="figure">Figure 2</ref>: Examples of event-driven proximity of bibliographic networks methods measure the relation among multiple objects as a whole. However, they consider only the quantities of events and ignore their properties during the representation learning process.</p><p>Intuitively, objects involved in same events should be relevant. Meanwhile, objects involved in similar events should be relevant as well. As examples of two bibliographic networks illustrated in <ref type="figure">Figure 2</ref>. In <ref type="figure">Figure 2</ref>(a), author a 1 and a 2 published two papers together, they are both involved in event e 1 and e 2 . erefore, they are relevant. <ref type="figure">Figure 2</ref>(b) shows author a 1 and a 2 published papers with the same topic in the same venue. ey are involved in two similar events e 1 and e 2 , respectively. Hence, they should be relevant as well. e properties of events can facilitate capturing the semantic relevance among objects. e relevance among objects in HINs should be driven by both the number of their intersectional events (event-driven rst-order proximity) and the similarity between their events (event-driven second-order proximity).</p><p>In this paper, events are used to represent the relations among objects, and both event-driven rst-order and second-order proximities are used to measure the object relevance according to the quantities and properties of relations. We propose a new NRL framework, called Event2vec, to learn the object embeddings of HINs via two learning steps. e rst step uses an autoencoder to learn the event embeddings. Based on the event embeddings learned by the previous step, the second step obtains the object embeddings by preserving the event-driven proximities. We theoretically prove the leaning process which utilizes event embeddings to facilitate learning object embeddings is capable to preserve the event-driven proximities in the embedding space. e contributions of this paper are summarised as follows:</p><p>? We investigate the signi cance of properties of relations among multiple objects for learning HIN representations. ? We de ne the event-driven rst-order and second-order proximities to measure object relevance driven by quantities and properties of relations, respectively. ? We propose a new NRL framework called Event2vec to learn the object embeddings of HINs and theoretically prove Event2vec can preserve the event-driven rst-order and second-order proximities in the embedding space. ? Experiments on four real-world datasets and three network analysis tasks are conducted to demonstrate the e ectiveness of Event2vec. e rest of this paper is organized as follows. Section 2 reviews the related work. In Section 3, we give the de nition of the problem. e details of the proposed framework are given in Section 4. Section 5 presents the experimental results. Finally, we conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>e related work is in the area of network representation learning. Early works in NRL community were mainly designed for homogeneous information networks, which contain only a single type of objects and links. However, HINs are more ubiquitous in most complex real-world scenarios. Recently, representation learning for HINs has a racted increasing interest in the NRL community. We review the works in representation learning for homogeneous information networks and heterogeneous information networks in the following tow sub-sections, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Homogeneous Information Network Representation Learning</head><p>Many works have been proposed to learn representations of homogeneous information networks. ey can be classi ed into matrix factorization-based methods, probability-based methods, and deep learning-based methods. Matrix factorization-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23]</ref> represent relations between pairwise objects in the form of a matrix, e.g., adjacent matrix, Laplacian matrix, and factorize the matrix to obtain object embeddings using eigen-decomposition. Probability-based methods such as DeepWalk <ref type="bibr" target="#b21">[22]</ref> and node2vec <ref type="bibr" target="#b11">[12]</ref> use random walks to sample paths from the network and calculate object co-occurrence probabilities which are used to learn object embeddings via the Skip-gram model <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. LINE <ref type="bibr" target="#b25">[26]</ref> preserves both rst-order and second-order proximities of networks by minimizing the Kullback-Leibler divergence of two joint probability distributions for each pair nodes. Deep learning-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">29]</ref> use adjacent matrix, or object co-occurrence probability matrix, or graph convolution as input, and learn object embeddings via a deep neural network. ose methods are e ective to capture the structural and semantic information of homogeneous information networks. However, they fail to capture the complete semantic information of HINs since they ignore the di erent semantics of relations among di erent types of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Heterogenous Information Network Representation Learning</head><p>Researchers in NRL eld have increasingly engaged in HIN representation learning recently. e success of applying metapath <ref type="bibr" target="#b24">[25]</ref> in HIN analysis has motivated some researchers to carry out metapath-based methods to learn representations of HINs. Metap-ath2vec <ref type="bibr" target="#b6">[7]</ref> and HIN2Vec <ref type="bibr" target="#b9">[10]</ref> extend the Skip-gram model to learn the embeddings of HINs by employing the metapath-based random walks. HINE <ref type="bibr" target="#b13">[14]</ref> optimizes the de ned objective function which aims to preserve the metapath-based proximities. However, they only consider the relations between pairwise objects. In order to capture the complete semantics of relations among multiple objects, hyperedges have been used to represent the relations among  objects. HEBE <ref type="bibr" target="#b12">[13]</ref> preserves the proximites of the objects by modeling the relations among objects as hyperedges. DHNE <ref type="bibr" target="#b27">[27]</ref> is a hyperedge-based method that preserves both rst-order and secondorder hypergraph structural information through a semi-supervised neural network model. e aforementioned methods consider only the number of relations among objects while overlooking the impact of their properties. However, the properties of relations are important for NRL to capture the semantic information of HINs. On the contrary, Event2vec is able to consider both quantities and properties of relations. Given the bibliographic network shown in <ref type="figure" target="#fig_3">Figure 3</ref>(a), <ref type="figure" target="#fig_3">Figure  3</ref>(b) shows the results of representation learning using DeepWalk, DHNE, and Event2vec which demonstrate that the properties of relations among multiple objects can facilitate capturing the original network structural and semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>In this section, we formally de ne the problem of representation learning for HINs. Firstly, we give the de nition of HIN as presented below. D 1. (Heterogeneous Information Network <ref type="bibr" target="#b23">[24]</ref>). Given an information network G = (V , E,T ), where V is a set of vertexes, E is a set of links, and T is a set of object types and link types.</p><formula xml:id="formula_0">Let ?( ) : V ? T V ? T be an object type mapping function and ? (r ) : E ? T E ? T be a link mapping function. If |T V | + |T E | &gt; 2, we say that G is a heterogeneous information network. Note that if |T V | + |T E | = 2,</formula><p>it is degraded to a homogeneous information network.  2. (Events <ref type="bibr" target="#b12">[13]</ref>). An event e ? ? is an indecomposable unit formed by a set of objects, representing the consistent and complete semantic information of relation among multiple objects. ? i denotes the set of events that contain object i . <ref type="figure">Figure 2</ref>(a), the relation among a 1 , a 2 , p 1 , and c 1 is a complete semantic unit, denoted as an event e 2 . D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">(Incident Matrix</head><p>). An incident matrix H |V |?|? | is a matrix that shows the relationship between objects V and events ? in which each row represents an object and each column represents an event. If object i belongs to event e j , then  (Event-driven First-order Proximity (EFP)). e event-driven rst-order proximity of object i and j is de ned to be the ratio of the number of their intersectional events and the number of their unioned events:</p><formula xml:id="formula_1">H i, j = 1, otherwise H i, j = 0. Given an HIN with |T V | types of objects, there are |T V | inci- dent matrices {H t } |T V | t =1 in which each H t</formula><formula xml:id="formula_2">s 1 i, j = |? i ? ? j | |? i ? ? j | .<label>(1)</label></formula><p>In <ref type="figure">Figure 2</ref>(a), a 1 and a 2 have the EFP since they are contained in two same events e 1 and e 2 , s 1 1,2 = 2/2 = 1. EFP considers the relevance among objects driven by the quantities of their relations. Larger s 1 i, j of object i and object j indicates their stronger EFP. As a result, i and j should be closer in the embedding space. D 5. (Event-driven Second-order Proximity (ESP)). e event-driven second-order proximity of object i and j is de ned to be the average cosine similarity of their non-intersectional events:</p><formula xml:id="formula_3">s 2 i, j = sim(? i , ? j ) = 1 |? i ? ? j | e ?? i ,k ?? j ,e k sim(e, k).<label>(2)</label></formula><p>where sim(e, k) denotes the cosine similarity between e and k.</p><p>In <ref type="figure">Figure 2</ref>(b), a 1 and a 2 have the ESP since e 1 and e 2 are similar, s 2 1,2 = 1 2 sim(e 1 , e 2 ). ESP considers the relevance among objects driven by the properties of relations. Larger s 2 i, j of object i and object j indicates their stronger ESP. erefore, i and j should be closer in the embedding space. |V |, and preserve both event-driven rst-order and secondorder proximities of objects in the embedding space R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVENT2VEC</head><p>In this section, we introduce the proposed Event2vec to learn the object embeddings of HINs. As shown in <ref type="figure">Figure 4</ref>, a er generating events from the input pairwise-based HIN, the representation learning process of Event2vec consists of two steps. e rst step tries to learn the event embeddings. e second step obtains the object embeddings based on the learned event embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Event Generating</head><p>In this section, we introduce the event generating algorithm. We rst de ne the event identi er q for each link r of the original HIN by de ning the mapping function ?(r ) : r ? q. en the links with same event identi er are merged into an event e. e event identi er q for event e can be de ned as a sub-set objects of e. e choices of those objects are based on the characteristics of the network in question. For example, for a bibliographic network that contains three types of objects (author, paper, and venue), an incident that authors published a paper in a venue is an indecomposable semantic unit. erefore, the relation among authors, a paper, and a venue should be regarded as an event. en the event identi er can be de ned as every paper since one event just corresponds to one paper.</p><p>Once the event identi ers are de ned, events can be generated using Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Event Generating</head><p>Input:</p><formula xml:id="formula_4">HIN G = (V , E,T ), Event identi er mapping function ? Output: Events set ? begin ? ?? ?; for r ? E do q ?? ?(r ); for e ? ? do if ?(e) = q then e ?? e ? r ;</formula><p>update ? using e; break;</p><p>if ? is ? or ?(e) q, ?e ? ? then ? ?? ? ? r ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning Event Embeddings</head><p>We use an autoencoder model to learn the event embeddings. It is worth to note that other methods such as singular value decomposition <ref type="bibr" target="#b10">[11]</ref>, stacked denoising autoencoders <ref type="bibr" target="#b28">[28]</ref>, variational autoencoder <ref type="bibr" target="#b5">[6]</ref>, can also be adapted to learn the event embeddings. As shown in <ref type="figure">Figure 5</ref>, di erent types of objects require learning di erent mapping functions to map events into the same embedding space. Taking the incident matrices {H t } |T V | t =1 as inputs, we design a single hidden layer autoencoder model with |T V | sub-encoder components and |T V | sub-decoder components. Each component is designed for one type of object. e size of the input layer of each sub-encoder component is equal to the number of objects with the same type. e same scheme works for the output layer of each sub-decoder component. e encoder module maps the incident matrices {H t } |T V | t =1 from all sub-encoder components to the same embedding space. en, the embedding of each event Z i is obtained in the hidden layer by summing up the event embeddings {Z t i } |T V | t =1 generated from sub-encoder components:</p><formula xml:id="formula_5">Z t i = ? (W t * H t i + b t ),<label>(3)</label></formula><formula xml:id="formula_6">Z i = |T V | t =1 Z t i ,<label>(4)</label></formula><formula xml:id="formula_7">H t i = ? (? t * Z i +b t ),<label>(5)</label></formula><p>where t is the index for object types, ? is the sigmoid function, W and b are weights and bias of encoder modules and? andb are weights and bias of decoder modules.</p><p>e training objective of the autoencoder is to minimize the reconstruction error between inputs and outputs:</p><formula xml:id="formula_8">L r ec = |? | i=1 |T V | t =1 ||H t i ?? t i || 2 2<label>(6)</label></formula><p>We may not observe all links of the HINs. erefore, it is more meaningful for the autoencoder to reconstruct the non-zero elements of incident matrices correctly. Hence, we impose more penalty to the reconstruction error of the non-zero elements, following the suggestions from the previous paper <ref type="bibr" target="#b29">[29]</ref>. e reconstruction error function is revised as below.</p><formula xml:id="formula_9">L * r ec = |? | i=1 |T V | t =1 ||(H t i ?? t i ) ? t i || 2 2 = |T V | t =1 ||(H t ?? t ) ? t || 2 2 ,<label>(7)</label></formula><p>where is the Hadamard product,</p><formula xml:id="formula_10">? t i = {? t i, j } |V t | j=1 . We de ne ? t i, j = 1, if H t i j = 0, ?, otherwise. with ? &gt; 1.</formula><p>To avoid over ing, we add the L2-norm regularizer term to the loss function L r e where</p><formula xml:id="formula_11">L r e = |T V | t =1 (||W t || 2 2 + ||? t || 2 2 ).<label>(8)</label></formula><p>en the nal objective function is shown as follows:</p><formula xml:id="formula_12">L = L * r ec + ? L r e ,<label>(9)</label></formula><p>where ? ? 0 is the penalty rate.  e stochastic gradient descent algorithm is used to train the proposed autoencoder. e partial derivatives of L with respect to W and? can be calculated as follows:</p><formula xml:id="formula_13">?L W t = ?L * r ec ?? t + ? ?L r e ?? t , ?L W t = ?L * r ec ?W t + ? ?L r e ?W t ,<label>(10)</label></formula><p>e above derivatives of Eq.10 are obtained iteratively using the back-propagation algorithm (BP) <ref type="bibr" target="#b18">[19]</ref> during the training process.</p><p>A er training the model, the event embeddings can be obtained in the hidden layer by repeating the encoder process. Events that contain many identical objects will obtain similar embeddings using autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning Object Embeddings</head><p>e object embeddings are obtained based on the event embeddings learned from the above sub-section. As previously discussed, large s 1 i, j and s 2 i, j both deduce object i and j are relevant. To capture the relevance of objects, we need to preserve both their EFP and ESP. Based on this motivation, we obtain the object embedding i by taking the average of event embeddings {z e }, e ? ? i as below.</p><formula xml:id="formula_14">i = 1 |? i | e ?? i z e .<label>(11)</label></formula><p>Rewriting the above equation into matrix form as below, which we obtain Y t ,</p><formula xml:id="formula_15">Y t = (D t ) ?1 H t Z t ,<label>(12)</label></formula><p>where D t is the diagonal matrix that contains the t-th type of object degrees.</p><p>rough the above learning process, the learned object embeddings satisfy the following lemma, L</p><p>1. e similarity between embedding i of object i and embedding j of j is proportional to their event-driven rst-order proximity s 1 i, j and second-order proximity s 2 i, j . P . Given object i and j , the cosine similarity between their embeddings i and j is shown as below.</p><formula xml:id="formula_16">sim( i , j ) = sim( 1 |? i | e ?? i z e , 1 |? j | k ?? j z k ) ? e ?? i ,k ?? j sim(z e , z k ) ? 1 |? i ? ? j | ( e ?? i ,k ?? j ,e=k sim(z e , z k ) + e ?? i ,k ?? j ,e k sim(z e , z k )) = |? i ? ? j | |? i ? ? j | + sim(? i , ? j ) = s 1 i, j + s 2 i, j .<label>(13)</label></formula><p>Hence, objects that have many intersectional events and/or similar events will obtain similar embeddings. Speci cally, the larger s 1 i, j and/or s 2 i, j of objects i and j , the more similar embeddings they have. erefore, Event2vec is able to preserve both EFP and ESP of objects in the embedding space.</p><p>Finally, we present the Event2vec in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Event2vec</head><p>Input: e incident matrix H , the object degrees diagonal </p><formula xml:id="formula_17">matrix {D t } |T V | t =1 and the parameter ? Output: Event embeddings Z, object embeddings Y begin Initialize {W} |T V | t =1 , {?} |T V | t =1 , {b} |T V | t =1 , {b} |T V | t =1 ; repeat</formula><formula xml:id="formula_18">|T V | t =1 , {?} |T V | t =1 , {b} |T V | t =1 , {b} |T V | t =1</formula><p>using BP based on Eq.10; until the terminating condition is met Calculate Y by Eq.12; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Complexity Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>is section reports experimental results of Event2vec. We use four real-world datasets to evaluate our method on three network analysis tasks including network reconstruction, link prediction, and node classi cation. Speci cally, network reconstruction task is used to evaluate the performance of NRL methods for preserving structural information and link prediction and node classi cation are used to evaluate the performance of simultaneously preserving both the original network structural and semantic information. e source code of Event2vec is available at h ps:// github.com/ fuguoji/ event2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate our method on four real-world datasets, including DBLP <ref type="bibr" target="#b24">[25]</ref>, Douban <ref type="bibr" target="#b32">[32]</ref>, IMDB 1 and Yelp 2 . e brief information of each dataset is shown as follows.</p><p>? DBLP: DBLP is a bibliographic network in computer science collected from four research areas: database, data mining, machine learning, and information retrieval. In the dataset, 4057 authors, 20 venues and 100 papers are labeled with one of the four research areas.</p><p>? Douban: Douban was collected from a user review website Douban in China. We extracted a sub-network containing four types of objects for our experiments. ? IMDB: IMDB is a link dataset collected from the Internet Movie Data. e network used in the experiment contains four types of objects. In the dataset, 1357 movies are labeled with at least one of the 23 labels. ? Yelp: e dataset was extracted from a user review website in America, Yelp, containing four types of objects.</p><p>All four datasets are used in network reconstruction and link prediction tasks, but only DBLP and IMDB are used in the node classi cation task since only these two datasets provide the ground truth of object labels. e detailed statistics of datasets are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Algorithms</head><p>We compare our method with ve state-of-the-art methods, including DeepWalk <ref type="bibr" target="#b21">[22]</ref>, node2vec <ref type="bibr" target="#b11">[12]</ref>, LINE <ref type="bibr" target="#b25">[26]</ref>, metapath2vec <ref type="bibr" target="#b6">[7]</ref>, and DHNE <ref type="bibr" target="#b27">[27]</ref>. e rst three are homogeneous NRL methods, they are widely used in learning representations of homogeneous information networks. Metapath2vec is a metapath-based method designed for learning representations of HINs. DHNE is a recent NRL method using hyperedges to model the relations among multiple objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Parameter Settings</head><p>In the experiments, the representation size is uniformly set as 64 for all methods. As same as the se ing in the previous paper ( <ref type="bibr" target="#b27">[27]</ref>), for Deepwalk and node2vec, we set the window size, walk length and the number of walks on each vertex as 10, 40, and 10, respectively. For LINE, the number of negative samples is set as 5 and the learning rate as 0.025. For metapath2vec, we follow the suggestions from the papers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">32]</ref>, the metapaths we chose for DBLP are "APA" and "APCPA", for Douban and IMBD are "MUM", "MAM" and "MDM", for Yelp are "BUB", "BLB" and "BCB". For DHNE, following the se ing on paper <ref type="bibr" target="#b27">[27]</ref> we use one-layer full connection layer to learn tuplewise similarity function, the size of hidden layer is set as 64 and the size of fully connection layer is set as the sum of the embedding length from all types, 256. e parameter ? in DHNE is tuned by grid search from {0.01, 0.1, 1, 2, 5, 10} and the learning rate is set as 0.025.</p><p>For the Event2vec, we use one hidden layer autoencoder for all experiments, the size of the hidden layer is 64 which equals the representation size. e parameter ? is set as 30, 100, 2, 80 for DBLP, Douban, IMDB, and Yelp respectively. e learning rate is set as 0.025 for all experiments. e event identi er for DBLP, Douban, IMDB, and Yelp are set as paper, movie, movie, and business, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Network Reconstruction</head><p>e network reconstruction task can be used to evaluate the performance of preserving original network structural information. In this section, the embeddings of objects obtained by the embedding methods are used to predict the links of the original networks, speci cally, we predict the links among objects based on cosine similarity of their embeddings. e evaluation metrics used in  network reconstruction task is AUC <ref type="bibr" target="#b8">[9]</ref>. We independently run each experiment ten times and present the average performance of network reconstruction on all four datasets in <ref type="table" target="#tab_3">Table 2</ref>. e standard deviation is less than 0.015 for all experiments. We have the following observations from the results:</p><p>? Event2vec obtains the best performance on DBLP and IMDB. e AUC values of Douban and Yelp obtained by Event2vec are 0.872 and 0.973, respectively. Overall, the results demonstrate that Event2vec can preserve the original network structure information e ectively. ? DHNE and Event2vec are both hyperedge-based NRL methods. However, Event2vec outperforms DHNE on all four datasets which demonstrate Event2vec is more e ective to preserve the pair-wise based structural information of HINs via preserving the EFP and ESP. ? Deepwalk and node2vec obtain good performance on all four datasets and perform be er than Event2vec on Douban and Yelp. ey can preserve the structural information of HINs e ectively. However, they do not excel at preserving the semantic information, as shown in both the below link prediction and node classi cation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Link Prediction</head><p>Link prediction can be used to evaluate the performance of NRL algorithms on capturing the implicit relevance of objects. e be er performance on link prediction, the more e ective NRL algorithm is. We present the link prediction task on object embeddings obtained on four datasets by all NRL algorithms. Speci cally, we predict the links among objects based on the cosine similarity of their embeddings. e evaluation metrics used in this task is AUC. We randomly split the edges of the original HIN for training and testing. e training set contains 80% edges of the original network and the testing set contains the le 20%. Each experiment is independently run 10 times and the average performances on the testing set are shown in <ref type="table" target="#tab_4">Table 3</ref>. e standard deviation is less than 0.015 for all experiments. for link prediction on all datasets. e previous task, network reconstruction, has shown DeepWalk and node2vec can work e ectively for preserving the structural information of HINs, while the results of link prediction demonstrate that their weakness for preserving the semantic information. As a contrary, it shows that Event2vec is able to e ectively preserve both the structural and semantic information. ? DHNE and Event2vec both consider the relationships among multiple objects as a whole. e di erence between them is that Event2vec considers both the quantities and properties of relations during the representation learning process, while DHNE considers only the former. However, Event2vec signi cantly outperforms DHNE on all four datasets. It demonstrates the properties of relations can facilitate capturing the implicit semantic information.</p><p>Furthermore, we repeat the link prediction task on DBLP with di erent sparsity. e ratio of remaining edges are varied from 10% to 90% with a step 10%, and the rest is used to form the testing set. Each experiment is independently conducted ten times and the average performance is shown in <ref type="figure" target="#fig_8">Figure 6</ref>. We can observe that Event2vec obtains the best performance on all sparse networks. Event2vec can work e ectively on sparse networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Node Classi cation</head><p>Node classi cation in network analysis is an important task in many applications such as user classi cation in social networks, movie classi cation in movie-user networks, and so on. In this section, we conduct the node classi cation tasks on DBLP and IMDB to evaluate the e ectiveness of NRL algorithms on preserving original HIN information. e be er node classi cation performance NRL algorithm obtain, the be er e ectiveness it has. e embeddings of objects generated from di erent methods are used as input features to classify the objects, and the classi er used in our experiments is logistic regression. We randomly sample 10% to 90% of the labeled objects as the training set and use the rest labeled objects as the testing set. For DBLP, since each labeled object only receives one label, we use the precision and AUC as the evaluation metrics. For IMDB, each labeled movie has at least one label. erefore, Micro-f1 and Macro-f1 are used as the evaluation metrics. Each experiment is conducted ten times and the average performance is reported in <ref type="figure" target="#fig_12">Figure 7</ref>. e standard deviation is less than 0.015 for all experiments.</p><p>From the experimental results, we can observe that:</p><p>? Event2vec performs be er than all baselines for node classi cation task on DBLP and IMDB. It demonstrates that Event2vec is e ective to preserve the original structural and semantic information of the original HINs. ? Event2vec achieves good performance when there are few labeled objects, even outperforming the performance of some baselines obtained on the cases of vast labeled objects. It demonstrates the robustness of the predictable power of object embeddings obtained by Event2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Parameter Sensitivity Analysis</head><p>We conduct node classi cation task experiments on DBLP to study the parameter sensitivity in this section. <ref type="figure" target="#fig_13">Figure 8</ref> shows using 50% as training set and le as the testing set, the node classi cation performance (AUC) as a function of one of the chosen parameter when xing the others. <ref type="figure" target="#fig_13">Figure 8(a)</ref> shows that the performance of node classi cation quickly improves as the number of embedding dimension d increases at the beginning, and become stable when the d is larger than 32. e impact of embedding dimension is reasonable since too small d is inadequate for embodying rich information of HINs. However, when the d is large enough, increasing d will not enrich the network information embodied in the embeddings. <ref type="figure" target="#fig_13">Figure 8</ref>(b) shows that when ? is set as 1, which means the nonzero elements are as important as zero elements of the incident matrix, the performance of node classi cation is poor. As discussed in section 4, the autoencoder reconstructs the existing link correctly is more important. erefore, when we set a larger penalty weight to the non-zero elements, the performance of node classi cation quickly improves and nally becomes stable when the penalty rate is large enough.</p><p>With xing the number of neurons in each layer to 64, we conduct the experiments to study the sensitivity of the number of hidden layers of autoencoder and present the results at <ref type="figure" target="#fig_13">Figure 8(c)</ref>. e results of <ref type="figure" target="#fig_13">Figure 8</ref>(c) show that the performance of node classication reduces as the number of hidden layers increases and nally as worse as random guessing when the number of hidden layers is larger than 7. Comparing to the single hidden layer autoencoder, deeper autoencoders are easier to over-t the data. It demonstrates that simple models are su cient to capture the original structural and semantic information of HINs well in our proposed NRL framework. Furthermore, simple models perform usually more robust to their hyper-parameters and are less prone to over-ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we showed the relevance among multiple objects in HINs should be considered as a whole and such relevance is driven by both the quantities and properties of relations. However, the existing NRL methods consider only the quantities of relations and ignore the impact of their properties. To tackle this issue, we de ned the EFP and ESP to measure the object relevance according to the quantities and properties of relations, respectively. A new NRL framework called Event2vec was proposed to learn the object embeddings for HINs, which was theoretically proved that it is able to preserve both the EFP and ESP during the learning process.</p><p>To evaluate the performance of the proposed method, we conducted three network analysis tasks on four real-world datasets. e results of network reconstruction showed that Event2vec can e ectively preserve the original structural information of HINs. Event2vec signi cantly outperformed the baselines on link prediction and node classi cation tasks that demonstrated Event2vec is e ective to preserve both structural and semantic information of HINs. DeepWalk and node2vec were shown to work e ectively on network reconstruction task as well. However, the results of link prediction and node classi cation indicated their weakness for preserving the semantic information of HINs. Overall, the experimental results demonstrated the e ectiveness of Event2vec for preserving the original structural and semantic information of HINs through preserving the EFP and ESP. Future work includes exploring more e cient event generating strategies and more powerful models which are used to learn event embeddings in our framework. Since the computational complexity of event generating is O(|E||?|). Designing more e cient event generating strategies can reduce the computational cost and improve the scalability of Event2vec. is paper used the traditional autoencoder to learn event embeddings, while advanced autoencoders such as denoising autoencoders <ref type="bibr" target="#b28">[28]</ref>, variational autoencoders <ref type="bibr" target="#b5">[6]</ref> or other models may work be er for this task. Using more powerful models to learn event embeddings in Event2vec can further improve its performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>? 2016 Copyright held by the owner/author(s). 123-4567-24-567/08/06. . . $15.00 DOI: 10.475/123 4 Example of a bibliographic network to illustrate the difference between pairwise and hyperedge-based networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Example of using DeepWalk((b) le ), DHNE((b) middle) and Event2vec((b) right) to learn the 2-dimensonal object embeddings. In the embedding space, a 3 , a 4 , and a 5 should be close since they have cooperations in publishing papers; a 1 and a 2 published same topic papers in the same venue, they have the semantic relevance and hence should be close as well. Event2vec obtains the best performance in capturing the above object relevance. e properties of relations facilitate capturing the object relevance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 (</head><label>1</label><figDesc>a) gives an example of HINs, i.e., a tiny bibliographic network containing three types of objects (author, paper, and venue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>D</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>represents the relationship among t-th type of objects and all events.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>D 6 .</head><label>6</label><figDesc>(Heterogeneous Information Network Representation Learning). Given an HIN G = (V , E,T ), HIN representation learning aims to learn a mapping function f : V ? Z ? R d where d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>e framework of Event2vec. e framework of autoencoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>e</head><label></label><figDesc>computational complexity of event generating is O(|E||?|), where |E| is the number of links in HIN and |?| is the number of generated events. e computational complexity of training autoencoder is O(|V |dbI ), where |V | is the number of objects in HIN, d is the representation size, b is the batch size and I is the number of iterations. e computational complexity of generating object embeddings is O(|V ||?|d). en the total computational complexity of Event2vec is O(|E||?|) + O(|V |dbI ) + (|V ||?|d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Link prediction results on DBLP of di erent sparsity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Top: node classi cation results on DBLP using Precision and AUC as evaluation metrics; bottom: node classi cation results on IMDB using Micro-f1 and Macro-f1 as evaluation metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Parameter sensitivity in node classi cation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Description of four datasets.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell cols="2">Object type</cell><cell></cell><cell></cell><cell>#(V)</cell><cell></cell><cell>#(E)</cell></row><row><cell>DBLP</cell><cell>author</cell><cell>paper</cell><cell>venue</cell><cell>term</cell><cell cols="2">14475 14376</cell><cell>20</cell><cell>8920 170794</cell></row><row><cell>Douban</cell><cell>user</cell><cell>movie</cell><cell>actor</cell><cell>director</cell><cell>3022</cell><cell>6977</cell><cell>3004</cell><cell>789 214392</cell></row><row><cell>IMDB</cell><cell>user</cell><cell>movie</cell><cell>actor</cell><cell>director</cell><cell>943</cell><cell cols="3">1360 42275 918 136093</cell></row><row><cell>Yelp</cell><cell>user</cell><cell cols="5">business location category 14085 14037</cell><cell>62</cell><cell>575 247698</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>AUC of network reconstruction.</figDesc><table><row><cell>Datasets</cell><cell cols="3">DBLP Douban IMDB Yelp</cell></row><row><cell>Event2vec</cell><cell>0.982</cell><cell>0.872</cell><cell>0.987 0.924</cell></row><row><cell>DeepWalk</cell><cell>0.964</cell><cell>0.930</cell><cell>0.975 0.973</cell></row><row><cell>node2vec</cell><cell>0.946</cell><cell>0.912</cell><cell>0.929 0.955</cell></row><row><cell>LINE</cell><cell>0.535</cell><cell>0.644</cell><cell>0.564 0.514</cell></row><row><cell cols="2">metapath2vec 0.884</cell><cell>0.891</cell><cell>0.846 0.738</cell></row><row><cell>DHNE</cell><cell>0.503</cell><cell>0.714</cell><cell>0.739 0.549</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>AUC of link prediction.</figDesc><table><row><cell>Methods</cell><cell cols="3">DBLP Douban IMDB Yelp</cell></row><row><cell>Event2vec</cell><cell>0.901</cell><cell>0.823</cell><cell>0.894 0.862</cell></row><row><cell>DeepWalk</cell><cell>0.794</cell><cell>0.677</cell><cell>0.839 0.841</cell></row><row><cell>node2vec</cell><cell>0.709</cell><cell>0.618</cell><cell>0.652 0.783</cell></row><row><cell>LINE</cell><cell>0.697</cell><cell>0.710</cell><cell>0.748 0.531</cell></row><row><cell cols="2">metapath2vec 0.551</cell><cell>0.589</cell><cell>0.909 0.616</cell></row><row><cell>DHNE</cell><cell>0.632</cell><cell>0.761</cell><cell>0.811 0.546</cell></row><row><cell cols="4">From the results, we have the following observations:</cell></row><row><cell cols="4">? Event2vec signi cantly outperforms all baselines on DBLP,</cell></row><row><cell cols="4">Douban, and Yelp, and obtains competitive performance</cell></row><row><cell cols="4">against metapath2vec on IMDB. Event2vec is e ective to</cell></row><row><cell cols="4">capture the implicit object relevance.</cell></row><row><cell cols="4">? Event2vec performs be er than DeepWalk and node2vec</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">h p://komarix.org/ac/ds/ 2 h ps://www.yelp.com/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on World Wide Web</title>
		<meeting>the 22nd International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: problems, techniques and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1616" to="1637" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Learning Graph Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Arti cial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Heterogeneous Neural A entive Factorization Machine for Rating Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="833" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">e bene ts of Facebook friends: Social capital and college students use of online social network sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nicole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cli</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Mediated Communication</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1143" to="1168" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An introduction to ROC analysis. Pa ern Recognition Le ers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Fawce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="861" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1797" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Singular value decomposition and least squares solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische mathematik</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="403" to="420" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Embedding learning with events in heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2428" to="2441" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Heterogeneous information network embedding for meta path based proximity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Mamoulis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05291</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning latent representations of nodes for classifying in heterogeneous social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Web search and data mining</title>
		<meeting>the 7th ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ranking-based classi cation of heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Danilevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1298" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised learning over heterogeneous information networks by ensemble of meta-graph guided random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Arti cial Intelligence</title>
		<meeting>the 26th International Joint Conference on Arti cial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classi cation with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Rey Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">E cient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Rey Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey of heterogeneous information network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="17" to="37" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Commi ee</title>
		<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Commi ee</meeting>
		<imprint>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10146</idno>
		<title level="m">Structural Deep Embedding for Hyper-Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Di usion of following links in microblogging networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2093" to="2106" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predicting Social Links for New Users across Aligned Heterogeneous Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1289" to="1294" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recommendation in heterogeneous information network via dual similarity regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="35" to="48" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
