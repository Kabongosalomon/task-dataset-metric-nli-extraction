<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Row, Multi-Span Distant Supervision For Table+Text Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwajeet</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saneem</forename><surname>Chemmengath</surname></persName>
							<email>saneem.cg@in.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Gupta</surname></persName>
							<email>yashgupta@cse.iitb.ac.in</email>
							<affiliation key="aff1">
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaydeep</forename><surname>Sen</surname></persName>
							<email>jaydesen@in.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
							<email>soumen.chakrabarti@gmail.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Row, Multi-Span Distant Supervision For Table+Text Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Question answering (QA) over tables and text linked from table elements, also called Text-TableQA, has witnessed significant research in recent years, because tables are often found embedded in documents along with related text. HybridQA and OTT-QA are the two bestknown TextTableQA datasets, with questions that are best answered by combining information from both table cells and linked text passages. A common challenge in both datasets, and TextTableQA in general, is that the training instances include just the question and answer, where the gold answer may match not only multiple table rows but also multiple text spans within the scope of a row and its associated text. This leads to a noisy multiinstance training regime. We present MITQA, a transformer-based TextTableQA system that is explicitly designed to cope with distant supervision along both these axes, through a multi-instance loss objective, together with careful curriculum design. Our experiments show that the proposed multi-instance distant supervision approach helps MITQA get much better EM and F1 scores than existing baselines for both HybridQA and OTT-QA, with MITQA currently being at the top of Hy-bridQA leaderboard with a held out test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Question answering (QA) over tables and text linked from table elements, also called Text-TableQA, has witnessed significant research in recent years, because tables are often found embedded in documents along with related text. HybridQA and OTT-QA are the two bestknown TextTableQA datasets, with questions that are best answered by combining information from both table cells and linked text passages. A common challenge in both datasets, and TextTableQA in general, is that the training instances include just the question and answer, where the gold answer may match not only multiple table rows but also multiple text spans within the scope of a row and its associated text. This leads to a noisy multiinstance training regime. We present MITQA, a transformer-based TextTableQA system that is explicitly designed to cope with distant supervision along both these axes, through a multi-instance loss objective, together with careful curriculum design. Our experiments show that the proposed multi-instance distant supervision approach helps MITQA get much better EM and F1 scores than existing baselines for both HybridQA and OTT-QA, with MITQA currently being at the top of Hy-bridQA leaderboard with a held out test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer-based question answering (QA) methods have evolved rapidly in recent years to handle open-domain, multi-hop reasoning over retrieved context paragraphs. Many existing QA datasets and benchmarks measure performance over homogeneous data sources, such as text <ref type="bibr" target="#b22">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b5">Chen et al., 2017a;</ref><ref type="bibr" target="#b17">Joshi et al., 2017;</ref><ref type="bibr" target="#b12">Dua et al., 2019)</ref> and more recently tables <ref type="bibr" target="#b21">(Pasupat and Liang, 2015;</ref><ref type="bibr" target="#b30">Zhong et al., 2017;</ref><ref type="bibr" target="#b20">Liang et al., 2017;</ref><ref type="bibr" target="#b16">Herzig et al., 2020;</ref><ref type="bibr" target="#b28">Yin et al., 2020)</ref>. Even though real-world documents often contain tables embedded in free form text, QA over such a hybrid corpus, i.e., a combination of tables and text -a.k.a. TextTableQA -remains relatively unexplored. As illustrated in <ref type="figure">Figure 1</ref>, even a relatively simple table from Wikipedia often references several entities, definitions or descriptions from the table elements. A question may be best answered by matching some parts of it to table elements and other parts to linked text spans. Existing Transformer-based QA solutions need significant modifications to score such heterogeneous corpus units. A key challenge is to reduce the cognitive burden of supervision to (question, answer) pairs, without humans having to identify the specific table cell or text span where the answer was mentioned. In TextTableQA, such 'distant' supervision is particularly challenging because it occurs along two distinct axes: (1) There could be multiple rows and associated passages that mention the answer string; and (2) Even for a specific table row with linked passages, the same candidate answer may occur in multiple text spans. Many of them may spurious and detrimental to system training.</p><p>In response, we present MITQA -a Text-TableQA system specifically engineered to address the above challenges. MITQA defines each table row, together with linked passages, as the fundamental retrieval unit. To adapt to memory-hungry Transformer networks, constrained by the number of input tokens they can efficiently process, MITQA uses a novel query-informed passage filter to prepare a contextual representation of each retrieval unit. MITQA then uses an early interaction (cross attention) Transformer network to score retrieval units. While training MITQA, its most salient features are multi-instance loss functions and data engineering curricula to tackle distant supervision, along both the multi-row and multispan axes. Many of the above challenges are not faced by homogeneous text-only or table-only QA systems. We report on extensive experiments on two recent TextTableQA challenge data sets, Hy-bridQA and OTT-QA, where our system outper- <ref type="figure">Figure 1</ref>: An instance of question answering over hybrid context of table and text (from HybridQA). Gold answer in correct context is highlighted in blue and gold answer appearing in irrelevant context is highlighted in red. The context used to arrive at the answer in the correct passage is shaded in yellow. The relevant row to be retrieved is shaded green and irrelevant rows are shaded red. forms baselines and is currently at the top of Hy-bridQA 1 leaderboard. Our source code is included with the submission and will be open-sourced, post publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>TableQA has gained much popularity in recent years, resulting in diverse approaches including semantic parsing-based <ref type="bibr" target="#b21">(Pasupat and Liang, 2015;</ref><ref type="bibr" target="#b30">Zhong et al., 2017;</ref><ref type="bibr" target="#b20">Liang et al., 2017;</ref><ref type="bibr" target="#b19">Krishnamurthy et al., 2017;</ref> and more recently BERT-based <ref type="bibr" target="#b10">(Devlin et al., 2018)</ref> systems for table encoding by, inter alia, <ref type="bibr" target="#b16">Herzig et al. (2020)</ref>; <ref type="bibr" target="#b28">Yin et al. (2020)</ref>; <ref type="bibr">Glass et al. (2021a)</ref>. A more realistic application scenario is "Text-TableQA" where tables are often embedded in documents and a natural language query needs to combine information from a table as well as its correlated textual context to find an answer.</p><p>HybridQA ) pioneered a Text-TableQA benchmark, with Wikipedia tables linked to relevant free-form text passages (e.g., Wikipedia entity definition pages). They curated questions which need information from both tables and text to answer correctly. They also proposed HYBRIDER as the first system in TextTableQA with an F1 score of 50%, leaving much scope for improvement. The OTT-QA <ref type="bibr" target="#b7">(Chen et al., 2021)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>benchmark extended</head><p>HybridQA to an open domain setting where a system needs to retrieve a relevant set of tables and passages first before trying to answer a question. Moreover, the links from table and passage are not provided explicitly. To our knowledge, no existing TextTableQA system <ref type="bibr" target="#b7">(Chen et al., , 2021</ref><ref type="bibr" target="#b31">Zhong et al., 2022)</ref> attempts to handle the challenge of multiple candidate instances arising from distant supervision during system training, owing to multiple matching table rows and multiple matching spans within a row and its linked text. Our experiments with HybridQA and OTT-QA show that superior handling of multi-instance matches by MITQA improves QA accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>T denotes a set of tables, each table being denoted as t. Title, caption, and other available metadata of table t is accessed as t.meta. <ref type="table">Table t</ref> has t.rows rows and t.cols columns. Its column headers are denoted t.hdr. (Row headers may also assume a similar salient role, but we limit notation to column headers for simplicity of exposition.) [N ] denotes the set of indices {1, . . . , N }. For r ? [t.rows], the rth row is denoted t[r, ]. For c ? [t.cols], the cell at position (r, c) is written as t[r, c]. The cth column header cell is denoted t.hdr <ref type="bibr">[c]</ref>. The set of passages linked with the row r of table t is denoted by t[r, ].psg. A passage p is a sequence of tokens. The set of all token spans in p is denoted by spans(p). One token span is denoted ? ? spans(p). A set of such spans is denoted ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task Definition</head><p>Given a question q (modeled as a sequence of tokens) and a table t together with linked text, the task is to find a relevant row r, and then an answer text a, which can be a cell from t[r, ], or a span from spans(t[r, ].psg). In HybridQA, the table t and associated linked passages are provided along with the question q. In contrast, for OTT-QA, the  <ref type="table">Each row of table  may have multiple  linked passages.   ? TableSplitter splits each table  into several records, each  containing a row, table header</ref> and metadata .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TableSplitter</head><p>? PassageFilter prunes linked passages until they fit the input limit.</p><p>? RowRetriever retrieves top-k rows with cells or spans containing the right answer.</p><p>Multi-Row Training ? AnswerExtractor performs RC over of the the k row cells and passage text to answer the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AnswerExtractor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Span Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RowRetriever</head><p>Top-k Answers ? Using the confidence scores from RowRetriever and AnswerExtractor. Joint Row+Span Reranker selects the best possible among the k answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: a</head><p>Question: q Answer Candidates correct table t and linked passages need to retrieved from a corpus of tables and initially unconnected passages-a more challenging setting. <ref type="figure" target="#fig_1">Figure 2</ref> shows the overall architecture of MITQA. In some workloads (e.g., HybridQA), a question comes already associated with a table and its linked text. In other "open domain" workloads (e.g., OTT-QA), tables and linked passages must be retrieved from a large corpus by a TableRetriever. The Ta-bleSplitter segments the table t into retrieval units, each comprising one row r (i.e., all cells in t[r, ]) and its linked passages t[r, ].psg. For data sets (like OTTQA) which are not provided pre-linked, the RowPassageLinker module links spans in table cells to corpus passages to prepare the retrieval units. To score retrieval units, we will use an early interaction (cross-attention) Transformer network, to which we will feed the question and a retrieval unit, suitably encoded into text. Rather than naive truncation, or expensive hierarchical encodings, we use a question-sensitive PassageFilter to select a subset of passages PassageFilter(t, r, q) ? t[r, ].psg to retain with each candidate row. The RowRetriever can then identify the most relevant retrieval units. Next, an AnswerExtractor module selects the answer span as a cell from t[r, ] or as a token span from a passage p ? t[r, ].psg linked to the row t[r, ]. Distant supervision (as described above), and the consequent need for multi-instance learning, are handled by three modules: RowRetriever, An-swerExtractor, and a final RowSpanReranker. RowRetriever employs a special loss function that can handle spurious matches of the gold answer in multiple rows and associated retrieval units <ref type="bibr" target="#b11">(Dietterich et al., 1997;</ref><ref type="bibr" target="#b0">Andrews et al., 2003</ref>). An-swerExtractor employs a data programming <ref type="bibr" target="#b23">(Ratner et al., 2016)</ref> curriculum to a similar end. The final Reranker module refines the score for each answer candidate, based on a learned weighted combination of RowRetriever and AnswerExtractor confidence scores. We describe the most important components of MITQA in Section 4 and defer the rest to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">System Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MITQA System Architecture</head><p>In this section we first describe the modules shown in <ref type="figure" target="#fig_1">Figure 2</ref>, that are shared for closed-domain <ref type="table">(table  and linked</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PassageFilter</head><p>The total tokens in passages linked to a row can be large, exceeding the input capacity of BERT-like models. Efforts <ref type="bibr" target="#b3">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b29">Zaheer et al., 2020)</ref> have recently been made to remove these capacity limits, but at the cost of additional complexity, unsuited for our fine-grained application to table rows. In any case, the query has a critical role in determining the utility of each passage linked to a row. Our PassageFilter module orders the linked passages such that the prefix that fits within the input capacity of a BERT-like model is likely to be the most valuable for judging the relevance of a row. More details are in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RowRetriever</head><p>Given question q and table t, the task of RowRetriever is to identify the correct row r from which the answer can be obtained, either as a cell t[r, c] from the cth column, or a span from a passage in t[r, ].psg. We implement RowRetriever by training a BERT-based sequence classification model <ref type="bibr" target="#b10">(Devlin et al., 2018</ref>) on a binary classification task with correct rows to be labelled as 1s and the rest as 0s. Suppose the columns of t are indexed leftto-right using index c. Then t.hdr <ref type="bibr">[c]</ref> and t <ref type="bibr">[r, c]</ref> are the header and cell in column c. The input x to the 1: 59.5% 3: 6.6% 2:12.8% 4: 4.5% 5: 3.1% &gt;5: 13.4% <ref type="figure">Figure 3</ref>: Distribution of number of rows containing the answer-text in the training set of HybridQA. "2: 12.8%" in the chart means that 12.8% instances of training set has exactly 2 rows with answer-text appearing in them.</p><p>BERT encoder is fashioned as:</p><formula xml:id="formula_0">[CLS] q [SEP] c?[t.cols] t.hdr[c] is t[r, c] [DOT] [SEP] t.meta [DOT] p?PassageFilter(t,r,q) p [DOT] (1)</formula><p>where ' ' is the concatenation operator and 'is' is literally the word 'is'.</p><p>[DOT] and [SEP] are separator tokens. In words, we concatenate: (1) the question q;</p><p>(2) phrases of the form "header is cellvalue", over all columns; (3) table metadata (title etc.); and (4) passages linked to the given row, that survive through PassageFilter; before passing into a BERT-Large encoder in a specific format to get a suitable latent states. The [CLS] embedding output by BERT is sent to a feed-forward neural network to make the label prediction. During inference, all {question, row} pairs are passed through this sequence classifier. The row with the largest score for class 1 is identified as the chosen row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distant supervision of RowRetriever:</head><p>A row retrieval system that expects supervision in the form of gold rows exacts a high cognitive burden from annotator in preparing training instances. In the case of HybridQA and OTT-QA, we only have final answer-text as supervision, not relevant row/s, cell/s or text span/s. Given a table with connected passages and a question, we identify potential gold rows by exact string matching answer-text on rows (cells and linked texts).</p><p>As depicted in <ref type="figure">Figure 3</ref>, for HybridQA, ?40% of the training instances have the problem of multiple rows containing the correct answer text. For some instances, the gold answer appears in 19 rows! Multi-instance (-row) training: A naive way is to label all matches with label 1 and the rest with label 0 for training. This reduces the performance of the RowRetriever as a large chunk of training data gets incorrect labels. To address the issue of multiple potentially correct rows we map this problem into a multiple-instance learning setup <ref type="bibr" target="#b11">(Dietterich et al., 1997;</ref><ref type="bibr" target="#b0">Andrews et al., 2003)</ref>, with questionrow pairs as instances and potential correct rows for a question forming a bag. We are given a question q and table t, with row subset B ? t.rows labeled 1 (relevant) and the rest, t.rows \ B labeled 0 (irrelevant). RowRetriever applied to the retrieval unit of row r is modeled as a function f (x r ), where x r is the text constructed in Eqn. (1) from row r. Let (y r , f (x r )) be the binary cross-entropy classification loss, where y r ? {0, 1} is the gold label of instance x i . For a given table and a question, we define the row retriever loss as</p><formula xml:id="formula_1">min r?B (1, f (x r )) + r / ?B (0, f (x r )). (2)</formula><p>The intuition is that RowRetriever can avoid a loss if it assigns a large score to any one of the rows in B, whereas it must assign small score to all rows not in B. Apart from this multi-instance loss function, we also deployed a form of curriculum learning <ref type="bibr" target="#b4">(Bengio et al., 2009</ref>). In early epochs, we only use instances whose labels we are most confident about: negative rows, and questions with only one positive row. In later epochs, we increase the fraction of instances with multiple relevant rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">AnswerExtractor</head><p>In TextQA, answer extraction is solved by a reading comprehension (RC) module <ref type="bibr" target="#b2">(Baradaran et al., 2020)</ref>. An RC module is usually trained with the query, the passage, and the start and end token positions of the span in the passage where the gold answer is found. In MITQA, neither start and end index of the span is available (when the answer is a passage span), nor are the table cell coordinates (when the answer is in a table cell). Furthermore, Algorithm 1 Multi-span AnswerExtractor training.</p><formula xml:id="formula_2">Input: training instances D={(q, t, r ? , ?[r ? ])} 1: D 1 ? {(q, t, r ? , ?[r ? ])?D : ?[r ? ] = 1} 2: AE init ? train AnswerExtractor on D 1 initial model based on 'easy' cases 3: D &gt;1 ? {(q, t, r ? , ?[r ? ])?D : ?[r ? ] &gt; 1} 4: D ? ? collects 'denoised' instances 5: for (q, t, r ? , ?[r ? ]) ? D &gt;1 do 6: ? * ? argmax ???[r ? ] AnswerExtractor AE1 (q, t[r ? , ].psg, ?) ? * is the best span among ?[r ? ] as per initial model AE init 7: D ? D ? (q, t, r ? , {? * }) 8: AE final ? train AnswerExtractor on D 1 ? D 9: return AE final refined model</formula><p>high level supervision of whether the correct answer is a table cell or passage span, is also not available. This makes the training of AnswerExtractor a challenging task. We tackle this challenge using a multi-span training paradigm.</p><p>Multi-instance (-span) training: Recent systems <ref type="bibr" target="#b10">(Devlin et al., 2018;</ref><ref type="bibr">Segal et al., 2020)</ref> simply consider the first span matching the gold answer text as the correct span and use that for training. This is often an incorrect policy. In <ref type="figure">Figure 1, the</ref>   <ref type="bibr" target="#b23">(Ratner et al., 2016)</ref>, we propose a multispan training (MST) paradigm for AnswerExtractor, shown in Algorithm 1. Assuming there is a sufficient number of single-match instances, we train an initial model AE1 on these. We then use this initial model AE1 to score spans from the noisy instances in D &gt;1 . Note that this is different from end-task inference, because we are in a highly constrained output space -we know the answer can Algorithm 2 Joint row+span reranker training.</p><p>Input: Trained RowRetriever and AnswerExtractor; K: number of rows to retain; K : number of spans to retain; search space of combining weights W; development fold</p><formula xml:id="formula_3">D = {(q, t, a)} for w ? W do grid search for weights w D ? ? for (q, t, a) ? D do R = {(r, s)} ? top-K rows from RowRetriever(q, t, K) with scores for (r, s) ? R do ? = {(?, s st , s en )} ? AnswerExtractor(q, t, r, K ) s ? s s st s en score(r, ?) ? w ? s combo score r ? ? argmax r score(r, ?) D ? D ? {(q, t, r ? , a)} perf(w) ? evaluate AnswerExtractor on D return argmax w perf(w)</formula><p>only be among the few choices. The best-scoring span ? * should therefore give us a 'denoised' instance. These, combined with the earlier singlespan instances, give us a much better training set on which we can train another answer extractor, leading to the final model AE2. Appendix A.4 has more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">RowRetriever feedback (RF)</head><p>In Algorithm 1, note that a single row r ? is identified in each instance as relevant. As we have noted before, this is not directly available from training data, because the gold answer may match multiple rows, with no certificate that they are evidence rows. A trivial approach involves invoking Algorithm 1 on all rows containing the gold answer. As expected, this method produced a sub-optimal An-swerExtractor. Instead, we use the trained RowRetriever to identify the most probable row as r ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Joint row+span reranker (RSR)</head><p>The final piece in MITQA combines the confidence scores of RowRetriever and AnswerExtractor. Despite the efforts outlined in the preceding sections, they are both imperfect. E.g., if we retain the top five rows from RowRetriever, gold row recall jumps 8-9% compared to using only the top one row. To recover from such situations, we retain the top five rows, along with their relevance scores. These rows are sent to AnswerExtractor, which outputs its own set of scores for candidate answer spans. The row+answer reranker implements a joint selection across RowRetriever and AnswerExtractor, through a linear combination of their scores, to select the best overall answer. The weights in the combination are set using a development fold. These weights can be selected using either grid search or gradient descent, after pinning module outputs. We do a grid search, shown as Algorithm 2. We shall see that such reranking leads to significant accuracy improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Modules for open-domain applications</head><p>TableRetriever: For open-domain scenarios where questions are not accompanied by tables, this module retrieves the tables most relevant to a given question. For this task, we linearize the tables using different special delimiters to distinguish header information, cells and rows. we also prefix the table title in front of the linearized table with a separator. Then we train a dense passage retriever (DPR) <ref type="bibr" target="#b18">(Karpukhin et al., 2020)</ref> to give a higher score for <ref type="table">a table if it is  relevant to the question while computing the dot  product of the encoded table and question. Details  about table linearization and DPR training are in  Appendix A.1.</ref> RowPassageLinker: This module iterates over each row of the tables retrieved by TableRetriever and links relevant passages to the row. For every cell in the row, RowPassageLinker first searches for nearest neighbour in the passage corpus using a BM25 retriever <ref type="bibr" target="#b6">(Chen et al., 2017b)</ref>. Similar to <ref type="bibr" target="#b7">Chen et al. (2021)</ref>, RowPassageLinker additionally uses a pre-trained GPT-2 model as context generator for each row and uses the generated context to retrieve more relevant passages from passage corpus. Details are in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>HybridQA  is the first large scale multi-hop QA dataset that requires reasoning over hybrid contexts of tables and text. It contains 62,682 instances in the train set, 3466 instances in the dev set and 3463 instances in the test set. HybridQA provides the relevant table and its linked passages with each question, so TableRetriever and RowPassageLinker are not needed.</p><p>OTT-QA <ref type="bibr" target="#b7">(Chen et al., 2021)</ref>  Multiple rows containing the answer text pose a major challenge for question answering on these datasets. In HybridQA, ?40% instances have more than one row in the table matching the answer text exactly. This makes learning to retrieve the most relevant row nontrivial.</p><p>Multiple answer spans pose additional challenges. Further analysis on HybridQA revealed that ?34.5% instances in the training set have multiple answer spans. Details are in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines and competing methods</head><p>We compare MITQA's performance with HY-BRIDER , CARP <ref type="bibr" target="#b31">(Zhong et al., 2022)</ref>, <ref type="bibr">MATE (Eisenschlos et al., 2021)</ref> and the methods proposed by <ref type="bibr" target="#b7">Chen et al. (2021)</ref>: iterative/fusion retrieveal (IR/FR) + single/cross block reader (SBR/CBR). Appendix B.2 has details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance summary</head><p>HybridQA: In <ref type="table">Table 1</ref>, we compare the performance of the proposed models on the dev and test sets of HybridQA dataset. We evaluate the performance in terms of exact match (EM) and F1 scores between predicted answer and ground truth answer. We observe that MITQA, which incorporates passage filtering, multi instance training and joint row+span reranking achieves the best performance on dev as well as test set in terms of both EM and F1. The final best model achieves ?21% absolute improvement over HYBRIDER in both EM and F1 on the test splits. At the time of writing, our system also has a ?4% lead in both EM and F1 over the next best submission on the public leaderboard. Our system outperforms MATE (Eisenschlos et al., 2021) (a contemporary work reporting performance on HybridQA dataset) by ?1.5-2%.</p><p>OTT-QA: In <ref type="table" target="#tab_3">Table 2</ref>, we compare the performance of the best performing method, MITQA, on the dev and test sets of OTT-QA dataset. We report the final answer prediction performance in terms of exact match (EM) and F1 scores. <ref type="table" target="#tab_3">Table 2</ref> shows MITQA achieves the best performance on dev as well as test set in terms of both EM and F1. It delivers ?10% absolute improvement over the best performing baseline by <ref type="bibr" target="#b7">(Chen et al., 2021)</ref> in both EM and F1 on the test splits. It also achieves ?4% higher EM on test set when compared to the very recent CARP <ref type="bibr" target="#b31">(Zhong et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">MITQA ablation setup</head><p>MITQA is a complex system with many modules working in concert. It starts from a base system (RATQA, see below) and then adds several enhancements. In this section, we compile a list of these enhancements, show their effects on performance, and analyze the results. <ref type="table">Table-</ref>text Question Answering (RATQA) is a minimal ablation of MITQA. RATQA includes a BERTLARGE <ref type="bibr" target="#b10">(Devlin et al., 2018)</ref> based row retriever trained on standard cross-entropy loss and a BERTLARGE based answer extractor. The answer extractor is trained with all the rows having a string match with the answer text. During inference, we get the best 10.3 13.0 9.7 12.8 IR+SBR <ref type="bibr" target="#b7">(Chen et al., 2021)</ref> 7.9 11.1 9.6 13.1 FR+SBR <ref type="bibr" target="#b7">(Chen et al., 2021)</ref> 13.8 17.2 13.4 16.2 IR+CBR <ref type="bibr" target="#b7">(Chen et al., 2021)</ref> 14.4 18.5 16.9 20.9 FR+CBR <ref type="bibr" target="#b7">(Chen et al., 2021)</ref> 28.1 32.5 27.2 31.5 CARP ? <ref type="bibr" target="#b31">(Zhong et al., 2022)</ref> 33.2 38.6 32.5 38.5 MITQA 40.0 45.1 36.4 41.9 row from the retriever and apply AnswerExtractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RATQA: Row retrieval Augmented</head><p>MIL: This is the novel multi instance loss function (Section 4.2) used to deal with multiple rows getting incorrect labels if they contain the answer text. Without MIL i.e. if a naive cross entropy loss is used, they lead to a noisy training regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RF:</head><p>As described in Sec. 4.4, we use a pre-trained row retriever to score rows in the train set. This score is used to select the most relevant row while constructing the training data for AnswerExtractor. For the control case (no RF), we create separate instances for AnswerExtractor from all rows where the gold answer text occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MST:</head><p>Multi-span answer extractor training (Algorithm 1) is used. For the control case, the leftmost answer span is used.</p><p>RSR: Algorithm 2 is used for joint row+span reranking, with K=5. For the control case, K=1.</p><p>PF: PassageFilter (Sec. 4.1 and Appendix A.3) is used to select a limited number of tokens to attach to a linearized row, to fit within the input capacity of BERT. In the control setting without PF, we concatenate connected passages in left-to-right cell order while constructing the context, and retain the largest prefix accepted by BERT.  <ref type="table">Table 3</ref>: Ablations of MITQA, starting from the RATQA baseline and progressing to the full MITQA system. <ref type="table">Table 3</ref> shows the results of ablation experiments. In the rest of this section, we will discuss the key takeaways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">MITQA ablation results and analysis</head><p>Benefits of retrieving row, then span: Comparing HYBRIDER in <ref type="table">Table 1</ref> and RATQA in <ref type="table">Table 3</ref>, we see that our strategy to retrieve correct rows first works better than HYBRIDER, producing ?12% F1 score improvement even without any other enhancements and without retriever feedback. This shows that identifying the correct/best rows is of utmost importance and brings large benefits. <ref type="table">Table 3</ref> also gives evidence that training with the new Multi Instance Loss helps RowRetriever increase overall F1 score beyond passage ranking alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Row Training (MIL) benefits:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Span Training (MST) benefits: Multi</head><p>Span Training (Algorithm 1) usually boosts performance by 0.5-1%. This demonstrates the effectiveness of training on denoised data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Row+Span Reranking (RSR) benefits:</head><p>Beyond multi-span training (MST) of AnswerExtractor, the joint row+span reranker (RSR) im-K   proves F1 score as compared to model variations not applying these strategies. In fact, these enhancements can be applied together -as seen in <ref type="table">Table 3</ref>, model variations with MST+RSR produce the best results. PassageFilter (PF) benefits: While designing PassageFilter, our intent was to minimize the damage from discarded text. Comparing RATQA+PF against RATQA, we find that not only is Passage-Filter effective in this role, but it can, in fact, increase F1 score by pruning irrelevant passages before invoking RowRetriever and AnswerExtractor. Retriever Feedback (RF) benefits: In all ablations of MITQA that include multi-row training, RF acts as a positive influence, always yielding better F1 scores than ablations without RF. This translates to better AnswerExtractor performance using less data. With RF, the model is only trained on the best row, while without RF, thrice as much training data is available, but it is more noisy. This also demonstrates the superiority of our row retriever in enhancing answer extractor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Performance of additional modules</head><p>TableRetriever: Given a question, TableRetriever retrieves top-k tables from ?400K tables provided in the corpus of OTT-QA. <ref type="table" target="#tab_6">Table 4</ref> gives the hit rates at top-k predictions for various values of k. RowRetriever: In <ref type="table" target="#tab_7">Table 5</ref>, we present row retrieval accuracy of our models on the dev split of HybridQA dataset. We also present ablations corresponding to all the modules affecting the accuracy i.e. MIL and PF. We observe that passage filtering improves the row retrieval accuracy by ?3%.</p><p>Changing standard cross entropy loss to multi instance loss (Section 4.2) further boosts the row retriever accuracy by ?2%.</p><p>PassageFilter: We find that average number of tokens in the context for the dev set is 585, with 49% examples exceeding BERT's maximum token  <ref type="figure" target="#fig_3">Figure 4</ref>: The benefit of MITQA over HYBRIDER. count of 512 (thus needing truncation). We see that, if we follow our passage ranking and filtering strategy before truncation, the answer is retained in the truncated context in around ?1-2% more dev set examples. Interestingly, the observed performance gain for our answer extractor is slightly larger than this. This can be attributed to the fact that with passage ranking, the correct span more often appears as the first one and gets correctly chosen during back-propagation training for answer extraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">An anecdotal example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>TextTableQA requires reasoning over table cells and linked passage contents. Weak supervision poses a challenge: the target answer might be mentioned in multiple row cells and/or as multiple spans in linked passages. We design a novel QA pipeline that uses multiple row and multiple answer based novel training strategies to identify correct rows first and then use the row cells for relevant passage lookup. We propose efficient strategies for filtering linked passages to retain the most relevant ones for the question, and a novel re-ranker to rank the answers obtained from different rows and their respective linked passages. Our system, MITQA, performs better than recent systems on HybridQA and OTT-QA benchmarks, with large improvements in F1 scores. We have also tried different combinations of our proposed strategies to substantiate the benefit from each of them separately. In future, we would like to explore the following directions: (1) answering complex nu-merical questions over hybrid context of table and text, (2) handling more complex table with structural hierarchies, and (3) enhancing MITQA to provide interpretable explanations for answers.</p><p>Although MITQA achieves the best results for Text-TableQA benchmarks to date, it still has some limitations, owing to its design, and the type of training data it can access.</p><p>Design policy: We have designed MITQA as a collection of trainable modules, which are used in a specific sequence. This design has helped us to focus our innovations in specific modules such as multi-row training for RowRetriever, multi-span training for AnswerExtractor, etc., with an eye to boost overall accuracy. However, the modular design also means that MITQA is not fully end-to-end trainable. Therefore MITQA is, in principle, susceptible to compounding error propagation across modules. We view this as an acceptable trade-off while working on HybridQA and OTT-QA, but other data sets may force us to revisit this decision.</p><p>Types of queries: TextTableQA, being a relatively new task, has only two major benchmarks available (HybridQA and OTT-QA), where OTT-QA is an open domain extension of HybridQA. Therefore, the types of queries to which MITQA during training are limited to effectively a single large benchmark (HybridQA). HybridQA -and consequently OTT-QA -corpora are similar to Wikipedia articles, not confined to any specific domain. Further experiments in specific verticals, such as Finance, Retail, and Health are needed to check if MITQA affords practical cross-domain adaptation.</p><p>Moreover, only a small fraction of queries in HybridQA and OTT-QA need aggregation. Due to their rareness, we have not considered handling aggregation queries through MITQA, which needs additional work in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Row, Multi-Span Distant Supervision For Table+Text Question Answering (Appendix)</head><p>A Further details of MITQA modules</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 TableRetriever and its training</head><p>In the open domain QA setting (like in OTT-QA) where a designated table t and linked passages t <ref type="bibr">[ , ]</ref>.psg are not provided, we employ the module TableRetriever(q) to retrieve the most promising tables T q ? T , where T is the corpus of tables.</p><p>The training of the TableRetriever module follows the original DPR work <ref type="bibr" target="#b18">(Karpukhin et al., 2020)</ref> and its recent application <ref type="bibr" target="#b15">(Glass et al., 2021b)</ref>, where we first index the linearized tables with Anserini. TableRetriever is trained using triplet loss over instances of the form q, t ? , t , where t ? is a ground-truth table and t is a hard negative <ref type="bibr">(Robinson et al., 2021)</ref> -an irrelevant table that scores highly with respect to the current scoring model.</p><p>To collect hard negative tables t , we retrieve a pool of tables from a BM25 text retrieval system, and remove the gold table if it is retrieved. The surviving tables are considered 'hard'. To further enhance the robustness of TableRetriever, we select the hard negative table at random from some number of top-scoring hard negative tables.</p><p>The tables and the questions are encoded independently using the same BERTBASE <ref type="bibr" target="#b10">(Devlin et al., 2018)</ref> model. We later calculate the inner product of question embedding and embedding of all tables to locate the top-scoring relevant tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 RowPassageLinker</head><p>For each table t?T q returned by TableRetriever and every row r in table t, we use a RowPassageLinker(t, r) to retrieve the most appropriate passages (from a large corpus of text) and link them to appropriate cells t <ref type="bibr">[r, c]</ref>. RowPas-sageLinker first searches for nearest neighbour of the cell text in the passage corpus using a BM25 retriever <ref type="bibr" target="#b6">(Chen et al., 2017b)</ref> and retrieves 10 passages. Similar to <ref type="bibr" target="#b7">Chen et al. (2021)</ref>, RowPas-sageLinker additionally uses a pre-trained GPT-2 model to generate text from row t[r, * ]. and uses the generated text as context to retrieve 10 more relevant passages from the passage corpus. Specifically, the model takes in the text of t[r, c] as input and outputs additional augmented queries, which are then fed again to the BM25 retriever as queries, to retrieve additional relevant passages. The GPT-2 model is fine-tuned on the supervised pairs of table row (i.e., t[r, ]), header (i.e., t.hdr) and their hyperlinks (t[r, ] + t.hdr, hyperlink) from in-domain (HybridQA) tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 PassageFilter (PF) and its training</head><p>Given a question, table, and a set of passages connected to cells in the table, PassageFilter ranks the passages based on their relevance to the question. We use Sentence-BERT <ref type="bibr" target="#b24">(Reimers and Gurevych, 2019)</ref> to get question and passage embeddings and we perform asymmetric semantic search to rank the passages. Asymmetric semantic search is a feature in Sentence-BERT that allows to find a longer passage/document based on a short question.</p><p>Passage ranking plays a vital role in row retrieval as well as answer extraction. BERT encoders (used in RowRetriever and AnswerExtractor) have a limitation that they cannot process sequences of length more than 512 tokens. Passage ranking ensures that even if we truncate the context to fit BERT, we are unlikely to lose passages most relevant to the question. Because we do not have supervision about which passages should be ranked higher, we train PassageFilter on a similar task of passage ranking given a query on the MS MARCO Passage Retrieval dataset <ref type="bibr" target="#b1">(Bajaj et al., 2016)</ref>.</p><p>Moreover, in case the context contains multiple spans, passage filtering helps to bring the correct answer span at the top, thus reducing the possibility of noisy labels. This is particularly important, because the basic model of answer extractor without multi span training (MST), back-propagates through the first span in the passage matching with the gold answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 AnswerExtractor text linearization</head><p>A training instance for answer extraction consists of a token sequence generated by concatenating linearized row contents and passages (linked to cells in the row), together with start and end span indexes of the ground truth answer. We linearize a row as "&lt;column-header&gt; is &lt;cell-content&gt;". This simple linearization bypasses the need to introduce new additional special tokens as columnheader and row delimiters, and avoids computationally intensive training of their embeddings. The concatenated sequential context often exceeds BERT's 512-token limit. We reduce the proba-bility of the passage containing the ground truth answer getting truncated, by using PassageFilter <ref type="figure">(Appendix A.3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Details on Experiments</head><p>In this section, we give further details on our experimental approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Datasets</head><p>HybridQA is the first large scale multi-hop QA dataset that requires reasoning over hybrid contexts of tables and text. It contains 62,682 instances in the train set, 3466 instances in the dev set and 3463 instances in the test set. For the test set, ground truth answers are not available. The authors employ Amazon Mechanical Turk crowd-workers to generate questions based on Wikipedia tables with cells linked to Wikipedia pages. We split the tables into rows with column headers attached. This enables us to pose the QA problem as row retrieval and answer extraction from the retrieved row.</p><p>OTT-QA extends over HybridQA to make it a large-scale open-domain QA dataset over tables and text which needs table and page retrieval before question answering. This dataset provides 400k tables and 5 million passages as corpus. It has 41,469 questions in the training set, 2,214 questions in the dev set, and 2,158 questions in the test set. According to <ref type="bibr" target="#b7">(Chen et al., 2021</ref>) , a remarkable difference from original HybridQA is that a proportion of questions actually have multiple plausible inference chains in the open-domain setting.</p><p>Multiple rows containing the answer text pose a major challenge for question answering on these datasets. As depicted in <ref type="figure">Figure 3</ref>, for HybridQA, ?40% instances have more than one row in the table matching the answer text exactly. This makes retrieving the most relevant row highly nontrivial.</p><p>Multiple answer spans pose additional challenges. Further analysis on HybridQA revealed that ?34.5% instances in the training set have multiple answer spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Baselines and Competing Methods</head><p>HYBRIDER We compare our model's performance with the standard HYBRIDER  baseline. HYBRIDER uses a two phase process of linking and reasoning to answer questions over heterogeneous context of table and text. This approach attempts to use cell as a unit for linking, hopping and answer prediction.</p><p>Iterative and Block Retrieval These models are proposed by <ref type="bibr" target="#b7">Chen et al. (2021)</ref> and are combinations of Iterative/Fusion retrievers and Single/Cross readers. Fusion retrieval uses "early fusion" strategy to group tables and passages as fused blocks before retrieval. Single Block Reader feeds top-k blocks independently to the reader and selecting the best answer. Cross Block Reader concatenates top-k blocks together to the reader, and generates a single joint answer string.</p><p>MATE MATE (Eisenschlos et al., 2021) models the structure of large Web tables. It uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. To apply it on HybridQA, the authors propose P ointR, which expands a cell using description of its enitities, selects an appropriate expanded cell and then reads the answer from it.</p><p>CARP CARP <ref type="bibr" target="#b31">(Zhong et al., 2022</ref>) is a chaincentric reasoning and pre-training framework for Other baselines These can be found on the respective challenge leaderboards. 2 There are no linked papers to the submissions as yet. We compare MITQA's test performance against all of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Implementation Details</head><p>MITQA is implemented using Pytorch version 1.8 and Huggingface's transformers 3 <ref type="bibr" target="#b27">(Wolf et al., 2020)</ref> library. We train our models using two NVIDIA A100 GPUs. We train the row retriever and answer extractor for 5 epochs and select the best model based on dev fold performance. We optimize the model parameters using AdamW algorithm with a learning rate of 5?10 ?5 and a batch size of 24. We set per-GPU train batch size to 16 while training the answer extractor. We evaluate final answers using EM (exact match) and F1 metrics.</p><p>Average Runtime: Overall training of MITQA takes approximately 24 hours on A100 gpu.</p><p>2 HybridQA: https://competitions.codalab. org/competitions/24420 OTT-QA: https://competitions.codalab.org/ competitions/27324 3 https://huggingface.co/ BP plc ... England . It is one of the world 's seven oil and gas supermajors , whose performance in 2012 made it the world 's sixthlargest oil and gas ... and the company with the world's 12thlargest revenue (turnover) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>What is the rank of the company whose performance in 2012 made it the company with the world's 12th-largest revenue ( turnover )? Answer: 9 Rank Name Headquarters  <ref type="figure">Figure 5</ref>: MITQA is able to extract answer even if the answer is only present in the table as a cell value. The correct answer is highlighted in blue. Despite having other numbers in the table and phrases mentioning ranks like 'seven', 'sixth-largest', etc. in the passage MITQA was able to predict the correct answer from the table.</p><p>Hyperparameter Details: We tune hyperparameters based on loss on validation set. We use the following range of values for selecting the best hyper-parameter ? Batch Size: 8, 16, 32 ? Learning Rate: 1e-3, 1e-4, 1e-5, 1e-6, 3e-3, 3e-4, 3e-5, 3e-6, 5e-3, 5e-4, 5e-5, 5e-6 C Anecdotes of Gains C.1 Answer in <ref type="table">Table Cell</ref> We present in <ref type="figure">Figure 5</ref> an example where MITQA is able to predict the answer correctly even when the correct answer is in a table cell and not a span in the passages. <ref type="figure">Figure 6</ref> shows an example where MST leads the model to train on the correct answer span, thereby leading to a less noisy training regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Benefits of Multi Span Training (MST)</head><p>Question: What was the mascot of the college of Ryan Quigley ? Answer: Eagles Context: Original NFL team is Chicago Bears . Player is Ryan Quigley . Pos is P . College is Boston College . Conf is ACC . Ryan Andrew Quigley ( born January 26 , 1990 ) is an American football punter who is currently a free agent . He was signed by the Chicago Bears after going undrafted in the 2012 NFL Draft . He played college football at Boston College . He has played for the New York Jets , Philadelphia Eagles(7.73) , Jacksonville Jaguars , Arizona Cardinals and Minnesota Vikings . The 2011 Boston College Eagles(0.03) football team represented Boston College in the 2011 NCAA Division I FBS football season . The Eagles(6.27) were led by third year head coach Frank Spaziani and played their home games at Alumni Stadium . ... <ref type="figure">Figure 6</ref>: Benefits from MAT. The model loss is shown in brackets along with the spans. It is clear that the correct mention (in blue) rightly gets the lowest loss while the ones which are irrelevant (in red) have higher losses. Contexts that can potentially help answer the question are underlined. The first 'Eagles' in entirely irrelevant as it refers to a different team. The second one is the best answer by far. The third occurrence refers to the correct team, but lacks as good a context as the second (for model learning).  <ref type="figure">Figure 7</ref>: The benefit of Row Span Re-ranker. The correct answer is highlighted in blue and the incorrect answer is highlighted in red. Both 'Piero Alva' and 'Renzo Revoredo' played in the same stadium ('Universitario de Deportes'). But only 'Piero Alva' (answer after reranking) has retired while 'Renzo Revoredo' (answer before reranking) has not. Thus, the RSR helps rank the correct answers higher than the incorrect ones in similar contexts and confusing scenarios.</p><p>C.3 Benefits of Row Span Re-ranker (RSR) <ref type="figure">Figure 7</ref> depicts an instance where RSR is able to rectify the error made by MITQA. The incorrect answer also appeared in a context very similar to the context of correct answer but Multi-Row Reranker is able to rank the correct answer higher than the incorrect answer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>MITQA system sketch. TableRetriever and RowPassageLinker are not shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>text provided, as in HybridQA) and open-domain (OTT-QA) applications. After that we describe TableRetriever and RowPassageLinker that are needed for open-domain scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>shows how MITQA can outperform at answering questions where the context might cause confusion to both retriever and reader because of multiple matches of important question keywords. As shown, HYBRIDER got confused by the presence of 'Cornwall' in the first row and produced an incorrect answer 'Lanner'. In contrast, MITQA predicts the correct answer 'Veor'. Appendix C shows more examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>correct answer, '2018', occurs multiple times in t[r ? , ].psg, where r ? is the relevant row. There is absolutely no guarantee that the first span in t[r ? , ].psg matching the gold answer text will be true evidence for answering the question. Therefore, using the first, or all, matches for training AnswerExtractor can introduce large volumes of training noise and degrade its accuracy.Let ?[r ? ] be the set of spans in t[r ? , ].psg that match the gold answer. Our problem is when ?[r ? ] &gt;1. Inspired by data programming methods</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>extends HybridQA. It is a large-scale open-domain QA dataset over tables and text which needs table and passage retrieval before question answering. This dataset provides 400K tables and 5M passages as corpus. It has 42K questions in the training set, 2K questions in the dev set, and 2K questions in the test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>= 0.8) 54.3 61.4 56.2 63.3 39.1 45.7 37.5 44.4 44.0 50.7 43.8 50.6 POINTR + MATE ? (Eisenschlos et al., 2021) 68.6 74.2 66.9 72.3 62.8 71.9 62.8 72.9 63.4 71.0 62.8 70.Table 1: End-task performance on dev and test folds of HybridQA, comparing prior systems against MITQA.</figDesc><table><row><cell></cell><cell></cell><cell>Table</cell><cell></cell><cell>Passage</cell><cell></cell><cell>Total</cell></row><row><cell></cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell></row><row><cell></cell><cell>EM</cell><cell>F1 EM</cell><cell>F1 EM</cell><cell>F1 EM</cell><cell>F1 EM</cell><cell>F1 EM</cell><cell>F1</cell></row><row><cell>Table-Only</cell><cell cols="7">14.7 19.1 14.2 18.8 2.4 4.5 2.6 4.7 8.4 12.1 8.3 11.7</cell></row><row><cell>Passage-Only</cell><cell cols="7">9.2 13.5 8.9 13.8 26.1 32.4 25.5 32.0 19.5 25.1 19.1 25.0</cell></row><row><cell cols="8">HYBRIDER (? 2</cell></row><row><cell>MITQA</cell><cell cols="7">68.1 73.3 68.5 74.4 66.7 75.6 64.3 73.3 65.5 72.7 64.3 71.9</cell></row><row><cell>Dev</cell><cell>Test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">EM F1 EM F1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">HYBRIDER (Top-1) (Chen et al., 2020) 8.9 11.3 8.4 10.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HYBRIDER (best Top-K)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? -Systems contemporary to MITQA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>End-task performance on dev and test folds of OTT-QA. IR=iterative retriever, FR=fusion retriever. SBR=single block reader, CBR=cross block reader. Best numbers overall are in bold.? -Systems contem- porary to MITQA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table retrieval</head><label>retrieval</label><figDesc></figDesc><table><row><cell></cell><cell>accuracy (%)</cell></row><row><cell>1</cell><cell>41.28</cell></row><row><cell>5</cell><cell>68.15</cell></row><row><cell>10</cell><cell>76.51</cell></row><row><cell>50</cell><cell>88.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>TableRetriever HITS@K, OTT-QA dev set.</figDesc><table><row><cell cols="3">Ablations Row Retrieval</cell></row><row><cell>MIL</cell><cell>PF</cell><cell>Accuracy (%)</cell></row><row><cell></cell><cell></cell><cell>81.39</cell></row><row><cell></cell><cell></cell><cell>84.30</cell></row><row><cell></cell><cell></cell><cell>86.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>RowRetriever accuracy, HybridQA dev fold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Veor Rugby Football Club is a Cornish and ... as champions of Cornwall 1 at the end of the 2018-19 season .Camborne is a town in Cornwall . ... formerly one of the richest tin mining ... Which team of the Cornwall League 1 comes from a town that is known for its tin mining?</figDesc><table><row><cell cols="2">Answer: Veor</cell><cell></cell><cell>Lanner is a village and civil parish in</cell></row><row><cell cols="3">Cornwall_League_1_4</cell><cell>west Cornwall , England</cell></row><row><cell>Team</cell><cell cols="2">Ground Town/Village</cell></row><row><cell>Lanner</cell><cell>-</cell><cell>Lanner</cell></row><row><cell>Veor</cell><cell>Wheal Gerry</cell><cell>Camborne</cell></row></table><note>Question:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>table-and-text question answering. It first extracts explicit hybrid chain to reveal the intermediate reasoning process leading to the answer across table and text. The hybrid chain then provides a guidance for QA, and explanation of the intermediate reasoning process.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Answer: Piero Alva</head><label></label><figDesc>Piero Alva ( born 14 February 1979 in Lima ) is a Peruvian international football striker . He currently Retired . What now retired Peruvian football player was able to play in a 80,000-capacity stadium, for 11 years before being transferred ?Renzo Revoredo Zuazo ( born 11 May  1986  in Lima ) is a Peruvian footballer who plays for Sporting Cristal ...</figDesc><table><row><cell>Question: Club Universitario de Deportes .. In 2000 , they opened the 80,000-capacity stadium Estadio Monumen</cell><cell>Player Piero Alva 2011_Sporting_Cristal_season_0 Stadium Date 15 June 2011 Renzo Revoredo Universitario de Deportes Universitario 10 August de Deportes 2011</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/ competitions/24420#results</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey on machine reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razieh</forename><surname>Baradaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razieh</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Amirkhani</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2001.01582</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Open question answering over tables and text</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hybridqa: A dataset of multi-hop question answering over tabular and textual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Findings of EMNLP 2020</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iterative search for weakly supervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1273</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2669" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Solving the multiple instance problem with axis-parallel rectangles. Artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lozano-P?rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="31" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno>abs/1903.00161</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mate: Multiview attention for table transformer efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Martin Eisenschlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maharshi</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Samarth Bharadwaj, and Nicolas Rodolfo Fauceglia. 2021a. Capturing row and column semantics in transformer based question answering over tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Canim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saneem</forename><surname>Chemmengath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwajeet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishav</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.96</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1212" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust retrieval augmented generation for zero-shot slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetano</forename><surname>Rossiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Faisal Mahbub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gliozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1939" to="1949" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana, Dominican Republic. Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe? Krzysztof</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
		</author>
		<title level="m">Tapas: Weakly supervised table parsing via pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1705.03551</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1516" to="1526" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>abs/1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">quickly. NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3567" to="3575" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<title level="m">Suvrit Sra, and Stefanie Jegelka. 2021. Contrastive learning with hard negative samples. ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Amir Globerson, and Jonathan Berant. 2020. A simple and effective model for answering multi-span questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Segal</surname></persName>
		</author>
		<imprint>
			<pubPlace>Avia Efrat, Mor Shoham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tabert: Pretraining for joint understanding of textual and tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Big Bird: Transformers for longer sequences. NeurIPS, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reasoning over hybrid chain for table-and-text open domain qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
