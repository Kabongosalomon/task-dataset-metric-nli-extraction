<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Image Homography Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
							<email>ddetone@magicleap.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Magic Leap, Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
							<email>tmalisiewicz@magicleap.com</email>
							<affiliation key="aff1">
								<orgName type="department">Magic Leap, Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
							<email>arabinovich@magicleap.com</email>
							<affiliation key="aff2">
								<orgName type="department">Magic Leap, Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Image Homography Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a deep convolutional neural network for estimating the relative homography between a pair of images. Our feed-forward network has 10 layers, takes two stacked grayscale images as input, and produces an 8 degree of freedom homography which can be used to map the pixels from the first image to the second. We present two convolutional neural network architectures for HomographyNet: a regression network which directly estimates the real-valued homography parameters, and a classification network which produces a distribution over quantized homographies. We use a 4-point homography parameterization which maps the four corners from one image into the second image. Our networks are trained in an end-to-end fashion using warped MS-COCO images. Our approach works without the need for separate local feature detection and transformation estimation stages. Our deep models are compared to a traditional homography estimator based on ORB features and we highlight the scenarios where HomographyNet outperforms the traditional technique. We also describe a variety of applications powered by deep homography estimation, thus showcasing the flexibility of a deep learning approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Sparse 2D feature points are the basis of most modern Structure from Motion and SLAM techniques <ref type="bibr" target="#b8">[9]</ref>. These sparse 2D features are typically known as corners, and in all geometric computer vision tasks one must balance the errors in corner detection methods with geometric estimation errors. Even the simplest geometric methods, like estimating the homography between two images, rely on the error-prone corner-detection method.</p><p>Estimating a 2D homography (or projective transformation) from a pair of images is a fundamental task in computer vision. The homography is an essential part of monocular SLAM systems in scenarios such as:</p><p>? Rotation only movements ? Planar scenes ? Scenes in which objects are very far from the viewer It is well-known that the transformation relating two images undergoing a rotation about the camera center is a homography, and it is not surprising that homographies are essential for creating panoramas <ref type="bibr" target="#b2">[3]</ref>. To deal with planar and mostly-planar scenes, the popular SLAM algorithm ORB-SLAM <ref type="bibr" target="#b13">[14]</ref> uses a combination of homography estimation and fundamental matrix estimation. Augmented Reality applications based on planar structures and homographies have been well-studied <ref type="bibr" target="#b15">[16]</ref>. Camera calibration techniques using planar structures <ref type="bibr" target="#b19">[20]</ref> also rely on homographies.</p><p>The traditional homography estimation pipeline is composed of two stages: corner estimation and robust homography estimation. Robustness is introduced into the corner detection stage by returning a large and over-complete set of points, while robustness into the homography estimation step shows up as heavy use of RANSAC or robustification of the squared loss function. Since corners are not as reliable as man-made linear structures, the research community has put considerable effort into adding line features <ref type="bibr" target="#b17">[18]</ref> and more complicated geometries <ref type="bibr" target="#b7">[8]</ref> into the feature detection step. What we really want is a single robust algorithm that, given a pair of images, simply returns the homography relating the pair. Instead of manually engineering corner-ish features, line-ish features, etc, is it possible for the algorithm to learn its own set of primitives? We want to go even further, and add the transformation estimation step as the last part of a deep learning pipeline, thus giving us the ability to learn the entire homography estimation pipeline in an end-to-end fashion.</p><p>Recent research in dense or direct featureless SLAM algorithms such as LSD-SLAM <ref type="bibr" target="#b5">[6]</ref> indicates promise in using a full image for geometric computer vision tasks. Concurrently, deep convolutional networks are setting state-of-the-art benchmarks in semantic tasks such as image classification, semantic segmentation and human pose estimation. Additionally, recent works such as FlowNet <ref type="bibr" target="#b6">[7]</ref>, Deep Semantic Matching <ref type="bibr" target="#b0">[1]</ref> and Eigen et al.'s Multi-Scale Deep Network <ref type="bibr" target="#b4">[5]</ref> present promising results for dense geometric computer vision tasks like optical flow and depth estimation. Even robotic tasks like visual odometry are being tackled with convolutional neural networks <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this paper, we show that the entire homography estimation problem can be solved by a deep convolutional neural network (See <ref type="figure" target="#fig_0">Figure 1</ref>). Our contributions are as follows: we present a new VGG-style <ref type="bibr" target="#b16">[17]</ref> network for the homography estimation task. We show how to use the 4-point parameterization <ref type="bibr" target="#b1">[2]</ref> to get a well-behaved deep estimation problem. Because deep networks require a lot of data to be trained from scratch, we share our recipe for creating a seemingly infinite dataset of (I A , I B , H AB ) training triplets from an existing dataset of real images like the MS-COCO dataset. We present an additional formulation of the homography estimation problem as classification, which produces a distribution over homographies and can be used to determine the confidence of an estimated homography.  HomographyNet is a Deep Convolutional Neural Network which directly produces the Homography relating two images. Our method does net require separate corner detection and homography estimation steps and all parameters are trained in an end-to-end fashion using a large dataset of labeled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE 4-POINT HOMOGRAPHY PARAMETERIZATION</head><p>The simplest way to parameterize a homography is with a 3x3 matrix and a fixed scale. The homography maps <ref type="bibr">[u, v]</ref>, the pixels in the left image, to <ref type="bibr">[u , v ]</ref>, the pixels in the right image, and is defined up to scale (see <ref type="bibr">Equation 1)</ref>.</p><formula xml:id="formula_0">? ? u v 1 ? ? ? ? ? H 11 H 12 H 13 H 21 H 22 H 23 H 31 H 32 H 33 ? ? ? ? u v 1 ? ?<label>(1)</label></formula><p>However, if we unroll the 8 (or 9) parameters of the homography into a single vector, we'll quickly realize that we are mixing both rotational and translational terms. For example, the submatrix [H 11 H 12 ; H 21 H 22 ], represents the rotational terms in the homography, while the vector [H 13 H 23 ] is the translational offset. Balancing the rotational and translational terms as part of an optimization problem is difficult.</p><p>We found that an alternate parameterization, one based on a single kind of location variable, namely the corner location, is more suitable for our deep homography estimation task. The 4-point parameterization has been used in traditional homography estimation methods <ref type="bibr" target="#b1">[2]</ref>, and we use it in our modern deep manifestation of the homography estimation problem (See <ref type="figure">Figure 2</ref>). Letting ?u 1 = u 1 ?u 1 be the u-offset for the first corner, the 4-point parameterization represents a homography as follows:</p><formula xml:id="formula_1">H 4point = ? ? ? ? ?u 1 ?v 1 ?u 2 ?v 2 ?u 3 ?v 3 ?u 4 ?v 4 ? ? ? ?<label>(2)</label></formula><p>Equivalently to the matrix formulation of the homography, the 4-point parameterization uses eight numbers. Once the displacement of the four corners is known, one can easily convert H 4point to H matrix . This can be accomplished in a number of ways, for example one can use the normalized Direct Linear Transform (DLT) algorithm <ref type="bibr" target="#b8">[9]</ref>, or the function getPerspectiveTransform()in OpenCV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATA GENERATION FOR HOMOGRAPHY ESTIMATION</head><p>Training deep convolutional networks from scratch requires a large amount of data. To meet this requirement, we generate a nearly unlimited number of labeled training examples by applying random projective transformations to a large dataset of natural images 1 . The process is illustrated in <ref type="figure">Figure 3</ref> and described below.</p><p>To generate a single training example, we first randomly crop a square patch from the larger image I at position p (we avoid the borders to prevent bordering artifacts later in the data generation pipeline). This random crop is I p . Then, the four corners of Patch A are randomly perturbed by values within the range [-?, ?]. The four correspondences define a homography H AB . Then, the inverse of this homography H BA = (H AB ) ?1 is applied to the large image to produce image I . A second patch I p is cropped from I at position p. The two grayscale patches, I p and I p are then stacked channelwise to create the 2-channel image which is fed directly into our ConvNet. The 4-point parameterization of H AB is then used as the associated ground-truth training label.</p><p>Managing the training image generation pipeline gives us full control over the kinds of visual effects we want to model. For example, to make our method more robust to motion blur, we can apply such blurs to the image in our training set. If we want the method to be robust to occlusions, we can insert random occluding shapes into our training images. We experimented with in-painting random occluding rectangles into our training images, as a simple mechanism to simulate real occlusions. <ref type="bibr" target="#b0">1</ref> In our experiments, we used cropped MS-COCO <ref type="bibr" target="#b12">[13]</ref> images, although any large-enough dataset could be used for training</p><formula xml:id="formula_2">1-to-1 mapping 128x128x64 128x128x64 64x64x64 64x64x Max Pooling Conv1 Conv2 Conv3 Conv4 3x3 3x3 3x3 3x3 128x128x2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Images</head><p>Deep Image Homography Es The simplest way to parameterize a homography is with a 3x3 matrix and a fixed scale (see <ref type="bibr">Equation 1</ref>). However, if we unroll the 8 (or 9) parameters of the homography into a single vector, well quickly realize that we are mixing both rotational and translational terms. For example, the submatrix [H 11 H 12 ; H 21 H 22 ], represents the rotational terms in the homography, while the vector [H 13 H 23 ] is the translational offset. Balancing the rotational and translational terms as part of an optimization problem is difficult.</p><formula xml:id="formula_3">0 @ u 1 0 v 1 0 1 1 A ? 0 @ H 11 H 12 H 13 H 21 H 22 H 23 H 31 H 32 H 33 1 A 0 @ u 1 v 1 1 1 A (1)</formula><p>We found that an alternate parameterization, one based on a single kind of location variable, namely the corner location, is more suitable for our deep homography estimation task. The 4-point parameterization has been used in traditional homography estimation methods <ref type="bibr" target="#b1">[2]</ref>, and we use it in our modern deep manifestation of the homography estimation problem (See <ref type="figure">Figure 2</ref>). Letting u 1 = u 1 0 u 1 be the u-offset for the first corner, the 4-point parameterization represents the Homography as follows:</p><formula xml:id="formula_4">H 4point = 0 B B @ u 1 v 1 u 2 v 2 u 3 v 3 u 4 v 4 1 C C A<label>(2)</label></formula><p>Equivalently to the matrix formulation of the homography, the 4-point parameterization is represented by eight numbers. In other words, once the displacement of the four corners is known, only a single closed form transformation is needed for the 8-dof homography. This can be accomplished in a number of ways, for example one can use the normalized Direct Linear Transform (DLT) algorithm <ref type="bibr" target="#b8">[9]</ref>, or the function getPerspectiveTransform()in OpenCV.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATA GENERATION FOR HOMOGRAPHY ESTIMATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE 4-POINT HOMOGRAPHY PARAMETERIZATION</head><p>The simplest way to parameterize a homography is with a 3x3 matrix and a fixed scale (see <ref type="bibr">Equation 1</ref>). However, if we unroll the 8 (or 9) parameters of the homography into a single vector, well quickly realize that we are mixing both rotational and translational terms. For example, the submatrix [H11H12; H21H22], represents the rotational terms in the homography, while the vector [H13H23] is the translational offset. Balancing the rotational and translational terms as part of an optimization problem is difficult.</p><formula xml:id="formula_5">0 @ u10 v10 1 1 A ? 0 @ H11 H12 H13 H21 H22 H23 H31 H32 H33 1 A 0 @ u1 v1 1 1 A (1)</formula><p>We found that an alternate parameterization, one based on a single kind of location variable, namely the corner location, is more suitable for our deep homography estimation task. The 4-point parameterization has been used in traditional homography estimation methods <ref type="bibr" target="#b1">[2]</ref>, and we use it in our modern deep manifestation of the homography estimation problem (See <ref type="figure">Figure 2</ref>). Letting u1 = u10 u1 be the u-offset for the first corner, the 4-point parameterization represents the Homography as follows:</p><formula xml:id="formula_6">H4point = 0 B B @ u1 v1 u2 v2 u3 v3 u4 v4 1 C C A<label>(2)</label></formula><p>Equivalently to the matrix formulation of the homography, the 4-point parameterization is represented by eight numbers. In other words, once the displacement of the four corners is known, only a single closed form transformation is needed for the 8-dof homography. This can be accomplished in a number of ways, for example one can use the normalized Direct Linear Transform (DLT) algorithm <ref type="bibr" target="#b8">[9]</ref>, or the function getPerspectiveTransform()in OpenCV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATA GENERATION FOR HOMOGRAPHY ESTIMATION</head><p>Training deep convolutional networks from scratch requires a large amount of data. To meet this requirement, we generate a nearly unlimited number of labeled training examples by applying random projective transformations to a large dataset of natural images 1 . This procedure is detailed below.</p><p>To generate a single training example, we first randomly crop a square patch from the larger image I at position p (we avoid the borders to prevent bordering artifacts later in the data generation pipeline). This random crop is Ip. Then, the four corners of Patch A are randomly perturbed by values within the range [-?, ?]. The four correspondences define a homography H AB . Then, the inverse of this homography H BA = (H AB ) 1 is applied to the large image to produce image I 0 . A second patch I 0 p is cropped from I 0 at position p. The two grayscale patches, Ip and I 0 p are then stacked channelwise to create the 2-channel image which is fed directly into our ConvNet. The 4-point parameterization of H AB is then used as the associated ground-truth training label. The process is illustrated in <ref type="figure">Figure 3</ref>.</p><p>Managing the training image generation pipeline gives us full control over the kinds of visual effects we want to model. For example, to make our method more robust to motion blur, we can apply such blurs to the image in our training set. If we want the method to be robust to occlusions, we can insert random occluding shapes into our training images. We experimented with in-painting random occluding rectangles into our training images, as a simple mechanism to simulate real occlusions. <ref type="bibr" target="#b0">1</ref> In our experiments, we used cropped MS-COCO <ref type="bibr" target="#b14">[15]</ref> images, although any large-enough dataset could be used for training  </p><formula xml:id="formula_7">H (u1,v1) (u1',v1') (?u1, ?v1) (?u2, ?v2) (?u3,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE 4-POINT HOMOGRAPHY PARAMETERIZATION</head><p>The simplest way to parameterize a homography is with a 3x3 matrix and a fixed scale (see <ref type="bibr">Equation 1</ref>). However, if we unroll the 8 (or 9) parameters of the homography into a single vector, well quickly realize that we are mixing both rotational and translational terms. For example, the submatrix [H11H12; H21H22], represents the rotational terms in the homography, while the vector [H13H23] is the translational offset. Balancing the rotational and translational terms as part of an optimization problem is difficult. </p><p>We found that an alternate parameterization, one based on a single kind of location variable, namely the corner location, is more suitable for our deep homography estimation task. The 4-point parameterization has been used in traditional homography estimation methods <ref type="bibr" target="#b1">[2]</ref>, and we use it in our modern deep manifestation of the homography estimation problem (See <ref type="figure">Figure 2</ref>). Letting u1 = u10 u1 be the u-offset for the first corner, the 4-point parameterization represents the Homography as follows:</p><formula xml:id="formula_9">H4point = 0 B B @ u1 v1 u2 v2 u3 v3 u4 v4 1 C C A<label>(2)</label></formula><p>Equivalently to the matrix formulation of the homography, the 4-point parameterization is represented by eight numbers. In other words, once the displacement of the four corners is known, only a single closed form transformation is needed for the 8-dof homography. This can be accomplished in a number of ways, for example one can use the normalized Direct Linear Transform (DLT) algorithm <ref type="bibr" target="#b8">[9]</ref>, or the function getPerspectiveTransform()in OpenCV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATA GENERATION FOR HOMOGRAPHY ESTIMATION</head><p>Training deep convolutional networks from scratch requires a large amount of data. To meet this requirement, we generate a nearly unlimited number of labeled training examples by applying random projective transformations to a large dataset of natural images 1 . This procedure is detailed below.</p><p>To generate a single training example, we first randomly crop a square patch from the larger image I at position p (we avoid the borders to prevent bordering artifacts later in the data generation pipeline). This random crop is Ip. Then, the four corners of Patch A are randomly perturbed by values within the range [-?, ?]. The four correspondences define a homography H AB . Then, the inverse of this homography H BA = (H AB ) 1 is applied to the large image to produce image I 0 . A second patch I 0 p is cropped from I 0 at position p. The two grayscale patches, Ip and I 0 p are then stacked channelwise to create the 2-channel image which is fed directly into our ConvNet. The 4-point parameterization of H AB is then used as the associated ground-truth training label. The process is illustrated in <ref type="figure">Figure 3</ref>.</p><p>Managing the training image generation pipeline gives us full control over the kinds of visual effects we want to model. For example, to make our method more robust to motion blur, we can apply such blurs to the image in our training set. If we want the method to be robust to occlusions, we can insert random occluding shapes into our training images. We experimented with in-painting random occluding rectangles into our training images, as a simple mechanism to simulate real occlusions.  <ref type="figure" target="#fig_6">Fig. 2: 4</ref>-point parameterization. We use the 4-point parameterization of the homography. There exists a 1-to-1 mapping between the 8-dof "corner offset" matrix and the representation of the homography as a 3x3 matrix.</p><formula xml:id="formula_10">H (u1,v1) (u1',v1') (?u1, ?v1) (?u2, ?v2) (?u3,</formula><p>Step 1: Randomly crop at position p. This is Patch A.</p><p>Step 2: Randomly perturb four corners of Patch A.</p><p>Step 3: Compute H AB given these correspondences.</p><p>Step 4: Apply (H AB ) -1 = H BA to the image, and crop again at position p, this is Patch B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Image Homography Network H AB</head><p>Step 5: Stack Patch A and Patch B channel-wise and feed into the network. Set H AB as the target vector. <ref type="figure">Fig. 3</ref>: Training Data Generation. The process for creating a single training example is detailed. See Section III for more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONVNET MODELS</head><p>Our networks use 3x3 convolutional blocks with Batch-Norm <ref type="bibr" target="#b9">[10]</ref> and ReLUs, and are architecturally similar to Oxfords VGG Net <ref type="bibr" target="#b16">[17]</ref> (see <ref type="figure" target="#fig_0">Figure 1</ref>). Both networks take as input a two-channel grayscale image sized 128x128x2. In other words, the two input images, which are related by a homography, are stacked channel-wise and fed into the network. We use 8 convolutional layers with a max pooling layer (2x2, stride 2) after every two convolutions. The 8 convolutional layers have the following number of filters per layer: 64, 64, 64, 64, 128, 128, 128, 128. The convolutional layers are followed by two fully connected layers. The first fully connected layer has 1024 units. Dropout with a probability of 0.5 is applied after the final convolutional layer and the first fully-connected layer. Our two networks share the same architecture up to the last layer, where the first network produces real-valued outputs and the second network produces discrete quantities (see <ref type="figure" target="#fig_6">Figure 4</ref>).</p><p>The regression network directly produces 8 real-valued numbers and uses the Euclidean (L2) loss as the final layer during training. The advantage of this formulation is the simplicity; however, without producing any kind of confidence value for the prediction, such a direct approach could be prohibitive in certain applications.</p><p>The classification network uses a quantization scheme, has a softmax at the last layer, and we use the cross entropy loss function during training. While quantization means that there is some inherent quantization error, the network is able to produce a confidence for each of the corners produced by the method. We chose to use 21 quantization bins for each of the 8 output dimensions, which results in a final layer with 168 output neurons. <ref type="figure">Figure 6</ref> is a visualization of the corner confidences produced by our method -notice how the confidence is not equal for all corners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>We train both of our networks for about 8 hours on a single Titan X GPU, using stochastic gradient descent (SGD) with momentum of 0.9. We use a base learning rate of 0.005 and decrease the learning rate by a factor of 10 after every 30,000 iterations. The networks are trained for for 90,000 total iterations using a batch size of 64. We use Caffe <ref type="bibr" target="#b10">[11]</ref>, a popular open-source deep learning package, for all experiments.</p><p>To create the training data, we use the MS-COCO Training Set. All images are resized to 320x240 and converted to grayscale. We then generate 500,000 pairs of image patches sized 128x128 related by a homography using the method described in Section III. We choose ? = 32, which means that each corner of the 128x128 grayscale image can be perturbed by a maximum of one quarter of the total image edge size. We avoid larger random perturbations to avoid extreme transformations. We did not use any form of pre-training; the weights of the networks were initialized to random values and trained from scratch. We use the MS-COCO validation set to monitor overfitting, of which we found very little.</p><p>To our knowledge there are no large, publicly available homography estimation test sets, thus we evaluate our homography estimation approach on our own Warped MS-COCO 14 Test Set. To create this test set, we randomly chose 5000 images from the test set and resized each image to grayscale 640x480, and generate a pairs of image patches sized 256x256 2 and corresponding ground truth homography, using the approach described in <ref type="figure">Figure 3</ref> with ? = 64.</p><p>We compare the Classification and Regression variants of the HomographyNet with two baselines. The first baseline is a classical ORB <ref type="bibr" target="#b14">[15]</ref> descriptor + RANSAC + getPerspectiveTransform() OpenCV Homography computation. We use the default OpenCV parameters in the traditional homography estimator. This estimates ORB features at multiple scales and uses the top 25 scoring matches as input to the RANSAC estimator. In scenarios where too few ORB features are computed, the ORB+RANSAC approach outputs an identity estimate. In scenarios where the ORB+RANSAC's estimate is too extreme, the 4-point homography estimate is clipped at <ref type="bibr">[-64,64</ref>]. The second baseline uses a 3x3 identity matrix for every pair of images in the test set.</p><p>Since the HomographyNets expect a fixed sized 128x128x2 input, the image pairs from the Warped MS-COCO 14 Test Set are resized from 256x256x2 to 128x128x2 before being passed Loss: Euclidean (L2) <ref type="figure">Figure   Fig</ref> through the network. The 4-point parameterized homography output by the network is then multiplied by a factor of two to account for this. When evaluating the Classification HomographyNet, the corner displacement with the highest confidence is chosen.</p><formula xml:id="formula_11">1 2 ||p(x) ? q(x)|| 2 ? ? 3.Comparison</formula><p>The results are reported in <ref type="figure" target="#fig_8">Figure 5</ref>. We report the Mean Average Corner Error for each approach. To measure this metric, one first computes the L2 distance between the ground truth corner position and the estimated corner position. The error is averaged over the four corners of the image, and the mean is computed over the entire test set. While the regression network performs the best, the classification network can produce confidences and thus a meaningful way to visually debug the results. In certain applications, it may be critical to have this measure of certainty.</p><p>We visualize homography estimations in <ref type="figure" target="#fig_9">Figure 7</ref>. The blue squares in column 1 are mapped to a blue quadrilateral in column 2 by a random homography generated from the process described in Section III. The green quadrilateral is the estimated homography. The more closely the blue and green quadrilateral align, the better. The red lines show the top scoring matches of ORB features across the image patches. A similar visualization is shown in columns 3 and 4, except the Deep Homography Estimator is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPLICATIONS</head><p>Our Deep Homography Estimation system enables a variety of interesting applications. Firstly, our system is fast. It runs at over 300fps with a batch size of one (i.e. real-time inference mode) on an NVIDIA Titan X GPU, which enables a host of applications that are simply not possible with a slower system. The recent emergence of specialized embedded hardware for deep networks will enable applications on many embedded systems or platforms with limited computational power which cannot afford an expensive and power-hungry desktop GPU. These embedded systems are capable of running much larger networks such as AlexNet <ref type="bibr" target="#b11">[12]</ref> in real-time, and should have no problem running the relatively light-weight HomographyNets.</p><p>Secondly, by formulating homography estimation as a machine learning problem, one can build application-specific homography estimation engines. For example, a robot that navigates an indoor factory floor using planar SLAM via homography estimation could be trained solely with images captured from the robot's image sensor of the indoor factory. While it is possible to optimize a feature detector such as ORB to work in specific environments, it is not straightforward. Environment and sensor-specific noise, motion blur, and occlusions which might restrict the ability of a homography estimation algorithm can be tackled in a similar fashion using a ConvNet. Other classical computer vision tasks such as image mosaicing (as in <ref type="bibr" target="#b18">[19]</ref>) and markerless camera tracking systems     <ref type="figure">Fig. 6</ref>: Corner Confidences Measure. Our Classification HomographyNet produces a score for each potential 2D displacement of each corner. Each corner's 2D grid of scores can be interpreted as a distribution.</p><p>for augmented reality (as in <ref type="bibr" target="#b15">[16]</ref>) could also benefit from HomographyNets trained on image pair examples created from the target system's sensors and environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper we asked if one of the most essential computer vision estimation tasks, namely homography estimation, could be cast as a learning problem. We presented two Convolutional Neural Network architectures that are able to perform well on this task. Our end-to-end training pipeline contains two additional insights: using a 4-point corner parameterization of homographies, which makes the parameterizations coordinates operate on the same scale, and using a large dataset of real image to synthetically create an seemingly unlimited-sized training set for homography estimation. We hope that more geometric problems in vision will be tackled using learning paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional Homography Estimation</head><p>Deep Image Homography Estimation The ORB features either concentrate on small regions or cannot detect enough features and perform poorly relative to the HomographyNet, which is uneffected by these phenomena. Row 3: Both methods give reasonably good homography estimates. Row 4: A small amount of Gaussian noise is added to the image pair in row 3, deteriorating the results produced by the traditional method, while our method is unaffected by the distortions. Rows 5-6: The traditional approach extracts well-distributed ORB features, and also outperforms the deep method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Deep Image Homography Estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Deep Image Homography Estimation. HomographyNet produces the Homography relating two images. Our method doe estimation steps and all parameters are trained in an end-to-end fas II. THE 4-POINT HOMOGRAPHY PARAMETERIZATION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>Deep Image Homography Estimation. HomographyNet is a Deep Convolutional Neural Network which directly produces the Homography relating two images. Our method does net require separate corner detection and homography estimation steps and all parameters are trained in an end-to-end fashion using a large dataset of labeled images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 :</head><label>1</label><figDesc>Deep Image Homography Estimation. HomographyNet is a Deep Convolutional Neural Network which directly produces the Homography relating two images. Our method does net require separate corner detection and homography estimation steps and all parameters are trained in an end-to-end fashion using a large dataset of labeled images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>. 4 :</head><label>4</label><figDesc>Classification HomographyNet vs Regression HomographyNet. Our VGG-like Network has 8 convolutional layers and two fully connected layers. The final layer is 8x21 for the classification network and 8x1 for the regression network. The 8x21 output can be interpreted as four 21x21 corner distributions. See Section IV for full ConvNet details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 :</head><label>5</label><figDesc>Homography Estimation Comparison on Warped MS-COCO 14 Test Set. The mean average corner error is computed for various approaches on the Warped MS-COCO 14 Test Set. The HomographyNet with the regression head performs the best. The far right bar shows the error computed if the identity transformation is estimated for each test pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 :</head><label>7</label><figDesc>Traditional Homography Estimation vs Deep Image Homography Estimation. In each of the 12 examples, blue depicts the ground truth region. The left column shows the output of ORB-based Homography Estimation, the matched features in red, and the resulting mapping in green of the cropping. The right column shows the output of the HomographyNet (regression head) in green. Rows 1-2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Max Pooling Max Pooling Max Pooling Conv1 Conv2 Conv3 Conv4 Conv5 Conv6 Conv7 Conv8 1024 8x11FC 128x128x2 Input Images FC 16x16x128 H Deep Image Homography Estimation using ConvNets</head><label></label><figDesc>Training deep convolutional networks from scratch requires a large amount of data. To meet this requirement, we generate a nearly unlimited number of labeled training examples by</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>app of n T cro avo data fou wit a h</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H B</cell></row><row><cell>128x128x64 3x3</cell><cell>128x128x64 3x3</cell><cell>64x64x64 3x3</cell><cell>64x64x64 3x3</cell><cell>32x32x128 3x3</cell><cell>32x32x128 3x3</cell><cell>16x16x128 3x3</cell><cell>Softmax</cell><cell>ima The wis our use is i M full For we If w real into exp inse</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 I any</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fig eter betw of t</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Max Pooling Max Pooling Max Pooling Conv1 Conv2 Conv3 Conv4 Conv5 Conv6 Conv7 Conv8 1024 8x11FC 128x128x2 Input Images FC 16x16x128</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Softmax</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x3</cell><cell>3x3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x3</cell><cell>3x3</cell><cell>16x16x128</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>32x32x128</cell><cell>32x32x128</cell></row><row><cell>3x3</cell><cell></cell><cell cols="2">3x3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>64x64x64</cell><cell>64x64x64</cell></row><row><cell>128x128x64</cell><cell cols="3">128x128x64</cell></row><row><cell></cell><cell cols="2">H4point =</cell><cell>?v3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(?u4, ?v4)</cell></row><row><cell></cell><cell>Hmatrix =</cell><cell>(</cell><cell>H11 H12 H13 H21 H22 H23 H31 H32 H33</cell><cell>(</cell></row></table><note>1-to-1 mapping Fig. 2: 4-point parameterization. We use the 4-point param- eterization of the homography. There exists a 1-to-1 mapping between the 8-dof "corner offset" matrix and the representation of the homography as a 3x3 matrix.H Deep Image Homography Estimation using ConvNets</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our experiments, we used cropped MS-COCO<ref type="bibr" target="#b14">[15]</ref> images, although any large-enough dataset could be used for training</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We found that very few ORB features were detected when the patches were sized 128x128, while the HomographyNets had no issues working at the smaller scale.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep Semantic Matching for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>abs/1604.01827</idno>
		<imprint>
			<date type="published" when="2016-04" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Parameterizing homographies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<idno>CMU-RI-TR- 06-11</idno>
		<imprint>
			<date type="published" when="2006-03" />
			<publisher>Robotics Institute</publisher>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automatic panoramic image stitching using invariant features. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="59" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring representation learning with cnns for frame to frame ego-motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Costante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Valigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Ciarfuglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1406.2283</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">LSD-SLAM: Largescale direct monocular SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discovering higher level structure in visual slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chekhlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">521540518</biblScope>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Orbslam: A versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Markerless tracking using planar structures in the scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Augmented Reality</title>
		<meeting>International Symposium on Augmented Reality</meeting>
		<imprint>
			<date type="published" when="2000-10" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time monocular SLAM with straight lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video mosaics for virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
