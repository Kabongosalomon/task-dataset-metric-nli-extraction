<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analyzing Learned Molecular Representations for Property Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Yang</surname></persName>
							<email>yangk@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swanson</surname></persName>
							<email>swansonk@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Coley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Eiden</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Guzman-Perez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hopper</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kelley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Mathea</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Palmer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Settels</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klavs</forename><surname>Jensen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">?Department of Chemical Engineering</orgName>
								<orgName type="laboratory">?Computer Science and Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">United States ?BASF SE</orgName>
								<address>
									<postCode>67063</postCode>
									<settlement>Ludwigshafen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">?Amgen Inc</orgName>
								<address>
									<postCode>02141</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Novartis Institutes for BioMedical Research</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analyzing Learned Molecular Representations for Property Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Advancements in neural machinery have led to a wide range of algorithmic solutions for molecular property prediction. Two classes of models in particular have yielded promising results: neural networks applied to computed molecular fingerprints or expert-crafted descriptors, and graph convolutional neural networks that construct a learned molecular representation by operating on the graph structure of the molecule.</p><p>However, recent literature has yet to clearly determine which of these two methods is superior when generalizing to new chemical space. Furthermore, prior research has 1 arXiv:1904.01561v5 [cs.LG] 20 Nov 2019 rarely examined these new models in industry research settings in comparison to existing employed models. In this paper, we benchmark models extensively on 19 public and 16 proprietary industrial datasets spanning a wide variety of chemical endpoints.</p><p>In addition, we introduce a graph convolutional model that consistently matches or outperforms models using fixed molecular descriptors as well as previous graph neural architectures on both public and proprietary datasets. Our empirical findings indicate that while approaches based on these representations have yet to reach the level of experimental reproducibility, our proposed model nevertheless offers significant improvements over models currently used in industrial workflows.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Molecular property prediction, one of the oldest cheminformatics tasks, has received new attention in light of recent advancements in deep neural networks. These architectures either operate over fixed molecular fingerprints common in traditional QSAR models, or they learn their own task-specific representations using graph convolutions. <ref type="bibr" target="#b1">[1]</ref><ref type="bibr" target="#b2">[2]</ref><ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref><ref type="bibr" target="#b5">[5]</ref><ref type="bibr" target="#b6">[6]</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref> Both approaches are reported to yield substantial performance gains, raising state-of-the-art accuracy in property prediction.</p><p>Despite these successes, many questions remain unanswered. The first question concerns the comparison between learned molecular representations and fingerprints or descriptors.</p><p>Unfortunately, current published results on this topic do not provide a clear answer. Wu et al. <ref type="bibr" target="#b2">2</ref> demonstrate that convolution-based models typically outperform fingerprint-based models, while experiments reported in Mayr et al. <ref type="bibr" target="#b12">12</ref> report the opposite. Part of these discrepancies can be attributed to differences in evaluation setup, including the way datasets are constructed. This leads us to a broader question concerning current evaluation protocols and their capacity to measure the generalization power of a method when applied to a new chemical space, as is common in drug discovery. Unless special care is taken to replicate this distributional shift in evaluation, neural models may overfit the training data but still score highly on the test data. This is particularly true for convolutional models that can learn a poor molecular representation by memorizing the molecular scaffolds in the training data and thereby fail to generalize to new ones. Therefore, a meaningful evaluation of property prediction models needs to account explicitly for scaffold overlap between train and test data in light of generalization requirements.</p><p>In this paper, we aim to answer both of these questions by designing a comprehensive evaluation setup for assessing neural architectures. We also introduce an algorithm for property prediction that outperforms existing strong baselines across a range of datasets.</p><p>The model has two distinctive features: (1) It operates over a hybrid representation that combines convolutions and descriptors. This design gives it flexibility in learning a task specific encoding, while providing a strong prior with fixed descriptors. (2) It learns to construct molecular encodings by using convolutions centered on bonds instead of atoms, thereby avoiding unnecessary loops during the message passing phase of the algorithm.</p><p>We extensively evaluate our model and other recently published neural architectures with over 850 experiments on 19 publicly available benchmarks from Wu et al. <ref type="bibr" target="#b2">2</ref> and Mayr et al. <ref type="bibr" target="#b12">12</ref> and on 16 proprietary datasets from Amgen, Novartis, and BASF (Badische Anilin und Soda Fabrik). Our goal is to assess whether the models' performance on the public datasets and their relative ranking are representative of their ranking on the proprietary datasets. We demonstrate that under a scaffold split of training and testing data, the relative ranking of the models is consistent across the two classes of datasets. We also show that a scaffoldbased split of the training and testing data is a good approximation of the temporal split commonly used in industry in terms of the relevant metrics. By contrast, a purely random split is a poor approximation to a temporal split, confirming the findings of Sheridan. <ref type="bibr" target="#b13">13</ref> To put the performance of current models in perspective, we report bounds on experimental error and show that there is still room for improving deep learning models to match the accuracy and reproducibility of screening results.</p><p>Building on the diversity of our benchmark datasets, we explore the impact of molecular representation with respect to the dataset characteristics. We find that a hybrid representation yields higher performance and generalizes better than either convolution-based or fingerprint-based models. We also note that on small datasets (up to 1000 training molecules) fingerprint models can outperform learned representations, which are negatively impacted by data sparsity. Beyond molecular representation issues, we observe that hyperparameter selection plays a crucial role in model performance, consistent with prior work. <ref type="bibr" target="#b14">14</ref> We show that Bayesian optimization yields a robust, automatic solution to this issue. The addition of ensembling further improves accuracy, again consistent with the literature. <ref type="bibr" target="#b15">15</ref> Our experiments show that our model achieves consistently strong out-of-the-box performance and even stronger optimized performance across a wide variety of public and proprietary datasets. Our model achieves comparable or better performance on 11 out of 19 public datasets and on 15 out of 16 proprietary datasets compared to all baseline models. Furthermore, no single baseline model is clearly superior across the remaining 8 public datasets, and the relative performance of the baseline models often varies from dataset to dataset, whereas our model is consistently strong across datasets. These results indicate that our model, and learned molecular fingerprints in general, are applicable and ready to be used as a powerful tool for chemists actively working on drug discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Since the core element of our model is a graph encoder architecture, our work is closely related to previous work on graph encoders, such as those for social networks <ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b16">16</ref> or for chemistry applications. <ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref> Common approaches to molecular property prediction today involve the application of well-known models like support vector machines 25 or random forests 26 to expert-engineered descriptors or molecular fingerprints, such as the Dragon descriptors <ref type="bibr" target="#b27">27</ref> or Morgan (ECFP) fingerprints. <ref type="bibr" target="#b28">28</ref> One direction of advancement is the use of domain expertise to improve the base feature representation of molecular descriptors <ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref> to drive better performance. <ref type="bibr" target="#b12">12</ref> Additionally, many studies have leveraged explicit 3D atomic coordinates to improve performance further. <ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref> The other main line of research is the optimization of the model architecture, whether the model is applied to descriptors or fingerprints <ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b37">37</ref> or is directly applied to SMILES 38 strings 12 or the underlying graph of the molecule. <ref type="bibr" target="#b1">[1]</ref><ref type="bibr" target="#b2">[2]</ref><ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref><ref type="bibr" target="#b5">[5]</ref><ref type="bibr" target="#b6">[6]</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref> Our model belongs to the last category of models, known as graph convolutional neural networks. In essence, such models learn their own expert feature representations directly from the data, and they have been shown to be very flexible and capable of capturing complex relationships given sufficient data. <ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b4">4</ref> In a direction orthogonal to our own improvements, Ishiguro et al. <ref type="bibr" target="#b39">39</ref> also make a strong improvement to graph neural networks. Liu et al. <ref type="bibr" target="#b40">40</ref> also evaluate their model against private industry datasets, but we cannot compare against their method directly owing to dataset differences. <ref type="bibr" target="#b40">40</ref> The property prediction models most similar to our own are encapsulated in the Message Passing Neural Network (MPNN) framework presented in Gilmer et al.. <ref type="bibr" target="#b4">4</ref> We build upon this basic framework by adopting a message-passing paradigm based on updating representations of directed bonds rather than atoms. Additionally, we further improve the model by combining computed molecule-level features with the molecular representation learned by the MPNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>We first summarize MPNNs in general using the terminology of Gilmer et al., <ref type="bibr" target="#b4">4</ref> and then we expand on the characteristics of Directed MPNN (D-MPNN) <ref type="bibr" target="#b19">19</ref> used in this paper. (D-MPNN is originally called structure2vec in Dai et al.. <ref type="bibr" target="#b19">19</ref> In this paper, we refer to it as Directed MPNN to show it is a variant of the generic MPNN architecture.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message Passing Neural Networks</head><p>An MPNN is a model which operates on an undirected graph G with node (atom) features</p><p>x v and edge (bond) features e vw . MPNNs operate in two phases: a message passing phase, which transmits information across the molecule to build a neural representation of the molecule, and a readout phase, which uses the final representation of the molecule to make predictions about the properties of interest.</p><p>More specifically, the message passing phase consists of T steps. On each step t, hidden states h t v and messages m t v associated with each vertex v are updated using message function M t and vertex update function U t according to:</p><formula xml:id="formula_0">m t+1 v = w?N (v) M t (h t v , h t w , e vw ) h t+1 v = U t (h t v , m t+1 v )</formula><p>where N (v) is the set of neighbors of v in graph G, and h 0 v is some function of the initial atom features x v . The readout phase then uses a readout function R to make a property prediction based on the final hidden states according t?</p><formula xml:id="formula_1">y = R({h T v |v ? G}).</formula><p>The output? may be either a scalar or a vector, depending on whether the MPNN is designed to predict a single property or multiple properties (in a multitask setting).</p><p>During training, the network takes molecular graphs as input and makes an output prediction for each molecule. A loss function is computed based on the predicted outputs and the ground truth values, and the gradient of the loss is backpropagated through the readout phase and the message passing phase. The entire model is trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Directed MPNN</head><p>The main difference between the Directed MPNN (D-MPNN) <ref type="bibr" target="#b19">19</ref> and the generic MPNN described above is the nature of the messages sent during the message passing phase. Rather than using messages associated with vertices (atoms), D-MPNN uses messages associated with directed edges (bonds). The motivation of this design is to prevent totters 41 , that is, to avoid messages being passed along any path of the form v 1 v 2 ? ? ? v n where v i = v i+2 for some i. Such excursions are likely to introduce noise into the graph representation. Using <ref type="figure" target="#fig_0">Figure 1</ref> as an illustration, in D-MPNN, the message 1 ? 2 will only be propagated to nodes 3 and 4 in the next iteration, whereas in the original MPNN it will be sent to node 1 as well, creating an unnecessary loop in the message passing trajectory. Compared to the atom based message passing approach, this message passing procedure is more similar to belief propagation in probabilistic graphical models. <ref type="bibr" target="#b42">42</ref> We refer to Dai et al. <ref type="bibr" target="#b19">19</ref> for futher discussion about the connection between D-MPNN and belief propagation.</p><p>The D-MPNN works as follows. The D-MPNN operates on hidden states h t vw and messages m t vw instead of on node based hidden states h t v and messages m t v . Note that the direction of messages matters (i.e., h t vw and m t vw are distinct from h t wv and m t wv ). The corresponding message passing update equations are thus</p><formula xml:id="formula_2">m t+1 vw = k?{N (v)\w} M t (x v , x k , h t kv ) h t+1 vw = U t (h t vw , m t+1 vw ).</formula><p>Observe that message m t+1 vw does not depend on its reverse message m t wv from the previous iteration. Prior to the first step of message passing, we initialize edge hidden states with</p><formula xml:id="formula_3">h 0 vw = ? (W i cat(x v , e vw ))</formula><p>where W i ? R h?h i is a learned matrix, cat(x v , e vw ) ? R h i is the concatenation of the atom features x v for atom v and the bond features e vw for bond vw, and ? is the ReLU activation function. <ref type="bibr" target="#b43">43</ref> We choose to use relatively simple message passing functions M t and edge update functions U t . Specifically, we define M t (x v , x w , h t vw ) = h t vw and we implement U t with the same neural network on every step,</p><formula xml:id="formula_4">U t (h t vw , m t+1 vw ) = U (h t vw , m t+1 vw ) = ? (h 0 vw + W m m t+1 vw )</formula><p>where W m ? R h?h is a learned matrix with hidden size h. Note that the addition of h 0 vw on every step provides a skip connection to the original feature vector for that edge.</p><p>Finally, we return to an atom representation of the molecule by summing the incoming bond features according to</p><formula xml:id="formula_5">m v = k?N (v) h T kv h v = ? (W a cat(x v , m v ))</formula><p>where W a ? R h?h is a learned matrix.</p><p>Altogether, the D-MPNN message passing phase operates according to</p><formula xml:id="formula_6">h 0 vw = ? (W i cat(x v , e vw ))</formula><p>followed by</p><formula xml:id="formula_7">m t+1 vw = k?{N (v)\w} h t kv h t+1 vw = ? (h 0 vw + W m m t+1 vw )</formula><p>for t ? {1, . . . , T }, followed by</p><formula xml:id="formula_8">m v = w?N (v) h T vw h v = ? (W a cat(x v , m v )).</formula><p>The readout phase of the D-MPNN is the same as the readout phase of a generic MPNN.</p><p>In our implementation of the readout function R, we first sum the atom hidden states to obtain a feature vector for the molecule</p><formula xml:id="formula_9">h = v?G h v .</formula><p>Finally, we generate property predictions? = f (h) where f (?) is a feed-forward neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Featurization</head><p>Our model's initial atom and bond features are listed in <ref type="table" target="#tab_0">Tables 1 and 2</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D-MPNN with Features</head><p>Next, we discuss further extensions and optimizations to improve performance. Although an MPNN should ideally be able to extract any information about a molecule that might be relevant to predicting a given property, two limitations may prevent this in practice. First, many property prediction datasets are very small, i.e., on the order of only hundreds or thousands of molecules. With so little data, MPNNs are unable to learn to identify and extract all features of a molecule that might be relevant to property prediction, and they are susceptible to overfitting to artifacts in the data. Second, most MPNNs use fewer message passing steps than the diameter of the molecular graph, i.e. T &lt; diam(G), meaning atoms that are a distance of greater than T bonds apart will never receive messages about each other. This results in a molecular representation that is fundamentally local rather than global in nature, meaning the MPNN may struggle to predict properties that depend heavily on global features.</p><p>In order to counter these limitations, we introduce a variant of the D-MPNN that incorporates 200 global molecular features that can be computed rapidly in silico using RDKit.</p><p>The neural network architecture requires that the features are appropriately scaled to prevent features with large ranges dominating smaller ranged features, as well as preventing issues where features in the training set are not drawn from the same sample distribution as features in the testing set. To prevent these issues, a large sample of molecules was used to fit cumulative density functions (CDFs) to all features. CDFs were used as opposed to simpler scaling algorithms mainly because CDFs have the useful property that each value has the same meaning: the percentage of the population observed below the raw feature value. Min-max scaling can be easily biased with outliers and Z-score scaling assumes a normal distribution which is most often not the case for chemical features, especially if they are based on counts.</p><p>The CDFs were fit to a sample of 100k compounds from the Novartis internal catalog using the distributions available in the scikit-learn package, <ref type="bibr" target="#b45">45</ref> a sample of which can be seen in <ref type="figure" target="#fig_2">Figure 2</ref>. One could do a similar normalization using publicly available databases such as ZINC <ref type="bibr" target="#b46">46</ref> and PubChem. 47 scikit-learn was used primarily due to the simplicity of fitting and the final application. However, more complicated techniques could be used in the future to fit to empirical CDFs, such as finding the best fit general logistic function, which has been shown to be successful for other biological datasets. <ref type="bibr" target="#b48">48</ref> No review was taken to remove odd distributions. For example, azides are hazardous and rarely used outside of a few specific reactions, as reflected in the fr azide distribution in <ref type="figure" target="#fig_2">Figure 2</ref>. As such, since the sample data was primarily used for chemical screening against biological targets, the distribution used here may not accurately reflect the distribution of reagents used for chemical synthesis. For the full list of calculated features, please refer to the Supporting Information.</p><p>To incorporate these features, we modify the readout phase of the D-MPNN to apply the feed-forward neural network f to the concatenation of the learned molecule feature vector h and the computed global features h f ,?</p><formula xml:id="formula_10">= f (cat(h, h f )).</formula><p>This is a very general method of incorporating external information and can be used with any MPNN and any computed features or descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Optimization</head><p>The performance of MPNNs, like most neural networks, can depend greatly on the settings of the various model hyperparameters, such as the hidden size of the neural network lay-(a) fr azide</p><formula xml:id="formula_11">(b) Kappa2</formula><p>(c) fr pyridine (d) BalabanJ <ref type="figure" target="#fig_2">Figure 2</ref>: Four example distributions fit to a random sample of 100,000 compounds used for biological screening in Novartis. Note that some distributions for discrete calculations, such as fr pyridine, are not fit especially well. This is an active area for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ers. Thus to maximize performance, we perform hyperparameter optimization via Bayesian</head><p>Optimization 14 using the Hyperopt 49 Python package. We specifically optimize our model's depth (number of message-passing steps), hidden size (size of bond message vectors), number of feed-forward network layers, and dropout probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensembling</head><p>A common technique in machine learning for improving model performance is ensembling,</p><p>where the predictions of multiple independently trained models are combined to produce a more accurate prediction. <ref type="bibr" target="#b15">15</ref> We apply this technique by training several copies of our model, each initialized with different random weights, and then averaging the predictions of these models (each with equal weight) to generate an ensemble prediction.</p><p>Since prior work did not report performance using ensembling, all direct comparisons we make to prior work use a single D-MPNN model for a fair comparison. However, we also report results using an ensemble to illustrate the maximum possible performance of our model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>We implement our model using the PyTorch 50 deep learning framework. All code for the D-MPNN and its variants is available in our GitHub repository. <ref type="bibr" target="#b51">51</ref>   <ref type="table" target="#tab_3">Table 3</ref>. It is worth noting that for some datasets, the number of compounds in <ref type="table" target="#tab_5">Table 4</ref>   None of our splits on classification datasets are stratified; we do not enforce class balance.</p><p>Compared to random splits, the scaffold splits have more class imbalance on average, but are not excessively imbalanced; we analyze this class balance quantitatively in the Additional Dataset Statistics section of the Supporting Information.</p><p>Compared to a random split, a scaffold split is a more challenging and realistic evaluation setting as shown in <ref type="figure" target="#fig_0">Figures 11 and 13</ref>. This allows us to use a scaffold split as a proxy for the chronological split present in real-world property prediction data, where one trains a model on past data to make predictions on future data, although chronological splits are still preferred when available. However, as chronological information is not available for most public datasets, we use a scaffold-based split for all evaluations except for our direct comparison with the MoleculeNet models from Wu et al., 2 for which we use their original data splits.</p><p>Baselines. We compare our model to the following baselines: ? Random forest on binary Morgan fingerprints.</p><p>? Feed-forward network (FFN) on binary Morgan fingerprints using the same FFN architecture that our D-MPNN uses during its readout phase.</p><p>? FFN on count-based Morgan fingerprints.</p><p>? FFN on RDKit-calculated descriptors.  <ref type="bibr" target="#b12">12</ref> we modified the authors' original code with their guidance in order to run their code on all of the datasets, not just on the ChEMBL dataset they experimented with.</p><p>We tuned learning rates and hidden dimensions in addition to the extensive hyperparameter search already present in their code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>In the following sections, we analyze the performance of our model on both public and proprietary datasets. Specifically, we aim to answer the following questions:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to Baselines</head><p>After optimizing our model, we compare our best single (non-ensembled) model on each dataset against models from prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to MoleculeNet</head><p>We first compare our D-MPNN to the best model from MoleculeNet 2,62 on the same datasets and splits on which Wu et al. <ref type="bibr" target="#b2">2</ref> evaluate their models. We were unable to reproduce their original data splits on BACE, Toxcast, and QM7, but we have evaluated our model against their original splits on all of the other datasets. The splits are a mix of random, scaffold, and time splits, as indicated in <ref type="figure" target="#fig_5">Figure 3</ref>.</p><p>Overall on the 10 datasets where the MoleculeNet models only use 2D information, i.e. all datasets except the QM and PDBbind datasets, our D-MPNN is significantly better than the best MoleculeNet models on 5 datasets, is not significantly different on 3 datasets, and is significantly worse on 2 datasets. This indicates that D-MPNN tends to outperform even the best MoleculeNet models, with the added benefit that the D-MPNN model architecture is the same for every dataset while the best MoleculeNet model architecture differs between datasets.</p><p>Furthermore, we note that there are two cases in which our D-MPNN may underperform.</p><p>The first is the MUV dataset, which is large but extremely imbalanced; only 0.  Comparison to <ref type="bibr">Mayr et al. 12</ref> In addition, we compare D-MPNN to the baseline from Mayr et al. <ref type="bibr" target="#b12">12</ref> in <ref type="figure" target="#fig_7">Figure 4</ref>. We reproduced the features from their best model on each dataset using their scripts or equivalent packages. <ref type="bibr" target="#b63">63</ref> We then ran their code and hyperparameter optimization directly on the classification datasets, and we modified their code to run on regression datasets with the authors' guidance. <ref type="bibr" target="#b63">63</ref> On most classification datasets, we obtain similar performance to Mayr et al.. <ref type="bibr" target="#b12">12</ref> On regression datasets, the baseline from Mayr et al. <ref type="bibr" target="#b12">12</ref>   The parameters of the simple baseline models are also out-of-the-box defaults. We make this comparison in order to demonstrate the strong out-of-the-box performance of our model across a wide variety of datasets. Finally, we include the performance of the automatically optimized version of our model as a reference. <ref type="figure" target="#fig_8">Figure 5</ref> shows that even without optimization, our D-MPNN provides an excellent starting point on a wide variety of datasets and targets, though it can be improved further with proper optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proprietary Datasets</head><p>We also ran our model on several private industry datasets, verifying that our model's strong performance on public datasets translates to real-world industrial datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amgen</head><p>We ran our model along with <ref type="bibr">Mayr</ref>  3. Rat liver microsomes intrinsic clearance (RLM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Human pregnane X receptor % activation at 2uM and 10uM (hPXR). In addition, we binarized the hPXR dataset according to Amgen's recommendations in order to evaluate on a classification dataset. Details of the datasets are shown in <ref type="table" target="#tab_9">Table 5</ref>.</p><p>Throughout the following, note that rPPB is in logit while Sol and RLM are in log 10 .  <ref type="figure" target="#fig_9">Figure 6</ref>. Thus our D-MPNN's strong performance on scaffold splits of public datasets can translate well to chronological splits of private industry datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BASF</head><p>We ran our model on 10 highly related quantum mechanical datasets from BASF. Each dataset contains 13 properties calculated on the same 30,733 molecules, varying the solvent in each dataset. Dataset details are in <ref type="table" target="#tab_10">Table 6</ref>.  For these datasets, we used a scaffold-based split because a chronological split was unavailable. We found that the model of Mayr et al. <ref type="bibr" target="#b12">12</ref> is numerically unstable on these datasets, and we therefore omit it from the comparison below. Once again we find that our model, originally designed to succeed on a wide range of public datasets, is robust enough to transfer to proprietary datasets as shown in <ref type="figure" target="#fig_10">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novartis</head><p>Finally, we ran our model on one proprietary dataset from Novartis as described in <ref type="table" target="#tab_11">Table 7</ref>.</p><p>As with other proprietary datasets, our D-MPNN outperforms the other baselines as shown in <ref type="figure" target="#fig_11">Figure 8</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Error</head><p>As a final "oracle" baseline, we compare our model's performance with to an experimental upper upper bound: the agreement between multiple runs of the same assay, which we refer to as the experimental error. <ref type="figure">Figure 9</ref> shows the R 2 of our model on the private Amgen regression datasets together with the experimental error; in addition, this graph shows the performance of Amgen's internal model using expert-crafted descriptors. Both models remain far less accurate than the corresponding ground truth assays. Thus there remains significant space for further performance improvement in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Split Type</head><p>We now justify our use of scaffold splits for performance evaluation. The ultimate goal of building a property prediction model is to predict properties on new chemistry in order to aid the search for drugs from new classes of molecules. On proprietary company datasets, performance on new chemistry is evaluated using a chronological split of the data, i.e., everything before a certain date serves as the training set while everything after that date serves as the test set. This approximates model performance on molecules that chemists are likely to investigate in the future. Since chronological data is typically unavailable for public datasets, we investigate whether we can use our scaffold split as a reasonable proxy for a chronological split, following the work of Sheridan. 13 <ref type="figure" target="#fig_0">Figure 10</ref> provides motivation for a scaffold split approach. As illustrated in the <ref type="figure">figure,</ref> train and test sets according to a chronological split share fewer molecular scaffolds than train and test sets split randomly. Since our scaffold split enforces zero molecular scaffold <ref type="figure">Figure 9</ref>: Comparison of Amgen's internal model and our D-MPNN (evaluated using a single run on a chronological split) to experimental error (higher = better). Note that the experimental error is not evaluated on the exact same time split as the two models since it can only be measured on molecules which were tested more than once, but even so the difference in performance is striking.</p><p>overlap between the train and test set, it should ideally provide a split that is at least as difficult as a chronological split.</p><p>As illustrated in <ref type="figure" target="#fig_0">Figures 11, 12</ref>, and 13, performance on our scaffold split is on average closer to performance on a chronological split on proprietary datasets from Amgen and Novartis and on the public PDBbind datasets. However, the results are noisy due to the nature of chronological splitting, where we only have a single data split, as opposed to random and scaffold splitting, which both have a random component and can generate different splits depending on the random seed. We can alleviate the problem with noise in chronological datasets by using a sliding time window to get different equally-sized splits, at the cost of significantly decreasing the dataset size. We report results on such sliding window splits in the Supporting Information, as the conclusions from these splits are qualitatively similar to those in the main paper. <ref type="figure" target="#fig_0">Figure 14</ref> shows the difference between a random split and a scaffold split on the publicly available datasets, further demonstrating that a scaffold split generally results in a more difficult, and ideally more useful, measure of performance. Therefore, all of our results are reported on a scaffold split rather than a random split in order to better reflect the generalization ability of our model on new chemistry. Nevertheless, it should be emphasized that the chronological split is still the ideal split on which to evaluate when it is available.</p><p>Thus we additionally report the results on chronological splits on all datasets where they are available.</p><p>Overall, our results confirm the findings of Sheridan 13 that scaffold and chronological splits are more difficult than random splits, and hence scaffold splits should be preferred over random splits during evaluation. Our findings differ somewhat from those in Sheridan 13 in that we find some evidence that chronological splits may actually be harder than scaffold splits. However, owing to the small number of datasets where chronological splits are available, further investigation is necessary on this point, ideally on a larger range of datasets. <ref type="figure" target="#fig_0">Figure 10</ref>: Overlap of molecular scaffolds between the train and test sets for a random or chronological split of four Amgen regression datasets. Overlap is defined as the percent of molecules in the test set which share a scaffold with a molecule in the train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablations</head><p>Finally, we analyze and justify our modeling choices and optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message Type</head><p>The most important distinction between our D-MPNN and related work is the nature of the messages being passed across the molecule. Most prior work uses messages centered on atoms whereas our D-MPNN uses messages centered on directed bonds. To isolate the effect of the message passing paradigm on property prediction performance, we implemented message passing on undirected bonds and on atoms as well, as detailed in the Supporting Information and in our code. <ref type="figure" target="#fig_0">Figure 15</ref> illustrates the differences in performance between these three types of message passing. While on average the method using directed bonds outperforms the alternatives, the results are largely not statistically significant, so more investigation is warranted on this point. <ref type="figure" target="#fig_0">Figure 11</ref>: Performance of D-MPNN on four Amgen regression datasets according to three methods of splitting the data (lower = better). The chronological split is significantly harder than both random and scaffold on Sol and hPXR, while the scaffold split is significantly harder than the random split on Sol only. <ref type="figure" target="#fig_0">Figure 12</ref>: Performance of D-MPNN on the Novartis regression dataset according to three methods of splitting the data (lower = better). The chronological split is significantly harder than the random split while the scaffold split is not. <ref type="figure" target="#fig_0">Figure 13</ref>: Performance of D-MPNN on the full (F), core (C), and refined (R) subsets of the PDBbind dataset according to three methods of splitting the data (lower = better). The chronological and scaffold splits are significantly harder than the random split in all cases except for the PDBbind-C scaffold split. <ref type="figure" target="#fig_0">Figure 14</ref>: Performance of D-MPNN on random and scaffold splits for several public datasets.</p><p>Only the results on PDBbind-C, HIV, ClinTox, and ChEMBL are not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RDKit Features</head><p>Next, we examined the impact of adding additional molecule-level features from RDKit to our model. <ref type="figure" target="#fig_0">Figure 16</ref> shows the effect on model performance.  Another interesting trend is the effect of adding features to the three PDBbind datasets.</p><p>The features appear to help on all three datasets, but the benefit is much more pronounced on the extremely small PDBbind-C (core) dataset than it is on the larger PDBbind-R (refined)</p><p>and PDBbind-F (full) datasets. This indicates that the features may help compensate for the lack of training data and thus may be particularly relevant in low-data regimes. In particular, we hypothesize that the features may help to regularize a representation derived from a small dataset: because the features are derived from more general chemical knowledge, they implicitly provide the model some understanding of a larger chemical domain. Thus, it is worthwhile to consider the addition of features both when they are particularly relevant to the task of interest and when the dataset is especially small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Optimization</head><p>To improve model performance, we performed Bayesian Optimization to select the best model hyperparameters for each dataset. <ref type="figure" target="#fig_0">Figure 17</ref> illustrates the benefit of performing this optimization, as model performance improves on virtually every dataset. Interestingly, some datasets are particularly sensitive to hyperparameters. While most datasets experience a moderate 2-5% improvement in performance following hyperparameter optimization, the quantum mechanics datasets (QM7, QM8, and QM9) and PCBA see dramatic improvements in performance, with our D-MPNN model performing 37% better on QM9 after optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensembling</head><p>To maximize performance, we trained an ensemble of models. For each dataset, we selected the best single model-i.e. the best hyperparameters along with the RDKit features if the features improved performance-and we trained five models instead of one. The results appear in <ref type="figure" target="#fig_0">Figure 19</ref>. On most datasets, ensembling only provides a small 1-5% benefit, but as with hyperparameter optimization, there are certain datasets, particularly the quantum mechanics datasets, which especially benefit from the effect of ensembling.   While each of the latter three optimizations (RDKit descriptors, hyperparameter optimization, and ensembling) on its own has limited benefits, altogether they significantly improve the model's performance on every dataset except MUV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Data Size</head><p>Finally, we analyze the effect of data size on the performance of our model, using the    In this paper, we performed an extensive comparison of molecular property prediction models based on either fixed descriptors or learned molecular representations by performing over 850 experiments on 19 public and 16 proprietary datasets. The strong performance of our model over these baselines, many of which use computed fingerprints or descriptors, demonstrate that learned molecular representations are indeed ready for "prime time" use in industrial property prediction settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>Nevertheless, several avenues for future research remain. When analyzing the performance of our D-MPNN, we found that it typically underperforms when either 1) the other models incorporate 3D information, as in MoleculeNet's best QM and PDBbind models, 2) the dataset is especially small, as in the PDBbind-C dataset, or 3) the classes are particularly imbalanced, as in the MUV dataset. One avenue of improvement is the incorporation of additional 3D information into our model, which currently includes only a very restricted and naive representation of such features. Another potential improvement is a principled pretraining approach, which some authors have already begun to explore. <ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b65">65</ref> Such an approach could enable models to transfer learning from large chemical datasets to much smaller datasets, thereby improving performance in limited data settings. Another direction for future research is to determine how to adapt models and training algorithms to classification datasets with extreme class imbalance. Finally, in addition to these potential improvements, our analysis of how estimation of model generalizability is affected by split type opens the door to future work in uncertainty quantification and domain of applicability assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We We would like to thank the other members of the computer science and chemical engineering groups in the Machine Learning for Pharmaceutical Discovery and Synthesis consortium for their helpful feedback throughout the research process. We would also like to thank the other industry members of the consortium for useful discussions regarding how to use our model in a real-world setting. We thank Nadine Schneider and Niko Fechner at Novartis for helping to analyze our model on internal Novartis data and for feedback on the manuscript.</p><p>We thank Ryan White, Stephanie Geuns-Meyer, and Florian Boulnois at Amgen Inc. for their help enabling us to run experiments on Amgen datasets. In addition, we thank Lior Hirschfeld for his work on our web-based user interface. <ref type="bibr" target="#b53">53</ref> Finally, we thank Zhenqin Wu for his aid in recreating the original data splits of Wu et al., 2 and we thank Andreas Mayr for helpful suggestions regarding adapting the model from Mayr et al. <ref type="bibr" target="#b12">12</ref> to new classification and regression datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting Information Available</head><p>The following files are available free of charge.</p><p>In our Supporting Information, we provide links to our code and to a demonstration of our web-based user interface. We also provide further comparisons of model performance on both scaffold-based and random splits of the data, and we provide tables with all raw performance numbers (including p-values) which appear in the charts in this paper. In addition, we analyze the class balance of classification datasets, and we provide a list of the RDKit calculated features used by our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of bond-level message passing in our proposed D-MPNN. (a): Messages from the orange directed bonds are used to inform the update to the hidden state of the red directed bond. By contrast, in a traditional MPNN, messages are passed from atoms to atoms (for example atoms 1, 3, and 4 to atom 2) rather than from bonds to bonds. (b): Similarly, a message from the green bond informs the update to the hidden state of the purple directed bond. (c): Illustration of the update function to the hidden representation of the red directed bond from diagram (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, respectively. The D-MPNN's initial node features x v are simply the atom features for that node, while the D-MPNN's initial edge features e vw are the bond features for bond vw. All features are computed using the open-source package RDKit. 44</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>? 2 ?</head><label>2</label><figDesc>The best model for each dataset from MoleculeNet by Wu et al. The best model from Mayr et al., 12 a feed-forward neural network on a concatenation of assorted expert-designed molecular fingerprints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>cross-validation folds while still keeping each part large enough to result in a meaningful AUC computation. We define statistical significance as p-value less than 0.05. Additionally, note that in all figures and tables, "D-MPNN" refers to the base D-MPNN model, "D-MPNN Features" refers to the D-MPNN with RDKit features, "D-MPNN Optimized" refers to the D-MPNN with RDKit features and optimized hyperparameters, and "D-MPNN Ensemble" refers to an ensemble of five D-MPNNs with RDKit features and optimized hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2% of samples are labeled as positives. Wu et al.<ref type="bibr" target="#b2">2</ref> also encountered great difficulty with this extreme class imbalance when experimenting with the MUV dataset; all other datasets we experiment on contain at least 1% positives (see the Supporting Information for full class balance information). The second exception is when there is auxiliary 3D information available, as in the three variants of the PDBbind dataset and in QM9. The current iteration of our D-MPNN does not use 3D coordinate information, and we leave this extension to future work. Thus it is unsurprising that our D-MPNN model underperforms models using 3D information on a protein binding affinity prediction task such as PDBbind, where 3D structure is key. Nevertheless, our D-MPNN model outperforms the best graph-based method in MoleculeNet on PDBbind and QM9. Moreover, we note that on another dataset that provides 3D coordinate information, QM8, our model outperforms the best model in MoleculeNet with or without 3D coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of our D-MPNN with features to the best models from Wu et al.. 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>performs poorly in comparison, despite extensive tuning. We hypothesize that this poor performance on regression in comparison to classification is the result of a large number of binary input features to the output feed-forward network; this hypothesis is supported by the similarly poor performance of our Morgan fingerprint FFN baseline. In addition, their method does not employ early stopping based on validation set performance and therefore may overfit to the training data in some cases; this may be the source of some numerical instability. Overall, our D-MPNN is significantly better than the Mayr et al. 12 model on 8 datasets, is not significantly different on 10 datasets, and is significantly worse on 1 dataset. This indicates that D-MPNN generally outperforms the Mayr et al. 12 model, especially on regression datasets.Out-of-the-Box Comparison of D-MPNN to Other BaselinesFor our final baseline comparison, we evaluate our model's performance "out-of-the-box," i.e. using all the default settings (hidden size = 300, depth = 3, number of feed-forward layers = 2, dropout = 0) without any hyperparameter optimization and without any additional features. For this comparison, we compare to a number of simple baseline models that use computed fingerprints or descriptors:1. Random forest (RF) with 500 trees run on Morgan (ECFP) fingerprints using radius 2 and hashing to a bit vector of size 2048.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of our best single model (i.e. optimized hyperparameters and RDKit features) to the model from Mayr et al.. 2. Feed-forward network (FFN) on Morgan fingerprints. 3. FFN on Morgan fingerprints which use substructure counts instead of bits. 4. FFN on RDKit descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of our unoptimized D-MPNN against several baseline models. We omitted the random forest baseline on PCBA, MUV, Toxcast, and ChEMBL due to large computational cost. The D-MPNN matches or outperforms all baselines on 11 of the 19 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of our D-MPNN against baseline models on Amgen internal datasets on a chronological data split. D-MPNN outperforms almost all of the baselines. Note that the ensembles were ensembles of 3 models rather than 5 for the Amgen datasets only. Also note that RF on Morgan and Mayr et al FFN were only run once on RLM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of our D-MPNN against baseline models on BASF internal regression datasets on a scaffold data split (higher = better). Our D-MPNN outperforms all baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of our D-MPNN against baseline models on the Novartis internal regression dataset on a chronological data split (lower = better). Our D-MPNN outperforms all baseline models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Comparison of performance of different message passing paradigms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Effect of adding molecule-level features generated with RDKit to our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 :</head><label>17</label><figDesc>Effect of performing Bayesian hyperparameter optimization on the depth, hidden size, number of fully connected layers, and dropout of the D-MPNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 :</head><label>18</label><figDesc>An illustration of ensembling models. On the left is a single model, which takes input and makes a prediction. On the right is an ensemble of 3 models. Each model takes the same input and makes a prediction independently, and then the predictions are averaged to generate the ensemble's prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>ChEMBL dataset. ChEMBL is a large dataset of 456,331 molecules on 1,310 targets, but is extremely sparse: only half of the 1,310 targets have at least 300 labels. For this analysis, we use the original scaffold-based split of Mayr et al., 12 containing 3 cross-validation folds. From</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 20 , 12 Figure 19 :</head><label>201219</label><figDesc>we hypothesize that our D-MPNN struggles on low-label targets in comparison to this baseline. As our D-MPNN model does not use any human-engineered fingerprints or descriptors and must therefore learn its features completely from scratch based on the input data, it would be unsurprising if the average ROC-AUC score of D-MPNN is worse than that of the feed-forward network running on human-engineered descriptors in Mayr et al.. Effect of using an ensemble of five models instead of a single model. When we filter the ChEMBL dataset by pruning low-data targets at different thresholds, we find that our D-MPNN indeed may outperform the best model of Mayr et al. 12 at larger data thresholds (Figure 20), though our results are not fully conclusive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 20 :</head><label>20</label><figDesc>Effect of data size on the performance of the model from Mayr et al.<ref type="bibr" target="#b12">12</ref> and of our D-MPNN model (higher = better). All comparisons besides the first are statistically significant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>thank the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, Amgen Inc., BASF, and Novartis for funding this research. The MIT authors are funded by MIT, in particular the MLPDS consortium; the authors at companies are funded by their respective organizations. This work was supported by the DARPA Make-It program under contract ARO W911NF-16-2-0023. The authors declare no competing financial interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Atom Features. All features are one-hot encodings except for atomic mass, which is a real number scaled to be on the same order of magnitude.</figDesc><table><row><cell>Feature</cell><cell>Description</cell><cell>Size</cell></row><row><cell>Atom type</cell><cell>Type of atom (ex. C, N, O), by atomic number.</cell><cell>100</cell></row><row><cell># Bonds</cell><cell>Number of bonds the atom is involved in.</cell><cell>6</cell></row><row><cell>Formal charge</cell><cell>Integer electronic charge assigned to atom.</cell><cell>5</cell></row><row><cell>Chirality</cell><cell>Unspecified, tetrahedral CW/CCW, or other.</cell><cell>4</cell></row><row><cell># Hs</cell><cell>Number of bonded Hydrogen atom.</cell><cell>5</cell></row><row><cell>Hybridization</cell><cell>sp, sp 2 , sp 3 , sp 3 d, or sp 3 d 2 .</cell><cell>5</cell></row><row><cell>Aromaticity</cell><cell>Whether this atom is part of an aromatic system.</cell><cell>1</cell></row><row><cell>Atomic mass</cell><cell>Mass of the atom, divided by 100.</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Bond Features. All features are one-hot encodings.</figDesc><table><row><cell>Feature</cell><cell>Description</cell><cell>Size</cell></row><row><cell>Bond type</cell><cell>Single, double, triple, or aromatic.</cell><cell>4</cell></row><row><cell>Conjugated</cell><cell>Whether the bond is conjugated.</cell><cell>1</cell></row><row><cell>In ring</cell><cell>Whether the bond is part of a ring.</cell><cell>1</cell></row><row><cell>Stereo</cell><cell>None, any, E/Z or cis/trans.</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Code for computing and using the RDKit feature CDFs is available in the Descriptastorus package. 52 Additionally, a web demonstration of our model's predictive capability on public datasets is available online. 53Experiments DataWe test our model on 19 publicly available datasets from Wu et al.<ref type="bibr" target="#b2">2</ref> and Mayr et al..<ref type="bibr" target="#b12">12</ref> These datasets range in size from less than 200 molecules to over 450,000 molecules. They include a wide range of regression and classification targets spanning quantum mechanics, physical chemistry, biophysics, and physiology. Detailed descriptions are provided in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Descriptions of the public datasets used in this paper. Summary statistics for all the datasets are provided inTable 4, and further details onthe datasets are available in Wu et al., 2 with the exception of the ChEMBL dataset which is described in Mayr et al.. 12 Additional information on the class balance of the classification datasets is provided in the Supporting Information. Although most classification datasets are reasonably balanced, the MUV dataset is particularly unbalanced, with only 0.2% of molecules classified as positive. This makes our model unstable, leading to the wide variation in performance on this dataset in the subsequent sections.</figDesc><table><row><cell>Dataset</cell><cell>Category</cell><cell>Description</cell></row><row><cell cols="3">QM7, QM8, QM9 Quantum Mechanics Computer-generated quantum mechanical properties</cell></row><row><cell>ESOL</cell><cell cols="2">Physical Chemistry Water solubility</cell></row><row><cell>FreeSolv</cell><cell cols="2">Physical Chemistry Hydration free energy in water</cell></row><row><cell>Lipophilicity</cell><cell cols="2">Physical Chemistry Octanol/water distribution coefficients</cell></row><row><cell>PDBbind</cell><cell>Biophysics</cell><cell>Protein binding affinity</cell></row><row><cell>PCBA</cell><cell>Biophysics</cell><cell>Assorted biological assays</cell></row><row><cell>MUV</cell><cell>Biophysics</cell><cell>Assorted biological assays</cell></row><row><cell>HIV</cell><cell>Biophysics</cell><cell>Inhibition of HIV replication</cell></row><row><cell>BACE</cell><cell>Biophysics</cell><cell>Inhibition of human ?-secretase 1</cell></row><row><cell>BBBP</cell><cell>Physiology</cell><cell>Ability to penetrate the blood-brain-barrier</cell></row><row><cell>Tox21</cell><cell>Physiology</cell><cell>Toxicity</cell></row><row><cell>ToxCast</cell><cell>Physiology</cell><cell>Toxicity</cell></row><row><cell>SIDER</cell><cell>Physiology</cell><cell>Side effects of drugs</cell></row><row><cell>ClinTox</cell><cell>Physiology</cell><cell>Toxicity</cell></row><row><cell>ChEMBL</cell><cell>Physiology</cell><cell>Biological assays</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>does not precisely match the numbers from Wu et al.. 2 This is because Wu et al. 2 included duplicate molecules in that count while we count the unique number of molecules. Additionally, we left out one or two molecules which could not be processed by RDKit. 44 However, the impact of removing these molecules is negligible on overall model performance. Furthermore, we have fewer molecules in QM7 because we used SMILES strings generated by Wu et al. 2 from the original 3D coordinates in the dataset, but the SMILES conversion process failed for ? 300 molecules. For this reason, we do not directly compare our model's performance on QM7 to the QM7 performance numbers reported by Wu et al.. 2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Summary statistics of the public datasets used in this paper. Note: PDBbind-F, PDBbind-C, and PDBbind-R refer to the full, core, and refined PDBbind datasets from Wu et al..<ref type="bibr" target="#b2">2</ref> Validation and Hyperparameter Optimization. Since many of the datasets are very small (two thousand molecules or fewer), we use a cross-validation approach to decrease noise in the results both while optimizing the hyperparameters and while determining final performance numbers. For consistency, we maintain the same approach for all of our datasets. Specifically, for each dataset, we use 20 iterations of Bayesian optimization on 10 randomly-seeded 80:10:10 data splits to determine the best hyperparameters, selecting hyperparameters based on validation set performance. We then evaluate the model by re-Molecules are partitioned into bins based on their Murcko scaffold calculated by RDKit.<ref type="bibr" target="#b44">44</ref> Any bins larger than half of the desired test set size are placed into the training set, in order to guarantee the scaffold diversity of the validation and test sets. All remaining bins are placed randomly into the training, validation, and test sets until each set has reached its desired size. As this latter process involves randomly placing scaffolds into bins, we are able to generate several different scaffold splits for evaluation.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Tasks Task Type # Compounds</cell><cell>Metric</cell></row><row><cell>QM7</cell><cell>1</cell><cell>Regression</cell><cell>6,830</cell><cell>MAE</cell></row><row><cell>QM8</cell><cell>12</cell><cell>Regression</cell><cell>21,786</cell><cell>MAE</cell></row><row><cell>QM9</cell><cell>12</cell><cell>Regression</cell><cell>133,885</cell><cell>MAE</cell></row><row><cell>ESOL</cell><cell>1</cell><cell>Regression</cell><cell>1,128</cell><cell>RMSE</cell></row><row><cell>FreeSolv</cell><cell>1</cell><cell>Regression</cell><cell>642</cell><cell>RMSE</cell></row><row><cell>Lipophilicity</cell><cell>1</cell><cell>Regression</cell><cell>4,200</cell><cell>RMSE</cell></row><row><cell>PDBbind-F</cell><cell>1</cell><cell>Regression</cell><cell>9,880</cell><cell>RMSE</cell></row><row><cell>PDBbind-C</cell><cell>1</cell><cell>Regression</cell><cell>168</cell><cell>RMSE</cell></row><row><cell>PDBbind-R</cell><cell>1</cell><cell>Regression</cell><cell>3,040</cell><cell>RMSE</cell></row><row><cell>PCBA</cell><cell>128</cell><cell>Classification</cell><cell>437,929</cell><cell>PRC-AUC</cell></row><row><cell>MUV</cell><cell>17</cell><cell>Classification</cell><cell>93,087</cell><cell>PRC-AUC</cell></row><row><cell>HIV</cell><cell>1</cell><cell>Classification</cell><cell>41,127</cell><cell>ROC-AUC</cell></row><row><cell>BACE</cell><cell>1</cell><cell>Classification</cell><cell>1,513</cell><cell>ROC-AUC</cell></row><row><cell>BBBP</cell><cell>1</cell><cell>Classification</cell><cell>2,039</cell><cell>ROC-AUC</cell></row><row><cell>Tox21</cell><cell>12</cell><cell>Classification</cell><cell>7,831</cell><cell>ROC-AUC</cell></row><row><cell>ToxCast</cell><cell>617</cell><cell>Classification</cell><cell>8,576</cell><cell>ROC-AUC</cell></row><row><cell>SIDER</cell><cell>27</cell><cell>Classification</cell><cell>1,427</cell><cell>ROC-AUC</cell></row><row><cell>ClinTox</cell><cell>2</cell><cell>Classification</cell><cell>1,478</cell><cell>ROC-AUC</cell></row><row><cell>ChEMBL</cell><cell>1,310</cell><cell>Classification</cell><cell>456,331</cell><cell>ROC-AUC</cell></row></table><note>training using the optimal hyperparameters and checking performance on the test set. Due to computational cost, we only use 3 splits for HIV, QM9, MUV, PCBA, and ChEMBL. When we run the best model from Mayr et al. 12 for comparative purposes, we optimize their model's hyperparameters with the same splits, using their original hyperparameter optimization script. Split Type. We evaluate all models on random and scaffold-based splits as well as on the original splits from Wu et al.2 and Mayr et al.. 12 The one exception is the model of Mayr et al., 12 which we only ran on scaffold-based splits, due to the large computational cost of optimizing their model. Results on scaffold-based splits are reported below while results on random splits are presented in the Supporting Information. Our scaffold split is similar to that of Wu et al..2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>This is because the scale of the errors can differ drastically between datasets. All results using R 2 , area under the receiver operating characteristic curve (ROC-AUC), or area under the precision recall curve (PRC-AUC) are displayed as plots showing the actual values. For RMSE and MAE, lower is better, while for R 2 , ROC-AUC, and PRC-AUC, higher is better.Table 4 indicates the metric used for each dataset. Tables showing the exact performance numbers for all experiments can be found in the Supporting Information. Note that the error bars on all plots show the standard error of the mean across multiple runs, where standard error is defined as the standard deviation divided by the square root of the number of runs. We evaluate statistical significance using two statistical tests: a one-sided Wilcoxon signed-rank test and a one-sided Welch's t-test. While the Wilcoxon test is stronger, as it is a paired test comparing performance molecule-by-molecule, it requires knowing per-molecule predictions, which we do not have easy access to for the models from MoleculeNet 2 and Mayr et al.. 12 Furthermore, comparisons between data split types inherently involves comparing performance on different test molecules, meaning a per-molecule test is not possible.</figDesc><table /><note>1. How does our model perform on both public and proprietary datasets compared to pub- lic benchmarks, and how close are we to the upper bound on performance represented by experimental reproducibility? 2. How should we be splitting our data, and how does the method of splitting affect our evaluation of the model's generalization performance? 3. What are the key elements of our model, and how can we maximize its performance? In the following sections, all results using root-mean-square error (RMSE) or mean abso- lute error (MAE) are displayed as plots showing change relative to a baseline model rather than showing absolute performance numbers.Therefore, for these comparison we use the weaker Welch's t-test and for all other compar- isons we use the Wilcoxon test. When using the Wilcoxon test for regression datasets, we directly compare test errors molecule-by-molecule. For the classification datasets, we divide all the test molecules into 30 equal parts, compute AUC on each part, and then use the Wilcoxon test on these AUC values. This subdivision of the test molecules into 30 parts gives the Wilcoxon test more strength than evaluating directly on the original 3 or 10 test</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>et al.'s 12 model and our simple baselines on four internal Amgen regression datasets. The datasets are as follows.</figDesc><table><row><cell>1. Rat plasma protein binding free fraction (rPPB).</cell></row><row><cell>2. Solubility in 0.01 M hydrochloric acid solution, pH 7.4 phosphate buffer solution, and</cell></row><row><cell>simulated intestinal fluid (Sol HCL, Sol PBS, and Sol SIF respectively).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Details on internal Amgen datasets. Note: ADME stands for absorption, distribution, metabolism, and excretion.</figDesc><table><row><cell>Category</cell><cell>Dataset</cell><cell cols="3"># Tasks Task Type # Compounds</cell><cell>Metric</cell></row><row><cell>ADME</cell><cell>rPPB</cell><cell>1</cell><cell>Regression</cell><cell>1,441</cell><cell>RMSE</cell></row><row><cell>Physical Chemistry</cell><cell>Solubility</cell><cell>3</cell><cell>Regression</cell><cell>18,007</cell><cell>RMSE</cell></row><row><cell>ADME</cell><cell>RLM</cell><cell>1</cell><cell>Regression</cell><cell>64,862</cell><cell>RMSE</cell></row><row><cell>ADME</cell><cell>hPXR</cell><cell>2</cell><cell>Regression</cell><cell>22,188</cell><cell>RMSE</cell></row><row><cell>ADME</cell><cell>hPXR (class)</cell><cell>2</cell><cell>Classification</cell><cell>22,188</cell><cell>ROC-AUC</cell></row><row><cell cols="6">For each dataset, we evaluate on a chronological split. Our model outperforms the</cell></row><row><cell cols="4">baselines on 4 out of the 5 datasets, as shown in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Details on internal BASF datasets. Note: R 2 is the square of Pearson's correlation coefficient.</figDesc><table><row><cell>Category</cell><cell>Dataset</cell><cell cols="4">Tasks Task Type # Compounds Metric</cell></row><row><cell>Quantum Mechanics</cell><cell>Benzene</cell><cell>13</cell><cell>regression</cell><cell>30,733</cell><cell>R 2</cell></row><row><cell>Quantum Mechanics</cell><cell>Cyclohexane</cell><cell>13</cell><cell>regression</cell><cell>30,733</cell><cell>R 2</cell></row><row><cell cols="2">Quantum Mechanics Dichloromethane</cell><cell>13</cell><cell>regression</cell><cell>30,733</cell><cell>R 2</cell></row><row><cell>Quantum Mechanics</cell><cell>DMSO</cell><cell>13</cell><cell>regression</cell><cell>30,733</cell><cell>R 2</cell></row><row><cell>Quantum Mechanics</cell><cell>Ethanol</cell><cell>13</cell><cell>regression</cell><cell>30,733</cell><cell>R 2</cell></row><row><cell>Quantum Mechanics</cell><cell>Ethyl acetate</cell><cell>13</cell><cell>regression</cell><cell>30,733</cell><cell>R 2</cell></row><row><cell>Quantum Mechanics</cell><cell>H2O</cell><cell>13</cell><cell>regression</cell><cell>30,733</cell><cell>R 2</cell></row><row><cell>Quantum Mechanics</cell><cell>Octanol</cell><cell>13</cell><cell>regression</cell><cell>30,733</cell><cell>R 2</cell></row><row><cell cols="2">Quantum Mechanics Tetrahydrofuran</cell><cell>13</cell><cell>regression</cell><cell>30,733</cell><cell>R 2</cell></row><row><cell>Quantum Mechanics</cell><cell>Toluene</cell><cell>13</cell><cell>regression</cell><cell>30,733</cell><cell>R 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Details on the internal Novartis dataset.</figDesc><table><row><cell>Category</cell><cell cols="5">Dataset Tasks Task Type # Compounds Metric</cell></row><row><cell>Physical Chemistry</cell><cell>logP</cell><cell>1</cell><cell>regression</cell><cell>20,294</cell><cell>RMSE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The results appear to be highly dataset-dependent. Some datasets, such as QM9 and ESOL, show marked improvement with the addition of features, while other datasets, such as PCBA and HIV, actually show worse performance with the features. We hypothesize that this is because the features are particularly relevant to certain tasks while possibly confusing and distracting the model on other tasks. This implies that our model's performance on a given dataset may be further optimized by selecting different features more relevant to the task of interest.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Number of public datasets where D-MPNN is statistically significantly better than, equivalent to, or worse than each baseline model. D-MPNN is better D-MPNN is same D-MPNN is worse # datasets</figDesc><table><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoleculeNet 2</cell><cell>5</cell><cell>3</cell><cell>2</cell><cell>10</cell></row><row><cell>Mayr et al. 12</cell><cell>8</cell><cell>10</cell><cell>1</cell><cell>19</cell></row><row><cell>RF on Morgan</cell><cell>9</cell><cell>1</cell><cell>4</cell><cell>15</cell></row><row><cell>FFN on Morgan</cell><cell>14</cell><cell>5</cell><cell>0</cell><cell>19</cell></row><row><cell>FFN on Morgan Counts</cell><cell>15</cell><cell>4</cell><cell>0</cell><cell>19</cell></row><row><cell>FFN on RDKit</cell><cell>8</cell><cell>5</cell><cell>4</cell><cell>19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8</head><label>8</label><figDesc>shows a summary of how our D-MPNN compares to each of the baseline models. Our model consistently matches or outperforms each baseline individually, and across all baselines, our model achieves comparable or better performance on 11 of the 19 public datasets: QM7, QM8, QM9, ESOL, FreeSolv, Lipophilicity, BBBP, PDBbind-F, PCBA, Tox21, and ClinTox. On the remaining 8 datasets, no single baseline model is consistently superior. Furthermore, our model's strong results transfer to proprietary datasets, where our model outperforms the random forest, feed-forward neural network, and Mayr et al. 12 models on 15 out of the 16 datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Regression Datasets (lower = better).(b) Classification Datasets (higher = better).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Classification Datasets (higher = better)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MoleculeNet: A Benchmark for Molecular Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">513530</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Molecular Graph Convolutions: Moving Beyond Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput.-Aided Mol. Des</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated Graph Sequence Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<title level="m">Supervised Classification with Graph Convolutional Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral Networks and Locally Connected Networks on Graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional Embedding of Attributed Molecular Graphs for Physical Property Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quantum-Chemical Insights from Deep Tensor Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interaction Networks for Learning about Objects, Relations and Physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-Scale Comparison of Machine Learning Methods for Drug Target Prediction on ChEMBL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steijaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ceulemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="5441" to="5451" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Time-Split Cross-Validation as a Method for Estimating the Goodness of Prospective Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="783" to="790" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Taking the Human Out of the Loop: A Review of Bayesian Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 2016</title>
		<meeting>the IEEE 2016</meeting>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="148" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ensemble Methods in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Multiple Classifier Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep Convolutional Networks on Graph-Structured Data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative Embeddings of Latent Variable Models for Structured Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deriving Neural Architectures from Sequence and Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09037</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01925</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">M. Grammar Variational Autoencoder. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>S?nchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Cent. Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Junction Tree Variational Autoencoder for Molecular Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning Multimodal Graph-to-Graph Translation for Molecular Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01070</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Support Vector Machine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dragon Software: An Easy Approach to Molecular Descriptor Calculations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Consonni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Todeschini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="237" to="248" />
			<pubPlace>Match</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extended-Connectivity Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kernels for Small Molecules and the Prediction of Mutagenicity, Toxicity and Anti-Cancer Activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Swamidass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="359" to="368" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chemopy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Freely Available Python Package for Computational Biology and Chemoinformatics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1092" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reoptimization of MDL Keys for Use in Drug Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Durant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Leland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Nourse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1273" to="1280" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mordred: A Molecular Descriptor Calculator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moriwaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kawashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cheminf</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schnet</surname></persName>
		</author>
		<title level="m">A Continuous-Filter Convolutional Neural Network for Modeling Quantum Interactions. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02144</idno>
		<title level="m">Covariant Compositional Networks for Learning Graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Von Lilienfeld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05532</idno>
		<title level="m">Machine Learning Prediction Errors Better than DFT Accuracy</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PotentialNet for Molecular Property Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Husic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Cent. Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1520" to="1530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ligand Biological Activity Predicted by Cleaning Positive and Negative Chemical Correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bassyouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Chemical Language and Information System. 1. Introduction to Methodology and Encoding Rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smiles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Graph Warp Module: An Auxiliary Module for Boosting the Power of Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01020</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boulnois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06236</idno>
		<title level="m">Chemi-net: A Graph Convolutional Network for Accurate Drug Property Prediction</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Extensions of Marginalized Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mah?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akutsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Perret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Machine learning</title>
		<meeting>the Twenty-First International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">RDKit: Open-Source Cheminformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Landrum</surname></persName>
		</author>
		<ptr target="https://rdkit.org/docs/index.html" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ZINC-A Free Database of Commercially Available Compounds for Virtual Screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Shoichet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="177" to="182" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PubChem Substance and Compound Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Thiessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gindulyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Shoemaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1202" to="1213" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">LRpath: A Logistic Regression Approach for Identifying Enriched Biological Groups in Gene Expression Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sartor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Leikauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Medvedovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="211" to="217" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Distributed Asynchronous Hyperparameter Optimization in Python</title>
		<ptr target="https://github.com/hyperopt/hyperopt" />
		<imprint>
			<date type="published" when="2019-05-24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<title level="m">Lerer, A. Automatic differentiation in PyTorch. 31st Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Message Passing Neural Networks for Molecule Property Prediction</title>
		<ptr target="https://github.com/swansonk14/chemprop" />
		<imprint>
			<date type="published" when="2019-05-24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Descriptor computation(chemistry) and (optional) storage for machine learning</title>
		<ptr target="https://github.com/bp-kelley/descriptastorus" />
		<imprint>
			<date type="published" when="2019-05-24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<ptr target="http://chemprop.csail.mit.edu" />
		<title level="m">Chemprop Machine Learning for Molecular Property Prediction</title>
		<imprint>
			<date type="published" when="2019-05-24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Greedy Function Approximation: A Gradient Boosting Machine. Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1189" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Logit Models from Economics and Other Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Cramer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep Architectures and Deep Learning in Chemoinformatics: The Prediction of Aqueous Solubility for Drug-like Molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lusci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pollastri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1563" to="1575" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Support-Vector Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep Neural Nets as a Method for Quantitative Structure-Activity Relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Svetnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="263" to="274" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Is Multitask Deep Learning Practical for Pharma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tudor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2068" to="2076" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Influence Relevance Voting: An Accurate and Interpretable Virtual High Throughput Screening Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Swamidass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-A</forename><surname>Azencott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gramajo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="756" to="766" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">ANI-1: An Extensible Neural Network Potential with DFT Accuracy at Force Field Computational Cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Roitberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3192" to="3203" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deep Learning for the Life Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eastman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<ptr target="https://github.com/yangkevin2/lsc_experiments" />
		<title level="m">Scripts for running lsc model on other datasets</title>
		<imprint>
			<date type="published" when="2019-05-24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06930</idno>
		<title level="m">Pre-training Graph Neural Networks with Kernels</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for Transferable Chemical Property Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hodas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
