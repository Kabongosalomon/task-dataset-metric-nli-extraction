<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Age Group and Gender Estimation in the Wild with Deep RoR Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
							<email>zhangkeit@ncepu.edu.cn.c.gaoiswiththe</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liru</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Sun</forename><surname>Miao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Xingfang</forename><surname>Yuan</surname></persName>
							<email>xyuan@mail.missouri.edu.t.x.haniswiththe</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Tony</forename><forename type="middle">X</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhenbing</forename><surname>Zhao</surname></persName>
							<email>zhaozhenbing@ncepu.edu.cn.b.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogang</forename><surname>Li</surname></persName>
							<email>baogangli@ncepu.edu.cn.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Communication Engineering</orgName>
								<orgName type="institution">North China Electric Power University</orgName>
								<address>
									<postCode>071000</postCode>
									<settlement>Baoding, Hebei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic and Communication Engineer-ing</orgName>
								<orgName type="institution">Electric Power University</orgName>
								<address>
									<postCode>071000</postCode>
									<settlement>Baoding, Hebei</settlement>
									<country>North China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic and Communication Engineer-ing</orgName>
								<orgName type="institution">Electric Power University</orgName>
								<address>
									<postCode>071000</postCode>
									<settlement>Baoding, Hebei</settlement>
									<country>North China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Engi-neering</orgName>
								<orgName type="institution">University of Missouri</orgName>
								<address>
									<postCode>65211</postCode>
									<settlement>Columiba</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Electrical and Computer En-gineering</orgName>
								<orgName type="institution">University of Missouri</orgName>
								<address>
									<postCode>65211</postCode>
									<settlement>Columiba</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Electrical and Computer En-gineering</orgName>
								<orgName type="institution">University of Missouri</orgName>
								<address>
									<postCode>65211</postCode>
									<settlement>Columiba</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Department of Electronic and Communication Engineer-ing</orgName>
								<orgName type="institution">Electric Power University</orgName>
								<address>
									<postCode>071000</postCode>
									<settlement>Baoding, Hebei</settlement>
									<country>North China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Department of Electronic and Communication Engineering</orgName>
								<orgName type="institution">North China Electric Power University</orgName>
								<address>
									<postCode>071000</postCode>
									<settlement>Baoding, Hebei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Age Group and Gender Estimation in the Wild with Deep RoR Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Manuscript received 28 Aug. 2017; revised XX XXX. 2017.</note>
					<note>IEEE TRANSACTIONS ON L A T E X CLASS FILES, VOL. XX, NO. X, AUGUST 2017 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-age and gender estimation</term>
					<term>Adience</term>
					<term>RoR</term>
					<term>weighted loss</term>
					<term>pre-training</term>
					<term>ImageNet</term>
					<term>IMDB-WIKI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatically predicting age group and gender from face images acquired in unconstrained conditions is an important and challenging task in many real-world applications. Nevertheless, the conventional methods with manually-designed features on in-the-wild benchmarks are unsatisfactory because of incompetency to tackle large variations in unconstrained images. This difficulty is alleviated to some degree through Convolutional Neural Networks (CNN) for its powerful feature representation. In this paper, we propose a new CNN based method for age group and gender estimation leveraging Residual Networks of Residual Networks (RoR), which exhibits better optimization ability for age group and gender classification than other CNN architectures. Moreover, two modest mechanisms based on observation of the characteristics of age group are presented to further improve the performance of age estimation. In order to further improve the performance and alleviate overfitting problem, RoR model is pre-trained on ImageNet firstly, and then it is fune-tuned on the IMDB-WIKI-101 data set for further learning the features of face images, finally, it is used to fine-tune on Adience data set. Our experiments illustrate the effectiveness of RoR method for age and gender estimation in the wild, where it achieves better performance than other CNN methods. Finally, the RoR-152+IMDB-WIKI-101 with two mechanisms achieves new state-of-the-art results on Adience benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig.1(a)</ref> <p>is the overview of RoR architecture for age classification with weighted loss layer. The images from Adience data set represent some challenges of age and gender estimation from real-world, unconstrained images. RoR architecture is adopted for feature learning. In weighted loss layer, we use different loss weight instead of equal loss weight based on aging curve. The green circles stand for the original loss of every age group, and the red circles are denoted as different loss weight of every age group. <ref type="figure">Fig.1(b)</ref> is the pipeline of our framework. The RoR model is pre-trained on ImageNet firstly, and then it is fune-tuned on the IMDB-WIKI-101 data set for further learning the features of face images, finally, it is used to fine-tune on Adience data set for age and gender estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A GE and gender, two of the key facial attributes, play very foundational roles in social interactions, making age and gender estimation from a single face image an important task in intelligent applications, such as access control, humancomputer interaction, law enforcement, marketing intelligence and visual surveillance, etc <ref type="bibr" target="#b0">[1]</ref>.</p><p>Over the last decade, most methods used manually-designed features and statistical models <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> to estimate age and gender <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and they achieved respectable results on the benchmarks of constrained images, such as FG-NET <ref type="bibr" target="#b10">[11]</ref> and MORPH <ref type="bibr" target="#b11">[12]</ref>. However, manuallydesigned features based methods behave unsatisfactorily on recent benchmarks of unconstrained images, namely "in-thewild" benchmarks, including Public Figures <ref type="bibr" target="#b12">[13]</ref>, Gallagher group photos <ref type="bibr" target="#b13">[14]</ref>, Adience <ref type="bibr" target="#b14">[15]</ref> and the apparent age data set LAP <ref type="bibr" target="#b15">[16]</ref> for these features' ineptitude to approach large variations in appearance, noise, pose and lighting.</p><p>Deep learning, especially deep Convolutional Neural Networks (CNN) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, arXiv:1710.02985v1 [cs.CV] 9 Oct 2017 <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, has proven itself to be a strong competitor to the more sophisticated and highly tuned methods <ref type="bibr" target="#b26">[27]</ref>. Although unconstrained photographic conditions bring about various challenges to age and gender prediction in the wild, we can still enjoy great improvements brought by CNNs <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b0">[1]</ref>. The optimization ability of neural networks is critical to the performance of age and gender estimation, while existing CNNs designed for age and gender estimation only have several layers, which severely limit the development of age and gender estimation. Therefore, we construct a very deep CNN, Residual networks of Residual networks (RoR) <ref type="bibr" target="#b42">[43]</ref>, for age group and gender estimation in the wild. To begin with, we construct RoR with different residual block types, and analyze the effects of drop-path, dropout, maximum epoch number, residual block type and depth in order to promote the learning capability of CNN. In addition, analysis of the characteristics of age estimation suggests two modest mechanisms, pretrained CNN by gender and weighted loss layer, to further increase the accuracy of age estimation, as shown in <ref type="figure">Fig. 1(a)</ref>. Moreover, in order to further improve the performance and alleviate over-fitting problem on small scale data set, we train RoR model on ImageNet firstly, and then fine-tune it on IMDB-WIKI-101 data set, thirdly, we use the model to further fine-tune on Adience data set. <ref type="figure">Fig. 1(b)</ref> shows the pipeline of our framework. Finally, through massive experiments on Adience data set, our RoR model achieves the new state-ofthe-art results on Adience data set.</p><p>The remainder of the paper is organized as follows. Section II briefly reviews related work for age and gender estimation methods and deep convolutional neural networks. The proposed RoR age and gender estimation method and the two mechanisms are described in Section III. Experimental results and analysis are presented in Section IV, leading to conclusions in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Age and gender estimation</head><p>In the past twenty years, human age and gender estimation from face image has benefited tremendously from the evolutionary development in facial analysis. Early methods for age estimation were based on geometric features calculating ratios between different measurements of facial features <ref type="bibr" target="#b43">[44]</ref>. Geometry features can separate baby from adult easily but are unable to distinguish between adult and elderly people. Therefore, Active Appearance Model (AAM) based methods <ref type="bibr" target="#b10">[11]</ref> incorporated geometric and texture features to achieve desired result. However, these pixel-based methods are not suitable for in-the-wild images which have large variations in pose, illumination, expression, aging, cosmetics and occlusion. After 2007, most existing methods used manually-designed features in this field, such as Gabor <ref type="bibr" target="#b3">[4]</ref>, LBP <ref type="bibr" target="#b44">[45]</ref>, SFP <ref type="bibr" target="#b4">[5]</ref>, and BIF <ref type="bibr" target="#b5">[6]</ref>. Based on these manually-designed features, regression and classification methods are used to predict the age or gender of face images. SVM based methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref> are used for age group and gender classification. For Regression, linear regression <ref type="bibr" target="#b6">[7]</ref>, SVR <ref type="bibr" target="#b7">[8]</ref>, PLS <ref type="bibr" target="#b8">[9]</ref>, and CCA <ref type="bibr" target="#b9">[10]</ref> are the most popular methods for accurate age prediction. However, all of these methods were only proven effective on constrained benchmarks, and could not achieve respectable results on the benchmarks in the wild <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>Recent research on CNN showed that CNN model can learn a compact and discriminative feature representation when the size of training data is sufficiently large, so an increasing number of researchers start to use CNN for age and gender estimation. Yi et al. <ref type="bibr" target="#b27">[28]</ref> first proposed a CNN based age and gender estimation method, Multi-Scale CNN. Wang et al. <ref type="bibr" target="#b28">[29]</ref> extracted CNN features, and employed different regression and classification methods for age estimation on FG-NET and MORPH. Levi et al. <ref type="bibr" target="#b29">[30]</ref> used CNN for age and gender classification on unconstrained Adience benchmark. Ekmekji <ref type="bibr" target="#b30">[31]</ref> proposed a chained gender-age classification model by training age classifiers on each gender separately. With the development of deeper CNNs, Liu et al. <ref type="bibr" target="#b31">[32]</ref> addressed the apparent age estimation problem by fusing two kinds of models, real-value based regression models and Gaussian label distribution based GoogLeNet on LAP data set. Antipov et al. <ref type="bibr" target="#b32">[33]</ref>   <ref type="bibr" target="#b35">[36]</ref> in loss function for CNN training and obtained better age estimation result. VGG-16 architecture and SVR <ref type="bibr" target="#b36">[37]</ref> were used for age estimation on top of the CNN features. Deep EXpectation (DEX) formulation <ref type="bibr" target="#b0">[1]</ref> was proposed for age estimation based on VGG-16 architecture and a classification followed by a expected value formulation, and it got good results on FG-NET, MORPH, Adience and LAP data sets. Iqbal et al. <ref type="bibr" target="#b37">[38]</ref> proposed a local face description, Directional Age-Primitive Pattern(DAPP), which inherits discernible aging cue information and achieved higher accuracy on Adience data set. Recently, Hou et al. used the R-SAAFc2+IMDB-WIKI <ref type="bibr" target="#b38">[39]</ref> method, and achieved the state-of-the-art results on Adience benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep convolutional neural networks</head><p>It is widely acknowledged that the performance of CNN based age and gender estimation relies heavily on the optimization ability of the CNN architecture, where deeper and deeper CNNs have been constructed. From 5-conv+3fc AlexNet <ref type="bibr" target="#b16">[17]</ref> to the 16-conv+3-fc VGG networks <ref type="bibr" target="#b20">[21]</ref> and 21-conv+1-fc GoogleNet <ref type="bibr" target="#b24">[25]</ref>, then to thousand-layer ResNets, both the accuracy and depth of CNNs were promptly increasing. With a dramatic rise in depth, residual networks (ResNets) <ref type="bibr" target="#b25">[26]</ref> achieved the state-of-the-art performance at ILSVRC 2015 classification, localization, detection, and COCO detection, segmentation tasks. Then in order to alleviate the vanishing gradient problem and further improve the performance of ResNets, Identity Mapping ResNets (Pre-ResNets) <ref type="bibr" target="#b46">[47]</ref> simplified the residual networks training by BN-ReLU-conv order. Huang and Sun et al. <ref type="bibr" target="#b47">[48]</ref> proposed Stochastic Depth residual networks (SD), which randomly dropped a subset of layers and bypassed them with shortcut connections for every mini-batch to alleviate over-fitting and reduce vanishing gradient problem. In order to dig the optimization ability of residual networks family, Zhang et al. <ref type="bibr" target="#b42">[43]</ref> proposed Residual Networks of Residual Networks architecture (RoR), which added shortcuts level by level based on residual networks, and achieved the state-of-the-art results on low-resolution image data sets such as CIFAR-10, CIFAR-100 <ref type="bibr" target="#b48">[49]</ref> and SVHN <ref type="bibr" target="#b49">[50]</ref> at that time. Instead of sharply increasing the feature map dimension, PyramidNet <ref type="bibr" target="#b39">[40]</ref> gradually increases the feature map dimension at all units and gets superior generalization ability. DenseNet <ref type="bibr" target="#b40">[41]</ref> uses densely connected paths to concatenate the input features with the output features, and enables each micro-block to receive raw information from all previous micro-blocks. To enjoy the benefits from both path topologies of ResNets and DenseNet, Dual Path Network <ref type="bibr" target="#b41">[42]</ref> shares common features while maintaining the flexibility to explore new features through dual path architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we describe the proposed RoR architecture with two modest mechanisms for age group and gender classification. Our methodology is essentially composed of four steps: Constructing RoR architecture for improving optimization ability of model, pre-training with gender and training with weighted loss layer for promoting the performance of age group classification, pre-training on ImageNet and further fine-tuning on IMDB-WIKI-101 data set for alleviating overfitting problem and improving the performance of age group and gender classification. In the following, we describe the four main components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network architecture</head><p>RoR <ref type="bibr" target="#b42">[43]</ref> is based on a hypothesis: The residual mapping of residual mapping is easier to optimize than original residual mapping. To enhance the optimization ability of residual networks, RoR can optimize the residual mapping of residual mapping by adding shortcuts level by level based on residual networks. By experiments, Zhang et al. <ref type="bibr" target="#b42">[43]</ref> argued that the optimization ability of Pre-RoR is better than RoR with the same number of layers, so we choose Pre-RoR in this paper except pre-training on ImageNet or IMDB-WIKI.</p><p>In order to train the high-resolution Adience data set, we first construct RoR based on the basic Pre-ResNets for Adience, and denote this kind of RoR as Pre-RoR. Pre-ResNets <ref type="bibr" target="#b46">[47]</ref> include two types of residual block designs: basic residual block and bottleneck residual block. <ref type="figure" target="#fig_2">Fig. 2</ref> shows the Pre-RoR with basic block constructed based on original Pre-ResNets with L basic blocks. The shortcuts in these L original residual blocks are denoted as the final-level shortcuts. To start with, we add a shortcut above all basic blocks, and this shortcut can be called root shortcut or firstlevel shortcut. We use 64, 128, 256 and 512 filters sequentially in the convolutional layers, and each kind of filter has different number (L 1 , L 2 , L 3 , L 4 , respectively) of basic blocks which form four basic block groups. Furthermore, we add a shortcut above each basic block group, and these four shortcuts are <ref type="bibr">16 16</ref> 7?7,conv,64   called second-level shortcuts. Then we can continue adding shortcuts as the inner-level shortcuts. Lastly, the shortcuts in basic residual blocks are regarded as the final-level shortcuts. Let m denote a shortcut level number. In this paper, we choose level number m=3 according to the analysis of Zhang et al. <ref type="bibr" target="#b42">[43]</ref>, so the RoR has root-level, middle-level and final-level shortcuts, shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><formula xml:id="formula_0">image ?? 8-d/2-d</formula><p>The junctions which are located at the end of each residual block group can be expressed by the following formulations.</p><formula xml:id="formula_1">x L1+1 =g(x 1 ) + h(x L1 ) + F (x L1 , W L1 ) x L1+L2+1 =g(x L1+1 ) + h(x L1+L2 )+ F (x L1+L2 , W L1+L2 ) x L1+L2+L3+1 =g(x L1+L2+1 ) + h(x L1+L2+L3 )+ F (x L1+L2+L3 , W L1+L2+L3 ) x L+1 =g(x 1 ) + g(x L1+L2+L3+1 ) + h(x L )+ F (x L , W L )<label>(1)</label></formula><p>where x l and x l+1 are input and output of the l-th block, and F  is a residual mapping function, h(x l ) = x l and g(x l ) = x l are both identity mapping functions. g(x l ) expresses the identity mapping of first-level and second-level shortcuts, and h(x l ) denotes the identity mapping of the final-level shortcuts. g(x l ) function is type B projection shortcut.</p><p>For bottleneck block, He al et. <ref type="bibr" target="#b46">[47]</ref> used a stack of three layers instead of two layers that first reduce the dimensions and then re-increase it. Both basic block and bottleneck block have similar time complexity, so we can get deeper networks easily through bottleneck. In this paper, we also construct a Pre-RoR based on bottleneck Pre-ResNets. The architecture details of Pre-RoR with bottleneck blocks are shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. We use k to control the output dimensions of the blocks. He et al. <ref type="bibr" target="#b46">[47]</ref> chose k=4 led to the results that the input and output planes of these shortcuts are very different. Since the zero-padding (Type A) shortcut will bring more deviation and projection (Type B) shortcut will aggravate over-fitting, our RoR adopts k=4, k=2 and k=1 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pre-training with gender</head><p>Like face recognition, age estimation can be easily affected by many intrinsic and extrinsic factors. Some of the most important factors include identity, gender and ethnicity, together with other factors like Pose, Illumination and Expression (PIE). We can alleviate the effects of these factors by using large data sets in the wild, but the existing data sets for age estimation are generally relatively small. To some extent, gender affects age judgments. On the one hand, the aging process of men slightly differs from women due to different longevity, hormones, skin thickness, etc. On the other hand, women are more likely to hide their real age by using makeup. So real-world age estimations for men and women are not exactly the same. Guo et al. <ref type="bibr" target="#b9">[10]</ref> and Ekmekji <ref type="bibr" target="#b30">[31]</ref> first manually separated the data set according to the gender labels, then trained an age estimator on each subset separately. Inspired by this, we train CNN by gender initially, then replace the gender prediction layer with age prediction layer, and finetune the whole CNN structure at last.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training with weighted loss layer</head><p>There are some diversities lying between general image classification and age estimation. Firstly, the different classes in general image classification are uncorrelated, but the age groups have a sequential relationship between labels. These interrelated age groups are more difficult to distinguish. Secondly, human aging processes show variations in different age ranges. For example, aging processes between mid-life adults and children are not equivalent. In this paper, we will analyze the law of human aging, and do age estimation under its guidance. For human, it is easier to distinguish who is the older one out of two people than to determine the persons' actual ages. Based on this characteristic and age-ordered groups, we define y i , i=1,2...,K, where K is the number of age group labels. Then for a given age group k ? K, we separate the data set into two subsets X + k and X ? k as follows:</p><formula xml:id="formula_2">X + k = {(x i , +1)|y i &gt; k} X ? k = {(x i , +1)|y i ? k}<label>(2)</label></formula><p>Next, we use the two subsets to learn a binary classifier that can be considered as a query: "Is the face older than age group k?" There are eight classes (0-2, 4-6, <ref type="bibr">8-13, 15-20, 25-32, 38-43, 48-53, 60-)</ref> in Adience data set, so we can choose k=1,2,...,7. By doing so, we get seven binary-class data sets, and the results of these binary classifiers can form a human aging curve which represents the human aging process. We execute some experiments on folder0 of Adience data set with 4c2f CNN described in <ref type="bibr" target="#b29">[30]</ref> (just using two classes instead of eight classes), and the aging curve is described in <ref type="figure">Fig. 4</ref> We discover that the 4th, 5th and 6th results are smaller than the others. As a conclusion, the aging process of smaller and greater age group is faster than intermediate age groups, so it is harder to distinguish intermediate age groups comparing to smaller and greater age groups.</p><p>Through above analysis, we realize the 4th, 5th, 6th and 7th groups are more difficult to estimate, so we apply higher loss weights to these age groups. Thus, we define four different settings of loss weight distributions for optimal results, as shown in <ref type="table" target="#tab_4">Table I</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pre-training on ImageNet</head><p>Due to using small scale data sets for age and gender estimation, the over-fitting problem is easy to occur during training, so we use RoR network training ImageNet data set to obtain the basic feature expression model firstly. And then we use the pre-trained RoR model to fine-tune on the Adience data set, so as to alleviate the over-fitting problem brought by the direct training on Adience.</p><p>The preceding data sets using RoR were all small scale image data sets, in this paper we first conduct experiments on large scale and high-resolution image data set, ImageNet. We evaluate our RoR method on the ImageNet 2012 classification data set <ref type="bibr" target="#b50">[51]</ref>, which contains 1.28 million high-resolution training images and 50,000 validation images with 1000 object categories. During training of RoR, we notice that RoR is slower than ResNets. So instead of training RoR from scratch, we use the ResNets models from <ref type="bibr" target="#b51">[52]</ref> for pre-training. The weights from pre-trained ResNets models remain unchanged, but the new added weights are initialized as in <ref type="bibr" target="#b53">[53]</ref>. In addition, SD is not used here because SD makes RoR difficult to converge on ImageNet. Then we replace the 1000 classes prediction layer with age and gender prediction layer, and finetune the whole RoR structure on Adience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Fine-tuning on IMDB-WIKI-101</head><p>In order to make the RoR model further learn the feature expression of facial images and also reduce the over-fitting problem, we use large-scale face image data set IMDB-WIKI-101 <ref type="bibr" target="#b0">[1]</ref> to fine-tune the model after pre-training on ImageNet.</p><p>IMDB-WIKI is the largest publicly available data set for age estimation of people in the wild, containing more than half million images with accurate age labels, whose age ranges from 0 to 100. For the IMDB-WIKI data set, the images were crawled from IMDb and Wikipedia, where IMDB contains 460723 images of 20,284 celebrities and Wikipedia contains 62328 images. As the images of IMDB-WIKI data set are obtained directly from the website, the IMDB-WIKI data set contains many low-quality images, such as human comic images, sketch images, severe facial mask, full body images, multi-person images, blank images, and so on. The example images are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Those bad images seriously affect the network learning effect. Therefore, in this paper, we spend a week manually removing the low quantity images by four people. In our removing process we mainly consider: a) the bad images, which are not standard face images from the IMDB-WIKI data set and b) the images with wrong age labels, especially the age images from 0 to 10 years old. The remaining IMDB-WIKI dataset remains 440607 images. The data set after cleaning is divided into 101 classes representing the age of each age, which we name IMDB-WIKI-101 data set.</p><p>Firstly, we replace the 1000 classes prediction layer on ImageNet with 101 classes prediction layer for age prediction, and fine-tune the RoR structure on IMDB-WIKI-101. When fine-tuning the RoR model, the IMDB-WIKI-101 data set is randomly divided into 90% for training and 10% for testing. Then we replace the 101 classes prediction layer with age and gender prediction layer, and fine-tune the whole RoR structure on Adience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, extensive experiments are conducted to present the effectiveness of the proposed RoR architecture, two mechanisms, pre-training on ImageNet and further fine-tuning on IMDB-WIKI-101 data set. The experiments are conducted on unconstrained age group and gender data set, Adience <ref type="bibr" target="#b14">[15]</ref>. Firstly, we introduce our experimental implementation. Secondly, we empirically demonstrate the effectiveness of two mechanisms for age group classification. Thirdly, we analyze different Pre-RoR models for age group and gender classification. Fourthly, we improve the performance of age and gender estimation by pre-training on ImageNet with RoR models. Furthermore, the RoR model are fine-tuned on IMDB-WIKI-101 data set for learning the feature expression of face images. Finally, the results of our best models are compared with several state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation</head><p>For Adience data set, we do experiments by using 4c2f-CNN <ref type="bibr" target="#b29">[30]</ref>, VGG <ref type="bibr" target="#b20">[21]</ref>, Pre-ResNets <ref type="bibr" target="#b46">[47]</ref>, our Pre-RoR architectures, respectively. 4c2f-CNN: The CNN structure described in <ref type="bibr" target="#b29">[30]</ref> is denoted as baseline for the experiments with two mechanisms. Compared to the original 4c2f-CNN in <ref type="bibr" target="#b29">[30]</ref>, our baseline adds preprocessing of data by subtracting the mean and dividing the standard deviation. VGG: We choose VGG-16 in <ref type="bibr" target="#b20">[21]</ref> to construct age group and gender classifiers. Pre-ResNets: We use Pre-ResNets-34, Pre-ResNets-50 and Pre-ResNets-101 in <ref type="bibr" target="#b46">[47]</ref> as the basic architectures. Pre-RoR: We use the basic block and bottleneck block Pre-ResNets in <ref type="bibr" target="#b46">[47]</ref> to construct RoR architecture. The original Pre-ResNets contain four groups (64 filters, 128 filters, 256 filters and 512 filters) of residual blocks, the feature map sizes are 56, 28, 14 and 7, respectively. Pre-RoR with basic blocks includes Pre-RoR-34 (34 layers), Pre-RoR-58 (58 layers) and Pre-RoR-82 (82 layers). Pre-RoR with bottleneck blocks includes RoR-50 (50 layers) and RoR-101 (101 layers). Each residual block group in different Pre-RoR has different number of residual blocks, as shown in <ref type="table" target="#tab_4">Table II</ref>. Pre-RoR contains four middle-level residual blocks (every middle-level residual block contains some final-level residual blocks) and one rootlevel residual block (the root-level residual block contains four middle-level residual blocks). We adopt BN-ReLU-conv order, as shown in <ref type="figure" target="#fig_2">Fig. 2 and Fig. 3</ref>. Our implementations are based on Torch 7 with one Nvidia Geforce Titan X. We initialize the weights as in <ref type="bibr" target="#b25">[26]</ref>. We use SGD with a mini-batch size of 64 for these architectures except Pre-RoR with neckbottle block where we use mini-batch size 32. The total epoch number is 164. The learning rate starts from 0.1, and is divided by a factor of 10 after epoch 80 and 122. We use a weight decay of 1e-4, momentum of 0.9, and Nesterov momentum with 0 dampening <ref type="bibr" target="#b51">[52]</ref>. For stochastic depth drop-path method, we set p l with the linear decay rule of p 0 = 1 and p L =0.5 <ref type="bibr" target="#b47">[48]</ref>.</p><p>The entire Adience collection includes 26,580 256?256 color facial images of 2,284 subjects, with eight classes of age groups and two classes of gender. Testing for both age and gender classification is performed using a standard five-fold, subject-exclusive cross-validation protocol, defined in <ref type="bibr" target="#b14">[15]</ref>. We use the in-plane aligned version of the faces, originally used in <ref type="bibr" target="#b54">[54]</ref>. For data augmentation, VGG, PreResNets and Pre-RoR use scale and aspect ratio augmentation <ref type="bibr" target="#b51">[52]</ref> instead of scale augmentation used in 4c2f-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effectiveness of two mechanisms</head><p>In this section, we do age group classification experiments on folder0 of Adience data set with two mechanisms based on 4c2f-CNN architecture, and the results are described in <ref type="figure" target="#fig_5">Fig. 6</ref>. Here, we report the exact accuracy(correct age group predicted) and 1-off accuracy (correct or adjacent age group predicted) as <ref type="bibr" target="#b14">[15]</ref>.</p><p>Previously, we use 4c2f-CNN with each mechanism individually. In <ref type="figure" target="#fig_5">Fig. 6</ref>, 4c2f-CNN pre-training by gender (4c2f-CNNpt) achieves apparent progress compared to 4c2f-CNN without pre-training. And then, <ref type="figure" target="#fig_5">Fig. 6</ref> also shows that 4c2f-CNN with loss weight distribution LW3 (4c2f-CNN-LW3) achieves best performance among all the loss weight distributions on folder0 of Adience data set, so we will choose LW3 as the loss weight distribution in the following experiments. Finally, we combine above the two mechanisms to predict age group and <ref type="figure" target="#fig_5">Fig. 6</ref> shows that 4c2f-CNN combined of pretraining by gender and loss weight distribution LW3 together (4c2f-CNN-pt-LW3) achieves better performance than other models. These experiments demonstrate the effectiveness of pre-training method by gender and weighted loss layer for promoting performance of age group classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Age group and gender classification by Pre-RoR</head><p>In order to find the optimal model of Pre-RoR on Adience data set, we do a lot of comparative experiments with folder0 validation, and then we evaluate the effect of SD, dropout, shortcut type, block type, maximum epoch number and depth for age estimation results. Firstly, basic blocks are used in experiments, and the results of different architectures are shown in <ref type="table" target="#tab_4">Table III</ref>. We do some experiments by Pre-ResNets-34 (34 convolutional layers) with and without SD. Because Adience data set only has about 26,580 high-resolution images, over-fitting is a critical problem. In <ref type="table" target="#tab_4">Table III</ref>, the performance of Pre-ResNets-34 with SD is better than that without SD, which means SD alleviates the effect of over-fitting. We then use Pre-RoR-34 +SD to estimate age and gender. Pre-RoR-34+SD outperforms Pre-ResNets-34+SD, because RoR can promote the learning capability of residual networks. To further reduce over-fitting, we try dropout between convolutional layers in residual blocks, but the result of Pre-RoR-34+SD+dropout shows that dropout method in RoR does not make a big difference. This is consistent with WRN <ref type="bibr" target="#b55">[55]</ref>. Zhang et al. <ref type="bibr" target="#b42">[43]</ref> noted that extra parameters would escalate over-fitting and the zero-padding (type A) would bring more deviation, so shortcut Type A should be used in the final-level and Type B should be used in other levels (called Type A+B). <ref type="table" target="#tab_4">Table III</ref> shows that the Pre-RoR-34+SD with Type A+B has better performance than Pre-RoR-34+SD which uses Type B in all levels. <ref type="figure" target="#fig_6">Fig. 7</ref> shows that the test errors by Pre-ResNets-34, Pre-ResNets-34+SD and Pre-RoR-34+SD (Type A+B) at different training epochs with folder0 validation. Zhang et al. <ref type="bibr" target="#b42">[43]</ref> proofed that maximum epoch number of 500 is necessary to optimize RoR on CIFAR-10 and CIFAR-100, but the results of Pre-RoR-34+SD with 300 epochs show that 164 for maximum epoch number is enough for Adience data set. Generally, ResNets <ref type="bibr" target="#b25">[26]</ref> and RoR <ref type="bibr" target="#b42">[43]</ref> can improve performance by increasing depth. We estimate age and gender by Pre-RoR-58+SD and Pre-RoR-82+SD. The age estimation result of Pre-RoR-58+SD is better than Pre-RoR-34+SD, but Pre-RoR-82+SD is worse than Pre- RoR-58+SD, which is caused by degradation. Gender estimation gets better when adding more layers, since degradation is less critical for binary classification.</p><p>Secondly, we use bottleneck blocks instead of basic blocks, and the results of different architectures are shown in <ref type="table" target="#tab_4">Table IV</ref> and <ref type="table">Table V</ref>. We do some experiments by Pre-ResNets-50+SD (Type B, k=4) and Pre-RoR-50+SD (Type A+B, k=4). As can be observed, the performance of Pre-RoR-50+SD (Type A+B, k=4) is worse than Pre-ResNets-50+SD (Type B, k=4). When we use type A in final levels, the input and output planes of these shortcuts are very different, the zero-padding (type A) will bring more deviation. So we reduce the output dimensions by using k=2 and k=1. The results of Pre-RoR-50+SD (Type A+B, k=2) and Pre-RoR-50+SD (Type A+B, k=1) show that deviation problem is largely alleviated by reducing dimensions. The performance of Pre-RoR-50+SD (Type A+B, k=2) is better than Pre-RoR-50+SD (Type A+B, k=1), because reducing dimensions also reduces parameters and the optimizing ability of networks. Pre-RoR-50+SD (Type A+B, k=2) achieves the balance of deviation and over-fitting problems, but it can not catch up Pre-RoR with basic blocks because of these two problems.</p><p>We do the same experiments by increasing the depth to 101 convolutional layers. We find the similar results shown in <ref type="table">Table V</ref> as the networks with 50 convolutional layers in <ref type="table" target="#tab_4">Table IV</ref>. Pre-RoR-101+SD (Type A+B, k=2) achieves the best performance, and also outperforms Pre-RoR-50+SD (Type A+B, k=2).</p><p>In above experiments, we only use one folder to analyze different network architectures. Now we will demonstrate the generality of our method by using standard five-fold, subject-exclusive cross-validation protocol. In the following experiments, we only use Type A+B for Pre-RoR+SD. The age cross-validation results of Pre-RoR+SD (Type A+B) with different block types and depths are shown in <ref type="table" target="#tab_4">Table VI</ref>, where we achieve the similar results with folder0 validation. The performance of Pre-RoR+SD with basic block is better than Pre-RoR+SD with bottleneck block. We analyze that this is because of deviation by zero-padding. Our Pre-ROR-58+SD achieves the best performance, which outperforms 4c2f-CNN by 18.8% and 5.7% on exact and 1-off accuracy of Adience data set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Age group and gender classification by Pre-training on ImageNet</head><p>Because we can not find the well-trained Pre-ResNets on the web, we construct RoR based on the well-trained ResNets from <ref type="bibr" target="#b51">[52]</ref> for ImageNet. The well-trained ResNets from <ref type="bibr" target="#b51">[52]</ref> use Type B in the residual blocks, so we use Type B in all levels of RoR. We use SGD with a mini-batch size of 128 (18 layers and 34 layers) or 64 (101 layers) or 48 (152 layers) for 10 epochs to fine-tune RoR. The learning rate When we use pre-trained RoR model to fine-tune on Adience, we replace the 1000 classes prediction layer with age or gender prediction layer. We use SGD with a mini-batch size of 64 for 120 epochs to fine-tune on Adience. The learning rate starts from 0.01 and is divided by a factor of 10 after epoch 80. Based on the analysis of above section, we find deep Pre-RoR maybe outperform very deep Pre-RoR, so we use RoR-34 instead of deeper RoR as the basic pre-trained model. The results of different methods are shown in <ref type="table" target="#tab_4">Table VIII</ref>. We do some experiments by ResNets-34 and RoR-34. The results of ResNets-34 and RoR-34 by Pre-training on ImageNet are better than the results of ResNets-34 and RoR-34, because pretraining on ImageNet can reduce over-fitting problem. When we add SD method in these experiments, the performance are promoted too. Especially, RoR-34+SD by Pre-training on ImageNet achieves very competitive performance, which outperforms Pre-RoR-34+SD. These experiments verify the effectiveness of pre-training on ImageNet for age group and gender classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Age group and gender classification by fine-tuning on IMDB-WIKI-101</head><p>As the amount of training data strongly affects the accuracy of the trained models, there is a greater need for large datasets. Thus, we use IMDB-WIKI-101 to further fine-tune the RoR model. After pre-training on the ImageNet, we further finetune the RoR model on the IMDB-WIKI-101. The epoch is set to 120. The learning rate starts from 0.01 and is divided by a factor of 10 after epoch 60 and 90. When we use finetuned RoR model to fine-tune on Adience, we replace the 101 classes prediction layer with age or gender prediction layer. The epoch is set to 60. The learning rate is set to 0.0001. As shown in <ref type="table" target="#tab_4">Table IX</ref>, with the IMDB-WIKI-101 data set fine-tuning, both the performances of ResNets-34 and RoR-34 model have been significantly improved. This shows that having a large data set with face age images results in better performance. The performance of RoR-34 fine-tuning on the IMDB-WIKI-101 data set reaches the age exact accuracy of 66.74%(1-off 97.38%) compared to 60.29% (1-off 92.44%) when training directly on the Adience data set. That is competitive performance on Adience data set for age group and gender classification in the wild.</p><p>When we only use ImageNet data set to pre-train the RoR-34 model, the age estimation results on Adience with stochastic depth algorithm are better than without stochastic depth algorithm. However, when we first use the ImageNet dataset to pre-train the RoR-34 network, and then use the IMDB-WIKI-101 data set to fine-tune the RoR-34 network, the age estimation results on the Adience with stochastic depth algorithm are worse than without stochastic depth algorithm. The reason is that the ImageNet dataset is an object image dataset, the network can learn the feature expression of general object, adding the stochastic depth algorithm to the original network is effective for the results. However, the IMDB-WIKI-101 is a large-scale face image data set. The RoR-34 network can fully learn the characteristics of face images from the IMDB-WIKI-101 data set, which reduces the problem of overfitting. After adding stochastic depth algorithm, the original structure of the network will be changed, so the network needs to relearn the characteristics of facial image parameters, that is the reason why the results with SD are not better than the results without SD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparisons with state-of-the-art results of age group and gender classification on Adience</head><p>To begin with, we use 4c2f-CNN, VGG-16, Pre-ResNets, our RoR+SD by Pre-training on ImageNet and Pre-RoR+SD architectures to estimate gender. In addition, we use IMDB-WIKI-101 dataset to fine-tune the ResNets-34 and RoR-34 for gender estimation. The gender cross-validation results by  different methods are shown in <ref type="table" target="#tab_12">Table X</ref>. RoR-34+SD achieves a competitive accuracy 92.43% by only pretraining on Ima-geNet, and RoR-34+IMDB-WIKI achieves the best accuracy 93.24%, which outperforms 4c2f-CNN [30] by 6.44%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Exact Accuracy(%) SVM-dropout <ref type="bibr" target="#b14">[15]</ref> 79.3?0.0 4c2f-CNN <ref type="bibr" target="#b29">[30]</ref> 86 Then, we use 4c2f-CNN, VGG-16, Pre-ResNets, our RoR-34+SD by Pre-training on ImageNet and Pre-RoR-58+SD (Type A+B) architectures with the two mechanisms to estimate age. Furthermore, we use IMDB-WIKI-101 dataset to finetune the ResNets-34 and RoR-34, and then with the two mechanisms for further age estimation on Adience. <ref type="table" target="#tab_4">Table XI</ref> compares the state-of-the-art methods for age group classification on Adience data set. We find that the accuracy increases with the large-scale face image dataset fine-tuning the network, and two mechanisms will further improve each architecture, which demonstrates the versatility of two mechanisms in different models. <ref type="figure" target="#fig_7">Fig. 8</ref> shows the test errors by Pre-ROR-58+SD and Pre-ROR-58+SD with two mechanisms at different training epochs with folder0 validation. In addition, we notice that the effect of RoR-34+IMDB-WIKI with two mechanisms is a little better than RoR-34+IMDB-WIKI without two mechanisms. We argue that this is because of well-trained model by IMDB-WIKI. As shown in <ref type="table" target="#tab_4">Table XI</ref>, without using ImageNet and IMDB-WIKI101 datasets, the accuracy of Pre-ROR-58+SD with two mechanisms is better than 64.0?4.2% of DEX which pretrained on ImageNet and IMDB-WIKI (523,051 face images) <ref type="bibr" target="#b0">[1]</ref>. Although DEX can achieve competitive results, it needs very large data set IMDB-WIKI for pre-training. Our method can learn age and gender representation from scratch without the IMDB-WIKI and achieve the best performance. Our VGG-16 with two mechanisms also outperforms DEX (also based on VGG-16) which only pre-trained on ImageNet but without IMDB-WIKI. These results demonstrate that our method can improve the optimization ability of networks and alleviate over-fitting on Adience data set. Moreover, by pretraining on ImageNet RoR-34+SD with two mechanisms also achieves 63.76?4.18% of accuracy, which is very close to the accuracy in <ref type="bibr" target="#b0">[1]</ref>, so we have reason to believe that better performance can be achieved by pre-training on more extra data sets. Particularly, our RoR-34+IMDB-WIKI with two mechanisms obtains a single-model accuracy of 66.91?2.51% , and the 1-off accuracy of 97.49?0.76% on Adience. But the singlemodel accuracy is slightly lower than the accuracy in <ref type="bibr" target="#b38">[39]</ref>. Because compared with VGG used in <ref type="bibr" target="#b38">[39]</ref> RoR-34 is small. So we use RoR-152+IMDB-WIKI to repeat the experiments, we get the new state-of-the-art performance (a single-model accuracy of 67.34?3.56%) to our best knowledge now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposes a new Residual networks of Residual networks (RoR) architecture for high-resolution facial images age and gender classification in the wild. Two modest mechanisms, pre-training by gender and training with weighted loss layer, are used to improve the performance of age estimation. Pre-training on ImageNet is used to alleviate over-fitting. Further fine-tuning on IMDB-WIKI-101 is for the purpose of learning the features of face images. By RoR or Pre-RoR with two mechanisms, we obtain new state-of-the-art performance on Adience data set for age group and gender classification in the wild. Through empirical studies, this work not only significantly advances the age group and gender classification performance, but also explores the application of RoR on large scale and high-resolution image classifications in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. Fig.1(a) is the overview of RoR architecture for age classification with weighted loss layer. The images from Adience data set represent some challenges of age and gender estimation from real-world, unconstrained images. RoR architecture is adopted for feature learning. In weighted loss layer, we use different loss weight instead of equal loss weight based on aging curve. The green circles stand for the original loss of every age group, and the red circles are denoted as different loss weight of every age group. Fig.1(b) is the pipeline of our framework. The RoR model is pre-trained on ImageNet firstly, and then it is fune-tuned on the IMDB-WIKI-101 data set for further learning the features of face images, finally, it is used to fine-tune on Adience data set for age and gender estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Pre-RoR architecture with basic residual blocks. Pre-RoR has three levels, and it is constructed by adding shortcuts level by level based on basic Pre-ResNets. Leftmost shortcut is root-level shortcut, the rest four orange shortcuts are middle-level shortcuts, the blue shortcuts are final-level shortcuts. BN-ReLU-conv order in residual blocks is adopted. The fully-connected layer maps to the final soft-max layer for age or gender. Each basic residual block includes a stack of two convolutional layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Pre-RoR architecture with bottleneck residual blocks. If k=4, this is constructed based on original bottlencek Pre-ResNets architecture. The difference between this structure and Pre-RoR architecture with basic blocks is that its bottleneck block includes a stack of three convolutional layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The low-quality images in IMDB-WIKI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of 4c2f-CNN and 4c2f-CNN with two mechanisms on folder0 of Adience.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Results on folder0 of Adience by Pre-ResNets-34, Pre-ResNets-34+SD and Pre-RoR-34+SD (Type A+B) during training, corresponding to results inTable III. The blue curve of Pre-ResNets-34 shows that the over-fitting is very obvious. The green curve of Pre-RoR-34+SD) and the red curve of Pre-RoR-34+SD (Type A+B) shows the effectiveness of SD for reducing over-fitting. Pre-RoR-34+SD (Type A+B) displays stronger optimization ability of RoR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Results on folder0 of Adience by Pre-RoR-58 and Pre-ROR-58+SD with two mechanisms during training. The red curve of Pre-ROR-58+SD with two mechanisms converges earlier and achieves higher accuracy than Pre-RoR-58.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>improved the previous year's results fusing general model and children model on LAP. Huo et al. [34] proposed a novel method called Deep Age Distribution Learning(DADL) to use the deep CNN model to predict the age distribution. Hou et al. [35] proposed a VGG-16-like model with Smooth Adaptive Activation Functions (SAAF) to predict age group on Adience benchmark. Then he used the exact squared Earth Movers Distance(EMD2)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Pre-RoR with Basic Blocks</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>3?3,max pool</cell><cell></cell></row><row><cell></cell><cell>RB,64</cell><cell></cell></row><row><cell>1?1,conv,64</cell><cell>??</cell><cell></cell></row><row><cell></cell><cell></cell><cell>x x x</cell></row><row><cell></cell><cell>RB,64</cell><cell>64-d</cell><cell>BN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ReLU</cell></row><row><cell>1?1,conv,512,/8</cell><cell>RB,128</cell><cell cols="2">ReLU BN 3?3,conv,128,/2 1?1,conv,128,/2</cell></row><row><cell>1?1,conv, 128,/2</cell><cell>??</cell><cell cols="2">3?3,conv,128</cell></row><row><cell></cell><cell>RB,128</cell><cell></cell></row><row><cell></cell><cell>RB,256</cell><cell cols="2">Basic Residual Block</cell></row><row><cell>1?1,conv,256,/2</cell><cell>??</cell><cell></cell></row><row><cell></cell><cell>RB,256</cell><cell></cell></row><row><cell></cell><cell>RB,512</cell><cell></cell></row><row><cell>1?1,conv,512,/2</cell><cell>??</cell><cell></cell></row><row><cell></cell><cell>RB,512</cell><cell></cell></row><row><cell></cell><cell>BN</cell><cell></cell></row><row><cell></cell><cell>ReLU</cell><cell></cell></row><row><cell></cell><cell>Avg pool</cell><cell></cell></row><row><cell></cell><cell>fc</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Pre-RoR with Bottleneck Blocks</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>image</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">7?7,conv,64</cell><cell></cell></row><row><cell cols="3">3?3,max pool</cell><cell></cell></row><row><cell></cell><cell cols="2">16 RB,64?k</cell><cell></cell></row><row><cell>1?1,conv, 64?k</cell><cell>?? ??</cell><cell>L 1</cell><cell></cell></row><row><cell></cell><cell cols="2">16 RB,64?k</cell><cell>64?k-d</cell><cell>x x x</cell><cell>ReLU BN</cell></row><row><cell>1?1,conv, 512?k,/8</cell><cell cols="2">RB,128?k</cell><cell cols="2">ReLU BN 3?3,conv,128,/2 1?1,conv,128?k,/2 1?1,conv,128,/2</cell></row><row><cell>1?1,conv, 128?k,/2</cell><cell>??</cell><cell>L 2</cell><cell cols="2">3?3,conv,128 3?3,conv,128</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>BN</cell></row><row><cell></cell><cell cols="2">RB,128?k</cell><cell></cell><cell>ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">1?1,conv,128?k</cell></row><row><cell></cell><cell cols="2">RB,256?k</cell><cell></cell></row><row><cell>1?1,conv, 256?k,/2</cell><cell>??</cell><cell>L 3</cell><cell cols="2">Bottleneck Residual Block</cell></row><row><cell></cell><cell cols="2">RB,256?k</cell><cell></cell></row><row><cell></cell><cell cols="2">RB,512?k</cell><cell></cell></row><row><cell>1?1,conv, 512?k,/2</cell><cell>??</cell><cell>L 4</cell><cell></cell></row><row><cell></cell><cell cols="2">RB,512?k</cell><cell></cell></row><row><cell></cell><cell></cell><cell>L</cell><cell></cell></row><row><cell></cell><cell>BN</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ReLU</cell><cell></cell></row><row><cell></cell><cell>Avg pool</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">8-d/2-d fc</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.Fig. 4. The aging curve by binary classifiers. The curve expresses the aging rate. The lower the numerical value is, the more difficult it is to distinguish age group.</figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>95</cell><cell cols="2">93.74% 93.98%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy(%)</cell><cell>85 90</cell><cell></cell><cell></cell><cell>91.39%</cell><cell>87.09%</cell><cell></cell><cell>88.03%</cell><cell>92.97%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.31%</cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>75</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>k</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I FOUR</head><label>I</label><figDesc>DIFFERENT LOSS WEIGHT DISTRIBUTIONS.</figDesc><table><row><cell>Name</cell><cell>Loss Weight Distribution</cell></row><row><cell>LW0</cell><cell>(1,1,1,1,1,1,1,1)</cell></row><row><cell>LW1</cell><cell>(1,1,1,0.9,0.8,0.8, 0.9,1)</cell></row><row><cell>LW2</cell><cell>(1,1,1,1.1,1.2,1.2,1.1,1)</cell></row><row><cell>LW3</cell><cell>(1,1,1,1.3,1.5,1.5,1.3,1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II THE</head><label>II</label><figDesc>NUMBER OF RESIDUAL BLOCKS.</figDesc><table><row><cell>Block Type</cell><cell>Number of</cell><cell>Number of blocks in each</cell></row><row><cell></cell><cell>Layers</cell><cell>Group</cell></row><row><cell>Basic Block</cell><cell>34</cell><cell>3, 4, 6, 3</cell></row><row><cell>Basic Block</cell><cell>58</cell><cell>5, 6, 12, 5</cell></row><row><cell>Basic Block</cell><cell>82</cell><cell>7, 8, 14, 7</cell></row><row><cell>Bottleneck Block</cell><cell>50</cell><cell>3, 4, 6, 3</cell></row><row><cell>Bottleneck Block</cell><cell>101</cell><cell>3, 4, 23, 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III AGE</head><label>III</label><figDesc>AND GENDER CLASSIFICATION RESULTS ON ADIENCE BENCHMARK WITH BASIC BLOCK ARCHITECTURE.</figDesc><table><row><cell>Method</cell><cell>Age Exact</cell><cell>Age</cell><cell>Gender Ac-</cell></row><row><cell></cell><cell>Accu-</cell><cell>1-off(%)</cell><cell>curacy(%)</cell></row><row><cell></cell><cell>racy(%)</cell><cell></cell><cell></cell></row><row><cell>Pre-ResNets-34 (Type B)</cell><cell>58.81</cell><cell>88.31</cell><cell>90.23</cell></row><row><cell>Pre-ResNets-34+SD</cell><cell>59.56</cell><cell>90.43</cell><cell>89.91</cell></row><row><cell>(Type B)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre-RoR-34+SD (Type</cell><cell>60.21</cell><cell>91.14</cell><cell>90.72</cell></row><row><cell>B)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre-RoR-</cell><cell>59.87</cell><cell>88.68</cell><cell>90.32</cell></row><row><cell>34+SD+dropout (Type</cell><cell></cell><cell></cell><cell></cell></row><row><cell>B)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre-RoR-34+SD (Type</cell><cell>61.56</cell><cell>91.59</cell><cell>90.78</cell></row><row><cell>A+B)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre-RoR-34+SD (Type</cell><cell>61.52</cell><cell>91.56</cell><cell>90.84</cell></row><row><cell>A+B) 300 epochs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre-RoR-58+SD (Type</cell><cell>62.48</cell><cell>92.31</cell><cell>90.85</cell></row><row><cell>A+B)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre-RoR-82+SD (Type</cell><cell>61.78</cell><cell>92.15</cell><cell>90.87</cell></row><row><cell>A+B)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV AGE</head><label>IV</label><figDesc>AND GENDER CLASSIFICATION RESULTS ON ADIENCE BENCHMARK WITH 50-LAYER BOTTLENECK BLOCK ARCHITECTURE.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Age Exact</cell><cell>Age</cell><cell>Gender</cell></row><row><cell></cell><cell></cell><cell>Acc(%)</cell><cell>1-off(%)</cell><cell>Acc(%)</cell></row><row><cell>Pre-ResNets-50+SD</cell><cell></cell><cell>60.05</cell><cell>88.98</cell><cell>89.82</cell></row><row><cell>(Type B) k=4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pre-RoR-50+SD (Type</cell><cell>58.62</cell><cell>90.10</cell><cell>88.71</cell></row><row><cell>A+B) k=4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pre-RoR-50+SD (Type</cell><cell>61.68</cell><cell>91.63</cell><cell>88.92</cell></row><row><cell>A+B) k=2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pre-RoR-50+SD (Type</cell><cell>61.12</cell><cell>91.14</cell><cell>90.03</cell></row><row><cell>A+B) k=1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell></row><row><cell cols="5">AGE AND GENDER CLASSIFICATION RESULTS ON ADIENCE BENCHMARK</cell></row><row><cell cols="5">WITH 101-LAYER BOTTLENECK BLOCK ARCHITECTURE.</cell></row><row><cell>Method</cell><cell></cell><cell>Age Exact</cell><cell>Age</cell><cell>Gender</cell></row><row><cell></cell><cell></cell><cell>Acc(%)</cell><cell>1-off(%)</cell><cell>Acc(%)</cell></row><row><cell>Pre-ResNets-101+SD</cell><cell></cell><cell>59.16</cell><cell>89.61</cell><cell>89.12</cell></row><row><cell>(Type B) k=4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre-RoR-101+SD</cell><cell>(Type</cell><cell>60.46</cell><cell>90.95</cell><cell>88.37</cell></row><row><cell>A+B) k=4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre-RoR-101+SD</cell><cell>(Type</cell><cell>62.26</cell><cell>91.54</cell><cell>89.15</cell></row><row><cell>A+B) k=2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre-RoR-101+SD</cell><cell>(Type</cell><cell>60.49</cell><cell>91.14</cell><cell>89.41</cell></row><row><cell>A+B) k=1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI THE</head><label>VI</label><figDesc>AGE CROSS-VALIDATION RESULTS OF PRE-ROR WITH DIFFERENT BLOCK TYPES AND DEPTHS.</figDesc><table><row><cell>Method</cell><cell>Exact Acc(%)</cell><cell>1-off(%)</cell></row><row><cell>4c2f-CNN</cell><cell>52.62?4.37</cell><cell>88.61?2.27</cell></row><row><cell>VGG-16</cell><cell>54.64?4.76</cell><cell>54.64?4.76</cell></row><row><cell>Pre-ResNets-34</cell><cell>60.15?3.99</cell><cell>90.90?1.67</cell></row><row><cell>Pre-ResNets-34+SD</cell><cell>60.98?4.21</cell><cell>91.87?1.73</cell></row><row><cell>Pre-RoR-50+SD k=2</cell><cell>61.31?4.29</cell><cell>93.45?1.34</cell></row><row><cell>Pre-RoR-50+SD k=1</cell><cell>61.00?4.15</cell><cell>93.19?1.67</cell></row><row><cell>Pre-RoR-101+SD k=2</cell><cell>61.54?4.97</cell><cell>93.37?1.72</cell></row><row><cell>Pre-RoR-101+SD k=1</cell><cell>61.25?4.54</cell><cell>93.52?1.59</cell></row><row><cell>Pre-RoR-34+SD</cell><cell>62.35?4.69</cell><cell>93.55?1.90</cell></row><row><cell>Pre-RoR-58+SD</cell><cell>62.50?4.33</cell><cell>93.63?1.90</cell></row><row><cell>Pre-RoR-82+SD</cell><cell>62.14?4.10</cell><cell>93.68?1.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII VALIDATION</head><label>VII</label><figDesc>ERROR (%, 10-CROP TESTING) ON IMAGENET BY RESNETS AND ROR WITH DIFFERENT DEPTHS</figDesc><table><row><cell>Method</cell><cell>Top-1 Error</cell><cell>Top-5 Error</cell></row><row><cell>ResNets-18 [52]</cell><cell>28.22</cell><cell>9.42</cell></row><row><cell>RoR-18</cell><cell>27.84</cell><cell>9.22</cell></row><row><cell>ResNets-34 [26]</cell><cell>24.52</cell><cell>7.46</cell></row><row><cell>ResNets-34 [52]</cell><cell>24.76</cell><cell>7.35</cell></row><row><cell>RoR-34</cell><cell>24.47</cell><cell>7.13</cell></row><row><cell>ResNets-101 [26]</cell><cell>21.75</cell><cell>6.05</cell></row><row><cell>ResNets-101 [52]</cell><cell>21.08</cell><cell>5.35</cell></row><row><cell>RoR-101</cell><cell>20.89</cell><cell>5.24</cell></row><row><cell>ResNets-152 [26]</cell><cell>21.43</cell><cell>5.71</cell></row><row><cell>ResNets-152 [52]</cell><cell>20.69</cell><cell>5.21</cell></row><row><cell>RoR-152</cell><cell>20.55</cell><cell>5.14</cell></row><row><cell cols="3">starts from 0.001 and is divided by a factor of 10 after</cell></row><row><cell cols="3">epoch 5. For data augmentation, we use scale and aspect ratio</cell></row><row><cell cols="3">augmentation [52]. Both Top-1 and Top-5 error rates with 10-</cell></row><row><cell cols="3">crop testing are evaluated. From Table VII, our implementation</cell></row><row><cell cols="3">of residual networks achieves the best performance compared</cell></row><row><cell cols="3">to ResNets methods for single model evaluation on validation</cell></row><row><cell cols="3">data set. These experiments verify the effectiveness of RoR</cell></row><row><cell>on ImageNet.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII AGE</head><label>VIII</label><figDesc>GROUP AND GENDER CLASSIFICATION RESULTS ON ADIENCE BENCHMARK WITH ROR-34 BY PRE-TRAINING ON IMAGENET</figDesc><table><row><cell>Method</cell><cell></cell><cell>Age Exact Acc(%)</cell><cell>Age 1-off(%)</cell><cell>Gender Acc(%)</cell></row><row><cell cols="2">ResNets-34</cell><cell>59.39?4.45</cell><cell>91.98?1.57</cell><cell>90.12?1.48</cell></row><row><cell cols="2">ResNets-34 by</cell><cell>61.15?4.53</cell><cell>92.90?1.98</cell><cell>91.18?1.53</cell></row><row><cell cols="2">Pre-training on</cell><cell></cell><cell></cell></row><row><cell>ImageNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNets-</cell><cell></cell><cell>61.47?5.17</cell><cell>93.39?1.95</cell><cell>91.98?1.49</cell></row><row><cell>34+SD</cell><cell>by</cell><cell></cell><cell></cell></row><row><cell cols="2">Pre-training on</cell><cell></cell><cell></cell></row><row><cell>ImageNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoR-34</cell><cell></cell><cell>60.29?4.25</cell><cell>92.44?1.45</cell><cell>91.07?1.64</cell></row><row><cell cols="2">RoR-34 by Pre-</cell><cell>61.73?4.31</cell><cell>92.97?1.55</cell><cell>91.96?1.53</cell></row><row><cell cols="2">training on Im-</cell><cell></cell><cell></cell></row><row><cell>ageNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">RoR-34+SD</cell><cell>62.34?4.53</cell><cell>93.64?1.47</cell><cell>92.43?1.51</cell></row><row><cell>by</cell><cell>Pre-</cell><cell></cell><cell></cell></row><row><cell>training</cell><cell>on</cell><cell></cell><cell></cell></row><row><cell>ImageNet</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IX AGE</head><label>IX</label><figDesc>GROUP AND GENDER CLASSIFICATION RESULTS ON ADIENCE BENCHMARK WITH ROR-34 BY FINE-TUNING ON IMDB-WIKI-101</figDesc><table><row><cell cols="3">Method</cell><cell></cell><cell cols="3">Age Exact Acc(%)</cell><cell cols="2">Age 1-off(%)</cell><cell cols="2">Gender Acc(%)</cell></row><row><cell cols="4">ResNets-34+</cell><cell></cell><cell>66.63?3.04</cell><cell></cell><cell cols="2">97.20?0.65</cell><cell cols="2">93.17?1.57</cell></row><row><cell cols="4">IMDB-WIKI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">RoR-34+</cell><cell></cell><cell></cell><cell>66.42?2.64</cell><cell></cell><cell cols="2">97.35?0.65</cell><cell cols="2">92.90?1.76</cell></row><row><cell cols="4">IMDB-WIKI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+SD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">RoR-34+</cell><cell></cell><cell></cell><cell>66.74?2.69</cell><cell></cell><cell cols="2">97.38?0.65</cell><cell cols="2">93.24?1.77</cell></row><row><cell cols="4">IMDB-WIKI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Exact Accuracy(%)</cell><cell>45 50 55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Pre?RoR?58+SD</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Pre?RoR?58+SD with two mechanisms</cell></row><row><cell></cell><cell>35</cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>120</cell><cell>140</cell><cell>160</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE X THE</head><label>X</label><figDesc>GENDER CROSS-VALIDATION RESULTS BY DIFFERENT METHODS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XI THE</head><label>XI</label><figDesc>AGE CROSS-VALIDATION RESULTS BY DIFFERENT METHODS.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell>Exact Acc(%)</cell><cell>1-off(%)</cell></row><row><cell cols="2">SVM-dropout [15]</cell><cell></cell><cell>45.1?2.6</cell><cell>79.5?1.4</cell></row><row><cell cols="2">4c2f-CNN [30]</cell><cell></cell><cell>50.7?5.1</cell><cell>84.7?2.2</cell></row><row><cell>Chained</cell><cell cols="2">gender-age</cell><cell>54.5</cell><cell>84.1</cell></row><row><cell>CNN [31]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">R-SAAFc2 [35]</cell><cell></cell><cell>53.5</cell><cell>87.9</cell></row><row><cell cols="3">DEX w/o IMDB-WIKI</cell><cell>55.6?6.1</cell><cell>89.7?1.8</cell></row><row><cell>pretrain [1]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DEX w/ IMDB-WIKI pre-</cell><cell>64.0?4.2</cell><cell>96.60?0.90</cell></row><row><cell>train [1]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">RES-EMD [36]</cell><cell></cell><cell>62.2</cell><cell>94.3</cell></row><row><cell>DAPP [38]</cell><cell></cell><cell></cell><cell>62.2</cell><cell>-</cell></row><row><cell cols="2">R-SAAFc2(IMDB-</cell><cell></cell><cell>67.3</cell><cell>97.0</cell></row><row><cell>WIKI) [39]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4c2f-CNN</cell><cell></cell><cell></cell><cell>52.62?4.37</cell><cell>88.61?2.27</cell></row><row><cell cols="3">4c2f-CNN with two mech-</cell><cell>53.96?3.80</cell><cell>90.04?1.54</cell></row><row><cell>anisms</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VGG-16</cell><cell></cell><cell></cell><cell>54.64?4.76</cell><cell>89.93?1.87</cell></row><row><cell cols="3">VGG-16 with two mecha-</cell><cell>56.11?5.05</cell><cell>90.66?2.14</cell></row><row><cell>nisms</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pre-ResNets-34</cell><cell></cell><cell>60.15?3.99</cell><cell>90.90?1.67</cell></row><row><cell cols="3">Pre-ResNets-34 with two</cell><cell>61.89?4.16</cell><cell>93.50?1.33</cell></row><row><cell>mechanisms</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pre-RoR-58+SD</cell><cell></cell><cell>62.50?4.33</cell><cell>93.63?1.90</cell></row><row><cell cols="3">Pre-RoR-58+SD with two</cell><cell>64.17?3.81</cell><cell>95.77?1.24</cell></row><row><cell>mechanisms</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoR-34+SD</cell><cell>by</cell><cell>Pre-</cell><cell>62.34?4.53</cell><cell>93.64?1.47</cell></row><row><cell cols="2">training on ImageNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoR-34+SD</cell><cell>by</cell><cell>Pre-</cell><cell>63.76?4.18</cell><cell>94.92?1.42</cell></row><row><cell cols="3">training on ImageNet with</cell><cell></cell><cell></cell></row><row><cell cols="2">two mechanisms</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">RoR-34+ IMDB-WIKI</cell><cell>66.74?2.69</cell><cell>97.38?0.65</cell></row><row><cell>RoR-34+</cell><cell cols="2">IMDB-WIKI</cell><cell>66.91?2.51</cell><cell>97.49?0.76</cell></row><row><cell cols="3">with two mechanisms</cell><cell></cell><cell></cell></row><row><cell cols="3">RoR-152+ IMDB-WIKI</cell><cell>67.34?3.56</cell><cell>97.51?0.67</cell></row><row><cell cols="3">with two mechanisms</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the editor and the anonymous reviewers for their careful reading and valuable remarks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian estimation of beta mixture models with variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2160" to="2173" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational bayesian matrix factorization for bounded support data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Teschendorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="876" to="889" />
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Age classification on consumer images with gabor feature and fuzzy lda method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Biometrics</title>
		<meeting>International Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting age information from local spatially flexible patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="737" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human age estimation using bioinspired features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human age estimation with regression on discriminative aging manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="584" />
			<date type="published" when="2008-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image-based human age estimation by manifold learning and locally adjusted robust regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1178" to="1188" />
			<date type="published" when="2008-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous dimensionality reduction and human age estimation via kernel partial least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint estimation of age, gender and ethnicity: CCA vs. PLS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comparing different classifiers for automatic age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chrisina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="621" to="628" />
			<date type="published" when="2004-01" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Morph: a longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding images of groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ChaLearn looking at people 2015 new competitions: Age estimation and cultural event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhenvshky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generic object detection with dense neural patterns and regional</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.4316</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. -Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.6382</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Age estimation by multi-scale convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeply-Learned feature for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="534" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop</title>
		<meeting>CVPR Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for age and gender classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ekmekji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Research report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AgeNet: deeply learned regressor and classifier for robust apparent age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshop</title>
		<meeting>ICCV Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Apparent age estimation from face images combining general and children-specialized deep learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop</title>
		<meeting>CVPR Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep age distribution learning for apparent age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop</title>
		<meeting>CVPR Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Neural networks with smooth adaptive activation functions for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saltz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06557</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Squared Earth Mover&apos;s Distance-based Loss for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05916</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Some like it hot-visual guidance for preference prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07867</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Directional age-primitive pattern (DAPP) for human age group recognition and age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shoyaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ryu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ConvNets with Smooth Adaptive Activation Functions for Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dual Path Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Residual networks of residual networks: multilevel residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Age classification from facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="762" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic age classification with LBP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nabiyev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Computer and Information Sciences</title>
		<meeting>International Symposium on Computer and Information Sciences</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning local features for age estimation on real-life faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM international workshop on Multimodal Pervasive Video Analysis</title>
		<meeting>ACM international workshop on Multimodal Pervasive Video Analysis</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Identity mapping in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09382</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhenvshky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Toronto, ON, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Comput. Sci., Univ. of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.Sc. thesis</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Workshop Deep Learning and Unsupervised feature learning</title>
		<meeting>NIPS Workshop Deep Learning and Unsupervised feature learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Training and investigating residual nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<imprint>
			<pubPlace>Facebook AI Research, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avilable</forename></persName>
		</author>
		<ptr target="http://torch.ch/blog/2016/02/04/resnets.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4295" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
