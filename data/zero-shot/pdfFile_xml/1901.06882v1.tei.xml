<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton-based Action Recognition of People Handling Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunoh</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Eng</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Yun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
							<email>jongyoul@etri.re.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
							<email>jychoi@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Eng</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Skeleton-based Action Recognition of People Handling Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In visual surveillance systems, it is necessary to recognize the behavior of people handling objects such as a phone, a cup, or a plastic bag. In this paper, to address this problem, we propose a new framework for recognizing object-related human actions by graph convolutional networks using human and object poses. In this framework, we construct skeletal graphs of reliable human poses by selectively sampling the informative frames in a video, which include human joints with high confidence scores obtained in pose estimation. The skeletal graphs generated from the sampled frames represent human poses related to the object position in both the spatial and temporal domains, and these graphs are used as inputs to the graph convolutional networks. Through experiments over an open benchmark and our own data sets, we verify the validity of our framework in that our method outperforms the state-of-the-art method for skeleton-based action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition has attracted extensive attention in recent years, due to its many potential applications in video surveillance, human-robot interaction, and so on. Recognizing actions, however, is difficult, not only because of the common challenges of the domain, such as viewpoint changes and occlusions, but also because of the ambiguities of some actions. Therefore, human action recognition is still one of the most challenging tasks for computer vision. To tackle these challenges in human action recognition, many approaches using various modalities like appearance, optical-flows, depth, and skeleton have been proposed. Among them, pose-based representation using the skeleton simplifies learning of the actions by extracting the relevant high-level features. Moreover, the pose-based representation suffers relatively little from the intra-class variances, as the action from actor to actor varies less than in other representations <ref type="bibr" target="#b40">[40]</ref>. The development of low-cost depth sensors such as Microsoft Kinect and pose estimation Actions Graph Object Human GCNs <ref type="figure">Figure 1</ref>. Action recognition via object-related human poses. The pose graph is constructed by using spatial human poses (black dots and lines), spatial object poses (red dots and lines), and temporal connections (blue lines). In spatial and temporal domains, the graph is used as the input to GCNs.</p><p>algorithms <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29]</ref> provides an easier way to obtain skeletal information. It also contributes to the success of data-driven methods of skeleton-based action recognition. The applications of skeleton-based action recognition in real-world scenarios are difficult due to two major problems. First, conventional studies <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b39">39]</ref> for skeleton-based action recognition usually use constrained datasets, assuming that skeletal data are provided. Visual information in real-world scenarios, however, is captured in an unconstrained environment where human skeletons are not given. Thus, body movement should be interpreted through pose estimation algorithms to obtain skeletal data. Pose estimation is a challenging task, however, due to the rarity of appearance or missing or overlapping body parts. Given that pose estimation results are not always accurate, it becomes crucial to develop approaches dealing with inaccurate poses. Second, most approaches focusing only on human skeletons ignore contextual information like objects and scenes. Most meaningful actions in the real world, though, involve more than one object or person <ref type="bibr" target="#b24">[24]</ref>. Therefore, previous methods cannot fully understand the realworld actions.</p><p>Motivated by the aforementioned problems, we develop a framework for object-related human action recognition based on graph convolutional networks (OHA-GCN) as depicted in <ref type="figure">Figure 1</ref>. To understand the object-related human actions, our method builds the graph structure of both human and object poses. Human poses are extracted using the pose estimation method <ref type="bibr" target="#b3">[3]</ref>, and object poses are obtained by the pose heatmap and background subtraction. Using the obtained human and object poses, we model: 1) the human pose graph, where the movement of human body is considered; 2) the object-related human pose graph, where the relationship between human and object poses is represented. Our framework employs the two graphs of human poses and object-related human poses, generated in both the spatial and temporal domains.</p><p>To make the graphs more structurally complete, we explore a strategy of selecting informative frames, which discards ambiguous frames. After dividing the entire video into segments of equal length, we make a sequence of informative frames by sampling one informative frame from each segment. Using the confidence scores calculated by the pose estimation algorithm, we can decide which human poses have a more complete structure than the others. Then, the two types of the graphs are constructed from the sequence of the informative frames for pose-based representation. Using the human pose graph and object-related human pose graph, OHA-GCN runs in the architecture of the two stream to boost action recognition. The GCNs in each stream of our framework apply convolution filters to each graph. The nodes and their neighbors in each graph are applied to the convolutional filters. In experiments, our framework is validated using an open benchmark and our own data sets that includes an illegal rubbish dumping (IRD) dataset, which compiles realistic data in unconstrained environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Graph convolutional neural networks</head><p>Recent advancements in deep neural networks have led to the development of graph convolutional networks (GCNs) to understand the form of graph structures <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b26">26]</ref>. GCNs generalize convolutional neural networks (CNNs) from low-dimensional grids of images to high-dimensional domains represented by arbitrarily structured graphs. These tasks are categorized into two main categories: spectral perspective and spatial perspective methods. Spectral perspective methods convert graph data into a spectrum and apply CNNs to the spectral domain <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b22">22]</ref>. Different from the spectral perspective methods, spatial perspective methods directly use graph convolutions to define parameterized filters <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b26">26]</ref>. The convolution operation in the spatial perspective resembles the convolution operation on images. We propose a model using GCNs for action recognition following the concepts of the spatial perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Action recognition</head><p>Video-based action recognition is a challenging task because the concepts of the actions are highly abstract and consider both spatial and temporal dimensions. Conventional deep-learning-based approaches for action recognition rely mainly on 3D CNNs and two-stream CNNs. The methods based on the 3D CNNs propose a model that directly applies 3D convolution filters to RGB video sequences <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b33">33]</ref>. 3D CNNs, however, have problems in training due to the explosion of parameters, and their performance is only marginally improved compared to the traditional method <ref type="bibr" target="#b35">[35]</ref>. Methods based on the two-stream architecture of CNNs are proposed to solve these problems <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36]</ref>. In these methods, two CNNs are applied to process appearance and motion (optical flows) independently. These methods outperform traditional methods using hand-crafted features <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b35">35]</ref>. But, the heavy computation requirements for optical flows become a main limitation of the methods. Our approach is based only on pose information that can be extracted from RGB images. The pose-based representation can reduce computation time and simplify training for the recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Skeleton-based action recognition</head><p>Some methods for action recognition are based on skeletal data because human poses are highly relevant to human action. The methods can be categorized mainly into those based on hand-crafted features and those based on deeplearning features. The former methods design several features for understanding human joint motions <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b37">37]</ref>. For example, the human skeleton is represented as a point in the Lie group <ref type="bibr" target="#b34">[34]</ref>, encompassing rotations and translations between body parts. With the recent success of deep learning, many methods for skeleton-based action recognition have been proposed. Deep-learning-based methods usually use well-established neural models like recurrent neural networks (RNNs) <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b44">44]</ref> or CNNs <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18]</ref>. RNNs use their internal memory to process sequences of input data, making them applicable to skeleton-based action recognition. Zhang et al. <ref type="bibr" target="#b44">[44]</ref> select geometric features based on distances between human joints and use an RNN-based model. CNN-based methods propose applying the CNN directly by considering the skeleton sequence as an image array. Qiuhong et al. <ref type="bibr" target="#b16">[16]</ref> transform the skeleton sequence into three clips and learn the spatio-temporal feature based on deep CNNs. Still, generalizing the CNNs and RNNs to work on graph structures is challenging because they cannot fully understand the graphs. Since human poses are represented by a graph structure physically, the methods based on the CNNs and RNNs are limited in their ability to use the poses. Recently, the GCN-based methods have been suggested to understand the human poses without loss of graph information <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b39">39]</ref>. Yan et al. <ref type="bibr" target="#b39">[39]</ref> propose a  ST-GCN that applies the GCNs for skeleton-based action recognition. This method can capture the motion information in dynamic skeleton sequences by constructing spatiotemporal graphs. However, ST-GCN does not consider how to cope with inaccurate poses and cannot understand contextual information. By contrast, our method can build the pose graphs with the contextual information using the reliable poses through the informative joint selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Object-related human action recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall scheme</head><p>The overall scheme of the proposed framework is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. Pose estimation and detection algorithm are applied to patches extracted from RGB video through a tracking algorithm. Then, we make a sequence of informative frames by selecting frames containing relatively accurate poses in the patches. Using a spatial-temporal graph of human poses and object-related human poses from the informative frames, the GCNs perform action recognition in the architecture of the two stream. We revisit the ST-GCN <ref type="bibr" target="#b39">[39]</ref> in Section 3.2. We then present how our method detects the objects handled by humans in Section 3.3 and give detailed descriptions of the object-related human action recognition (OHA-GCN) framework. Section 3.4 and Section 3.5 explain how to construct graphs of object-related human poses and apply the graphs on the GCNs, respectively. Finally, we present the strategy of selecting informative frames in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ST-GCN [39]</head><p>ST-GCN uses a spatial temporal graph to form hierarchical representation of the human skeleton. The spatial temporal graph consists of nodes and edges where each node corresponds to a joint of the human body, and each edge corresponds to the spatio-temporal connection of the node. In one single frame, they formulate a spatial graph which is represented by human body joints, and the spatial edge of the graph becomes natural connection between human body joints. Then, temporal edges are made by connecting the corresponding joints in consecutive frames.</p><p>The skeletal graph with N joints in one single frame and T frames of a video is represented by G = (V, E), where V = {v ti |t = 1, ..., T, i = 1, ..., N } is the node set. In terms of spatial domain, the graph convolution is written as</p><formula xml:id="formula_0">f out (v ti ) = vtj ?B(vti) 1 Z ti (v tj ) f in (v tj ) ? w(l ti (v tj )),<label>(1)</label></formula><p>where f in and f out are the input and output feature map, respectively. B(v ti ) is the neighbor set of a node v ti , where the 1-distance neighbors of the target node v ti are considered. w is the weight function which provides a weight vector to compute inner product with the input feature map. Because the number of the weight vectors is fixed, strategies to partition B(v ti ) into a fixed number of subsets have to be designed. l ti is a function mapping each node in neighborhood of v ti to its subset label. To prevent the different subsets from unbalancing the output, the cardinality of the subset Z ti (v tj ) is used as the normalizing term. In ST-GCN, many partitioning strategies are introduced, such as uni-labeling, distance partitioning, and spatial configuration. Among them, spatial configuration has achieved best performance, as it consider the gravity center of the human body. This strategy divide the neighboring nodes of target node into three subsets: 1) the target node itself; 2) centripetal subset: the nodes that are closer to the gravity center; 3) otherwise centrifugal subset. The implementation of the graph convolution <ref type="bibr" target="#b19">[19]</ref> is adopted for ST-GCN. A is the N ? N adjacency matrix where a graph structure is represented as matrix form by indicating connections between nodes. To implement ST-GCN with the spatial configuration partitioning, the Eq. <ref type="formula" target="#formula_0">( 1)</ref> is transformed into</p><formula xml:id="formula_1">f out = j ? ? 1 2 j A j ? ? 1 2 j f in W j ,<label>(2)</label></formula><p>where the input feature map f in is represented as a tensor (N, T, C) dimensions, C being the number of input channels. Then, the adjacency matrix is divided into three matrices: 1) A 0 , the self-connection of each node; 2) A 1 , the connections of the centripetal subset; 3) A 2 , the connections of the centrifugal subset. W j denotes the weight matrix, where weight vectors of multiple output channels are stacked. ? j whose element can be formulated as ? ii j = k (A ik j ) + ? is the diagonal node degree matrix of A j . ? is set to 0.001 to avoid empty rows in A j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detection of objects handled by humans</head><p>ST-GCN can capture dynamics of human poses by learning both the spatial and temporal patterns of skeletal data. ST-GCN has limitations, however, in understanding objectrelated human actions because it focuses on only the human poses. For example, the object-related actions, such as dumping, phoning, texting, and smoking require a specific object that acts as a main cue for recognition. Inspired by the issue, we create a new graph structure to form a hierarchical representation of the human and object poses. Before constructing the graphs, it is essential to detect the objects a human is handling.</p><p>An easy approach is to find objects that overlap with humans through the object detector, but this approach has several problems. First, objects in the background that are not related to humans, such as parked cars, are regarded as related objects. To solve this problem, we define a class of objects handled by humans and develop a new detector, but the shapes of the objects handled by humans vary widely. The objects handled by humans have a variety of shapes and are small compared to humans. Hence, conventional detectors cannot easily detect the objects handled by humans. Several methods have been presented to use detection networks to detect humans, objects, and their interaction <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b10">10]</ref>. Still, a pair consisting of a human and an object is required to train the model on each action, and the training operates independently for each frame, not for the whole video.</p><p>In this paper, we use a simple and efficient method to find an object handled by a human through the temporal property of a video. Since CCTV cameras are generally fixed and limited to the motions of Pan-Tilt-Zoom, it is possible to find a moving area effectively through background modeling and camera motion compensation <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43]</ref>. This moving area includes the areas of both the human and the object carried by the humans. Then, object area can then be detected by subtracting the human area from the moving area regardless of the shape of the object. Fortunately, a human area can be obtained by using Openpose, a 2D pose  The procedure for detecting the object handled by a human. A moving area is obtained by background subtraction with camera motion compensation and the human area is found by estimated joint and heatmap. Then, the object area is detected by subtracting the human area from the moving area. estimation algorithm providing the location of each joint. The algorithm provides not only the location but also the confidence scores of the joints and affinities between joints as a heatmap, which can then be used as a human area. <ref type="figure" target="#fig_3">Figure 3</ref> shows the procedure for detecting an object handled by a human. In order to detect the moving region, we use the ViBE algorithm <ref type="bibr" target="#b0">[1]</ref> which efficiently builds the background model by storing a set of pixel values from the past. Then, through the Openpose algorithm, we get each coordinate of the joint and the joint heatmaps. We aggregate the heatmap for all joints and all joint association into a human region map, which is subtracted from the moving area. The regions around each human hand are selected. Finally, we can construct a graph structure by connecting the center position of this region with the joints of the closest person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Graph for object-related human poses</head><p>We use our detection algorithm and Openpose to obtain the coordinates of the human joints and objects for creating a graph structure of the human and object poses. For human skeletal data, the pose estimation algorithm produces 25 joints in a single frame. We then formulate a human pose graph using all the human joints in a video sequence. In addition, the object position obtained in Section 3.3 is added to the human pose graph for modeling our object-related human pose graph. As a result, the object-related human pose graph consists of 26 nodes that represent human and object poses. We can model a spatial graph of the physical connection between joints in the human body and position of the object in a frame. The corresponding nodes in consecutive frames are then connected to formulate a temporal graph.</p><p>To avoid confusion, we use notation similar to that used in the ST-GCN. In our skeletal graph G = (V, E) for object-related human poses, the node set is denoted as V = {v ti |t = 1, ..., T, i = 1, ..., N } with N joints in one single frame. T is the number of informative frames sampled from the video. The details of the informative frames are explained in Section 3.6. The edge set is E = E S ? E T , which depicts two subsets for spatial and temporal connection. The subset for spatial connection is</p><formula xml:id="formula_2">E S = {e tij |v ti , v tj are connected, (i, j) ? O}, (3)</formula><p>where O is the set of the connections for natural human body and the connections between an object and the human body joints. The subset for temporal connection</p><formula xml:id="formula_3">E T = {e ti |v ti , v (t+1)i are connected}<label>(4)</label></formula><p>connects the same nodes which is temporally consecutive. An example of our spatial-temporal skeleton graph is shown in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>We consider how to connect the object with the human body. Many strategies of the connection are explored empirically, but we discuss three strategies which are representative of many cases in our view. First, we use the relationship between the whole joint and the object. This method, however, makes the graph-based network inefficient because the graph includes redundant information. Thus, we build a graph structure that is denoted as limbs, focusing on the specific body parts used for interactions. We also introduce a strategy using only human hands, where most object-related human actions occur. The three strategies are denoted as follows: 1) Body: connections between all the human joints and the object; 2) Limbs: connections between the human joints belonging to arms or legs and the object; 3) Hands: connections between the human joints of both hands and the object. Detailed experiments with each strategy are described in Section 4.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Two stream GCNs with human and object poses</head><p>Using only the human pose graph is limited to express only human body motion, ignoring human-object interactions. Based on the graph structure of both human and object-related human poses, we propose a framework of two-stream GCNs called OHA-GCN for object-related human action recognition. The OHA-GCN can boost the recognition for object-related actions, as the human poses and object-related human poses are jointly used. Humanrelated objects do not, however, appear in every scene. In this regard, the two-stream GCN is designed to use the object-related human pose stream only if human-related objects are found. In the absence of the objects, OHA-GCN uses only the human pose stream. If the object is properly detected, OHA-GCN recognizes actions through both The GCN in each stream takes the graph constructed in Section 3.4 as input and produces a video-level prediction for action recognition. The model of GCN is composed of 9 GCN layers. Each layer has 64, 64, 64, 128, 128, 128, 256, 256, and 256 channels, respectively. The global average pooling layer then follows and produces the feature vector. The final outputs are produced by Softmax for action prediction. Categorical cross-entropy loss is adopted for training of the GCN. We adopt evenly averaging Softmax scores for final predictions through empirical evaluation of different aggregation methods such as averaging and maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Informative frame selection strategy</head><p>The conventional skeleton-based action recognition methods assume that accurate human skeletons are provided by datasets. Real-world scenarios, however, do not provide skeletal data, and the estimated skeletal data might be inaccurate due to an unusual appearance or missing or overlapping body parts. Likewise, since our methods are based only on RGB sequences, the pose estimation results may be inaccurate. This issue motivates us to introduce a new sampling strategy that selects informative frames. Our informative frame selection strategy extracts a short sequence of sampled frames from a video sequence. The sampled frames preserve the joints enough to understand human poses with high confidence score from the Openpose al-gorithm. Based on confidence scores, we selectively discard frames containing ambiguous human poses. Then our method can select only those sequences with reliable human pose information.</p><p>Initially, the entire video is divided into T segments {S 1 , S 2 , ..., S T } at equal interval. We construct sequences of T frames by sampling one frame from each segment. Since consecutive frames have little variation and provide redundant pose information, the sampled sequences can reduce the redundancy. Each segment has M frames, so t-th segment S t has frames {F 1 t , F 2 t , ..., F M t }, where F m t is mth frame in the t-th segment.After applying the pose estimation algorithm to each frame F m t , we can obtain the confidence score c m ti of each human joint h m ti , where i = 1, ..., N and N is the number of joints. Then, we select the frame with the highest sum of confidence scores in the estimated joints within a segment because the higher the sum is, the more accurately the human joints are detected. Finally, the sequence of the sampled frames is used directly to construct the human pose graph on both the spatial and temporal domains. The node set of our skeletal graph can be written as</p><formula xml:id="formula_4">V = {v ti |v ti = h m * ti , m * = arg max m N i c m ti }. (5)</formula><p>To demonstrate that our frame selection algorithm performs better than the existing sampling strategy, we conduct an ablation study in Section 4.2.1. As a result, OHA-GCN has a relatively complete form of graph that includes reliable joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The existing datasets <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b38">38]</ref> are not suitable for use in evaluating our framework to recognize the object-related actions in real-world scenes for two reasons. First, the existing datasets for skeleton-based action recognition were captured in constrained environments. These datasets acquired from sensors are limited to indoor environments. Therefore, the constrained datasets are insufficient for modeling realistic scenes because these datasets cover only a few variations in dynamic environments. Second, these datasets usually include one actor in each image, which is not reflective of the scenes from real-world surveillance. In our evaluation, instead of using the existing datasets, we use two realistic datasets captured in unconstrained real-world environments, which contain only raw video clips without skeleton data. The two datasets are described below.</p><p>IRD dataset: As for our own dataset, we have constructed an illegal rubbish dumping (IRD) dataset that contains videos from CCTV cameras. The dataset is used for the purpose of the general surveillance, especially monitoring illegal dumping. The original videos are about 10 minutes long, and the resolution of the original video is 1280?720 pixels (HD). We made, however, several video clips for annotations from the original videos by extracting the region of the event. Each video clip, averaging 690 frames in length, is divided into two classes: 1) a garbage dumping action and 2) a normal event. Humans tagged the ground truth, such as the person, the object the person carries, the start time of the action of dumping trash, and the end time. Unlike other existing datasets for skeletonbased action recognition, this dataset is not limited to indoor environments and captured in unconstrained environments. There is a wide variation of conditions in dynamic environments, including viewpoint changes, illumination variance, and occlusions. Most of all, this dataset is suitable for evaluating our method for recognizing object-related human actions. We made a total of 1374 video clips and use 1193 of them for training and 181 for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICVL-4 dataset:</head><p>The ICVL dataset <ref type="bibr" target="#b15">[15]</ref> includes dynamic sub-actions of multiple people at multiple locations in the surveillance scenes. The actions are divided into 13 categories. The dataset contains 497 original videos, and one actor who is one of the people appearing in the video is marked by three sub-actions. For example, one person in the surveillance scene is standing, walking, and smoking. The original video is divided into several video clips containing a single actor. We use only these specific classes related to object-related actions, such as phoning, smoking, texting, and dumping, which will be called ICVL-4 in this paper. The reason we choose the specific classes is that we have to verify the effectiveness of our method in recognizing object-related human actions. The evaluation using the entire class is meaningless, as the proposed strategies are the only major differences between previous skeleton-based approach and OHA-GCN. Therefore, we only conduct experiments with only the data from 4 classes, focusing on the object-related actions. A training set of 805 video clips and a validation set of 86 video clips are used for our experiments.</p><p>Both the IRD and ICVL-4 datasets provide only RGB video clips and action labels. As our approach for action recognition is based on the skeleton, we use the pose estimation algorithm <ref type="bibr" target="#b3">[3]</ref> and our detection algorithm. Through the algorithms, 25 human joints are extracted, and the position of an object is obtained. There is a total of 26 nodes including both human joints and an object. We make each video clip as a (3, T, 26) tensor, where the first dimension of the tensor represents 3 channels for the 2D coordinates (x, y) and confidence scores. If more than two people exist in one video clip, we select the person with the highest sum of confidence scores as the actor for the recognition. T is the length of the frames, and 26 is the number of nodes here. <ref type="table">Table 1</ref>. Ablation study of the proposed framework over the IRD and ICVL-4 datasets. The baseline of our approach is ST-GCN <ref type="bibr" target="#b39">[39]</ref> which is the state of the art for the skeleton-based action recognition using GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>IRD (%) ICVL-4 (%) ST-GCN <ref type="bibr" target="#b39">[39]</ref>  We use the top-1 accuracy, which is a conventional evaluation method for both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental result 4.2.1 Ablation study</head><p>In the ablation study, we first investigate the role of the proposed strategy of selecting informative frames for skeleton-based action recognition. Second, we examine the strategy for how to connect the object to the human body in OHA-GCN in formulating object-related pose graphs. Finally, we verify the effectiveness of the object-related human pose in recognizing object-related actions. We will show improvement using the proposed components in our framework compared with the state-of-the-art method, ST-GCN <ref type="bibr" target="#b39">[39]</ref>, for skeleton-based action recognition.</p><p>Frame selection strategies. We compare the strategies of selecting frames on the IRD and ICVL-4 datasets: the proposed informative frame sampling and the existing uniform sampling. Informative samples are selected from the informative frame selection strategy, and uniform samples are obtained from the uniform sampling strategy used in ST-GCN <ref type="bibr" target="#b39">[39]</ref>. The uniform sampling selects T frames at the same interval from a video. The comparisons between uniform and informative samples are performed using the human pose (HP) stream and the object-related human pose with hands connection (OHP-hands) stream. Our results, as observed in <ref type="table">Table 1</ref>, show that the informative frame selection is a better strategy than uniform sampling. In particular, the informative frame selection in HP stream achieves a 5.5% (on IRD) and a 8% (on ICVL-4) improvement compared to ST-GCN using uniform samples. We argue that the improved performance comes from building more meaningful graphs by ignoring redundant information and focusing on accurate poses. Our methods use the informative frame selection strategy for the following experiments.</p><p>Graph construction strategies. Approaches to connecting the object to the human body are explored empirically. 'OHP-body' indicates connections between all the human joints and the object; 'OHP-limbs' indicates connections between the human joints belonging to arms or legs and the object; 'OHP-hands' denotes connections between the human joints of both hands and the object. 'OHP-body' shows lower performance than ST-GCN because the graph structure becomes complex and introduces redundant information. It means that the more abstracted form the objectrelated pose graph takes, the better its performance. As shown in <ref type="table">Table 1</ref>, 'OHP-hands' shows the best performance among all the strategies. We believe this result is reasonably good because object-related human actions usually occur in human hands. Consequently, we use the connections between human hands and an object in other experiments.</p><p>HP stream and OHP stream. We also compare the human pose stream with the object-related human pose stream in the <ref type="table">Table 1</ref>. The method using the object-related poses with the uniform sampling strategy (OHP-hands stream + uniform samples) does not improve performance compared to the human poses (HP stream) with the same strategy (ST-GCN; HP stream + uniform samples). While, The usage of object-related poses with the informative frame selection (OHP-hands stream + informative samples) achieves a 2.2% (on IRD) and a 1.2% (on ICVL-4) improvements compared to ST-GCN. The improvement is minimal, however, in the case of 'HP stream + informative samples'. This difference in the performances is the result of the fact that the frame selection strategy is more informative to the human pose graph than the object-related one. By combining the HP and OHP streams, the two-stream OHA-GCN achieves the best performance of a 80.11% and a 91.86% on the IRD and ICVL-4 datasets, respectively, which is a significant improvement over the state-of-the-art, ST-GCN for skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison with CNN methods using images</head><p>Our method is compared with other CNN-based methods, evaluated on the ICVL-4 datasets. Baseline CNNs are models of RGB modality, using only RGB images as input for the network. In fact, this is not a fair comparison because our approach is based on pose modality while CNN methods are based on RGB modality. Still, we want to show that even without the appearance information that is high dimensionality, our method can achieve a comparable performance. We establish baseline models whose architectures are LeNet <ref type="bibr" target="#b21">[21]</ref>, VGG16 <ref type="bibr" target="#b31">[31]</ref>, and ResNet50 <ref type="bibr" target="#b11">[11]</ref>, respectively. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the accuracy of our method is lower than some of CNN-based methods, as expected. This result is explained by the difference in the modalities. RGB modality has more information for actions while pose-based representation is a compressed representation of the human body. Still, we argue that our pose-based approach achieves not only fairly high performance but also faster speed than RGB-based models. To examine this argument, we perform speed comparisons with the RGB-based methods. The test is performed on GTX TITAN X GPU, and other details of our implemented environments are described in Section 4.3. Using the ICVL-4 datasets, we measure the average runtime that the networks take to process one frame. Our method can operate at around 3571 frames per second (FPS), much faster speed than the CNN-based methods. The LeNet works relatively quickly with light computation, but its accuracy is inferior to that of our method. The ResNet achieves better results than our methods, but cannot operate quickly due to heavy computation. Given these speed/accuracy trade-offs, our method has more advantages than other methods because it achieves the comparable results at high speed even without the RGB modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Applications</head><p>Not only do our methods show promising results, but they also open up the possibility of many applications, building a real-world surveillance system. As a number of people appear throughout the original video in both datasets, there are many actors and actions to be recognized. To prevent multiple people from obstructing an action of one actor in a video, we apply a tracking algorithm to extract the regions where a single person exists. For the tracking, we adopt the online tracking-by-detection method based on the Hungarian matching algorithm <ref type="bibr" target="#b42">[42]</ref> using pose information. If more than two people overlap in one patch obtained from tracking, we use the person whose joints have the highest sum of confidence scores as an actor for action recognition.</p><p>To examine the real-time performance of our proposed method in the surveillance systems, we analyze the runtime of the overall pipeline. Our method has been implemented with Pytorch deep learning framework, and the runtime is measured on GTX TITAN X GPU and Intel Core i7-6700K 4.0Ghz. For applications in real-world surveillance, we also use tracking algorithms whose runtime is 12.87 ms using only CPU. In our evaluation on the IRD dataset, foreground detection <ref type="bibr" target="#b3">[3]</ref> and object detection operate at up to 45.78 ms and 14.93 ms using only CPU, respectively. The frame size in the original videos is 1920?1080, but we resize it to 656?368 which is the size recommended by OpenP ose <ref type="bibr" target="#b3">[3]</ref>. The pose estimation algorithm takes 74.62 ms using one GPU, 38.02 ms using two GPUs, and 20.92 ms using three GPUs. The overall pipeline, including the runtime of OHA-GCN, takes around 94.78 ms (10.6 FPS) on average, which can be applied to real-time monitoring. The experimental results and runtime analysis imply that our approach can be successfully implemented in realworld scenarios, achieving real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Our main contributions are summarized as follows. First, we propose a new framework, object-related human action recognition (OHA-GCN), which uses the graphs of human poses and the object-related human poses to understand object-related human actions. Second, to overcome the difficulties of skeleton-based action recognition in real-world scenarios, we explore good strategies, including informative frame selection and construction of the object-related human poses. Third, we introduce a new dataset of illegal rubbish dumping (IRD), which compiles realistic data in unconstrained environments. Fourth, our framework can run in real time while achieving significantly better performance than the state-of-the-art algorithm for skeleton-based action recognition. It is noted that the proposed strategies offer meaningful insights into how to incorporate contextual information into human-object interaction recognition in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall scheme of the proposed framework that includes pipeline modules of tracking and detection for extracting informative human and object poses, pose estimation, graph construction of human and object-related human poses, and GCNs for recognition of object-related human actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The procedure for detecting the object handled by a human. A moving area is obtained by background subtraction with camera motion compensation and the human area is found by estimated joint and heatmap. Then, the object area is detected by subtracting the human area from the moving area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>For the construction of our skeletal graph, all of the dots are used as nodes and all of the lines as edges. In each frame, nodes and edges based on the natural human body are denoted as green. Left: The graph structure of human poses in terms of both the spatial and temporal domains. Blue lines represent the temporal connections of corresponding nodes in consecutive frames. Right: The example of the graph structure for object-related human poses in each frame. A red dot denotes the position of a human-related object. Red lines are the connections between all human body joints and the object. the human pose stream and the object-related human pose stream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Accuracy and speed comparisons with the CNN-based methods using RGB image inputs on the ICVL-4 dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Acc.(%) Speed (ms)</cell></row><row><cell>CNN (LeNet [21])</cell><cell>87.21</cell><cell>3.66</cell></row><row><cell>CNN (VGG16 [31])</cell><cell>95.35</cell><cell>24.56</cell></row><row><cell>CNN (ResNet50 [11])</cell><cell>96.51</cell><cell>17.26</cell></row><row><cell>ST-GCN [39]</cell><cell>80.23</cell><cell>0.1321</cell></row><row><cell>OHA-GCN (HP stream)</cell><cell>88.37</cell><cell>0.1306</cell></row><row><cell>OHA-GCN (OHP stream)</cell><cell>81.40</cell><cell>0.1319</cell></row><row><cell>OHA-GCN (Two Stream)</cell><cell>91.86</cell><cell>0.2447</cell></row><row><cell cols="3">HP: Human Pose, OHP: Object-related Human Pose</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ViBe: A Universal Background Subtraction Algorithm for Video Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1724" />
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting and Recognizing Human-Object Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relation Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-Time Action Detection in Video Surveillance using a Sub-Action Descriptor with Multi-Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Institute of Control, Robotics and Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="308" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Detection of moving objects with a moving camera using nonpanoramic background model. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-10" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1015" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A survey of advances in vision-based human motion capture and analysis. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kr?ger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="90" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph based skeleton motion representation and similarity measurement for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coupled action recognition and pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="37" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detection of Moving Objects with Non-stationary Cameras in 5.8ms: Bringing Motion Detection to Your Mobile Device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Online Scheme for Multiple Camera Multiple Target Tracking Based on Multiple Hypothesis Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="454" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scene conditional background update for moving object detection in a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="57" to="63" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On geometric features for skeleton-based action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
