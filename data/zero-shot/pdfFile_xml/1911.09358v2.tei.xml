<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE, Xiang Bai Senior Member, IEEE</roleName><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
						</author>
						<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. XX, NO. XX, SEPTEMBER 2019 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Object detection</term>
					<term>R-CNN</term>
					<term>multi-oriented object</term>
					<term>aerial image</term>
					<term>scene text</term>
					<term>pedestrian detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection has recently experienced substantial progress. Yet, the widely adopted horizontal bounding box representation is not appropriate for ubiquitous oriented objects such as objects in aerial images and scene texts. In this paper, we propose a simple yet effective framework to detect multi-oriented objects. Instead of directly regressing the four vertices, we glide the vertex of the horizontal bounding box on each corresponding side to accurately describe a multi-oriented object. Specifically, We regress four length ratios characterizing the relative gliding offset on each corresponding side. This may facilitate the offset learning and avoid the confusion issue of sequential label points for oriented objects. To further remedy the confusion issue for nearly horizontal objects, we also introduce an obliquity factor based on area ratio between the object and its horizontal bounding box, guiding the selection of horizontal or oriented detection for each object. We add these five extra target variables to the regression head of faster R-CNN, which requires ignorable extra computation time. Extensive experimental results demonstrate that without bells and whistles, the proposed method achieves superior performances on multiple multi-oriented object detection benchmarks including object detection in aerial images, scene text detection, pedestrian detection in fisheye images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O BJECT detection has achieved a considerable progress thanks to convolutional neural networks (CNNs). The state-of-the-art methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> usually aim to detect objects via regressing horizontal bounding boxes. Yet multioriented objects are ubiquitous in many scenarios. Examples are objects in aerial images and scene texts. Horizontal bounding box does not provide accurate orientation and scale information, which poses problem in real applications such as object change detection in aerial images and recognition of sequential characters for multi-oriented scene texts.</p><p>Recent advances in multi-oriented object detection are mainly driven by adaption of classical object detection methods using rotated bounding boxes <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> or quadrangles <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> to represent multi-oriented objects. Though these existing adaptions of horizontal object detection methods to multi-oriented object detection have achieved promising results, they still face some limitations. For detection using rotated bounding boxes, the accuracy of angle prediction is critical. A minor angle deviation leads to important IoU drop, resulting in inaccurate object detection. This problem is more prominent for detecting long oriented objects such as bridges and harbors in aerial images and Chinese text lines in scene images. The methods based on quadrangle regression usually have ambiguity in defining the ground-truth order of four vertices, yielding unexpected detection results for objects of some orientations. Some other methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>   <ref type="figure">Fig. 1</ref>. Pipeline of the proposed method. An image is fed into a CNN, which outputs a classification score (blue value), a horizontal bounding box, four length ratios between each segment s i and corresponding side, and an obliquity factor (green value) for each detection. Based on obliquity factor, we select horizontal box (in purple) or oriented detection (in orange) as the final result. Best viewed in electronic version.</p><p>horizontal object parts followed by a grouping process. Yet, such grouping process step is usually heuristic and timeconsuming. Describing an oriented object as its segmentation mask <ref type="bibr" target="#b11">[12]</ref> is another alternative solution. However, this often results in split and/or merged components, requiring a heavy and time-consuming post-processing. In this paper, we propose a simple yet effective framework to deal with multi-oriented object detection. Specifically, we propose to glide each vertex of the horizontal bounding box on the corresponding side to accurately describe a multi-oriented object. This results in a novel arXiv:1911.09358v2 [cs.CV] 8 Apr 2020 representation by adding four gliding offset variables to classical horizontal bounding box representation. Put it simply, we regress four length ratios that characterize the relative gliding offset (see <ref type="figure">Fig. 1</ref>) on each side of horizontal bounding box. Such representation may be less sensitive to offset prediction error than angle prediction error in rotated bounding box representation. By limiting the offset on the corresponding side of horizontal bounding box, we may facilitate offset learning and also avoid the confusion for sequential label points in directly regressing the four vertices of oriented objects. To further get rid of confusion issue for nearly horizontal objects, we also introduce an obliquity factor based on area ratio between the multi-oriented object and its horizontal bounding box. As depicted in <ref type="figure">Fig. 1</ref>, this obliquity factor guides us to select the horizontal detection for nearly horizontal objects and oriented detection for oriented objects. It is noteworthy that the proposed method only introduces five additional target variables, requiring ignorable extra computation time.</p><p>In summary, the main contribution of this paper are three folds: 1) We introduce a simple yet effective representation for oriented objects, which is rather robust to offset prediction error and does not have the confusion issue. 2) We propose an obliquity factor that effectively guides the selection of horizontal detection for nearly horizontal objects and oriented detection for others, remedying the confusion issue for nearly horizontal objects. 3) Without bells and whistles (e.g., cascade refinement or attention mechanism), the proposed method outperforms some state-of-the-art methods on multiple multi-oriented object detection benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep general object detection</head><p>Object detection aims to detect general objects in images with horizontal bounding boxes. Recent mainstream CNNbased methods can be roughly summarized into top-down and bottom-up methods. Top-down methods directly detect entire objects. They can be further categorized into two classes: two-stage and single-stage methods. R-CNN and its variances <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> are representative twostage methods. They first generate object proposals and then use the features of these proposals to predict object categories and refine the bounding boxes. YOLO and its variances <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, SSD <ref type="bibr" target="#b17">[18]</ref>, and RetinaNet <ref type="bibr" target="#b18">[19]</ref> are representative single-stage methods. They predict bounding boxes directly from deep feature maps instead of region proposals. Bottom-up methods rise recently by predicting object parts followed by a grouping process. CornerNet <ref type="bibr" target="#b19">[20]</ref>, ExtremeNet <ref type="bibr" target="#b20">[21]</ref>, and CenterNet <ref type="bibr" target="#b21">[22]</ref> are recently proposed in succession. They attempt to predict some keypoints of objects such as corners or extreme points, which are then grouped into bounding boxes. Center points are also used by <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> as supplemental information for grouping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-oriented object detection</head><p>Object detection in aerial images is chanllenging because of huge scale variations and arbitrary orientations. Extensive studies have been devoted to this task. The baselines on the popular dataset DOTA <ref type="bibr" target="#b22">[23]</ref> replace horizontal box regression of faster R-CNN with regression of four vertices of quadrangle representation. Many methods resort to rotated bounding box representation. Rotated RPN is exploited in <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, which involves more anchors and thus requires more runtime. Ding et al. <ref type="bibr" target="#b4">[5]</ref> propose an RoI transformer that transforms horizontal proposals to rotated ones, on which the rotated bounding box regression is performed. Azimi et al. <ref type="bibr" target="#b25">[26]</ref> adopt an image-cascade network to extract multiscale features. Yang et al. <ref type="bibr" target="#b26">[27]</ref> employ multi-dimensional attention to extract robust features, better coping with complex backgrounds. Zhang et al. <ref type="bibr" target="#b27">[28]</ref> propose to learn global and local contexts together to enhance the features.</p><p>Oriented scene text detection is a challenging problem due to arbitrary orientations. The mainstream CNN-based detectors can be roughly divided into regression-based and segmentation-based <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b28">[29]</ref> methods. We focus on regression-based methods. Most methods directly predict entire texts using rotated bounding box or quadrangle representation. Ma et al. <ref type="bibr" target="#b29">[30]</ref> employ rotated RPN in the framework of faster R-CNN <ref type="bibr" target="#b0">[1]</ref> to generate rotated proposals and further perform rotated bounding box regression. Liu et al. <ref type="bibr" target="#b30">[31]</ref> propose to use quadrangle sliding windows to match texts with perspective transformation. TextBoxes++ <ref type="bibr" target="#b5">[6]</ref> adopts vertex regression on SSD <ref type="bibr" target="#b17">[18]</ref>. RRD <ref type="bibr" target="#b31">[32]</ref> further improves TextBoxes++ <ref type="bibr" target="#b5">[6]</ref> by decoupling classification and bounding box regression on rotation-invariant and rotationsensitive features, respectively, making the regression more accurate for long texts. Both EAST <ref type="bibr" target="#b3">[4]</ref> and Deep direct regression <ref type="bibr" target="#b6">[7]</ref> perform rotated bounding box regression and/or vertex regression at each location.</p><p>Pedestrian detection in fisheye images is different from general pedestrian detection because pedestrians in fisheye images are often multi-oriented. Seidel et al. <ref type="bibr" target="#b32">[33]</ref> propose to transform omnidirectional images into perspective ones, on which the detection is applied. Such transformation introduces extra computation time. Based on the prior knowledge that objects in fisheye images are radial, Tamura et al. <ref type="bibr" target="#b33">[34]</ref> propose to train a general object detector with rotated images and then determine the orientations based on the relative positions of object centers w.r.t. the image center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparison with related works</head><p>Compared with the related works, the proposed method targets on general and ubiquitous multi-oriented object detection with a simple yet effective framework. By gliding the vertex of horizontal bounding box on each corresponding side and a novel divide-and-conquer selection scheme for nearly horizontal and oriented objects, the proposed method may better learn the offset for accurate multi-oriented object detection and does not suffer from confusion issue. Furthermore, the proposed method may be complementary and easily plugged into many existing methods focusing on enhancing features. To equip them with the proposed approach, we only need to replace rotated bounding box or vertex regression by regressing the four length ratios and obliquity factor in addition to horizontal bounding box. Such modification requires ignorable extra runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>CNN-based object detectors perform well on detecting horizontal objects but struggle on oriented ones, in particular </p><formula xml:id="formula_0">! " ! " # ! $ # ! $ ! % # ! &amp; # ! % ! &amp; ' " ' $ ' % ' &amp; (, * ? , - . / (, *, ,, ?, 0 " = ' " , , 0 $ = ' $ ? , 0 % = ' % , , 0 &amp; = ' &amp; ?</formula><formula xml:id="formula_1">box B h = (v 1 , v 2 , v 3 , v 4 ) = (x, y, w, h). We adopt (x, y, w, h, ? 1 , ? 2 , ? 3 , ? 4 ) to represent oriented objects.</formula><p>for long and dense oriented objects. Direct adaption using rotated bounding box B r regression tends to produce inaccurate results due to high sensitivity to angle prediction error. Regressing the four vertices of quadrangle representation does not suffer from this problem, but also fails on some cases because of the ambiguity in defining the order of four ground truth vertices to be regressed. We attempt to solve the general multi-oriented object detection by introducing a simple representation for oriented objects and a novel detection scheme that divides and conquers nearly horizontal and oriented object detection, respectively. Specifically, we propose to glide the vertex of horizontal bounding box B h on each corresponding side to accurately describe an oriented object. Put it simply, in addition to B h , we compute four length ratios that characterize the relative gliding offset on each side of B h . Besides, We also introduce an obliquity factor based on area ratio between multi-oriented object and its horizontal bounding box B h . Based on the estimated obliquity factor, we select the horizontal (resp. oriented) detection for a nearly horizontal (resp. oriented) object.</p><p>This simple yet effective framework only introduces five target variables compared with classical horizontal object detectors, requiring ignorable extra computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Oriented object representation</head><p>The proposed method relies on a simple representation for oriented objects and an effective selection scheme. An intuitive illustration of the proposed representation is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. For a given oriented object O (blue box in <ref type="figure" target="#fig_0">Fig. 2</ref>) and its corresponding horizontal bounding box B h (black box in <ref type="figure" target="#fig_0">Fig. 2</ref> </p><formula xml:id="formula_2">), let v i , i ? {1, 2, 3, 4} denote top,</formula><formula xml:id="formula_3">? {1,3} = s {1,3} /w, ? {2,4} = s {2,4} /h,<label>(1)</label></formula><p>where s i = v i ? v i denotes the distance between v i and v i , i.e., the length of segment s i = (v i , v i ) representing the gliding offset from v i to v i . It is noteworthy that all ? i is set to 1 for horizontal objects.  <ref type="figure">Fig. 3</ref>. Network architecture. We simply add five extra target variables (normalized to <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref> using the sigmoid funciton) to the head of faster R-CNN <ref type="bibr" target="#b0">[1]</ref>. K: number of classes; k: a certain class.</p><p>In addition to the simple representation in terms of (x, y, w, h, ? 1 , ? 2 , ? 3 , ? 4 ) for an oriented object O, we also introduce an obliquity factor characterizing the tilt degree of O. This is given by the area ratio r between O and B h :</p><formula xml:id="formula_4">r = |O| / |B h |,<label>(2)</label></formula><p>where | ? | denotes the cardinality. Nearly horizontal objects have a large obliquity factor r being close to 1, and the obliquity factor r for extremely slender and oriented objects are close to 0. Therefore, we can select the horizontal or oriented detection as the final result based on such obliquity factor r. Indeed, it is reasonable to represent nearly horizontal objects with horizontal bounding boxes. However, oriented detections are required to accurately describe oriented objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network architecture</head><p>The network architecture (see <ref type="figure">Fig. 3</ref>) is almost the same as faster R-CNN <ref type="bibr" target="#b0">[1]</ref>. We simply add five extra target variables (normalized to [0, 1] using the sigmoid funciton) to the head of faster R-CNN <ref type="bibr" target="#b0">[1]</ref>. Specifically, The input image is first fed into a backbone network to extract deep features and generate bounding box proposals with RPN <ref type="bibr" target="#b0">[1]</ref>. Then the regional features extracted via RoIAlign <ref type="bibr" target="#b34">[35]</ref> on proposals are passed through a modified R-CNN head to generate final results, including a horizontal bounding box (x, y, w, h), four variables (? 1 , ? 2 , ? 3 , ? 4 ) characterizing the oriented bounding box, and obliquity factor r that indicates whether the object is nearly horizontal or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ground-truth generation</head><p>The ground-truth for each object is composed of three components: classical horizontal bounding box representation (x,?,w,h), four extra variables (? 1 ,? 2 ,? 3 ,? 4 ) representing the oriented object, and the obliquity factorr. The horizontal bounding box ground-truth follows the pioneer work in <ref type="bibr" target="#b12">[13]</ref>, which is relative to the proposal. The ground-truth for the four extra variables (? 1 ,? 2 ,? 3 ,? 4 ) and obliquity factorr depend only on the underlying ground-truth object, and are directly calculated by Eq. <ref type="formula" target="#formula_3">(1)</ref> and <ref type="formula" target="#formula_4">(2)</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training objective</head><p>The proposed method involves loss for RPN stage and R-CNN stage. The loss of RPN is the same as that in <ref type="bibr" target="#b0">[1]</ref>. The loss L for R-CNN head contains a classification loss term L cls and a regression loss term L reg . The R-CNN loss L is given by</p><formula xml:id="formula_5">L = 1 N cls i L cls + 1 N reg i p * i ? L reg ,<label>(3)</label></formula><p>where N cls and N reg are the number of total proposals and positive proposals in a mini-batch fed into the head, respectively, and i denotes the index of a proposal in a minibatch. If the i-th proposal is positive, p * i is 1, otherwise it is 0. The regression loss L reg contains three terms for horizontal bounding box, four length ratios (? 1 , ? 2 , ? 3 , ? 4 ), and obliquity factor r regression, respectively. Put it simply, the regression loss L reg is given by</p><formula xml:id="formula_6">L reg = ? 1 ? L h + ? 2 ? L ? + ? 3 ? L r , L ? = 4 i=1 smooth L1 (? i ?? i ), L r = smooth L1 (r ?r),<label>(4)</label></formula><p>where L h is the loss for horizontal box regression, which is the same as that in <ref type="bibr" target="#b0">[1]</ref>, and ? 1 , ? 2 , and ? 3 are hyperparameters that balance the importance of each loss term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Inference</head><p>During testing phase, for a given image, the forward pass generates a set of (x, y, w, h, ? 1 , ? 2 , ? 3 , ? 4 , r) representing horizontal bounding boxes, four length ratios, and obliquity factors. For each candidate, if its obliquity factor r is larger than a threshold t r , indicating that the underlying object is nearly horizontal, we select the horizontal bounding box (x, y, w, h) as the final detection. Otherwise, we select the oriented one given by (x, y, w, h, ? 1 , ? 2 , ? 3 , ? 4 ). The nonmaximum suppression (NMS) process is also performed. Specifically, we first adopt the efficient horizontal NMS (with 0.5 IoU threshold) to get rid of some candidate proposals, followed by an oriented NMS (with 0.1 IoU threshold) on the significantly reduced number of candidate proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and evaluation protocols</head><p>DOTA <ref type="bibr" target="#b22">[23]</ref> is a large-scale and challenging dataset for object detection in aerial images with quadrangle annotations. It contains 2806 4000 ? 4000 images and 188, 282 instances of 15 object categories: plane, baseball diamond (BD), bridge, ground field track (GTF), small vehicle (SV), large vehicle (LV), ship, tennis court (TC), basketball court (BC), storage tank (ST), soccer-ball field (SBF), roundabout (RA), harbor, swimming pool (SP) and helicopter (HC). The official evaluation protocol of DOTA in terms of mAP is used.</p><p>HRSC2016 <ref type="bibr" target="#b35">[36]</ref> is dedicated for ship detection in aerial images, containing 1061 images annotated with rotated rectangles. We conduct experiments for the level-1 task which detects ship from backgrounds. The standard evaluation protocol of HRSC2016 in terms of mAP is used. <ref type="bibr" target="#b36">[37]</ref> is proposed for detecting long and oriented texts. It contains 300 training and 200 test images annotated in terms of text lines. Since the training set is rather small, following other methods, we also use HUST-TR400 <ref type="bibr" target="#b37">[38]</ref> during training. The standard evaluation protocol of MSRA-TD500 based on F-measure is used. <ref type="bibr" target="#b38">[39]</ref> is also a long text detection dataset, consisting of 8034 training images and 4229 test images annotated with text lines. This dataset is very challenging due to very large text scale variances. We evaluate the proposed method via the online evaluation platform in terms of F-measure. <ref type="bibr" target="#b39">[40]</ref> is a multi-target horizontal pedestrian tracking dataset, in which images are taken with fisheye cameras. The authors of <ref type="bibr" target="#b33">[34]</ref> extracted some frames and annotated the pedestrians with rotated rectangles for omnidirectional pedestrian detection. The standard miss rates at every false positive per image (FPPI) and log average miss rates (LAMRs) <ref type="bibr" target="#b40">[41]</ref> are adopted for benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSRA-TD500</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RCTW-17</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MW-18Mar</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The proposed method is implemented based on the project of "maskrcnn benchmark" 1 using 3 Titan Xp GPUs. For a fair comparison with other methods, we adopt ResNet101 <ref type="bibr" target="#b41">[42]</ref> for object detection in aerial images, where the batch size is set to 6 due to limited GPU memory. For the other experiments, ResNet50 is adopted, and the batch size is set to 12. In all experiments, the network is trained by SGD optimizer with momentum and weight decay set to 0.9 and 5 ? 10 ?4 , respectively. The learning rate is initialized with 7.5 ? 10 ?3 and divided by 10 at each learning rate decay step. The hyper-parameters ? 1 , ? 2 , and ? 3 in Eq. (4) are set to 1, 1, and 16, respectively. Without explicitly specifying, the hyper-parameter t r on obliquity factor guiding the selection of horizontal or oriented detection is set to 0.8. Some other application related settings are depicted in the corresponding sections.</p><p>We compare the proposed method with two baseline methods using rotated bounding box representation (denoted by RBox Reg.) and quadrangle representation (denoted by Vertex Reg.). For the RBox reg., based on horizontal prior boxes, similar with <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b29">[30]</ref>, we regress the object center (x, y), long and short side length (w , h ), and the angle ? between the long side and X-axis. For Vertex Reg., we follow <ref type="bibr" target="#b5">[6]</ref> by regressing the one-to-one vertex offset between each vertex of the prior box and its corresponding groundtruth vertex, which is ordered by minimizing the sum of vertex-wise Euclidean distances between the ground-truth oriented object and its horizontal bounding box. For a fair comparison, both baseline methods are implemented using similar settings with the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Object detection in aerial images</head><p>For the experiments on DOTA <ref type="bibr" target="#b22">[23]</ref>, we train the model for 50k steps, and the learning rate decays at {38k, 46k} steps. Random rotation with angle among {0, ?/2, ?, 3?/2} and class balancing are adopted for data augmentation. For the experiments on HRSC2016 <ref type="bibr" target="#b35">[36]</ref>, we train the model for 3.2k steps and decay the learning rate at 2.8k steps. Horizontal flipping is applied for data augmentation. For a fair comparison, the size of training/test images and the anchor settings on both datasets are kept the same as <ref type="bibr" target="#b4">[5]</ref>.</p><p>Overall results. Some qualitative results on DOTA and HRSC2016 are shown in <ref type="figure">Fig. 4 and Fig. 7(a)</ref>, respectively. We show all detected objects with classification scores above  <ref type="figure">Fig. 4</ref>. Some detection results of the proposed method on DOTA <ref type="bibr" target="#b22">[23]</ref>. The arbitrary-oriented objects are correctly detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 1</head><p>Quantitative comparison with other methods on DOTA. Ours-r means that the divide and conquer detection scheme based on obliquity factor r is not used. * indicates that the backbone network is light-head R-CNN <ref type="bibr" target="#b42">[43]</ref>. ? stands for evaluation using IoU threshold 0.7. Note that the runtime for oriented NMS is not included for all methods on this dataset. Otherwise, the proposed method using FPN runs at 9.4 FPS instead of 10.0 FPS.  Ablation study. We conduct ablation study on DOTA <ref type="bibr" target="#b22">[23]</ref>. The proposed method relies on a novel multi-oriented object representation composed of three components: horizontal bounding box (x, y, w, h), gliding offsets (? 1 , ? 2 , ? 3 , ? 4 ), and obliquity factor r. We begin with analyzing the quality of each individual component using Faster R-CNN head with FPN. Firstly, the proposed method achieves a good  <ref type="figure">Fig. 6</ref>. Qualitative comparison with baseline methods in detecting objects of different orientations (by rotating an input image with different angles). The meaning of colors is the same as that in <ref type="figure">Fig. 4</ref>.</p><p>performance with 76.22% mAP under horizontal bounding box evaluation. The small performance gap (i.e., 1.2% mAP) between oriented and horizontal object detection implies that the gliding offset regression is also quite accurate. We also explicitly evaluate the accuracy of gliding offset regression in terms of mean absolute error (MAE) for the correctly detected objects. As depicted in <ref type="figure">Fig. 5</ref>, the gliding offset regression is quite accurate for oriented objects, but is less precise for nearly horizontal objects (e.g.,r &gt; 0.8) for which potential confusion issue remains. This motivates us to regress the obliquity factor r to guide the selection of horizontal or oriented detection as the final detection result, helping to remedy the remaining confusion issue for nearly horizontal objects. Indeed, as shown in <ref type="figure">Fig. 5</ref>, the obliquity factor r regression is in general very accurate (MAE &lt; 5.3%). This quality analysis of each individual component of the proposed multi-oriented object representation confirms the effectiveness of the proposed method. Some qualitative comparison can be found in <ref type="figure">Fig. 6</ref>. We rotate an image with several different angles and test the proposed method and two baseline methods on the rotated images. The RBox reg. produces inaccurate results due to the imprecise angle regression. The Vertex reg. have difficulty for tilted objects at some orientations due to the confusion in defining the vertex order in training. The proposed method is able to accurately detect objects of any orientations.</p><p>The quantitative comparison with baseline methods is depicted in the middle of Tab. 1. The proposed method outperforms the two baseline methods by a large margin. Specifically, the proposed method outperforms the RBox reg. and Vertex reg. by 6.30% and 11.37% mAP at the cost of ignorable runtime. In fact, as depicted in Tab. 1, the proposed method is more efficient than both baseline methods producing more false detections. To further demonstrate the accuracy of the proposed method, we also conduct a benchmark using larger IoU threshold 0.7 in the evaluation system. As shown in Tab. 1, the improvement is even more significant, changing from 6.30% (resp. 11.37%) to 25.93% (resp. 15.98%). This further demonstrates the accuracy of the proposed method in detecting oriented objects.</p><p>We then assess the individual contribution of the proposed vertex gliding and divide-and-conquer detection scheme in the proposed method for multi-oriented object detection. To this end, we evaluate an alternative of the proposed method by discarding the divide-andconquer detection scheme based on obliquity factor r. As depicted in Tab. 1, the proposed representation in terms of (x, y, w, h, ? 1 , ? 2 , ? 3 , ? 4 ) contributes a lot to the improvement. The proposed detection scheme brings 0.59% and 1.06% mAP improvement with and without FPN <ref type="bibr" target="#b2">[3]</ref>, respectively. When larger IoU threshold 0.7 is used, the selection scheme yields 2.55% mAP improvement, confirming the effectiveness of the selection scheme based on obliquity factor r. Without the selection scheme, some nearly horizontal objects with inaccurate predicted gliding offsets (see <ref type="figure">Fig. 5</ref>) may be considered as correct (resp. incorrect) detection under evaluation with 0.5 (resp. 0.7) IoU threshold. This explains the more significant improvement of the selection scheme when a larger IoU threshold is used for evaluation.</p><p>We also analyze the effect of different thresholds t r of obliquity factor r on DOTA dataset using Faster R-CNN head with FPN. As depicted in Tab. 3, the performance is rather stable, especially for t r ? [0.75, 0.85]. The performance slightly decreases for smaller and larger t r . Indeed, with a very small threshold t r , horizontal bounding boxes are selected to represent some oriented objects, which leads to inaccurate detection. When a large threshold t r is adopted, the potential confusion issue for nearly horizontal objects remains, also resulting in decreased performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Long text detection in natural scenes</head><p>For oriented scene text detection on MSRA-TD500 <ref type="bibr" target="#b36">[37]</ref> and RCTW-17 <ref type="bibr" target="#b38">[39]</ref>, we apply the same data augmentation as SSD <ref type="bibr" target="#b17">[18]</ref>. Besides, we also randomly rotate the images with ?/2 to better handle vertical texts. The training images are randomly cropped and resized to some specific sizes. For MSRA-TD500, we randomly resize the short side of cropped images to {512, 768, 864}. For RCTW-17 <ref type="bibr" target="#b38">[39]</ref> containing many small texts, the short side is randomly resized to {960, 1200, 1400}. We first pre-train the model on Synth-Text <ref type="bibr" target="#b44">[45]</ref> for one epoch. Then we fine-tune the model for 4k <ref type="figure">Fig. 7</ref>. Some detection results of the proposed method on HRSC2016 <ref type="bibr" target="#b35">[36]</ref> in (a), MSRA-TD500 <ref type="bibr" target="#b36">[37]</ref> in (b-c), and RCTW-17 <ref type="bibr" target="#b38">[39]</ref> in (d-e). Some qualitative illustrations are given in <ref type="figure">Fig. 7(b-e</ref>). The proposed method correctly detect texts of arbitrary orientations. The quantitative comparisons with some state-of-theart methods on MSRA-TD500 and RCTW-17 are depicted in Tab. 4 and Tab. 5, respectively. The proposed method outperforms other competing methods and is more efficient on both datasets. Specifically, on MSRA-TD500, the proposed method under single scale test outperforms the multi-scale version of <ref type="bibr" target="#b6">[7]</ref> using larger extra training images by 0.5%, and improves <ref type="bibr" target="#b45">[46]</ref> by 2.9%. On RCTW-17, the proposed method outperforms the state-of-the-art method <ref type="bibr" target="#b7">[8]</ref> by 5.8% (resp. 0.9%) under single-scale (resp. multi-scale) test while being much more efficient.</p><formula xml:id="formula_7">(a) (b) (c) (d) (e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Pedestrian detection in fisheye images</head><p>We compare the proposed method with the two baseline methods RBox reg. and Vertex reg., classical horizontal box   <ref type="figure">Fig. 9</ref>. Evaluation on MW-18Mar <ref type="bibr" target="#b39">[40]</ref>. The numbers are the LAMRs.</p><p>regression (denoted by HBox reg.), and the method in <ref type="bibr" target="#b33">[34]</ref> on MW-18Mar <ref type="bibr" target="#b39">[40]</ref>. For a fair comparison with <ref type="bibr" target="#b33">[34]</ref>, we follow similar training and test settings with <ref type="bibr" target="#b33">[34]</ref>. Specifically, in all experiments, FPN is not used. All images are resized to 416 ? 416 during training and test. During training, We randomly rotate the images for data augmentation. The model is trained in total for 4k steps and the learning rate decays at 3k steps. Some qualitative results are illustrated in <ref type="figure">Fig. 8</ref>. The proposed method achieves more accurate results than all the baseline methods. The curve of missing rate with respect to the number of false positives per image is depicted in <ref type="figure">Fig. 9</ref>. The proposed method achieves lower missing rate than all the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a simple yet effective representation for oriented objects and a divide-and-conquer strategy to detect multi-oriented objects. Based on this, we build a robust and fast multi-oriented object detector. It accurately detects ubiquitous multi-oriented objects such as objects in arial images, scene texts, and pedestrians in fisheye images. Extensive experiments demonstrate that the proposed method outperforms some state-of-the-art methods on multiple benchmarks while being more efficient. In the future, we would like to explore the complementary of the proposed method with other approaches focusing on feature enhancement. One-stage multi-oriented object detector is also another direction which is worthy of exploitation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of proposed representation for an oriented object O based on four intersecting points {v i } between O and its horizontal bounding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) HBox reg. (b) RBox reg. (c) Vertex reg. (d) Ours Fig. 8. Qualitative illustrations of different methods on MW-18Mar [40].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>right, bottom, left intersecting point with its horizontal bounding box B h denoted by v i , i ? {1, 2, 3, 4}, respectively. The horizontal bounding box B h is also usually represented by (x, y, w, h), where (x, y) is the center, and w and h are the width and height, respectively. We propose to represent the underlying oriented object by (x, y, w, h, ? 1 , ? 2 , ? 3 , ? 4 ). The extra variables ? i , i ? {1, 2, 3, 4} are defined as follows:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>50.02 50.25 63.62 90.38 69.04 74.56 51.58 50.16 32.73 24.19 25.18 53.49 10.0</head><label></label><figDesc>77.13 17.70 64.05 35.30 38.02 37.16 89.41 69.64 59.28 50.30 52.91 47.89 47.40 46.30 54.13 -RoI Trans. * [5] -88.53 77.91 37.63 74.08 66.53 62.97 66.57 90.50 79.46 76.75 59.04 56.73 62.54 61.29 55.56 67.74 5.9 Ours 85.78 45.90 73.66 70.07 69.10 76.78 90.62 79.08 83.94 57.75 67.57 67.53 70.85 56.46 72.33 9.Azimi et al. [26] 81.36 74.30 47.70 70.32 64.89 67.82 69.98 90.76 79.06 78.20 53.64 62.90 67.02 64.17 50.23 68.16 -RoI Trans. * [5] 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56 -CADNet [28] 87.80 82.40 49.40 73.50 71.10 63.50 76.60 90.90 79.20 73.30 48.40 60.90 62.00 67.00 62.20 69.90 -R 2 CNN++ [27] 89.66 81.22 45.50 75.10 68.27 60.17 66.83 90.90 80.69 86.15 64.05 63.48 65.34 68.01 62.05 71.16 -RBox reg. 89.37 75.96 35.43 69.57 68.35 63.78 74.92 90.76 84.70 85.26 62.43 62.40 52.97 60.32 54.61 68.72 9.2 Vertex reg. 80.16 76.77 43.31 69.38 55.71 56.52 72.25 88.10 28.95 86.31 63.66 62.23 61.62 68.18 41.65 63.65 9.8 Ours * 90.02 84.41 49.80 77.93 72.23 72.52 85.81 90.85 79.21 86.61 59.01 69.15 66.30 71.22 55.67 74.05 7.1 Ours-r 89.40 85.08 52.00 77.40 72.68 72.89 86.41 90.74 78.80 86.79 57.84 70.42 67.73 71.64 56.63 74.43 10.RBox reg. ? 42.52 21.76 10.47 36.53 26.57 26.91 32.39 63.20 36.56 33.54 33.04 15.63 11.16 10.05 12.98 27.56 9.2 Vertex reg. ? 67.94 50.51 14.28 47.46 29.79 27.92 40.66 72.75 14.29 67.59 33.47 40.87 22.04 17.91 15.13 37.51 9.8 Ours * ? 77.98 53.21 12.52 68.87 47.25 46.07 54.83 90.45 68.00 68.45 56.44 40.12 28.59 22.47 19.13 50.29 7.1 Ours-r ? 67.66 50.37 17.07 60.60 48.74 49.00 61.59 88.98 68.84 74.83 48.30 48.03 32.58 23.78 23.75 50.94 10.0</figDesc><table><row><cell>Methods</cell><cell>FPN Plane BD Bridge GTF</cell><cell>SV</cell><cell>LV</cell><cell>Ship TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA Harbor SP</cell><cell>HC mAP FPS</cell></row><row><cell cols="10">FR-O [23] Ours-r 89.93 Ours ? -79.42 49 8.4 -77.32 59.75 15.95 67.63</cell></row></table><note>* - 89.95 86.37 45.79 73.44 71.44 68.20 75.96 90.72 79.63 85.03 58.56 70.19 68.28 71.34 54.45 72.8 Ours - 89.89 85.99 46.09 78.48 70.32 69.44 76.93 90.71 79.36 83.80 57.79 68.35 72.90 71.03 59.78 73.39 9.80 Ours 89.64 85.00 52.26 77.34 73.01 73.14 86.82 90.74 79.02 86.81 59.55 70.91 72.94 70.86 57.32 75.02 10.0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2</head><label>2</label><figDesc>Quantitative comparison with some state-of-the-art methods on HRSC2016.</figDesc><table><row><cell>Ours 88.2 0.6. As illustrated, the proposed method accurately detects mAP 75.7 79.6 84.3 86.2 87.4 both horizontal and oriented objects even under dense dis-tribution and/or being long. The quantitative comparisons with other methods on DOTA [23] and HRSC2016 [36] are depicted in Tab. 1 and Tab. 2, respectively. Without any extra network design such as cascade refinement and attention the performance to 75.02%. The proposed method using features is also beneficial for the proposed method, boosting by 5.65% mAP. FPN [3] that exploits better multi-scale 73.39% mAP, outperforming the state-of-the-art method [5] on DOTA, the proposed method without FPN [3] achieves more efficient in runtime. Specifically, For the experiment of-the-art methods on both DOTA and HRSC2016 and is mechanism, the proposed method outperforms some state-</cell><cell>FPN [3] improves the state-of-the-art method [27] by 3.86% mAP. For HRSC2016 dataset, the proposed method achieves 88.2% mAP, improving state-of-the-art methods by 2%. Experiments on different network architectures. To fur-ther demonstrate the versatility of the proposed method, we evaluate the proposed method on different networks. Concretely, we replace the faster R-CNN head by light-head R-CNN [43] head. As depicted in Tab. 1, using the same network on DOTA [23], the proposed method improves [5] by 4.49% and 4.75% mAP with and without FPN, respec-tively. The proposed method outperforms [5] by 1.2% mAP on HRSC2016 [36].</cell></row></table><note>* indicates that Light-head R-CNN is adopted.Methods RC2 [44] R 2 PN [25] RRD [32] RoI Trans.* [5] Ours*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Fig. 5. Mean absolute error (MAE) of obliquity factor r and gliding offset ? regression with respect to different ranges of ground-truth obliquity factors for the proposed method on DOTA.</figDesc><table><row><cell></cell><cell>20.00</cell><cell>r ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>19.84</cell><cell>18.97</cell></row><row><cell></cell><cell>15.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAE (%)</cell><cell>10.00</cell><cell>9.51</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.19</cell><cell></cell></row><row><cell></cell><cell>0.00 5.00</cell><cell>3.32</cell><cell>2.32</cell><cell>5.34</cell><cell>2.34</cell><cell>5.66</cell><cell>3.33</cell><cell>6.42</cell><cell>4.71</cell><cell>7.17</cell><cell>5.24</cell><cell>4.46</cell><cell>4.66</cell></row><row><cell></cell><cell></cell><cell cols="12">(0,0.3] (0.3,0.4] (0.4,0.5] (0.5,0.6] (0.6,0.7] (0.7,0.8] (0.8,0.9] (0.9,1.0]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Ground-truth obliquity factors</cell><cell></cell></row><row><cell>RBox reg.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vertex reg.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0 ?</cell><cell></cell><cell></cell><cell cols="2">40 ?</cell><cell></cell><cell></cell><cell></cell><cell cols="2">80 ?</cell><cell></cell><cell>120 ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3</head><label>3</label><figDesc>Ablation study on different thresholds tr of obliquity factor r. 0.65 0.70 0.75 0.80 0.85 0.90 0.95 w FPN 73.29 74.30 74.72 75.02 75.06 75.06 74.44 w/o FPN 71.76 72.42 73.24 73.39 73.37 72.59 72.47</figDesc><table /><note>tr</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 Quantitative</head><label>4</label><figDesc>14k) and decay the learning rate at 3k (resp. 10k) steps for MSRA-TD500 (resp. RCTW-17). During test, the short side of MSRA-TD500 images is resized to 768. For RCTW-17, the short side is set to 1200 for single scale test. We add extra scales of {512, 1024, 1280, 1560} for multi-scale test.</figDesc><table><row><cell cols="3">comparison with other methods on MSRA-TD500 [37]. MS stands for multi-scale test.</cell></row><row><cell>Methods Zhang et al. [12] SegLink [9] RRD [32] EAST [4] Border MS [47] TextField [29] Lyu et al. [10] CRAFT [48] MCN [11] Wang et al. [46] Direct MS [7] Ours</cell><cell cols="2">Precision Recall F-measure FPS 83.0 67.0 74.0 0.5 86.0 70.0 77.0 8.9 87.0 73.0 79.0 10.0 87.3 67.4 76.1 13.2 83.0 73.3 76.8 -87.4 75.9 81.3 5.2 87.6 76.2 81.5 5.7 88.2 78.2 82.9 8.6 88.0 79.0 83.0 -85.2 82.1 83.6 10.0 91.0 81.0 86.0 -88.8 84.3 86.5 15.0</cell></row><row><cell cols="3">TABLE 5 Quantitative comparison with other methods on RCTW-17 [39]. MS stands for multi-scale test.</cell></row><row><cell cols="2">Methods Official baseline [39] RRD [32] RRD MS Direct MS [7] Border MS [47] LOMO [8] LOMO MS Ours Ours MS</cell><cell>Precision Recall F-measure FPS 76.0 40.4 52.8 8.9 72.4 45.3 55.7 10.0 77.5 59.1 67.0 -76.7 57.9 66.0 -78.2 58.8 67.1 -80.4 50.8 62.3 4.4 79.1 60.2 68.4 -77.0 61.0 68.1 7.8 77.6 62.7 69.3 -</cell></row><row><cell>(resp.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. and Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">EAST: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-oriented and multi-lingual scene text detection with direct regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5406" to="5419" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Look more than once: An accurate detector for text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3482" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7553" to="7563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning markov clustering networks for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6936" to="6944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conf. on Comp. Vis</title>
		<meeting>of European Conf. on Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">CenterNet: Object detection with keypoint triplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning a rotation invariant detector with rotatable bounding box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<idno>arXiv preprint:1711.09405</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Asian Conf. on Comp</title>
		<meeting>of Asian Conf. on Comp</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">R2CNN++: Multi-dimensional attention based rotation invariant detector with robust anchor strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xian</surname></persName>
		</author>
		<idno>arXiv preprint:1811.07126</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">CAD-Net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Textfield: Learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3454" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Omnidetector: With neural networks to bounding boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Apitzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hirtz</surname></persName>
		</author>
		<idno>arXiv preprint:1805.08503</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Omnidirectional pedestrian detection by rotation invariant training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Horiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Murakami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Winter Conf. on Applications of Comp. Vis</title>
		<meeting>of IEEE Winter Conf. on Applications of Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Computer Vision</title>
		<meeting>of IEEE Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ship rotated bounding box space for ship extraction from high-resolution optical satellite images with complex backgrounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1074" to="1078" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ICDAR2017 competition on reading chinese text in the wild (RCTW-17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Document Analysis and Recognition</title>
		<meeting>of International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1429" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<ptr target="https://icat.vt.edu/mirrorworlds/challenge/index.html" />
		<title level="m">Mirror worlds challenge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. and Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Light-Head R-CNN: In defense of two-stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv preprint:1711.07264</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rotated region based cnn for ship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Image Processing</title>
		<meeting>of IEEE Intl. Conf. on Image essing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Arbitrary shape scene text detection with adaptive text region representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Accurate scene text detection through border semantics awareness and bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Character region awareness for text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp. Vis. and Patt. Rec</title>
		<meeting>of IEEE Conf. on Comp. Vis. and Patt. Rec</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9365" to="9374" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
