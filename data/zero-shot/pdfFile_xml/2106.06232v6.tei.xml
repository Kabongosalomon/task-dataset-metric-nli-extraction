<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GDI: Rethinking What Makes Reinforcement Learning Different from Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changnan</forename><forename type="middle">Xiao</forename><surname>Bytedance</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang Bytedance</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GDI: Rethinking What Makes Reinforcement Learning Different from Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning (DRL) via combining deep learning (DL) with reinforcement learning (RL), which has noticed that the distribution of the acquired data would change during the training process. DQN found this property might cause instability for training, so it proposed effective methods to handle the downside of the property. Instead of focusing on the unfavorable aspects, we find it critical for RL to ease the gap between the estimated data distribution and the ground truth data distribution while supervised learning (SL) fails to do so. From this new perspective, we extend the basic paradigm of RL called the Generalized Policy Iteration (GPI) into a more generalized version, which is called the Generalized Data Distribution Iteration (GDI). We see massive RL algorithms and techniques can be unified into the GDI paradigm, which can be considered as one of the special cases of GDI. We provide theoretical proof of why GDI is better than GPI and how it works. Several practical algorithms based on GDI have been proposed to verify its effectiveness and extensiveness. Empirical experiments prove our state-ofthe-art (SOTA) performance on Arcade Learning Environment (ALE), wherein our algorithm has achieved 9620.98% mean human normalized score (HNS), 1146.39% median HNS and 22 human world record breakthroughs (HWRB) using only 200M training frames. Our work aims to lead the RL research to step into the journey of conquering the human world records and seek real superhuman agents on both performance and efficiency. Preprint. Under review. , others . Dota 2 with large scale deep reinforcement learning // arXiv preprint arXiv:1912.06680. 2019. Cohn David A, Ghahramani Zoubin, Jordan Michael I. Active learning with statistical models // , others . Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures // arXiv preprint arXiv:1802.01561. 2018. Eysenbach Benjamin, Gupta Abhishek, Ibarz Julian, Levine Sergey. Diversity is all you need: Learning skills without a reward function // arXiv preprint arXiv:1802.06070. 2018. Garivier Aur?lien, Moulines Eric. On upper-confidence bound policies for non-stationary bandit problems // arXiv preprint arXiv:0805.3415. 2008. Haarnoja Tuomas, Zhou Aurick, Abbeel Pieter, Levine Sergey. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor // arXiv preprint arXiv:1801.01290. 2018.</p><p>Hafner Danijar, Lillicrap Timothy, Norouzi Mohammad, Ba Jimmy. Mastering atari with discrete world models // arXiv preprint arXiv:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning (ML) can be defined as improving some measure performance P at some task T according to the acquired data or experience E <ref type="bibr">(Mitchell, others, 1997)</ref>. As one of the three main components of ML <ref type="bibr">(Mitchell, others, 1997)</ref>, the training experiences matter in ML, which can be reflected from many aspects. For example, three major ML paradigms can be distinguished from the perspective of the different training experiences. Supervised learning (SL) is learning from a training set of labeled experiences provided by a knowledgable external supervisor <ref type="bibr" target="#b21">(Sutton, Barto, 2018)</ref>. Unsupervised learning (UL) is typically about seeking structure hidden in collections of unlabeled experiences (Sutton, <ref type="bibr" target="#b21">Barto, 2018)</ref>. Unlike UL or SL, reinforcement learning (RL) focuses on the problem that agents learn from experiences gained through trial-and-error interactions with a dynamic environment <ref type="bibr">(Kaelbling et al., 1996)</ref>. As <ref type="bibr">(Mitchell, others, 1997)</ref> said, there is no free lunch in the ML problem -no way to generalize beyond the specific training examples. The performance can only be improved through learning from the acquired experiences in ML problems <ref type="bibr">(Mitchell, others, 1997)</ref>. All of them have revealed the importance of the training experiences and thus the selection of the training distribution appears to be a fundamental problem in ML.</p><p>Recalling these three paradigms, SL and RL receive explicit learning signals from data. In SL, there is no way to make up the gap between the distribution estimated by the collected data and the ground truth without any domain knowledge unless collecting more data. Researchers have found RL explicitly and naturally transforming the training distribution <ref type="bibr" target="#b6">(Mnih et al., 2015)</ref>, which makes RL distinguished from SL. In the recent RL advances, many researchers <ref type="bibr" target="#b6">(Mnih et al., 2015)</ref> have realized that RL agents hold the property of changing the data distribution and massive works have revealed the unfavorable aspect of the property. Among those algorithms, <ref type="bibr">DQN (Mnih et al., 2015)</ref> firstly noticed the unique property of RL and considered it as one of the reasons for the training instability of DRL. After that, massive methods like replay buffer <ref type="bibr" target="#b6">(Mnih et al., 2015)</ref>, periodically updated target <ref type="bibr" target="#b6">(Mnih et al., 2015)</ref> and importance sampling <ref type="bibr" target="#b30">(Espeholt et al., 2018)</ref> have been proposed to mitigate the impact of the data distribution shift. However, after rethinking this property, we wonder whether changing the data distribution always brings unfavorable nature. What if we can control it? More precisely, what if we can control the ability to select superior data distribution for training automatically? Prior works in ML have revealed the great potential of this property. As <ref type="bibr">(Cohn et al., 1996)</ref> put it, when training examples are appropriately selected, the data requirements for some problems decrease drastically, and some NP-complete learning problems become polynomial in computation time <ref type="bibr">(Angluin, 1988;</ref><ref type="bibr">Baum, 1991)</ref>, which means that carefully selecting good training data benefits learning efficiency. Inspired by this perspective, instead of discussing how to ease the disadvantages caused by the change of data distribution like other prior works of RL, in this paper, we rethink the property distinguishing RL from SL and explore more effective aspects of it. One of the fundamental reasons RL holds the ability to change the data distribution is the change of behavior policies, which directly interact with the dynamic environments to obtain training data <ref type="bibr" target="#b6">(Mnih et al., 2015)</ref>. Therefore, the training experiences can be controlled by adjusting the behavior policies, which makes behavior selection the bridge between RL agents and training examples.</p><p>In the RL problem, the agent has to exploit what it already knows to obtain the reward, but it also has to explore to make better action selections in the future, which is called the exploration and exploitation dilemma <ref type="bibr" target="#b21">(Sutton, Barto, 2018)</ref>. Therefore, diversity is one of the main factors that should be considered while selecting the training examples. In the recent advances of RL, some works have also noticed the importance of the diversity of training experiences <ref type="bibr">(Badia et al., 2020a,b;</ref><ref type="bibr">Parker-Holder et al., 2020;</ref><ref type="bibr" target="#b8">Niu et al., 2011;</ref><ref type="bibr" target="#b3">Li et al., 2019;</ref><ref type="bibr">Eysenbach et al., 2018)</ref>, most of which have obtained diverse data via enriching the policy diversity. Among those algorithms, <ref type="bibr">DIAYN (Eysenbach et al., 2018)</ref> focused entirely on the diversity of policy via learning skills without a reward function, which has revealed the effect of policy diversity but ignored its relationship with the RL objective. DvD <ref type="bibr">(Parker-Holder et al., 2020)</ref> introduced a diversity-based regularizer into the RL objective to obtain more diverse data, which changed the optimal solution of the environment (Sutton, <ref type="bibr" target="#b21">Barto, 2018)</ref>. Besides, training a population of agents to gather more diverse experiences seems to be a promising approach. Agent57 <ref type="bibr" target="#b30">(Badia et al., 2020a)</ref> and NGU <ref type="bibr">(Badia et al., 2020b</ref>) trained a family of policies with different degrees of exploratory behaviors using a shared network architecture. Both of them have obtained SOTA performance at the cost of increasing the uncertainty of environmental transition, which leads to extremely low learning efficiency. Through those successes, it is evident that the diversity of the training data benefit the RL training. However, why does it perform better and whether more diverse data always benefit RL training? In other words, we have to explore the following question:</p><p>Does diverse data always benefit effective learning?</p><p>To investigate this problem, we seek inspiration from the natural biological processes. In nature, the population evolves typically faster than individuals because the diversity of the populations boosts more beneficial mutations which provide more possibility for acquiring more adaptive direction of evolution <ref type="bibr">(Pennisi, 2016)</ref>. Furthermore, beneficial mutations rapidly spread among the population, thus enhancing population adaptability <ref type="bibr">(Pennisi, 2016)</ref>. Therefore, an appropriate diversity brings high-value individuals, and active learning among the population promotes its prosperity. <ref type="bibr">1</ref> From this perspective, the RL agents have to pay more attention to experiences worthy of learning from.  DisCor <ref type="bibr">(Kumar et al., 2020)</ref>, which re-weighted the existing data buffer by the distribution that explicitly optimizes for corrective feedback, has also noticed the fact that the choice of the sampling distribution is of crucial importance for the stability and efficiency of approximation dynamic programming algorithms. Unfortunately, DisCor only changes the existing data distribution instead of directly controlling the source of the training experiences, which may be more important and also more complex. In conclusion, it seems that both expanding the capacity of policy space for behaviors and selecting suitable behavior policies from a diverse behavior population matter for efficient learning. This new perspective motivates us to investigate another critical problem:</p><p>How to select superior behaviors from the behavior policy space?</p><p>To address those problems, we proposed a novel RL paradigm called Generalized Data Distribution Iteration (GDI), which consists of two major process, the policy iteration operator T and the data distribution iteration operator E. Specifically, behaviors will be sampled from a policy space according to a selective distribution, which will be iteratively optimized through the operator E. Simultaneously, elite training data will be used for policy iteration via the operator T . More details about our methodology can see Sec. <ref type="bibr">3.</ref> In conclusion, the main contributions of our work are:</p><p>? A Novel RL Paradigm: Rethinking the difference between RL and SL, we discover RL can ease the gap between the sampled data distribution and the ground truth data distribution via adjusting the behavior policies. Based on the perspective, we extend GPI into GDI, a more general version containing a data optimization process. This novel perspective allows us to unify massive RL algorithms, and various improvements can be considered a special case of data distribution optimization, detailed in Sec. 3.</p><p>? Theoretical Proof of GDI: We provide sufficient theoretical proof of GDI. The effectiveness of the data distribution optimization of GDI has been proved on both first-order optimization and second-order optimization, and the guarantee of monotonic improvement induced by the data distribution optimization operator E has also been proved. More details can see Sec. <ref type="bibr">3</ref> and App. E.</p><p>? A General Practical Framework of GDI: Based on GDI, we propose a general practical framework, wherein behavior policy belongs to a soft -greedy space which unifies -greedy policies <ref type="bibr" target="#b27">(Watkins, 1989)</ref> and Boltzmann policies <ref type="bibr" target="#b28">(Wiering, 1999)</ref>. As a practical framework of GDI, a self-adaptable meta-controller is proposed to optimize the distribution of the behavior policies. More implementation details can see App. F and App. G.</p><p>? The State-Of-The-Art Performance: From Figs. 1, our approach has achieved 9620.98% mean HNS and 1146.39% median HNS, which achieves new SOTA. More importantly, our learning efficiency has approached the human level as achieving the SOTA performance within less than 1.5 months of game time. We have also illustrated the RL Benchmark on HNS in App. D.1 and recorded their scores in App. J.5.</p><p>? Human World Records Breakthrough: As our algorithms have achieved SOTA on mean HNS, median HNS and learning efficiency, we aim to lead RL research on ALE to step into a new era of conquering human world records and seeking the real superhuman agents. Therefore, we propose several novel evaluation criteria and an open challenge on the Atari benchmark based on the human world records. From Figs. 1, our method has surpassed 22 human world records, which has also surpassed all previous algorithms. We have also illustrated the RL Benchmark on human world records normalized scores (HWRNS), <ref type="bibr">SABER (Toromanoff et al., 2019)</ref>, HWRB in App. D. <ref type="bibr">2, D.3 and D.4, respectively.</ref> Relevant scores are recorded in App. J.6 and App. J.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>The RL problem can be formulated as a Markov Decision Process <ref type="bibr">(Howard, 1960, MDP)</ref> defined by (S, A, p, r, ?, ? 0 ). Considering a discounted episodic MDP, the initial state s 0 is sampled from the initial distribution ? 0 (s) : S ? ?(S), where we use ? to represent the probability simplex. At each time t, the agent chooses an action a t ? A according to the policy ?(a t |s t ) : S ? ?(A) at state s t ? S. The environment receives a t , produces the reward r t ? r(s, a) : S ? A ? R and transfers to the next state s t+1 according to the transition distribution p (s | s, a) : S ? A ? ?(S). The process continues until the agent reaches a terminal state or a maximum time step. Define the discounted state visitation distribution as d ? ?0 (s) = (1 ? ?)E s0??0 [ ? t=0 ? t P(s t = s|s 0 )]. The goal of reinforcement learning is to find the optimal policy ? * that maximizes the expected sum of discounted rewards, denoted by J (Sutton, <ref type="bibr" target="#b21">Barto, 2018)</ref>:</p><formula xml:id="formula_0">? * = argmax ? J ? = argmax ? E st?d ? ? 0 E ? [G t |s t ] = argmax ? E st?d ? ? 0 E ? ? k=0 ? k r t+k |s t<label>(1)</label></formula><p>where ? ? (0, 1) is the discount factor.</p><p>RL algorithms can be divided into off-policy manners <ref type="bibr" target="#b6">(Mnih et al., 2015</ref><ref type="bibr" target="#b5">(Mnih et al., , 2016</ref><ref type="bibr">Haarnoja et al., 2018;</ref><ref type="bibr" target="#b30">Espeholt et al., 2018</ref>) and on-policy manners <ref type="bibr" target="#b19">(Schulman et al., 2017b)</ref>. Off-policy algorithms select actions according to a behavior policy ? that can be different from the learning policy ?. On-policy algorithms evaluate and improve the learning policy through data sampled from the same policy. RL algorithms can also be divided into value-based methods <ref type="bibr" target="#b6">(Mnih et al., 2015;</ref><ref type="bibr" target="#b23">Van Hasselt et al., 2016;</ref><ref type="bibr" target="#b25">Wang et al., 2016;</ref><ref type="bibr">Hessel et al., 2017;</ref><ref type="bibr">Horgan et al., 2018)</ref> and policy-based methods <ref type="bibr" target="#b19">(Schulman et al., 2017b;</ref><ref type="bibr" target="#b5">Mnih et al., 2016;</ref><ref type="bibr" target="#b30">Espeholt et al., 2018;</ref><ref type="bibr">Schmitt et al., 2020)</ref>. In the value-based methods, agents learn the policy indirectly, where the policy is defined by consulting the learned value function, like -greedy, and the value function is learned by a typical GPI. In the policy-based methods, agents learn the policy directly, where the correctness of the gradient direction is guaranteed by the policy gradient theorem <ref type="bibr" target="#b21">(Sutton, Barto, 2018)</ref>, and the convergence of the policy gradient methods is also guaranteed <ref type="bibr">(Agarwal et al., 2019)</ref>. More background on RL can see App. B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generalized Data Distribution Iteration</head><p>Let's abstract our notations first, which is also summarized in App. A.</p><p>Define ? to be an index set, ? ? R k . ? ? ? is an index in ?. (?, B| ? , P ? ) is a probability space, where B| ? is a Borel ?-algebra restricted to ?. Under the setting of meta-RL, ? can be regarded as the set of all possible meta information. Under the setting of population-based training (PBT) <ref type="bibr">(Jaderberg et al., 2017)</ref>, ? can be regarded as the set of the whole population.</p><p>Define ? to be a set of all possible values of parameters. ? ? ? is some specific value of parameters. For each index ?, there exists a specific mapping between each parameter of ? and ?, denoted as ? ? , to indicate the parameters in ? corresponding to ?. Under the setting of linear regression y = w ? x, ? = {w ? R n } and ? = w. If ? represents using only the first half features to make regression, assume w = (w 1 , w 2 ), then ? ? = w 1 . Under the setting of RL, ? ? defines a parameterized policy indexed by ?, denoted as ? ? ? .</p><p>Define D def = {d ? ?0 | ? ? ?(A) S , ? 0 ? ?(S)} to be the set of all states visitation distributions. For the parameterized policies, denote D ?,?,?0 def = {d ? ? ? ?0 | ? ? ?, ? ? ?}. Note that (?, B| ? , P ? ) is a probability space on ?, which induces a probability space on D ?,?,?0 , with the probability measure given by P D (D ?0,?,?0 ) = P ? (? 0 ), ?? 0 ? B| ? .</p><p>We use x to represent one sample, which contains all necessary information for learning. For DQN, x = (s t , a t , r t , s t+1 ). For R2D2, x = (s t , a t , r t , . . . , s t+N , a t+N , r t+N , s t+N +1 ). For IMPALA, x also contains the distribution of the behavior policy. The content of x depends on the algorithm, but it's sufficient for learning. We use X to represent the set of samples. At training stage t, given the parameter ? = ? <ref type="bibr">(t)</ref> , the distribution of the index set P ? = P (t) ? and the distribution of the initial state ? 0 , we denote the set of samples as</p><formula xml:id="formula_1">X (t) ?0 def = d ? ? 0 ?P (t) D {x|x ? d ? ?0 } = ??P (t) ? {x|x ? d ? ? ?0 , ? = ? (t) ? } ??P (t) ? X (t)</formula><p>?0,? .</p><p>Now we introduce our main algorithm:</p><formula xml:id="formula_2">Algorithm 1 Generalized Data Distribution Iteration (GDI). Initialize ?, ?, P (0) ? , ? (0) . for t = 0, 1, 2, . . . do Sample {X (t) ?0,? } ??P (t) ? . ? (t+1) = T (? (t) , {X (t) ?0,? } ??P (t) ? ). P (t+1) ? = E(P (t) ? , {X (t) ?0,? } ??P (t) ? ). end for T defined as ? (t+1) = T (? (t) , {X (t) ?0,? } ??P (t) ?</formula><p>) is a typical optimization operator of RL algorithms, which utilizes the collected samples to update the parameters for maximizing some function L T . For instance, L T may contain the policy gradient and the state value evaluation for the policy-based methods, may contain generalized policy iteration for the value-based methods, may also contain some auxiliary tasks or intrinsic rewards for special designed methods.</p><formula xml:id="formula_3">E defined as P (t+1) ? = E(P (t) ? , {X (t) ?0,? } ??P (t) ?</formula><p>) is a data distribution optimization operator. It uses the samples {X</p><formula xml:id="formula_4">(t) ?0,? } ??P (t) ? to maximize some function L E , namely, P (t+1) ? = arg max P? L E ({X (t) ?0,? } ??P? ).</formula><p>When P ? is parameterized, we abuse the notation and use P ? to represent the parameter of P ? . If E is a first order optimization operator, then we can write E explicitly as</p><formula xml:id="formula_5">P (t+1) ? = P (t) ? + ?? P (t) ? L E ({X (t) ?0,? } ??P (t) ? ).</formula><p>If E is a second order optimization operator, like natural gradient, we can write E formally as</p><formula xml:id="formula_6">P (t+1) ? = P (t) ? + ?F(P (t) ? ) ? ? P (t) ? L E ({X (t) ?0,? } ??P (t) ? ), F(P (t) ? ) = ? P (t) ? log P (t) ? ? ? P (t) ? log P (t) ? ,</formula><p>where ? denotes the Moore-Penrose pseudoinverse of the matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Systematization of GDI</head><p>We can further divide all algorithms into two categories, GDI-I n and GDI-H n . n represents the degree of freedom of ?. I represents Isomorphism. We say one algorithm belongs to GDI-I n , if ? = ? ? , ?? ? ?. H represents Heterogeneous. We say one algorithm belongs to GDI-H n , if ? ?1 = ? ?2 , ?? 1 , ? 2 ? ?. We say one algorithm is "w/o E" if it doesn't have the operator E, in another word, its E is an identical mapping. Now we discuss the connections between GDI and some algorithms.</p><p>For DQN, RAINBOW, PPO and IMPALA, they are in GDI-I 0 w/o E. Let |?| = 1, WLOG, assume ? = {? 0 }. The probability measure P ? collapses to P ? (? 0 ) = 1. ? = {? ?0 }. E is an identical mapping of P (t) ? . T is the first order operator that optimizes the loss functions, respectively. For Ape-X and R2D2, they are in GDI-</p><formula xml:id="formula_7">I 1 w/o E. Let ? = { l | l = 1, . . . , 256}. P ? is uniform, P ? ( l ) = |?| ?1 .</formula><p>Since all actors and the learner share parameters, we have</p><formula xml:id="formula_8">? 1 = ? 2 for ? 1 , 2 ? ?, hence ? = ?? {? } = {? l }, ? l = 1, . . . , 256. E is an identical mapping, because P (t)</formula><p>? is always a uniform distribution. T is the first order operator that optimizes the loss functions.</p><formula xml:id="formula_9">For LASER, it's in GDI-H 1 w/o E. Let ? = {i| i = 1, . . . , K} to be the number of learners. P ? is uniform, P ? (i) = |?| ?1 . Since different learners don't share parameters, ? i1 ? ? i2 = ? for ?i 1 , i 2 ? ?, hence ? = i?? {? i }. E is an identical mapping. T can be formulated as a union of ? (t+1) i = T i (? (t) i , {X (t) ?0,? } ??P (t) ?</formula><p>), which represents optimizing ? i of ith learner with shared samples from other learners.</p><p>For PBT, it's in GDI-H n+1 , where n is the number of searched hyperparameters. Let ? = {h}?{i|i = 1, . . . , K}, where h represents the hyperparameters being searched and K is the population size</p><formula xml:id="formula_10">. ? = i=1,...,K {? i,h }, where ? i,h1 = ? i,h2 for ?(h 1 , i), (h 2 , i) ? ?.</formula><p>E is the meta-controller that adjusts h for each i, which can be formally written as P</p><formula xml:id="formula_11">(t+1) ? (?, i) = E i (P (t) ? (?, i), {X (t) ?0,(h,i) } h?P (t)</formula><p>? (?,i) ), which optimizes P ? according to the performance of all agents in the population. T can also be formulated as a union of T i , but is ?</p><formula xml:id="formula_12">(t+1) i = T i (? (t) i , {X (t) ?0,(h,i) } h?P (t)</formula><p>? (?,i) ), which represents optimizing the ith agent with only samples from the ith agent.</p><p>For NGU and Agent57, it's in GDI-I 2 . Let ? = {? i |i = 1, . . . , m} ? {? j |j = 1, . . . , n}, where ? is the weight of the intrinsic value function and ? is the discount factor. Since all actors and the learner share variables, ? = (?,?)?? {? (?,?) } = {? (?,?) } for ?(?, ?) ? ?. E is an optimization operator of a multi-arm bandit controller with UCB, which aims to maximize the expected cumulative rewards by adjusting P ? . Different from above, T is identical to our general definition ?</p><formula xml:id="formula_13">(t+1) = T (? (t) , {X (t) ?0,? } ??P (t) ?</formula><p>), which utilizes samples from different ?s to update the shared ?.</p><p>For Go-Explore, it's in GDI-H 1 . Let ? = {? }, where ? represents the stopping time of switching between robustification and exploration. ? = {? r } ? {? e }, where ? r is the robustification model and ? e is the exploration model. E is a search-based controller, which defines the next P ? for a better exploration. T can be decomposed into (T r , T e ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Monotonic Data Distribution Optimization</head><p>We see massive algorithms can be formulated as a special case of GDI. For the algorithms without a meta-controller, whose data distribution optimization operator E is trivially an identical mapping, the guarantee that the learned policy could converge to the optimal policy has been wildly studied, for instance, GPI in <ref type="bibr" target="#b21">(Sutton, Barto, 2018)</ref> and policy gradient in <ref type="bibr">(Agarwal et al., 2019)</ref>. But for the algorithms with a meta-controller, whose data distribution optimization operator E is non-identical, though most algorithms in this class show superior performance, it still lacks a general study on why the data distribution optimization operator E helps. In this section, with a few assumptions, we show that given the same optimization operator T , a GDI with a non-identical data distribution optimization operator E is always superior to a GDI w/o E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For brevity, we denote the expectation of</head><formula xml:id="formula_14">L E , L T for each ? ? ? as L E (?, ? ? ) = E x?? ? ? [L E ({X ?0,? })], L T (?, ? ? ) = E x?? ? ? [L T ({X ?0,? })], and denote the expectation of L E , L T for any P ? as L E (P ? , ?) = E ??P? [L E (?, ? ? )], L T (P ? , ?) = E ??P? [L T (?, ? ? )]. Assumption 1 (Uniform Continuous Assumption). For ? &gt; 0, ?s ? S, ? ? &gt; 0, s.t.|V ?1 (s) ? V ?2 (s)| &lt; , ? d ? (? 1 , ? 2 ) &lt; ?, where d ? is a metric on ?(A)</formula><p>S . If ? is parameterized by ?, then for</p><formula xml:id="formula_15">? &gt; 0, ?s ? S, ? ? &gt; 0, s.t.|V ? ? 1 (s) ? V ? ? 2 (s)| &lt; , ? ||? 1 ? ? 2 || &lt; ?.</formula><p>Remark. <ref type="bibr">(Dadashi et al., 2019)</ref> shows V ? is infinitely differentiable everywhere on ?(A) S if |S| &lt; ?, |A| &lt; ?. <ref type="bibr">(Agarwal et al., 2019)</ref> shows V ? is ?-smooth, namely bounded second order derivative, for direct parameterization. If ?(A) S is compact, continuity implies uniform continuity.</p><formula xml:id="formula_16">Assumption 2 (Formulation of E Assumption). Assume P (t+1) ? = E(P (t) ? , {X (t) ?0,? } ??P (t) ? ) can be written as P (t+1) ? (?) = P (t) ? (?) exp(?L E (?,? (t) ? )) Z (t+1) , Z (t+1) = E ??P (t) ? [exp(?L E (?, ? (t) ? ))].</formula><p>Remark. The assumption is actually general. Regarding ? as an action space and</p><formula xml:id="formula_17">r ? = L E (?, ? (t) ? ), when solving arg max P? E ??P? [L E (?, ? (t) ? )] = arg max P? E ??P? [r ? ]</formula><p>, the data distribution optimization operator E is equivalent to solving a multi-arm bandit (MAB) problem. For the first order optimization, <ref type="bibr" target="#b17">(Schulman et al., 2017a)</ref> shows that the solution of a KL-regularized version,</p><formula xml:id="formula_18">arg max P? E ??P? [r ? ] ? ?KL(P ? ||P (t) ? )</formula><p>, is exactly the assumption. For the second order optimization, let P ? = sof tmax({r ? }), <ref type="bibr">(Agarwal et al., 2019)</ref> shows that the natural policy gradient of a softmax parameterization also induces exactly the assumption.</p><formula xml:id="formula_19">Assumption 3 (First Order Optimization Co-Monotonic Assumption). For ? ? 1 , ? 2 ? ?, we have [L E (? 1 , ? ?1 ) ? L E (? 2 , ? ?2 )] ? [L T (? 1 , ? ?1 ) ? L T (? 2 , ? ?2 )] ? 0. Assumption 4 (Second Order Optimization Co-Monotonic Assumption). For ? ? 1 , ? 2 ? ?, ? ? 0 &gt; 0, s.t. ? 0 &lt; ? &lt; ? 0 , we have [L E (? 1 , ? ?1 ) ? L E (? 2 , ? ?2 )] ? [G ? L T (? 1 , ? ?1 ) ? G ? L T (? 2 , ? ?2 )] ? 0, where ? ? ? = ? ? + ?? ? ? L T (?, ? ? ) and G ? L T (?, ? ? ) = 1 ? [L T (?, ? ? ? ) ? L T (?, ? ? )].</formula><p>Under Assumption <ref type="formula" target="#formula_0">(1) (2) (3)</ref>, if T is a first order operator, namely a gradient accent operator, to maximize L T , GDI can be guaranteed to be superior to that w/o E. Under Assumption <ref type="formula" target="#formula_0">(1) (2) (4)</ref>, if T is a second order operator, namely a natural gradient operator, to maximize L T , GDI can also be guaranteed to be superior to that w/o E. Theorem 1 (First Order Optimization with Superior Target). Under Assumption <ref type="formula" target="#formula_0">(1)</ref> </p><formula xml:id="formula_20">(2) (3), we have L T (P (t+1) ? , ? (t+1) ) = E ??P (t+1) ? [L T (?, ? (t+1) ? )] ? E ??P (t) ? [L T (?, ? (t+1) ? )] = L T (P (t)</formula><p>? , ? (t+1) ). Proof. By Theorem 4 (see App. E), the upper triangular transport inequality, let f (?) = L T (?, ? ? ) and g(?) = L E (?, ? ? ), the proof is done.</p><p>Remark (Why Superior Target). In Algorithm 1, if E updates P (t) ? at time t, then the operator T at time t + 1 can be written as ? (t+2) = ? (t+1) + ?? ? (t+1) L T (P (t+1) ? , ? (t+1) ). If P (t) ? hasn't been updated at time t, then the operator T at time t + 1 can be written as ? (t+2) = ? (t+1) + ?? ? (t+1) L T (P (t) ? , ? (t+1) ). Theorem 1 shows that the target of T at time t + 1 becomes higher if P (t) ? is updated by E at time t. Remark (Practical Implementation). We provide one possible practical setting of GDI. Let L E (?, ? ? ) = J ? ? ? and L T (?, ? ? ) = J ? ? ? . E can update P ? by the Monte-Carlo estimation of J ? ? ? . T is to maximize J ? ? ? , which can be any RL algorithms.</p><p>Theorem 2 (Second Order Optimization with Superior Improvement). Under Assumption (1)</p><formula xml:id="formula_21">(2) (4), we have E ??P (t+1) ? [G ? L T (?, ? (t+1) ? )] ? E ??P (t) ? [G ? L T (?, ? (t+1) ? )], more specifically, E ??P (t+1) ? [L T (?, ? (t+1),? ? ) ? L T (?, ? (t+1) ? )] ? E ??P (t) ? [L T (?, ? (t+1),? ? ) ? L T (?, ? (t+1) ? )].</formula><p>Proof. By Theorem 4 (see App. E), the upper triangular transport inequality, let f (?) = G ? L T (?, ? ? ) and g(?) = L E (?, ? ? ), the proof is done. Remark (Why Superior Improvement). Theorem 2 shows that, if P ? is updated by E, the expected improvement of T is higher. <ref type="bibr">(Agarwal et al., 2019)</ref> shows that, for direct parameterization, the natural policy gradient gives ? (t+1) ? ? (t) exp( A ? (t) ), by Lemma 4 (see App. E), the performance difference lemma, V ? (s 0 )?V ? (s 0 ) = 1 1?? E s?d ? s 0 E a??(?|s) [A ? (s, a)], hence if we ignore the gap between the states visitation distributions of ? (t) and ? (t+1) , L E (?, ?</p><formula xml:id="formula_22">Remark (Practical Implementation). Let L E (?, ? ? ) = E s?d ? ? 0 E a??(?|s) exp( A ? (s,?))/Z [A ? (s, a)], where ? = ? ? ? . Let L T (?, ? ? ) = J ? ? ? . If we optimize L T (?, ? ? ) by natural gradient,</formula><formula xml:id="formula_23">(t) ? ) ? 1 1?? E s?d ? ? 0 [V ? (t+1) (s) ? V ? (t) (s)], where ? (t) = ? ? (t) ? .</formula><p>Hence, E is actually putting more measure on ? that can achieve more improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We begin this section by describing our experimental setup. Then we report and analyze our SOTA results on ALE, specifically, 57 games, which are summarized and illustrated in App. J. To further investigate the mechanism of our algorithm, we study the effect of several major components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>The overall training architecture is on the top of the Learner-Actor framework <ref type="bibr" target="#b30">(Espeholt et al., 2018)</ref>, which supports large-scale training. Additionally, the recurrent encoder with LSTM (Schmidhuber, 1997) is used to handle the partially observable MDP problem <ref type="bibr">(Bellemare et al., 2013)</ref>. burn-in technique is adopted to deal with the representational drift as , and we train each sample twice. A complete description of the hyperparameters can be found in App. I. We employ additional environments to evaluate the scores during training, and the undiscounted episode returns averaged over 32 environments with different seeds have been recorded. Details on ALE and relevant evaluation criteria can be found in App. H.</p><p>To illustrate the generality and efficiency of GDI, we propose one implementation of GDI-I 3 and GDI-H 3 , respectively. Let ? = {?|? = (? 1 , ? 2 , )}. The behavior policy belongs to a soft -greedy policy space, which contains -greedy policy and Boltzmann policy. We define the behavior policy ? ? ? as</p><formula xml:id="formula_24">? = (? 1 , ? 2 , ), ? ? ? = ? ? Softmax A 1 ? 1 + (1 ? ?) ? Softmax A 2 ? 2<label>(2)</label></formula><p>For GDI-I 3 , A 1 and A 2 are identical, so it is estimated by an isomorphic family of trainable variables. The learning policy is also ? ? ? . For GDI-H 3 , A 1 and A 2 are different, and they are estimated by two different families of trainable variables. Since GDI needn't assume A 1 and A 2 are learned from the same MDP, so we use two kinds of reward shaping to learn A 1 and A 2 respectively, which can be found in App. I.2. Full algorithm can be found in App. F.</p><p>The operator T is achieved by policy gradient, V-Trace and ReTrace <ref type="bibr" target="#b30">(Espeholt et al., 2018;</ref><ref type="bibr" target="#b7">Munos et al., 2016</ref>) (see App. B), which meets Theorem 1 by first order optimization.</p><p>The operator E, which optimizes P ? , is achieved by a variant of Multi-Arm Bandits (Sutton, Barto, 2018, MAB), where Assumption 2 holds naturally. More details can be found in App. G.  We construct a multivariate evaluation system to emphasize the superiority of our algorithm in all aspects, and more discussions on those evaluation criteria are in App. C and details are in App. H. Furthermore, to avoid any issues that aggregated metrics may have, App. J provides full learning curves for all games, as well as detailed comparison tables of raw and normalized scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Summary of Results</head><p>The aggregated results across games are reported in Tab. 1. Our agents obtain the highest mean HNS with an extraordinary learning efficiency from this table. Furthermore, our agents have achieved 22 human world record breakthroughs and more than 90 times the average human score of Atari games via playing from scratch for less than 1.5 months. Although Agent57 obtains the highest median HNS, it costs each of the agents more than 57 years to obtain such performance, revealing its low learning efficiency. It is obvious that there is no such world record achieved by a human who played for over 57 years. This is due to the fact that Agent57 fails to handle the balance between exploration and exploitation, thus collecting a large number of inferior samples, which further hinders the efficient-learning and makes it harder for policy improvement. Other algorithms gain higher learning efficiency than Agent57 but relatively lower final performance, such as NGU and R2D2, which acquire over 10B frames. Except for median HNS, our performance is better on all criteria than NGU and R2D2. In addition, other algorithms with 200M training frames are struggling to match our performance.</p><p>These results come from the following aspects:</p><p>1. Several games have been solved completely, achieving the historically highest score, such as RoadRunner, Seaquest, Jamesbond.</p><p>2. Massive games show enormous potentialities for improvement but fail to converge for lack of training, such as BeamRider, BattleZone, SpaceInvaders.</p><p>3. This paper aims to illustrate that GDI is general for seeking a suitable balance between exploration and exploitation, so we refuse to adopt any handcrafted and domain-specific tricks such as the intrinsic reward. Therefore, we suffer from the hard exploration problem, such as PrivateEye, Surround, Amidar.</p><p>Therefore, there are several aspects of potential improvement. For example, a more extensive training scale may benefit higher performance. More exploration techniques can be incorporated into GDI to handle those hard-exploration problems through guiding the direction of the acquired samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In the ablation study, we further investigate the effects of several properties of GDI. We set GDI-I 3 and GDI-H 3 as our baseline control group. To prove the effects of the data distribution optimization operator E, we set two ablation groups, which are Fixed Selection from GDI-I 0 w/o E and Random Selection from GDI-I 3 w/o E. To prove the capacity of the behavior policy space matters in GDI, we set two ablation groups, which are -greedy Selection ? = {?|? = ( )} and Boltzmann Selection ? = {?|? = (? )}. Both -greedy Selection and Boltzmann Selection implement E by the same MAB as our baselines'. More details on ablation study can see App. K.1.</p><p>From results in App. K.2, it is evident that both the data distribution optimization operator E and the capacity of the behavior policy space are critical. This is since if they lack the cognition to identify suitable experiences from various data, high variance and massive poor experiences will hinder the policy improvement, and if the RL agents lack the vision to find more examples to learn, they may ignore some shortcuts. To further prove the capacity of the policy space does bring more diverse data, we draw the t-SNE of GDI-I 3 , GDI-H 3 and Boltzmann Selection in App. K.3, from which we see GDI-I 3 and GDI-H 3 can explore more high-value states that Boltzmann selection has less chance to find. We also evaluate Fixed Selection and Boltzmann Selection in all 57 Atari games, and recorded the comparison tables of raw and normalized scores in App. K.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes a novel RL paradigm to effectively and adaptively trade-off the exploration and exploitation, integrating the data distribution optimization into the generalized policy iteration paradigm. Under this paradigm, we propose feasible implementations, which both have achieved new SOTA among all 200M scale algorithms on all evaluation criteria and obtained the best mean final performance and learning efficiency compared with all 10B+ scale algorithms. Furthermore, we have achieved 22 human world record breakthroughs within less than 1.5 months of game time. It implies that our algorithm obtains both superhuman learning performance and human-level learning efficiency. In the experiment, we discuss the potential improvement of our method in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Abbreviation and Notation</head><p>In this Section, we briefly summarize some common notations and abbreviations in our paper for the convenience of readers, which is illustraed in Tab. 2 and Tab. 3.  the states visitation distribution of ? with the initial state distribution ? 0 J ? the expectation of the returns with the states visitation distribution of ? V ? the state value function of ? Q ? the state-action value function of ? ? discount-rate parameter ? t temporal-difference error at t ? set of indexes ? one index in ? P ? one probability measure on ? ? set of all possible parameter values ? one parameter value in ? ? ? a subset of ?, indicates the parameter in ? being used by the index ? X set of samples x one sample in X D set of all possible states visitation distributions E the data distribution optimization operator T the RL algorithm optimization operator L E the loss function of E to be maximized, calculated by the samples set X L E expectation of L E , with respect to each sample x ? X L T the loss function of T to be maximized, calculated by the samples set X L T expectation of L T , with respect to each sample x ? X</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Background on RL</head><p>The RL problem can be formulated by a Markov decision process <ref type="bibr">(Howard, 1960, MDP)</ref> defined by the tuple (S, A, p, r, ?, ? 0 ). Considering a discounted episodic MDP, the initial state s 0 will be sampled from the distribution denoted by ? 0 (s) : S ? ?(S). At each time t, the agent choose an action a t ? A according to the policy ?(a t |s t ) : S ? ?(A) at state s t ? S. The environment receives the action, produces a reward r t ? r(s, a) : S ? A ? R and transfers to the next state s t+1 submitted to the transition distribution p (s | s, a) : S ? A ? ?(S). The process continues until the agent reaches a terminal state or a maximum time step. Define return</p><formula xml:id="formula_25">G t = ? k=0 ? k r t+k , state value function V ? (s t ) = E ? k=0 ? k r t+k |s t , state-action value function Q ? (s t , a t ) = E ? k=0 ? k r t+k |s t , a t , and advantage function A ? (s t , a t ) = Q ? (s t , a t ) ? V ? (s t ),</formula><p>wherein ? ? (0, 1) is the discount factor. The connections between V ? and Q ? is given by the Bellman equation,</p><formula xml:id="formula_26">T Q ? (s t , a t ) = E ? [r t + ?V ? (s t+1 )], where V ? (s t ) = E ? [Q ? (s t , a t )]</formula><p>. The goal of reinforcement learning is to find the optimal policy ? * that maximizes the expected sum of discounted rewards, denoted by J (Sutton, <ref type="bibr" target="#b21">Barto, 2018)</ref>:</p><formula xml:id="formula_27">? * = argmax ? J ? (? ) = argmax ? E ? [G t ] = argmax ? E ? [ ? k=0 ? k r t+k ]</formula><p>Model-free reinforcement learning (MFRL) has made many impressive breakthroughs in a wide range of Markov decision processes <ref type="bibr" target="#b24">(Vinyals et al., 2019;</ref><ref type="bibr" target="#b10">Pedersen, 2019;</ref><ref type="bibr">Badia et al., 2020a, MDP)</ref>. MFRL mainly consists of two categories, valued-based methods <ref type="bibr" target="#b6">(Mnih et al., 2015;</ref><ref type="bibr">Hessel et al., 2017)</ref> and policy-based methods <ref type="bibr" target="#b18">(Schulman et al., 2015</ref><ref type="bibr" target="#b19">(Schulman et al., , 2017b</ref><ref type="bibr" target="#b30">Espeholt et al., 2018)</ref>.</p><p>Value-based methods learn state-action values and select actions according to these values. One merit of value-based methods is to accurately control the exploration rate of the behavior policies by some trivial mechanism, such like -greedy. The drawback is also apparent. The policy improvement of valued-based methods totally depends on the policy evaluation. Unless the selected action is changed by a more accurate policy evaluation, the policy won't be improved. So the policy improvement of each policy iteration is limited, which leads to a low learning efficiency. Previous works equip valued-based methods with many appropriated designed structures, achieving a more promising learning efficiency <ref type="bibr" target="#b25">(Wang et al., 2016;</ref><ref type="bibr" target="#b12">Schaul et al., 2015;</ref><ref type="bibr" target="#b0">Kapturowski et al., 2018)</ref>.</p><p>In practice, value-based methods maximize J by policy iteration <ref type="bibr" target="#b21">(Sutton, Barto, 2018)</ref>. The policy evaluation is fulfilled by minimizing E ? [(G ? Q ? ) 2 ], which gives the gradient ascent direction E ? [(G ? Q ? )?Q ? ]. The policy improvement is usually achieved by -greedy. Q-learning is a typical value-based methods, which updates the state-action value function Q(s, a) with Bellman Optimality Equation <ref type="bibr" target="#b26">(Watkins, Dayan, 1992)</ref>:</p><formula xml:id="formula_28">? t = r t+1 + ? arg max a Q (s t+1 , a) ? Q (s t , a t ) Q (s t , a t ) ? Q (s t , a t ) + ?? t</formula><p>wherein ? t is the temporal difference error <ref type="bibr" target="#b20">(Sutton, 1988)</ref>, and ? is the learning rate.</p><p>A refined structure design of Q ? is achieved by <ref type="bibr" target="#b25">(Wang et al., 2016)</ref>. It estimates Q ? by a summation of two separated networks, Q ? = A ? + V ? .</p><p>Policy gradient (Williams, 1992, PG) methods is an outstanding representative of policy-based RL algorithms, which directly parameterizes the policy and updates through optimizing the following objective:</p><formula xml:id="formula_29">J (?) = E ? ? t=0 log ? ? (a t | s t ) R(? )</formula><p>wherein R(? ) is the cumulative return on trajectory ? . In PG method, policy improves via ascending along the gradient of the above equation, denoted as policy gradient:</p><formula xml:id="formula_30">? ? J (? ? ) = E ? ?? ? ? t=0 ? ? log ? ? (a t | s t ) R(? )</formula><p>One merit of policy-based methods is that they incorporate a policy improvement phase every training step, suggesting a higher learning efficiency than value-based methods. Nevertheless, policy-based methods easily fall into a suboptimal solution, where the entropy drops to 0 (Haarnoja et al., 2018). The actor-critic methods introduce a value function as the baseline to reduce the variance of the policy gradient <ref type="bibr" target="#b5">(Mnih et al., 2016)</ref>, but maintain the other characteristics unchanged.</p><p>Actor-Critic (Sutton, Barto, 2018, AC) reinforcement learning updates the policy gradient with an value-based critic, which can reduce variance of estimates and thus ensure more stable and rapid optimization.</p><formula xml:id="formula_31">? ? J (?) = E ? ? t=0 ? t ? ? log ? ? (a t | s t )</formula><p>wherein ? t is the critic to guide the improvement directions of policy improvement, which can be the state-action value function Q ? (s t , a t ), the advantage function A ? (s t , a t ) = Q ? (s t , a t ) ? V ? (s t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Retrace</head><p>When large scale training is involved, the off-policy problem is inevitable. Denote ? to be the behavior policy, ? to be the target policy, and c t = min{ ?t ?t ,c} to be the clipped importance sampling. For brevity, denote c [t:t+k] = k i=0 c t+i . ReTrace <ref type="bibr" target="#b7">(Munos et al., 2016)</ref> estimates Q(s t , a t ) by clipped per-step importance sampling</p><formula xml:id="formula_32">Q?(s t , a t ) = E ? [Q(s t , a t ) + k?0 ? k c [t+1:t+k] ? Q t+k Q], where ? Q t Q def = r t + ?Q(s t+1 , a t+1 ) ? Q(s t , a t ).</formula><p>The above operator is a contraction mapping, and Q converges to Q? ReT race that corresponds to some? ReT race .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Vtrace</head><p>Policy-based methods maximize J by policy gradient. It's shown (Sutton, Barto, 2018) that ?J = E ? [G? log ?]. When involved with a baseline, it becomes an actor-critic algorithm such as</p><formula xml:id="formula_33">?J = E ? [(G ? V ? )? log ?], where V ? is optimized by minimizing E ? [(G ? V ? ) 2 ], i.e. gradient ascent direction E ? [(G ? V ? )?V ? ].</formula><p>IMPALA <ref type="bibr" target="#b30">(Espeholt et al., 2018)</ref> introduces V-Trace off-policy actor-critic algorithm to correct for the discrepancy between target policy and behavior policy.</p><formula xml:id="formula_34">Denote ? t = min{ ?t ?t ,?}. V-Trace estimates V (s t ) by V?(s t ) = E ? [V (s t ) + k?0 ? k c [t:t+k?1] ? t+k ? V t+k V ], where ? V t V def = r t + ?V (s t+1 ) ? V (s t ).</formula><p>Ifc ??, the above operator is a contraction mapping, and V converges to V? that corresponds t? ?(a|s) = min {??(a|s), ?(a|s)} b?A min {??(b|s), ?(b|s)} .</p><p>The policy gradient is given by</p><formula xml:id="formula_35">E ? ? t (r t + ?V?(s t+1 ) ? V (s t ))? log ? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Background on ALE</head><p>Human intelligence is able to solve many tasks of different natures. In pursuit of generality in artificial intelligence, video games have become an important testing ground: they require a wide set of skills such as perception, exploration and control. Reinforcement Learning is at the forefront of this development, especially when combined with deep neural networks in DRL.</p><p>The Arcade Learning Environment (Bellemare et al., 2013, ALE) was proposed as a platform for empirically assessing agents designed for general competency across a wide range of games. It provides many different tasks ranging from simple paddle control in the ball game Pong to complex labyrinth exploration in Montezuma's Revenge which remains unsolved by general algorithms up to today. ALE offers an interface to a diverse set of Atari 2600 game environments designed to be engaging and challenging for human players. As <ref type="bibr">(Bellemare et al., 2013)</ref> put it, the Atari 2600 games are well suited for evaluating general competency in AI agents for three main reasons:</p><p>1. Varied enough to claim generality.</p><p>2. Each interesting enough to be representative of settings that might be faced in practice.</p><p>3. Each created by an independent party to be free of experimenter's bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Human Normalized Score</head><p>Agents are expected to perform well in as many games as possible without the use of game-specific information. Deep Q-Networks <ref type="bibr">(Mnih et al., 2015, DQN)</ref> was the first algorithm to achieve humanlevel control in a large number of the Atari 2600 games, measured by human normalized scores <ref type="bibr">(Bellemare et al., 2013, HNS)</ref>. Subsequently, using HNS to assess performance on Atari games has become one of the most widely used benchmarks in deep reinforcement learning , despite the human baseline scores potentially underestimating human performance relative to what is possible .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Human World Records Baseline</head><p>Except for comparing with the average human performance, a more common way to evaluate AI for games is to let agents compete against human world champions. Recent examples for DRL include the victory of OpenAI Five on <ref type="bibr">Dota 2 (Berner et al., 2019)</ref> or AlphaStar versus Mana for <ref type="bibr">StarCraft 2 (Vinyals et al., 2019)</ref>. In the same spirit, one of the most used metric for evaluating RL agents on Atari is to compare them to the human baseline introduced by (Bellemare et al., 2013). Previous works use the normalized human score, i.e. 0% is the score of a random player and 100% is the score of the human baseline, which allows to summarize the performance on the whole Atari set in one number, instead of individually comparing raw scores for each of the 57 games. However, it's obvious that this human baseline is far from being representative of the best human player, which means that using it to claim superhuman performance is misleading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Human World Records Normalized Score</head><p>As (Toromanoff et al., 2019) said, previous claims of superhuman performance of RL might not be accurate owing to comparing with the averaged performance of normal human instead of the human world records, which means there are still massive games of Atari where human champions outperform the RL agents. Thus, we believe the human world records normalized score (HWRNS) can serve as a more suitable evaluation criterion than the origin human normalized score, which directly compare the RL agents with the best human performance. HWRNS of a Atari game surpass 100% proves the fact that the DRL agents surpass the human world records and actually surpass the human on that game. When the mean HWRNS surpass 100% we can say the RL agents can reach and even surpass the highest level of humanity, and then we can say our algorithms really achieve the superhuman level control. Recommended by , we also adopt the capped HWRNS that each HWRNS will be capped below 200% as a evaluation criterion to avoid argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Learning Efficiency</head><p>The goal of reinforcement learning is to achieve human level control. It is reflected in two aspects. On the one hand, the RL agents can reach and even surpass the human world records, which is the central focus of massive studies. On the other hand, we should not ignore the essential pursuit of reinforcement learning is to master human learning ability, which acquire the RL agents to not only learn how to do but also learn how to learn efficiently. For example, human can achieve one world records of Atari within only few years or even few months, however present SOTA RL algorithms like Agent57 acquires tens of years to achieve similar results, which implies the fact that there is still much room to improve the learning efficiency of reinforcement learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Atari Benchmark</head><p>Artificial intelligence (AI) in video games is a longstanding research area. It studies how to learn human-level and even surpassing-human-level agents when playing video games. The Arcade Learning Environment (Bellemare et al., 2013, ALE) is a universal experiment platform for empirically assessing the general competency of agents across a wide range of games. In addition, ALE offers an interface to a diverse set of Atari 2600 game environments designed to engage and challenge human players. Agents are expected to perform well in as many games as possible without the use of game-specific information.</p><p>Since </p><formula xml:id="formula_36">Q? (s t , a t ) = E ? [Q (s t , a t ) + k?0 ? k c [t+1:t+k] ? Q t+k Q]<label>(3)</label></formula><p>where</p><formula xml:id="formula_37">c t = min ?t ?t ,c , c [t:t+k] = k i=0 c t+i and ? Q t Q def = r t + ?Q (s t+1 , a t+1 ) ? Q (s t , a t ).</formula><p>At the same time, PG methods is also booming, wherein AC framework is one of the brightest pearls. Asynchronous advantage actor-critic (Mnih et al., 2016, A3C) introduces a novel asynchronous training with several actors, wherein an entropy regularization term is introduced into the objective function to encourage the exploration. Importance-Weighted Actor Learner Architecture (Espeholt et al., 2018, IMPALA) is a novel large scale distributed training framework, which achieves stable learning by combining decoupled acting and learning with a novel V-trace off-policy correction method to estimate V (s):</p><formula xml:id="formula_38">V? (s t ) = E ? [V (s t ) + k?0 ? k c [t:t+k?1] ? t+k ? V t+k V ]<label>(4)</label></formula><p>where</p><formula xml:id="formula_39">? t = min ?t ?t ,? , ? V t V def = r t + ?V (s t+1 ) ? V (s t ).</formula><p>IMPALA reaches a new SOTA of policy-based methods on ALE. However, there still exist some hard-to-explore games with long horizon and sparse reward, like Montezuma's Revenge, which need better exploration ability, namely, a breakthrough on the method.</p><p>Go-Explore (Ecoffet et al., 2019) learns exploration and robustification separately, and achieves huge breakthroughs on games which acquire massive exploration. However, there still exist some extremely hard games like Skiing where the average human performs better than RL agents. Agent57 <ref type="bibr" target="#b30">(Badia et al., 2020a)</ref> firstly surpasses the average human performance in all 57 games, which is marked as a new milestone on ALE. Nevertheless, the breakthrough is achieved at the expense of tremendous training samples, called the low learning efficiency problem, which hinders the application of the method into real-world problems.</p><p>For solving the low learning efficiency problem, model-based methods are regarded as one solution. MuZero <ref type="bibr" target="#b31">(Schrittwieser et al., 2020)</ref> is based on the frameworks of AlphaZero, which combines MCTS with a learned model to make planning. It extends model-based RL to a range of logically complex and visually complex domains, and achieves a SOTA performance.</p><p>Unfortunately, both value-based SOTA method RAINBOW, policy-based SOTA method IMPALA, model-free SOTA method Agent57 and the model-based SOTA method MuZero fail to synchronously guarantee the learning efficiency and the final performance.</p><p>We concluded the SOTA results on the Atari benchmark and the corresponding learning efficiency in <ref type="figure">Figure 3</ref>. It's seen that our method reaches a new SOTA on both mean HNS and learning efficiency. Our final performance is competitive with the best model-free algorithm Agent57, and simultaneously achieves a better learning efficiency than the best model-based algorithm Muzero.          </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Theoretical Proof</head><p>For a monotonic sequence of numbers which satisfies a = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; b, we call it a split of interval <ref type="bibr">[a, b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 (Discretized Upper Triangular Transport Inequality for Increasing Functions in</head><formula xml:id="formula_40">R 1 ). Assume ? is a continuous probability measure supported on [0, 1]. Let 0 = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1 to be any split of [0, 1]. Define?(x i ) = ?([x i , x i+1 )). Defin? ?(x i ) =?(x i ) exp(x i )/Z, Z = i? (x i ) exp(x i ).</formula><p>Then there exists a probability measure ? :</p><formula xml:id="formula_41">{x i } i=0,...,n ? {x i } i=0,...,n ? [0, 1], s.t. ? ? ? ? ? ? ? ? ? ? ? ? ? j ?(x i , y j ) =?(x i ), i = 0, . . . , n; i ?(x i , y j ) =?(y j ), j = 0, . . . , n; ?(x i , y j ) = 0, i &gt; j.<label>(5)</label></formula><p>Then for any monotonic increasing function f :</p><formula xml:id="formula_42">{x i } i=0,...,n ? R, we have E?[f ] ? E?[f ].</formula><p>Proof of Lemma 1. For any couple of measures (?, ?), we say the couple satisfies Upper Triangular Transport Condition (UTTC), if there exists ? s.t. <ref type="formula" target="#formula_41">(5)</ref> holds.</p><formula xml:id="formula_43">Given 0 = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1, we prove the existence of ? by induction. Define? m (x i ) = ? ? ? ?([x i , x i+1 )), i &lt; m, ?([x i , 1)), i = m, 0, i &gt; m. Define? m (x i ) =? m (x i ) exp(x i )/Z m , Z m = i? m (x i ) exp(x i ).</formula><p>Noting if we prove that (? m ,? m ) satisfies UTTC for m = n, it's equivalent to prove the existence of ? in <ref type="formula" target="#formula_41">(5)</ref>.</p><p>To clarify the proof, we use x i to represent the point for?-axis in coupling and y j to represent the point for?-axis, but they are actually identical, i.e. x i = y j when i = j.</p><p>When m = 0, it's obvious that (? 0 ,? 0 ) satisfies UTTC, as</p><formula xml:id="formula_44">? 0 (x i , y j ) = 1, i = 0, j = 0, 0, else.</formula><p>Assume UTTC holds for m, i.e. there exists ? m s.t. (? m ,? m ) satisfies UTTC, we want to prove it also holds for m + 1.</p><p>By definition of? m , we have</p><formula xml:id="formula_45">? ? ?? m (x i ) =? m+1 (x i ), i &lt; m, ? m (x i ) =? m+1 (x i ) +? m+1 (x i+1 ), i = m, ? m (x m+1 ) =? m (x i ) =? m+1 (x i ) = 0, i &gt; m + 1.</formula><p>By definition of? m , we have</p><formula xml:id="formula_46">? ? ? ? ? ? ? ? ? ? ?? m (x i ) =? m+1 (x i ) ? Z m+1 Z m , i &lt; m, ? m (x i ) = ? m+1 (x i ) +? m+1 (x i+1 ) exp(x i ? x i+1 ) ? Z m+1 Z m , i = m, ? m (x m+1 ) =? m (x i ) =? m+1 (x i ) = 0, i &gt; m + 1.</formula><p>Multiplying ? m by Zm Zm+1 , we get the following UTTC</p><formula xml:id="formula_47">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? j Z m Z m+1 ? m (x i , y j ) = Z m Z m+1? m+1 (x i ), i &lt; m; j Z m Z m+1 ? m (x i , y j ) = Z m Z m+1 (? m+1 (x i ) +? m+1 (x i+1 )), i = m; j Z m Z m+1 ? m (x i , y j ) = 0, i = m + 1; j Z m Z m+1 ? m (x i , y j ) =? m+1 (x i ) = 0, i &gt; m + 1; i Z m Z m+1 ? m (x i , y j ) =? m+1 (y j ), j &lt; m; i Z m Z m+1 ? m (x i , y j ) =? m+1 (y i ) +? m+1 (y j+1 ) exp(y j ? y j+1 ), j = m; i Z m Z m+1 ? m (x i , y j ) = 0, j = m + 1; i Z m Z m+1 ? m (x i , y j ) =? m+1 (y j ) = 0, j &gt; m + 1; Z m Z m+1 ? m (x i , y j ) = 0, i &gt; j. By definition of Z m , Z m+1 ? Z m =? m+1 (x m+1 )(exp(x m+1 ) ? exp(x m )) &gt; 0,<label>(6)</label></formula><formula xml:id="formula_48">so we have Zm Zm+1? m+1 (x i ) &lt;? m+1 (x i ). Noticing that? m+1 (y i+1 ) exp(y i ? y i+1 ) &lt;? m+1 (y i+1 ) and Zm Zm+1? m+1 (x i ) &lt;? m+1 (x i ), we decompose the measure of Zm Zm+1 ? m at (x i , y m ) to (x i , y m )</formula><p>, (x i , y m+1 ) for i = 0, . . . , m ? 1, and complement a positive measure at (x i , y m+1 ) to make up the difference between Zm Zm+1? m+1 (x i ) and? m+1 (x i ). For i = m, we decompose the measure at (x m , y m ) to (x m , y m ), (x m , y m+1 ), (x m+1 , y m+1 ) and also complement a proper positive measure.</p><formula xml:id="formula_49">Now we define ? m+1 by ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? m+1 (x i , y j ) = Z m Z m+1 ? m (x i , y j ), i &lt; m and j &lt; m, ? m+1 (x i , y j ) = Z m Z m+1 ? m (x i , y j ) + Z m+1 ? Z m Z m+1? m+1 (x i ) ?? m+1 (y j ) ? m+1 (y j ) +? m+1 (y j+1 ) , i &lt; m and j = m, ? m+1 (x i , y j ) = Z m Z m+1 ? m (x i , y j ) + Z m+1 ? Z m Z m+1? m+1 (x i ) ?? m+1 (y j+1 ) ? m+1 (y j ) +? m+1 (y j+1 )</formula><p>, i &lt; m and j = m + 1,</p><formula xml:id="formula_50">? m+1 (x i , y j ) = 0, i &gt; j or i &gt; m + 1 or j &gt; m + 1, ? m+1 (x m , y m ) = u, ? m+1 (x m , y m+1 ) = v, ? m+1 (x m+1 , y m+1 ) = w,</formula><p>where we assume u, v, w to be the solution of the following equations</p><formula xml:id="formula_51">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? u + v + w =? m+1 (x m ) +? m+1 (x m+1 ), w u + v =? m+1 (x m+1 ) ? m+1 (x m ) , v + w u =? m+1 (x m+1 ) ? m+1 (x m ) , u, v, w ? 0.<label>(7)</label></formula><p>It's obvious that</p><formula xml:id="formula_52">? ? ? ? ? ? ? ? ? ? ? ? ? j ? m+1 (x i , y j ) =? m+1 (x i ) = 0, i &gt; m + 1, i ? m+1 (x i , y j ) =? m+1 (y j ) = 0, j &gt; m + 1, ?(x i , y j ) = 0, i &gt; j. For j &lt; m, since i Zm Zm+1 ? m (x i , y j ) =? m+1 (y j ), we have i ? m+1 (x i , y j ) =? m+1 (y j ), j &lt; m. For i &lt; m, since j Zm Zm+1 ? m (x i , y j ) = Zm Zm+1? m+1 (x i ) &lt;? m+1 (x i ), we add Zm+1?Zm Zm+1? m+1 (x i )? m+1(ym) ?m+1(ym)+?m+1(ym+1) , Zm+1?Zm Zm+1? m+1 (x i )? m+1(ym+1) ?m+1(ym)+?m+1(ym+1) to ? m+1 (x i , y m ), ? m+1 (x i , y m+1 ), respectively. So we have j ? m+1 (x i , y j ) =? m+1 (x i ), i &lt; m.</formula><p>For i = m, m + 1, since assumption (7) holds, we have u</p><formula xml:id="formula_53">+ v + w =? m+1 (x m ) + ? m+1 (x m+1 ), w u+v =? m+1(xm+1) ?m+1(xm) , it's obvious that u + v =? m+1 (x m ), w =? m+1 (x m+1 ), which is j ? m+1 (x i , y j ) =? m+1 (x i ), i = m, m + 1.</formula><p>For j = m, m + 1, we firstly have</p><formula xml:id="formula_54">j=m,m+1 i ? m+1 (x i , y j ) = j i ? m+1 (x i , y j ) ? j =m,m+1 i ? m+1 (x i , y j ) = i j ? m+1 (x i , y j ) ? j =m,m+1? m+1 (y j ) = i? m+1 (x i ) ? j =m,m+1? m+1 (y j ) = 1 ? (1 ?? m+1 (y m ) ?? m+1 (y m+1 )) =? m+1 (y m ) +? m+1 (y m+1 ).</formula><p>By definition of ? m+1 , we know ?m+1(xi,ym) ?m(xi,ym) =? m+1(xm+1) ?m+1(xm) for i &lt; m. By assumption <ref type="formula" target="#formula_51">(7)</ref>, we know v+w u =? m+1(xm+1) ?m+1(xm) . Combining three equations above together, we have</p><formula xml:id="formula_55">i ? m+1 (x i , y j ) =? m+1 (y j ), j = m, m + 1.</formula><p>Now we only need to prove assumption <ref type="formula" target="#formula_51">(7)</ref> holds. With linear algebra, we solve <ref type="formula" target="#formula_51">(7)</ref> and have</p><formula xml:id="formula_56">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? u = w 1 +? m+1(xm+1) ?m+1(xm) ?m+1(xm+1) ?m+1(xm) 1 +? m+1(xm+1) ?m+1(xm) , v = w? m+1(xm+1) ?m+1(xm) ?? m+1(xm+1) ?m+1(xm) ?m+1(xm+1) ?m+1(xm) 1 +? m+1(xm+1) ?m+1(xm) , w = (? m+1 (x m ) +? m+1 (x m+1 ))? m+1(xm+1) ?m+1(xm) 1 +? m+1(xm+1) ?m+1(xm) 1 +? m+1(xm+1) ?m+1(xm) 1 +? m+1(xm+1) ?m+1(xm)</formula><p>.</p><p>It's obvious that u, w ? 0. v ? 0 also holds, becaus?</p><formula xml:id="formula_57">? m+1 (x m+1 ) ? m+1 (x m ) ?? m+1 (x m+1 ) ? m+1 (x m ) =? m+1 (x m+1 ) exp(x m+1 ) ? m+1 (x m ) exp(x m ) ?? m+1 (x m+1 ) ? m+1 (x m ) =? m+1 (x m+1 ) ? m+1 (x m ) (exp(x m+1 ? x m ) ? 1) ? 0.<label>(8)</label></formula><p>So we can find a proper solution of assumption <ref type="formula" target="#formula_51">(7)</ref>.</p><p>So ? m+1 defined above satisfies UTTC for (? m+1 ,? m+1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By induction, for any</head><formula xml:id="formula_58">0 = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1, there exists ? s.t. UTTC (5) holds for (?,?).</formula><p>Then for any monotonic increasing function, since ?( </p><formula xml:id="formula_59">x i , y j ) = 0 when i &gt; j, we know ?(x i , y j )f (x i ) ? ?(x i , y j )f (y j ). Hence we have E?[f ] = i? (x i )f (x i ) = i j ?(x i , y j )f (x i ) ? i j ?(x i , y j )f (y j ) = j i ?(x i , y j )f (y j ) = j? (y j )f (y j ) = E?[f ].</formula><formula xml:id="formula_60">= x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1 to be any split of [0, 1]. Let f, g : {x i } i=0,.</formula><p>..,n ? R to be two co-monotonic functions that satisfy</p><formula xml:id="formula_61">(f (x i ) ? f (x j )) ? (g(x i ) ? g(x j )) ? 0, ? i, j. Define?(x i ) = ?([x i , x i+1 )). Defin? ?(x i ) =?(x i ) exp(g(x i ))/Z, Z = i? (x i ) exp(g(x i )).</formula><p>Then we have</p><formula xml:id="formula_62">E?[f ] ? E?[f ].</formula><p>Proof of Lemma 2. If the Upper Triangular Transport Condition (UTTC) holds for (?,?), i.e. there exists a probability measure ? :</p><formula xml:id="formula_63">{x i } i=0,...,n ? {x i } i=0,...,n ? [0, 1], s.t. ? ? ? ? ? ? ? ? ? ? ? ? ? j ?(x i , y j ) =?(x i ), i = 0, . . . , n; i ?(x i , y j ) =?(y j ), j = 0, . . . , n; ?(x i , y j ) = 0, g(x i ) &gt; g(y j ),</formula><p>then we finish the proof by</p><formula xml:id="formula_64">E?[f ] = i? (x i )f (x i ) = i j ?(x i , y j )f (x i ) ? i j ?(x i , y j )f (y j ) = j i ?(x i , y j )f (y j ) = j? (y j )f (y j ) = E?[f ], where ?(x i , y j )f (x i ) ? ?(x i , y j )f (y j ) is because of ?(x i , y j ) = 0, g(x i ) &gt; g(y j ) and (f (x i ) ? f (x j )) ? (g(x i ) ? g(x j )) ? 0.</formula><p>Now we only need to prove UTTC holds for (?,?).</p><p>Given 0 = x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1, we prove the existence of ? by induction. With g to be the transition function in the definition of?, we mimic the proof of Lemma 1 and sort (x 0 , . . . , x n ) in the increasing order of g, which is</p><formula xml:id="formula_65">g(x k0 ) ? g(x k1 ) ? ? ? ? ? g(x kn ). Defin? ? m (x ki ) = ? ? ? ?([x ki , min{1, x k l | x k l &gt; x ki , l ? m})), i ? m, x ki = min{x k l | l ? m}, ?([0, min{1, x k l | x k l &gt; x ki , l ? m})), i ? m, x ki = min{x k l | l ? m}, 0, i &gt; m. Define? m (x ki ) =? m (x ki ) exp(g(x ki ))/Z m , Z m = i? m (x ki ) exp(g(x ki )).</formula><p>To clarify the proof, we use x ki to represent the point for?-axis in coupling and y kj to represent the point for?-axis, but they are actually identical, i.e. x ki = y kj when i = j.</p><p>When m = 0, it's obvious that (? 0 ,? 0 ) satisfies UTTC, as ? 0 (x ki , y kj ) = 1, i = 0, j = 0, 0, else.</p><p>Assume UTTC holds for m, i.e. there exists ? m s.t. (? m ,? m ) satisfies UTTC, we want to prove it also holds for m + 1.</p><formula xml:id="formula_66">When x km+1 &gt; min{x k l | l ? m}, let x k * = max{x k l | x k l &lt; x km+1 , l ? m} to be the closest left neighbor of x km+1 in {x k l | l ? m}. Then we have? m (x k * ) =? m+1 (x k * ) +? m+1 (x k m+1 ). When x km+1 &lt; min{x k l | l ? m}, let x k * = min{x k l | l ? m} to be the leftmost point in {x k l | l ? m}. Then we have? m (x k * ) =? m+1 (x k * ) +? m+1 (x k m+1 ).</formula><p>In either case, we always have?</p><formula xml:id="formula_67">m (x k * ) =? m+1 (x k * ) +? m+1 (x km+1 ). By definition of? m and ? m , we have ? ? ?? m (x ki ) =? m+1 (x ki ), i ? m, k i = k * , ? m (x ki ) =? m+1 (x ki ) +? m+1 (x km+1 ), i ? m, k i = k * , ? m (x km+1 ) =? m (x ki ) =? m+1 (x ki ) = 0, i &gt; m + 1, ? ? ? ? ? ? ? ? ? ? ?? m (x ki ) =? m+1 (x ki ) ? Z m+1 Z m , i ? m, k i = k * , ? m (x ki ) = ? m+1 (x ki ) +? m+1 (x km+1 ) exp g(x ki ) ? g(x km+1 ) ? Z m+1 Z m , i ? m, k i = k * , ? m (x m+1 ) =? m (x i ) =? m+1 (x i ) = 0, i &gt; m + 1. If g(x k * ) = g(x km+1 ), it's easy to check that? m+1(xk m+1 ) ?m+1(x k * ) =? m+1(xk m+1 )</formula><p>?m+1(x k * ) , we can simply define the following ? m+1 which achieves UTTC for (? m+1 ,? m+1 ):</p><formula xml:id="formula_68">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? m+1 (x k * , y kj ) = ? m (x k * , y kj )? m+1 (x k * ) ? m+1 (x k * ) +? m+1 (x km+1 ) , j ? m, k j = k * , ? m+1 (x km+1 , y kj ) = ? m (x km+1 , y kj )? m+1 (x km+1 ) ? m+1 (x k * ) +? m+1 (x km+1 ) , j ? m, k j = k * , ? m+1 (x ki , y k * ) = ? m (x ki , y k * )? m+1 (y k * ) ? m+1 (y k * ) +? m+1 (y km+1 ) , i ? m, k i = k * , ? m+1 (x ki , y km+1 ) = ? m (x ki , y km+1 )? m+1 (y km+1 ) ? m+1 (y k * ) +? m+1 (x km+1 ) , i ? m, k i = k * , ? m+1 (x k * , y k * ) = ? m (x k * , y k * )? m+1 (x k * ) ? m+1 (x k * ) +? m+1 (x km+1 ) , ? m+1 (x km+1 , y km+1 ) = ? m (x km+1 , y km+1 )? m+1 (x km+1 ) ? m+1 (x k * ) +? m+1 (x km+1 ) , ? m+1 (x ki , y kj ) = 0, others. If g(x k * ) &lt; g(x km+1 )</formula><p>, recalling the proof of Lemma 1, it's crucial to prove inequalities <ref type="formula" target="#formula_47">(6)</ref> and <ref type="formula" target="#formula_57">(8)</ref>. Inequality <ref type="bibr">(6)</ref> guarantees that Zm Zm+1 &lt; 1, so we can shrinkage ? m entrywise by Zm Zm+1 and add some proper measure at proper points. Inequality (8) guarantees that (x m , y m ) can be decomposed to (x m , y m ), (x m , y m+1 ), (x m+1 , y m+1 ). Following the idea, we check that</p><formula xml:id="formula_69">Z m+1 ? Z m =? m+1 (x km+1 ) exp(g(x km+1 ) ? g(x k * )) &gt; 0, ? m+1 (x km+1 ) ? m+1 (x k * ) ?? m+1 (x km+1 ) ? m+1 (x k * ) =? m+1 (x km+1 ) exp(g(x km+1 )) ? m+1 (x k * ) exp(g(x k * )) ?? m+1 (x km+1 ) ? m+1 (x k * ) =? m+1 (x km+1 ) ? m+1 (x k * ) exp(g(x km+1 ) ? g(x k * )) ? 1 &gt; 0.</formula><p>Replacing x m , x m+1 in the proof of Lemma 1 by x k * , x km+1 , we can construct ? m+1 all the same way as in the proof of Lemma 1.</p><p>By induction, we prove UTTC for (?,?). The proof is done. </p><formula xml:id="formula_70">(f (x) ? f (y)) ? (g(x) ? g(y)) ? 0, ? x, y ? [0, 1]. f is continuous. Define ?(x) = ?(x) exp(g(x))/Z, Z = [0,1] ?(x) exp(g(x)</formula><p>).</p><p>Then we have</p><formula xml:id="formula_71">E ? [f ] ? E ? [f ]. Proof of Theorem 3. For ? &gt; 0, since f is continuous, f is uniformly continuous, so there exists ? &gt; 0 s.t. |f (x) ? f (y)| &lt; , ?x, y ? [0, 1]. We can split [0, 1] by 0 &lt; x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1 s.t. x i+1 ? x i &lt; ?. Define? and? as in Lemma 2. Since x i+1 ? x i &lt; ?,</formula><p>by uniform continuity and the definition of the expectation, we have</p><formula xml:id="formula_72">|E ? [f ] ? E?[f ]| &lt; , |E ? [f ] ? E?[f ]| &lt; , By Lemma 2, we have E?[f ] ? E?[f ]. So we have E ? [f ] &lt; E?[f ] + ? E?[f ] + &lt; E ? [f ] + 2 .</formula><p>Since is arbitrary, we prove </p><formula xml:id="formula_73">E ? [f ] ? E ? [f ].</formula><formula xml:id="formula_74">. Let 0 = x d 0 &lt; x d 1 &lt; ? ? ? &lt; x d n &lt; 1 to be any split of [0, 1], d = 1, . . . , p. Denote x i def = (x 1 i1 , . . . , x p ip ). Define?(x i ) = ?( d=1,...,p [x d i d , x d i d +1 )). Let f, g : {x i } i?{0,...,n} p ? R to be two co-monotonic functions that satisfy (f (x i ) ? f (x j )) ? (g(x i ) ? g(x j )) ? 0, ? i, j.</formula><p>Define?</p><formula xml:id="formula_75">(x i ) =?(x i ) exp(g(x i ))/Z, Z = i? (x i ) exp(g(x i )).</formula><p>Then there exists a probability measure ? :</p><formula xml:id="formula_76">{x i } i?{0,...,n} p ? {x j } j?{0,...,n} p ? [0, 1], s.t. j ?(x i , y j ) =?(x i ), ? i; i ?(x i , y j ) =?(y j ), ? j; ?(x i , y j ) = 0, g(x i ) &gt; g(y j ).</formula><p>Then we have</p><formula xml:id="formula_77">E?[f ] ? E?[f ].</formula><p>Proof of Lemma 3. The proof is almost identical to the proof of Lemma 2, except for the definition of (? m ,? m ) in R p .</p><p>Given {x i } i?{0,...,n} p , we sort x i in the increasing order of g, which is</p><formula xml:id="formula_78">g(x k0 ) ? g(x k1 ) ? ? ? ? ? g(x k (n+1) p ?1 ), where {k i } i?{0,...,(n+1) p ?1} is a permutation of {i} i?{0,...,n} p . For i, j ? {0, . . . , n} p , we define the partial order i &lt; j on {0, . . . , n} p , if ?0 ? d 0 ? n, s.t. i d ? j d , ?d &lt; d 0 and i d0 &lt; j d0 . It's obvious that ? ? ? ?i ? {0, . . . , n} p , i ? i, ?i, j ? {0, . . . , n} p , i &lt; j ? j ? i, ?i, j, k ? {0, . . . , n} p , i &lt; j, j &lt; k ? i &lt; k. We define i = j if i d = j d , ? 0 ? d ? n.</formula><p>So we define the partial order relation, and we can further define the min function and the max function on {0, . . . , n} p . Now using this partial order relation, we defin?</p><formula xml:id="formula_79">? m (x ki ) = ? ? ? ? ? ? ? ? ? ? ? ? ? k?ki,k&lt;min{k l |k l &gt;ki,l?m}? (x k ), i ? m, k i = min{k l | l ? m}, k&lt;min{k l |k l &gt;ki,l?m}? (x k ), i ? m, k i = min{k l | l ? m}, 0, i &gt; m.</formula><p>With this definition of? m , other parts are identical to the proof of Lemma 2. The proof is done. </p><formula xml:id="formula_80">(f (x) ? f (y)) ? (g(x) ? g(y)) ? 0, ? x, y ? [0, 1] p . f is continuous. Define ?(x) = ?(x) exp(g(x))/Z, Z = [0,1] p ?(x) exp(g(x)</formula><p>).</p><p>Let f, g : [0, 1] p ? R to be two co-monotonic functions that satisfy</p><formula xml:id="formula_81">(f (x) ? f (y)) ? (g(x) ? g(y)) ? 0, ? x, y ? [0, 1] p .</formula><p>Then we have</p><formula xml:id="formula_82">E ? [f ] ? E ? [f ].</formula><p>Proof of Theorem 4. For ? &gt; 0, since f is continuous, f is uniformly continuous, so there exists</p><formula xml:id="formula_83">? &gt; 0 s.t. |f (x) ? f (y)| &lt; , ?x, y ? [0, 1] p . We can split [0, 1] by 0 &lt; x 0 &lt; x 1 &lt; ? ? ? &lt; x n &lt; 1 s.t. x i+1 ? x i &lt; ?/ ? p. Define x d i = x i , ?0 ? d ? p. Define? and? as in Lemma 3. Since x i+1 ? x i &lt; ?/ ? p, |(x 0 i+1 , . . . , x p i+1 ) ? (x 0 i , . .</formula><p>. , x p i )| &lt; ?, by uniform continuity and the definition of the expectation, we have</p><formula xml:id="formula_84">|E ? [f ] ? E?[f ]| &lt; , |E ? [f ] ? E?[f ]| &lt; , By Lemma 3, we have E?[f ] ? E?[f ]. So we have E ? [f ] &lt; E?[f ] + ? E?[f ] + &lt; E ? [f ] + 2 .</formula><p>Since is arbitrary, we prove</p><formula xml:id="formula_85">E ? [f ] ? E ? [f ].</formula><p>Lemma 4 (Performance Difference Lemma). For any policies ?, ? and any state s 0 , we have</p><formula xml:id="formula_86">V ? (s 0 ) ? V ? (s 0 ) = 1 1 ? ? E s?d ? s 0 E a??(?|s) A ? (s, a) .</formula><p>Proof. See <ref type="bibr">(Kakade, Langford, 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Algorithm Pseudocode</head><p>For completeness, we provide the implementation pseudocode of GDI-I 3 , which is shown in Algorithm 2. For completeness, we provide the implementation pseudocode of GDI-H 3 , which is shown in Algorithm 3. </p><formula xml:id="formula_87">A = A ? (s t ) , V = V ? (s t ) , A = A ? E ? [A], Q =? + V. (9) ? = (? 1 , ? 2 , ), ? ? ? = ? ? Softmax A ? 1 Exploration +(1 ? ?) ? Softmax A ? 2 Exploitation<label>(10</label></formula><formula xml:id="formula_88">A 1 = A ?1 (s t ) , V 1 = V ?1 (s t ) , A 1 = A 1 ? E ? [A 1 ], Q 1 =? 1 + V 1 . A 2 = A ?2 (s t ) , V 2 = V ?2 (s t ) , A 2 = A 2 ? E ? [A 2 ], Q 2 =? 2 + V 2 . (11) ? = (? 1 , ? 2 , ), ? ? ? = ? ? Softmax A 1 ? 1 + (1 ? ?) ? Softmax A 2 ? 2<label>(12</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Adaptive Controller Formalism</head><p>In practice, we use a Bandits Controller (BC) to adaptively control the behavior sampling distribution. More details on Bandits can be found in <ref type="bibr">(Garivier, Moulines, 2008)</ref>. The whole algorithm is shown in Algorithm 4. As the behavior policy can be parameterized and thus sampling behaviors from the policy space is equivalent to sampling parameters x from parameter space.</p><p>Let's firstly define a bandit as B = Bandit <ref type="bibr">(mode, l, r, lr, d, acc, ta, to, w, N)</ref>.</p><p>? mode is the mode of sampling, with two choices, argmax and random, wherein argmax greedily chooses the behaviors with top estimated value from the policy space, and random samples behaviors according to a distribution calculated by Sof tmax(V ). ? l is the left boundary of the parameter space, and each x is clipped to x = max{x, l}.</p><p>? r is the right boundary of the parameter space, and each x is clipped to x = min{x, r}.</p><p>? acc is the accuracy of space to be optimized, where each x is located in the (min{max{x, l}, r} ? l)/acc th block.</p><p>? tile coding is a representation method of continuous space (Sutton, <ref type="bibr" target="#b21">Barto, 2018)</ref>, and each kind of tile coding can be uniquely determined by l, r, to and ta, wherein to represents the tile offset and ta represents the accuracy of the tile coding. ? to is the offset of each tile coding, which represents the relative offset of the basic coordinate system (normally we select the space to be optimized as basic coordinate system). ? ta is the accuracy of each tile coding, where each x is located in the (min{max{x ? to, l}, r} ? l)/ta th tile.</p><p>? M btt represents block-to-tile, which is a mapping from the block of the original space to the tile coding space. ? M ttb represents tile-to-block, which is a mapping from the tile coding space to the block of the original space.</p><p>? w is a vector in R (r?l)/ta , which represents the weight of each tile.</p><p>? N is a vector in R (r?l)/ta , which counts the number of sampling of each tile.</p><p>? lr is the learning rate.</p><p>? d is an integer, which represents how many candidates is provided by each bandit when sampling.</p><p>During the evaluation process, we evaluate the value of the ith tile by</p><formula xml:id="formula_89">V i = M btt (blocki) k w k len(M btt (block i ))<label>(13)</label></formula><p>During the training process, for each sample (x, g), where g is the target value. Since x locates in the jth tile of kth tile_coding, we update B by</p><formula xml:id="formula_90">? ? ? j = (min{max{x ? to k , l}, r} ? l)/ta k , w j ? w j + lr * (g ? V i ) N j ? N j + 1<label>(14)</label></formula><p>During the sampling process, we firstly evaluate B by (13) and get (V 1 , ..., V (r?l)/acc ). We calculate the score of ith tile by</p><formula xml:id="formula_91">score i = V i ? ?({V j } j=1,..., (r?l)/acc ) ?({V j } j=1,..., (r?l)/acc ) + c ? log(1 + j N j ) 1 + N i .<label>(15)</label></formula><p>For different modes, we sample the candidates by the following mechanism,</p><p>? if mode = argmax, find blocks with top-d scores, then sample d candidates from these blocks, one uniformly from a block;</p><p>? if mode = random, sample d blocks with scores as the logits without replacement, then sample d candidates from these blocks, one uniformly from a block;</p><p>In practice, we define a set of bandits </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Experiment Details</head><p>We evaluated all agents on 57 Atari 2600 games from the arcade learning environment <ref type="bibr">(Bellemare et al., 2013, ALE)</ref> by recording the average score of the population of agents during training. We demonstrate our multivariate evaluation system in Tab. 4, and we will describe more details in the following. Besides, all the experiment is accomplished using a single CPU with 92 cores and a single Tesla-V100-SXM2-32GB GPU.</p><p>Noting that episodes will be truncated at 100K frames (or 30 minutes of simulated play) as other baseline algorithms <ref type="bibr">(Hessel et al., 2017;</ref><ref type="bibr" target="#b30">Badia et al., 2020a;</ref><ref type="bibr">Schmitt et al., 2020;</ref><ref type="bibr">Badia et al., 2020b;</ref><ref type="bibr" target="#b0">Kapturowski et al., 2018)</ref>, and thus we calculate the mean game time over 57 games which is called Game Time. In addition to comparing the mean and median human normalized scores (HNS), we also report the performance based on human world records among these algorithms and the related learning efficiency to further emphasize the significance of our algorithm. Inspired by , human world records normalized score (HWRNS) and SABER are better descriptors for evaluating algorithms on human top level on Atari games, which simultaneously give rise to more challenges and lead the related research into a new journey to train the superhuman agent instead of just paying attention to the human average level. The learning is the ratio of the related evaluation criterion (such as HWRNS, HNS or SABER) and training frames. In this section we detail the hyperparameters we use to pre-process the environment frames received from the Arcade Learning Environment. The hyperparameters that we used in all experiments are almost the same as Agent57 <ref type="bibr" target="#b30">(Badia et al., 2020a)</ref>, NGU <ref type="bibr">(Badia et al., 2020b)</ref>, MuZero <ref type="bibr" target="#b31">(Schrittwieser et al., 2020)</ref> and <ref type="bibr">R2D2 (Kapturowski et al., 2018)</ref>. In Tab. 5 we detail these hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Value</head><p>Random  <ref type="table">Table 5</ref>: Atari pre-processing hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Hyperparameters Used</head><p>In this section we detail the hyperparameters we used , which is demonstrated in Tab. 6. We also include the hyperparameters we use for the UCB bandit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Value</head><p>Num. Frames 200M Replay 2 Num. Environments 160 GDI-I 3 Reward Shape log(abs(r) </p><formula xml:id="formula_92">+ 1.0) ? (2 ? 1 {r?0} ? 1 {r&lt;0} ) GDI-H 3 Reward Shape 1 log(abs(r) + 1.0) ? (2 ? 1 {r?0} ? 1 {r&lt;0} ) GDI-H 3 Reward</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Experimental Results</head><p>In this section, we report the performance of GDI-H 3 , GDI-I 3 and many well-known SOTA algorithms including both the model-based and model-free methods. First of all, we summarize the performance of all the algorithms over all the evaluation criteria of our evaluation system in App. J.1 which is mentioned in App. H. In the next three parts, we visualize the performance of GDI-H 3 , GDI-I 3 over HNS in App. J.2, HWRNS in App. J.3, SABER in App. J.4 via histogram. Furthermore, we details all the original scores of all the algorithms and provide raw data that calculates those evaluation criteria, wherein we first provide all the human world records in 57 Atari games and calculate the HNS in App. J.5, HWRNS in App. J.6 and SABER in App. J.7 of all 57 Atari games. We further provide all the evaluation curve of GDI-H 3 , GDI-I 3 over 57 Atari games in App. J.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1 Full Performance Comparison</head><p>In this part, we summarize the performance of all mentioned algorithms over all the evaluation criteria in Tab. 7. In the following sections we will detail the performance of each algorithm on all Atari games one by one.  <ref type="table">Table 7</ref>: Full performance comparison on Atari.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2 Figure of HNS</head><p>In this part, we begin to visualize the HNS using GDI-H 3 and GDI-I 3 in all 57 games. The HNS histogram of GDI-I 3 is illustrated in <ref type="figure" target="#fig_0">Fig. 12</ref>. The HNS histogram of GDI-H 3 is illustrated in <ref type="figure" target="#fig_0">Fig. 13</ref>. In addition, we mark the error bars in the histogram with respect to the random seed after running experiments multiple times.  <ref type="figure" target="#fig_0">Figure 13</ref>: HNS (%) of Atari 57 games using GDI-H 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.3 Figure of HWRNS</head><p>In this part, we begin to visualize the HWRNS <ref type="bibr" target="#b22">Toromanoff et al., 2019)</ref> using GDI-H 3 and GDI-I 3 in all 57 games. The HWRNS histogram of GDI-I 3 is illustrated in <ref type="figure" target="#fig_0">Fig. 14.</ref> The HWRNS histogram of GDI-H 3 is illustrated in <ref type="figure" target="#fig_0">Fig. 15</ref>. In addition, we mark the error bars in the histogram with respect to the random seed after running experiments multiple times.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.4 Figure of SABER</head><p>In this part, we begin to visualize the HWRNS <ref type="bibr" target="#b22">Toromanoff et al., 2019)</ref> using GDI-H 3 and GDI-I 3 in all 57 games. The HWRNS histogram of GDI-I 3 is illustrated in <ref type="figure" target="#fig_0">Fig. 16</ref>. The HWRNS histogram of GDI-H 3 is illustrated in <ref type="figure" target="#fig_0">Fig. 17</ref>. In addition, we mark the error bars in the histogram with respect to the random seed after running experiments multiple times.  <ref type="figure" target="#fig_0">Figure 17</ref>: SABER (%) of Atari 57 games using GDI-H 3 .     In this part, we detail the raw score of several representative SOTA algorithms including the SOTA 200M model-free algorithms, SOTA 10B+ model-free algorithms, SOTA model-based algorithms and other SOTA algorithms. <ref type="bibr">3</ref> Additionally, we calculate the human world records normalized world score (HWRNS) of each game with each algorithms. First of all, we demonstrate the sources of the scores that we used. Random scores are from <ref type="bibr" target="#b30">(Badia et al., 2020a)</ref>. Human world records (HWR) are form <ref type="bibr" target="#b22">Toromanoff et al., 2019)</ref>. Rainbow's scores are from <ref type="bibr">(Hessel et al., 2017)</ref>. IMPALA's scores are from <ref type="bibr" target="#b30">(Espeholt et al., 2018)</ref>. LASER's scores are from <ref type="bibr">(Schmitt et al., 2020)</ref>, no sweep at 200M. As there are many versions of R2D2 and NGU, we use original papers'. R2D2's scores are from . NGU's scores are from <ref type="bibr">(Badia et al., 2020b</ref>). Agent57's scores are from <ref type="bibr" target="#b30">(Badia et al., 2020a)</ref>. MuZero's scores are from <ref type="bibr" target="#b31">(Schrittwieser et al., 2020</ref>               </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.5 Atari Games</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Ablation Study</head><p>In this section, we firstly demonstrate the settings of our ablation studies. Then, we illustrate the performance graphs among all ablation cases in three representative Atari games. Lastly, we offer the t-SNE of three Atari games to further prove expansion of policy space in GDI brings more diverse data and more chance to obtain elite data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.1 Ablation Study Details</head><p>The details of ablation changes are listed in Tab. K.1. All the ablation studies are carried out using the evaluation system and 200M training frames. The operator T is achieved with Vtrace, Retrace and policy gradient. Except for the differences listed in the table, other settings and the shared hyperparameters remain the same in all ablation cases. The hyperparameters can see App. I.2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Videos</head><p>In the future, we will put all the videos of the performance of our algorithm on sites, which will contain the following:</p><p>? Our performance on all Atari games: We provide an example video for each game in the Atari games sweep, wherein our algorithm surpass all the existing SOTA algorithms and achieves SOTA. ? Adaptive Entropy Control: We show example videos for algorithms in the game James Bond, wherein the agents automatically learns how to trade off the exploitation policy and exploration. ? Human World Records Breakthroughs: We also provide an example video for 22 Atari games, wherein our algorithms achieved 22 human world records breakthroughs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Performance of SOTA algorithms of Atari 57 games on mean HNS(%) with corresponding learning efficiency and human world record breakthrough with corresponding game time. Details on those evaluation criteria can see App. H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Deep Q Network (Mnih et al., 2015, DQN) firstly achieves human level control of Atari games, reinforcement learning (RL) has brought the dawn of solving challenges of ALE and surpassing the human level control, which inspires researchers to pursuit more state-of-the-art(SOTA) performance. At the beginning, massive variants of DQN achieve new SOTA results. Double DQN (Van Hasselt et al., 2016) introduces independent target network to alleviating overestimation problem. Dueling DQN (Wang et al., 2016) adopts the dueling neural network architecture and achieved a new SOTA. RAINBOW (Hessel et al., 2017) combines various effective extensions of DQN and improves the learning efficiency and the final performance. Retrace(?) (Munos et al., 2016) takes the per-step importance sampling, off policy Q(?), and tree-backup(?) (Sutton, Barto, 2018) to estimate Q(s, a), resulting in a low variance estimation of Q(s, a):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>SOTA algorithms of Atari 57 games on mean and median HNS (%) and game time (year).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>SOTA algorithms of Atari 57 games on mean and median HNS (%) and corresponding training scale. D.1 RL Benchmarks on HNS We report several milestones of Atari benchmarks on HNS, including DQN (Mnih et al., 2015), RAINBOW (Hessel et al., 2017), IMPALA (Espeholt et al., 2018), LASER (Schmitt et al., 2020), R2D2 (Kapturowski et al., 2018), NGU (Badia et al., 2020b), Agent57 (Badia et al., 2020a), Go-Explore (Ecoffet et al., 2019), MuZero (Schrittwieser et al., 2020), DreamerV2 (Hafner et al., 2020), SimPLe (Kaiser et al., 2019) and Musile (Hessel et al., 2021). We summary mean HNS and median HNS of these algorithms marked with their game time (year), learning efficiency and training scale in Fig 2, 3 and 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>D. 2 RLFigure 5 :</head><label>25</label><figDesc>Benchmarks on HWRNS We report several milestones of Atari benchmarks on Human World Records Normalized Score (HWRNS), including DQN (Mnih et al., 2015), RAINBOW (Hessel et al., 2017), IMPALA (Espeholt et al., 2018), LASER (Schmitt et al., 2020), R2D2 (Kapturowski et al., 2018), NGU (Badia et al., 2020b), Agent57 (Badia et al., 2020a), Go-Explore (Ecoffet et al., 2019), MuZero (Schrittwieser et al., 2020), DreamerV2 (Hafner et al., 2020), SimPLe (Kaiser et al., 2019) and Musile (Hessel et al., SOTA algorithms of Atari 57 games on mean and median HWRNS (%) and corresponding game time (year).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>SOTA algorithms of Atari 57 games on mean and median HWRNS (%) and corresponding learning efficiency calculated by MEAN HWRNS/SOTA algorithms of Atari 57 games on mean and median HWRNS (%) and corresponding training scale. 2021). We summary mean HWRNS and median HWRNS of these algorithms marked with their game time (year), learning efficiency and training scale in Fig 5, 6 and 7. D.3 RL Benchmarks on SABER We report several milestones of Atari benchmarks on Standardized Atari BEnchmark for RL (SABER), including DQN (Mnih et al., 2015), RAINBOW (Hessel et al., 2017), IMPALA (Espeholt et al., 2018), LASER (Schmitt et al., 2020), R2D2 (Kapturowski et al., 2018), NGU (Badia et al., 2020b), Agent57 (Badia et al., 2020a), Go-Explore (Ecoffet et al., 2019), MuZero (Schrittwieser et al., 2020), DreamerV2 (Hafner et al., 2020), SimPLe (Kaiser et al., 2019) and Musile (Hessel et al., 2021). We summary mean SABER and median SABER of these algorithms marked with their game time (year), learning efficiency and training scale in Fig 8, 9 and 10. D.4 RL Benchmarks on HWRB We report several milestones of Atari benchmarks on HWRB, including DQN (Mnih et al., 2015), RAINBOW (Hessel et al., 2017), IMPALA (Espeholt et al., 2018), LASER (Schmitt et al., 2020),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>SOTA algorithms of Atari 57 games on mean and median SABER (%) and corresponding learning efficiency calculated by MEAN SABER/SOTA algorithms of Atari 57 games on mean and median SABER (%) and corresponding training scale. R2D2 (Kapturowski et al., 2018), NGU (Badia et al., 2020b), Agent57 (Badia et al., 2020a), Go-Explore (Ecoffet et al., 2019), MuZero (Schrittwieser et al., 2020), DreamerV2 (Hafner et al., 2020), SimPLe (Kaiser et al., 2019) and Musile(Hessel et al., 2021). We summary HWRB of these algorithms marked with their game time (year), learning efficiency and training scale inFig 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>SOTA algorithms of Atari 57 games on HWRB. HWRB of SimPLe is 0, so it's not shown in the up-rightfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>HWRNS (%) of Atari 57 games using GDI-I 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 :</head><label>15</label><figDesc>HWRNS (%) of Atari 57 games using GDI-H 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 :</head><label>18</label><figDesc>3 ? = {?|? = ( , ? 1 , ? 2 )} Uniform MAB GDI-H 3 GDI-H 3 ? = {?|? = ( , ? 1 , ? 2 )} Uniform MAB Fix Selection GDI-I 0 w/o E ? = {?|? = ( , ? 1 , ? 2 )} Delta Identical Mapping Random Selection GDI-I 3 w/o E ? = {?|? = ( , ? 1 , ? 2 )} Uniform Identical Mapping Boltzmann Selection GDI-I 1 ? = {?|? = (? )} Uniform MAB -greedy Selection GDI-I 1 ? = {?|? = ( )} Uniform MABK.2 Ablation ResultsIn this part, we show three representative experiments of different Atari games among all the ablation cases in Figs. 18. Evaluation curves of ablation study. For ease of comparison, all curves are smoothed with rate 0.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>9620.98 7810.6 2538.66 873.97 1741.36 3374.31 3169.90 4763.69 Median HNS(%) 1146.39 832.5 1077.47 230.99 454.91 1342.27 1208.11 1933.49   </figDesc><table><row><cell>Num. Frames</cell><cell cols="3">2E+8 2E+8 2E+8</cell><cell>2E+8</cell><cell cols="4">2E+8 1E+10 3.5E+10 1E+11</cell></row><row><cell cols="4">Game Time (year) 0.114 0.114 0.114</cell><cell>0.114</cell><cell>0.114</cell><cell>5.7</cell><cell>19.9</cell><cell>57</cell></row><row><cell>HWRB</cell><cell>22</cell><cell>17</cell><cell>5</cell><cell>4</cell><cell>7</cell><cell>15</cell><cell>8</cell><cell>18</cell></row><row><cell cols="4">Mean HNS(%) Mean HWRNS(%) 154.27 117.99 75.52</cell><cell>28.39</cell><cell cols="4">45.39 98.78 76.00 125.92</cell></row><row><cell cols="4">Median HWRNS(%) 50.63 35.78 24.86</cell><cell>4.92</cell><cell cols="4">8.08 33.62 21.19 43.62</cell></row><row><cell cols="4">Mean SABER(%) 71.26 61.66 48.74</cell><cell>28.39</cell><cell cols="4">36.78 60.43 50.47 76.26</cell></row><row><cell cols="4">Median SABER(%) 50.63 35.78 24.68</cell><cell>4.92</cell><cell cols="4">8.08 33.62 21.19 43.62</cell></row></table><note>GDI-H 3 GDI-I 3 Muesli RAINBOW LASER R2D2 NGU Agent57</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Experiment results of Atari. Muesli's scores are from (Hessel et al., 2021). RAINBOW's scores are from (Espeholt et al., 2018). LASER's scores are from (Schmitt et al., 2020), no sweep at 200M. R2D2's scores are from (Kapturowski et al., 2018). NGU's scores are from (Badia et al., 2020b). Agent57's scores are from (Badia et al., 2020a). More details on abbreviations and notations can see App. A and H. Full comparison among all algorithms can see App. J.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>: Abbreviation</cell></row><row><cell>Abbreviation</cell><cell>Description</cell></row><row><cell>SOTA</cell><cell>State-of-The-Art (Badia et al., 2020a)</cell></row><row><cell>RL</cell><cell>Reinforcement Learning (Sutton, Barto, 2018)</cell></row><row><cell>DRL</cell><cell>Deep Reinforcement Learning (Sutton, Barto, 2018)</cell></row><row><cell>GPI</cell><cell>Generalized Policy Iteration (Sutton, Barto, 2018)</cell></row><row><cell>PG</cell><cell>Policy Gradient (Sutton, Barto, 2018)</cell></row><row><cell>AC</cell><cell>Actor Critic (Sutton, Barto, 2018)</cell></row><row><cell>ALE</cell><cell>Atari Learning Environment (Bellemare et al., 2013)</cell></row><row><cell>HNS</cell><cell>Human Normalized Score (Bellemare et al., 2013)</cell></row><row><cell>HWRB</cell><cell>Human World Records Breakthrough</cell></row><row><cell>HWRNS</cell><cell>Human World Records Normalized Score</cell></row><row><cell>SABER</cell><cell>Standardized Atari BEnchmark for RL (Toromanoff et al., 2019)</cell></row><row><cell>CHWRNS</cell><cell>Capped Human World Records Normalized Score</cell></row><row><cell>WLOG</cell><cell>without loss of generality</cell></row><row><cell>w/o</cell><cell>without</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>: Notation</cell></row><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>s</cell><cell>state</cell></row><row><cell>a</cell><cell>action</cell></row><row><cell>S</cell><cell>set of all states</cell></row><row><cell>A</cell><cell>set of all actions</cell></row><row><cell>?</cell><cell>probability simplex</cell></row><row><cell>?</cell><cell>behavior policy</cell></row><row><cell>?</cell><cell>target policy</cell></row><row><cell>G t d ? ?0</cell><cell>cumulative discounted reward or return at t</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>SOTA algorithms of Atari 57 games on mean and median SABER (%) and corresponding game time (year).</figDesc><table><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60%</cell><cell></cell></row><row><cell></cell><cell>60%</cell><cell></cell><cell cols="2">GDI-I3 GDI-H3</cell><cell cols="3">Go-Explore</cell><cell cols="2">Muzero</cell><cell>Agent57</cell><cell></cell><cell>45%</cell><cell></cell><cell>GDI-H3</cell><cell>Go-Explore</cell><cell>Muzero</cell></row><row><cell>Mean SABER</cell><cell>40%</cell><cell></cell><cell cols="2">Muesli LASER RAINBOW</cell><cell cols="2">IMPALA</cell><cell cols="2">R2D2</cell><cell cols="2">NGU</cell><cell>Median SABER</cell><cell>30%</cell><cell></cell><cell>Muesli GDI-I3</cell><cell>R2D2</cell><cell>Agent57</cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell cols="2">DreamerV2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15%</cell><cell></cell><cell>NGU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LASER</cell></row><row><cell></cell><cell>0%</cell><cell>SimPLe</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0%</cell><cell>SimPLe</cell><cell>DreamerV2 RAINBOW</cell><cell>IMPALA</cell></row><row><cell></cell><cell>1E-04</cell><cell>1E-03</cell><cell>1E-02</cell><cell cols="2">1E-01</cell><cell>1E+00</cell><cell></cell><cell cols="2">1E+01</cell><cell>1E+02</cell><cell></cell><cell>1E-04</cell><cell>1E-03</cell><cell>1E-02</cell><cell>1E-01</cell><cell>1E+00</cell><cell>1E+01</cell><cell>1E+02</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Game Time (Years)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Game Time (Years)</cell></row><row><cell>Figure 8: Median SABER</cell><cell>60% 80% 40%</cell><cell cols="6">Go-Explore R2D2 Muzero NGU Agent57 LASER GDI-H3 GDI-I3 Muesli IMPALA RAINBOW</cell><cell></cell><cell></cell><cell></cell><cell>Median SABER</cell><cell>45% 60% 30%</cell><cell></cell><cell>Agent57</cell><cell>Muzero R2D2</cell><cell>Go-Explore</cell><cell>GDI-H3 Muesli GDI-I3</cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell cols="2">DreamerV2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15%</cell><cell></cell><cell>NGU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LASER</cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SimPLe</cell><cell></cell><cell>0%</cell><cell></cell><cell>DreamerV2 IMPALA</cell><cell>SimPLe RAINBOW</cell></row><row><cell></cell><cell>1E-12</cell><cell>1E-11</cell><cell>1E-10</cell><cell></cell><cell cols="2">1E-09</cell><cell cols="2">1E-08</cell><cell></cell><cell>1E-07</cell><cell></cell><cell>1E-13</cell><cell>1E-12</cell><cell>1E-11</cell><cell>1E-10</cell><cell>1E-09</cell><cell>1E-08</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Mean SABER Learning Efficiency</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Median SABER Learning Efficiency</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Lemma 2 (Discretized Upper Triangular Transport Inequality for Co-Monotonic Functions in R 1 ). Assume ? is a continuous probability measure supported on [0, 1]. Let 0</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Theorem 3 (Upper Triangular Transport Inequality for Co-Monotonic Functions in R 1 ). Assume ? is a continuous probability measure supported on [0, 1]. Let f, g : [0, 1] ? R to be two co-monotonic functions that satisfy</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Update ? via policy gradient and policy evaluation. if count mod d push = 0 then Push ? to PS. pull , M . Initialize ? as Eq. (9) and (10). Initialize {B m } m=1,...,M and sample ? as in Algorithm 4. Initialize count = 0, G = 0. Sample ? as in Algorithm 4 end if if count mod d pull = 0 then Pull ? from PS and update ?.</figDesc><table><row><cell>)</cell></row><row><cell>Algorithm 2 GDI-I 3 Algorithm.</cell></row><row><cell>Initialize Parameter Server (PS) and Data Collector (DC).</cell></row><row><cell>end if</cell></row><row><cell>count ? count + 1.</cell></row><row><cell>end while</cell></row><row><cell>// ACTOR</cell></row><row><cell>Initialize d end if</cell></row><row><cell>count ? count + 1.</cell></row><row><cell>end while</cell></row></table><note>// LEARNER Initialize d push . Initialize ? as Eq. (9) and (10). Initialize count = 0. while T rue do Load data from DC. Estimate qs and vs by proper off-policy algorithms. (For instance, ReTrace (B.1) for qs and V-Trace (B.2) for vs.)while T rue do Calculate ? ? ? (?|s). Sample a ? ? ? ? (?|s).s, r, done ? p(?|s, a).G ? G + r. if done then Update {B m } m=1,...,M with (?, G) as in Algorithm 4. Send data to DC and reset the environment. G ? 0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>, vs 2 .) Update ? 1 , ? 2 via policy gradient and policy evaluation, respectively. if count mod d push = 0 then Push ? 1 , ? 2 to PS. pull , M . Initialize ? 1 , ? 2 as Eq. (11) and (12). Initialize {B m } m=1,...,M and sample ? as in Algorithm 4. Initialize count = 0, G = 0. while T rue do Calculate ? ? ? (?|s). Sample a ? ? ? ? (?|s). s, r, done ? p(?|s, a). G ? G + r. if done then Update {B m } m=1,...,M with (?, G) as in Algorithm 4. Send data to DC and reset the environment. G ? 0. Sample ? as in Algorithm 4 end if if count mod d pull = 0 then Pull ? from PS and update ?.</figDesc><table><row><cell>)</cell></row><row><cell>Algorithm 3 GDI-H 3 Algorithm.</cell></row><row><cell>Initialize Parameter Server (PS) and Data Collector (DC).</cell></row><row><cell>end if</cell></row><row><cell>count ? count + 1.</cell></row><row><cell>end while</cell></row><row><cell>// ACTOR</cell></row><row><cell>Initialize d end if</cell></row><row><cell>count ? count + 1.</cell></row><row><cell>end while</cell></row></table><note>// LEARNER Initialize d push . Initialize ? as Eq. (11) and (12). Initialize count = 0. while T rue do Load data from DC. Estimate qs 1 , qs 2 and vs 1 , vs 2 by proper off-policy algorithms. (For instance, ReTrace (B.1) for qs1, qs 2 and V-Trace (B.2) for vs 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>B m = {B m } m=1,...,M . At each step, we sample d candidates {c m,i } i=1,...,d from each B m , so we have a set of m ? d candidates {c m,i } m=1,...,M ;i=1,...,d . Then we sample uniformly from these m ? d candidates to get x. At last, we transform the selected x to ? = {? 1 , ? 2 , } by ? 1,2 = 1 exp(x1,2)?1 and = x 3 When we receive (?, g), we transform ? to x by x 1,2 = log(1 + 1/? 1,2 ), and x 3 = . Then we update each B m by (14). Bandits Controller for m = 1, ..., M do Sample mode ? {argmax, random} and other initialization parameters Initialize B m = Bandit(mode, l, r, lr, d, acc, to, ta, w, N) Ensemble B m to constitute B Sample x from {c m,i } m=1,...,M ;i=1,...,d . Execute x and receive the return G. for m = 1, ..., M do Update B</figDesc><table><row><cell>Algorithm 4 end for</cell></row><row><cell>end while</cell></row></table><note>m end for while T rue do for m = 1, ..., M do Evaluate B m by (13). Sample candidates c m,1 , ..., c m,d from B m via (15) following its mode. end form with (x, G) by (14).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 4 :</head><label>4</label><figDesc>Multivariate evaluation system</figDesc><table><row><cell>Evaluation Criterion</cell><cell></cell><cell>Computing Formula</cell></row><row><cell>Game Time</cell><cell></cell><cell>Num.Frames 100000*2*24*365</cell></row><row><cell>HNS</cell><cell></cell><cell>G?Grandom Ghuman?Grandom</cell></row><row><cell cols="3">Human World Record Breakthrough i=1 (G Learning Efficiency 57 Related Evaluation Criterion Num.Frames</cell></row><row><cell>HWRNS</cell><cell></cell><cell>G?Grandom GHuman World Records?Grandom</cell></row><row><cell>SABER</cell><cell>min{</cell><cell>G?Grandom GHuman World Records?Grandom , 200%}</cell></row></table><note>i &gt;= G Human World Records )I.1 Atari pre-processing hyperparameters</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc>Table of Scores Based on Human Average RecordsIn this part, we detail the raw score of several representative SOTA algorithms including the SOTA 200M model-free algorithms, SOTA 10B+ model-free algorithms, SOTA model-based algorithms and other SOTA algorithms.2  Additionally, we calculate the Human Normalized Score (HNS) of each game with each algorithms. First of all, we demonstrate the sources of the scores that we used.</figDesc><table><row><cell cols="10">J.5.1 Comparison with SOTA 200M Model-Free Algorithms on HNS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Games</cell><cell cols="12">RND HUMAN RAINBOW HNS(%) IMPALA HNS(%) LASER HNS(%) GDI-I 3 HNS(%) GDI-H 3 HNS(%)</cell></row><row><cell>Scale</cell><cell></cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell cols="2">227.8 7127.8</cell><cell>9491.7</cell><cell cols="6">134.26 15962.1 228.03 35565.9 512.15 43384</cell><cell>625.45</cell><cell>48735</cell><cell>703.00</cell></row><row><cell cols="13">amidar Random scores and average human's scores are from (Badia et al., 2020a). Rainbow's scores are 5.8 1719.5 5131.2 299.08 1554.79 90.39 1829.2 106.4 1442 83.81 1065 61.81 assault 222.4 742 14198.5 2689.78 19148.47 3642.43 21560.4 4106.62 63876 12250.50 97155 18655.23 from (Hessel et al., 2017). IMPALA's scores are from (Espeholt et al., 2018). LASER's scores are asterix 210 8503.3 428200 5160.67 300732 3623.67 240090 2892.46 759910 9160.41 999999 12055.38</cell></row><row><cell cols="13">from (Schmitt et al., 2020), no sweep at 200M. As there are many versions of R2D2 and NGU, we asteroids 719 47388.7 2712.8 4.27 108590.05 231.14 213025 454.91 751970 1609.72 760005 1626.94</cell></row><row><cell cols="13">atlantis use original papers'. R2D2's scores are from (Kapturowski et al., 2018). NGU's scores are from 12850 29028.1 826660 5030.32 849967.5 5174.39 841200 5120.19 3803000 23427.66 3837300 23639.67 bank heist 14.2 753.1 1358 181.86 1223.15 163.61 569.4 75.14 1401 187.68 1380 184.84 (Badia et al., 2020b). Agent57's scores are from (Badia et al., 2020a). MuZero's scores are from battle zone 236 37187.5 62010 167.18 20885 55.88 64953.3 175.14 478830 1295.20 824360 2230.29</cell></row><row><cell cols="13">(Schrittwieser et al., 2020). DreamerV2's scores are from (Hafner et al., 2020). SimPLe's scores are beam rider 363.9 16926.5 16850.2 99.54 32463.47 193.81 90881.6 546.52 162100 976.51 422890 2551.09</cell></row><row><cell cols="13">berzerk form (Kaiser et al., 2019). Go-Explore's scores are form (Ecoffet et al., 2019). Muesli's scores are 123.7 2630.4 2545.6 96.62 1852.7 68.98 25579.5 1015.51 7607 298.53 14649 579.46 bowling 23.1 160.7 30 5.01 59.92 26.76 48.3 18.31 201.9 129.94 205.2 132.34 form (Hessel et al., 2021). In the following we detail the raw scores and HNS of each algorithms on boxing 0.1 12.1 99.6 829.17 99.96 832.17 100 832.5 100 832.50 100 832.50</cell></row><row><cell cols="3">breakout 57 Atari games. centipede 2090.9 12017 1.7 30.5</cell><cell>417.5 8167.3</cell><cell cols="9">1443.75 787.34 2727.92 747.9 2590.97 61.22 11049.75 90.26 292792 2928.65 155830 1548.84 195630 1949.80 864 2994.10 864 2994.10</cell></row><row><cell cols="2">chopper command 811</cell><cell>7387.8</cell><cell>16654</cell><cell>240.89</cell><cell>28255</cell><cell cols="7">417.29 761699 11569.27 999999 15192.62 999999 15192.62</cell></row><row><cell>crazy climber</cell><cell cols="12">10780.5 36829.4 168788.5 630.80 136950 503.69 167820 626.93 201000 759.39 241170 919.76</cell></row><row><cell>defender</cell><cell cols="2">2874.5 18688.9</cell><cell>55105</cell><cell cols="9">330.27 185203 1152.93 336953 2112.50 893110 5629.27 970540 6118.89</cell></row><row><cell>demon attack</cell><cell>152.1</cell><cell>1971</cell><cell cols="10">111185 6104.40 132826.98 7294.24 133530 7332.89 675530 37131.12 787985 43313.70</cell></row><row><cell>double dunk</cell><cell>-18.6</cell><cell>-16.4</cell><cell>-0.3</cell><cell>831.82</cell><cell>-0.33</cell><cell>830.45</cell><cell>14</cell><cell>1481.82</cell><cell>24</cell><cell>1936.36</cell><cell>24</cell><cell>1936.36</cell></row><row><cell>enduro</cell><cell>0</cell><cell>860.5</cell><cell>2125.9</cell><cell>247.05</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell cols="4">14330 1665.31 14300 1661.82</cell></row><row><cell>fishing derby</cell><cell>-91.7</cell><cell>-38.8</cell><cell>31.3</cell><cell>232.51</cell><cell>44.85</cell><cell>258.13</cell><cell>45.2</cell><cell>258.79</cell><cell>59</cell><cell>285.71</cell><cell>65</cell><cell>296.22</cell></row><row><cell>freeway</cell><cell>0</cell><cell>29.6</cell><cell>34</cell><cell>114.86</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>34</cell><cell>114.86</cell><cell>34</cell><cell>114.86</cell></row><row><cell>frostbite</cell><cell>65.2</cell><cell>4334.7</cell><cell>9590.5</cell><cell>223.10</cell><cell>317.75</cell><cell>5.92</cell><cell cols="3">5083.5 117.54 10485</cell><cell>244.05</cell><cell>11330</cell><cell>263.84</cell></row><row><cell>gopher</cell><cell cols="2">257.6 2412.5</cell><cell cols="10">70354.6 3252.91 66782.3 3087.14 114820.7 5316.40 488830 22672.63 473560 21964.01</cell></row><row><cell>gravitar</cell><cell>173</cell><cell>3351.4</cell><cell>1419.3</cell><cell>39.21</cell><cell>359.5</cell><cell>5.87</cell><cell>1106.2</cell><cell>29.36</cell><cell>5905</cell><cell>180.34</cell><cell>5915</cell><cell>180.66</cell></row><row><cell>hero</cell><cell cols="3">1027 30826.4 55887.4</cell><cell cols="6">184.10 33730.55 109.75 31628.7 102.69 38330</cell><cell>125.18</cell><cell>38225</cell><cell>124.83</cell></row><row><cell>ice hockey</cell><cell>-11.2</cell><cell>0.9</cell><cell>1.1</cell><cell>101.65</cell><cell>3.48</cell><cell>121.32</cell><cell>17.4</cell><cell cols="2">236.36 44.94</cell><cell>463.97</cell><cell>47.11</cell><cell>481.90</cell></row><row><cell>jamesbond</cell><cell>29</cell><cell>302.8</cell><cell>19809</cell><cell>72.24</cell><cell>601.5</cell><cell cols="7">209.09 37999.8 13868.08 594500 217118.70 620780 226716.95</cell></row><row><cell>kangaroo</cell><cell>52</cell><cell>3035</cell><cell>14637.5</cell><cell>488.05</cell><cell>1632</cell><cell>52.97</cell><cell cols="3">14308 477.91 14500</cell><cell>484.34</cell><cell>14636</cell><cell>488.00</cell></row><row><cell>krull</cell><cell cols="2">1598 2665.5</cell><cell>8741.5</cell><cell>669.18</cell><cell>8147.4</cell><cell cols="7">613.53 9387.5 729.70 97575 8990.82 594540 55544.92</cell></row><row><cell>kung fu master</cell><cell cols="2">258.5 22736.3</cell><cell>52181</cell><cell cols="9">230.99 43375.5 191.82 607443 2701.26 140440 623.64 1666665 7413.57</cell></row><row><cell>montezuma revenge</cell><cell>0</cell><cell>4753.3</cell><cell>384</cell><cell>8.08</cell><cell>0</cell><cell>0.00</cell><cell>0.3</cell><cell>0.01</cell><cell>3000</cell><cell>63.11</cell><cell>2500</cell><cell>52.60</cell></row><row><cell>ms pacman</cell><cell cols="2">307.3 6951.6</cell><cell>5380.4</cell><cell>76.35</cell><cell cols="3">7342.32 105.88 6565.5</cell><cell>94.19</cell><cell>11536</cell><cell>169.00</cell><cell>11573</cell><cell>169.55</cell></row><row><cell cols="3">name this game 2292.3 8049</cell><cell>13136</cell><cell cols="6">188.37 21537.2 334.30 26219.5 415.64 34434</cell><cell>558.34</cell><cell>36296</cell><cell>590.68</cell></row><row><cell>phoenix</cell><cell cols="2">761.5 7242.6</cell><cell cols="10">108529 1662.80 210996.45 3243.82 519304 8000.84 894460 13789.30 959580 14794.07</cell></row><row><cell>pitfall</cell><cell cols="2">-229.4 6463.7</cell><cell>0</cell><cell>3.43</cell><cell>-1.66</cell><cell>3.40</cell><cell>-0.6</cell><cell>3.42</cell><cell>0</cell><cell>3.43</cell><cell>-4.345</cell><cell>3.36</cell></row><row><cell>pong</cell><cell>-20.7</cell><cell>14.6</cell><cell>20.9</cell><cell>117.85</cell><cell>20.98</cell><cell>118.07</cell><cell>21</cell><cell>118.13</cell><cell>21</cell><cell>118.13</cell><cell>21</cell><cell>118.13</cell></row><row><cell>private eye</cell><cell cols="2">24.9 69571.3</cell><cell>4234</cell><cell>6.05</cell><cell>98.5</cell><cell>0.11</cell><cell>96.3</cell><cell>0.10</cell><cell>15100</cell><cell>21.68</cell><cell>15100</cell><cell>21.68</cell></row><row><cell>qbert</cell><cell cols="3">163.9 13455.0 33817.5</cell><cell cols="6">253.20 351200.12 2641.14 21449.6 160.15 27800</cell><cell>207.93</cell><cell>28657</cell><cell>214.38</cell></row><row><cell>riverraid</cell><cell cols="3">1338.5 17118.0 22920.8</cell><cell cols="6">136.77 29608.05 179.15 40362.7 247.31 28075</cell><cell>169.44</cell><cell>28349</cell><cell>171.17</cell></row><row><cell>road runner</cell><cell>11.5</cell><cell>7845</cell><cell>62041</cell><cell>791.85</cell><cell>57121</cell><cell cols="7">729.04 45289 578.00 878600 11215.78 999999 12765.53</cell></row><row><cell>robotank</cell><cell>2.2</cell><cell>11.9</cell><cell>61.4</cell><cell>610.31</cell><cell>12.96</cell><cell>110.93</cell><cell>62.1</cell><cell cols="5">617.53 108.2 1092.78 113.4 1146.39</cell></row><row><cell>seaquest</cell><cell cols="3">68.4 42054.7 15898.9</cell><cell>37.70</cell><cell>1753.2</cell><cell>4.01</cell><cell>2890.3</cell><cell>6.72</cell><cell cols="4">943910 2247.98 1000000 2381.57</cell></row><row><cell>skiing</cell><cell cols="3">-17098 -4336.9 -12957.8</cell><cell cols="6">32.44 -10180.38 54.21 -29968.4 -100.86 -6774</cell><cell>80.90</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell cols="2">1236.3 12326.7</cell><cell>3560.3</cell><cell>20.96</cell><cell>2365</cell><cell>10.18</cell><cell>2273.5</cell><cell>9.35</cell><cell>11074</cell><cell>88.70</cell><cell>9105</cell><cell>70.95</cell></row><row><cell>space invaders</cell><cell>148</cell><cell>1668.7</cell><cell>18789</cell><cell cols="9">1225.82 43595.78 2857.09 51037.4 3346.45 140460 9226.80 154380 10142.17</cell></row><row><cell>star gunner</cell><cell>664</cell><cell>10250</cell><cell cols="10">127029 1318.22 200625 2085.97 321528 3347.21 465750 4851.72 677590 7061.61</cell></row><row><cell>surround</cell><cell>-10</cell><cell>6.5</cell><cell>9.7</cell><cell>119.39</cell><cell>7.56</cell><cell>106.42</cell><cell>8.4</cell><cell>111.52</cell><cell>-7.8</cell><cell>13.33</cell><cell>2.606</cell><cell>76.40</cell></row><row><cell>tennis</cell><cell>-23.8</cell><cell>-8.3</cell><cell>0</cell><cell>153.55</cell><cell>0.55</cell><cell>157.10</cell><cell>12.2</cell><cell>232.26</cell><cell>24</cell><cell>308.39</cell><cell>24</cell><cell>308.39</cell></row><row><cell>time pilot</cell><cell cols="2">3568 5229.2</cell><cell>12926</cell><cell cols="9">563.36 48481.5 2703.84 105316 6125.34 216770 12834.99 450810 26924.45</cell></row><row><cell>tutankham</cell><cell>11.4</cell><cell>167.6</cell><cell>241</cell><cell>146.99</cell><cell>292.11</cell><cell>179.71</cell><cell>278.9</cell><cell cols="2">171.25 423.9</cell><cell>264.08</cell><cell>418.2</cell><cell>260.44</cell></row><row><cell>up n down</cell><cell cols="12">533.4 11693.2 125755 1122.08 332546.75 2975.08 345727 3093.19 986440 8834.45 966590 8656.58</cell></row><row><cell>venture</cell><cell>0</cell><cell>1187.5</cell><cell>5.5</cell><cell>0.46</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>2035</cell><cell>171.37</cell><cell>2000</cell><cell>168.42</cell></row><row><cell>video pinball</cell><cell>0</cell><cell cols="11">17667.9 533936.5 3022.07 572898.27 3242.59 511835 2896.98 925830 5240.18 978190 5536.54</cell></row><row><cell>wizard of wor</cell><cell cols="2">563.5 4756.5</cell><cell>17862.5</cell><cell>412.57</cell><cell>9157.5</cell><cell cols="7">204.96 29059.3 679.60 64239 1519.90 63735 1506.59</cell></row><row><cell>yars revenge</cell><cell cols="3">3092.9 54576.9 102557</cell><cell cols="9">193.19 84231.14 157.60 166292.3 316.99 972000 1881.96 968090 1874.36</cell></row><row><cell>zaxxon</cell><cell>32.5</cell><cell>9173.3</cell><cell>22209.5</cell><cell cols="9">242.62 32935.5 359.96 41118 449.47 109140 1193.63 216020 2362.89</cell></row><row><cell>MEAN HNS(%)</cell><cell>0.00</cell><cell>100.00</cell><cell></cell><cell>873.97</cell><cell></cell><cell>957.34</cell><cell></cell><cell>1741.36</cell><cell></cell><cell>7810.6</cell><cell></cell><cell>9620.98</cell></row><row><cell cols="2">Learning Efficiency 0.00</cell><cell>N/A</cell><cell></cell><cell>4.37E-08</cell><cell></cell><cell>4.79E-08</cell><cell></cell><cell>8.71E-08</cell><cell></cell><cell>3.91E-07</cell><cell></cell><cell>4.70E-07</cell></row><row><cell cols="2">MEDIAN HNS(%) 0.00</cell><cell>100.00</cell><cell></cell><cell>230.99</cell><cell></cell><cell>191.82</cell><cell></cell><cell>454.91</cell><cell></cell><cell>832.5</cell><cell></cell><cell>1146.39</cell></row><row><cell cols="2">Learning Efficiency 0.00</cell><cell>N/A</cell><cell></cell><cell>1.15E-08</cell><cell></cell><cell>9.59E-09</cell><cell></cell><cell>2.27E-08</cell><cell></cell><cell>4.16E-08</cell><cell></cell><cell>5.73E-08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 8 :</head><label>8</label><figDesc>Score table of SOTA 200M model-free algorithms on HNS. J.5.2 Comparison with SOTA 10B+ model-free algorithms on HNS</figDesc><table><row><cell>Games</cell><cell cols="10">R2D2 HNS(%) NGU HNS(%) AGENT57 HNS(%) GDI-I 3 HNS(%) GDI-H 3 HNS(%)</cell></row><row><cell>Scale</cell><cell>10B</cell><cell></cell><cell>35B</cell><cell></cell><cell>100B</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell cols="7">109038.4 1576.97 248100 3592.35 297638.17 4310.30 43384</cell><cell>625.45</cell><cell>48735</cell><cell>703.00</cell></row><row><cell>amidar</cell><cell cols="7">27751.24 1619.04 17800 1038.35 29660.08 1730.42 1442</cell><cell>83.81</cell><cell>1065</cell><cell>61.81</cell></row><row><cell>assault</cell><cell cols="10">90526.44 17379.53 34800 6654.66 67212.67 12892.66 63876 12250.50 97155 18655.23</cell></row><row><cell>asterix</cell><cell cols="10">999080 12044.30 950700 11460.94 991384.42 11951.51 759910 9160.41 999999 12055.38</cell></row><row><cell>asteroids</cell><cell cols="10">265861.2 568.12 230500 492.36 150854.61 321.70 751970 1609.72 760005 1626.94</cell></row><row><cell>atlantis</cell><cell cols="10">1576068 9662.56 1653600 10141.80 1528841.76 9370.64 3803000 23427.66 3837300 23639.67</cell></row><row><cell>bank heist</cell><cell cols="7">46285.6 6262.20 17400 2352.93 23071.5 3120.49 1401</cell><cell>187.68</cell><cell>1380</cell><cell>184.84</cell></row><row><cell>battle zone</cell><cell cols="10">513360 1388.64 691700 1871.27 934134.88 2527.36 478830 1295.20 824360 2230.29</cell></row><row><cell>beam rider</cell><cell cols="10">128236.08 772.05 63600 381.80 300509.8 1812.19 162100 976.51 422390 2548.07</cell></row><row><cell>berzerk</cell><cell cols="7">34134.8 1356.81 36200 1439.19 61507.83 2448.80 7607</cell><cell>298.53</cell><cell>14649</cell><cell>579.46</cell></row><row><cell>bowling</cell><cell>196.36</cell><cell cols="3">125.92 211.9 137.21</cell><cell>251.18</cell><cell cols="2">165.76 201.9</cell><cell>129.94</cell><cell>205.2</cell><cell>132.34</cell></row><row><cell>boxing</cell><cell>99.16</cell><cell>825.50</cell><cell>99.7</cell><cell>830.00</cell><cell>100</cell><cell>832.50</cell><cell>100</cell><cell>832.50</cell><cell>100</cell><cell>832.50</cell></row><row><cell>breakout</cell><cell cols="4">795.36 2755.76 559.2 1935.76</cell><cell>790.4</cell><cell>2738.54</cell><cell>864</cell><cell>2994.10</cell><cell>864</cell><cell>2994.10</cell></row><row><cell>centipede</cell><cell cols="10">532921.84 5347.83 577800 5799.95 412847.86 4138.15 155830 1548.84 195630 1949.80</cell></row><row><cell cols="11">chopper command 960648 14594.29 999900 15191.11 999900 15191.11 999999 15192.62 999999 15192.62</cell></row><row><cell>crazy climber</cell><cell cols="10">312768 1205.59 313400 1208.11 565909.85 2216.18 201000 759.39 241170 919.76</cell></row><row><cell>defender</cell><cell cols="10">562106 3536.22 664100 4181.16 677642.78 4266.80 893110 5629.27 970540 6118.89</cell></row><row><cell>demon attack</cell><cell cols="10">143664.6 7890.07 143500 7881.02 143161.44 7862.41 675530 37131.12 787985 43313.70</cell></row><row><cell>double dunk</cell><cell>23.12</cell><cell cols="2">1896.36 -14.1</cell><cell>204.55</cell><cell>23.93</cell><cell>1933.18</cell><cell>24</cell><cell>1936.36</cell><cell>24</cell><cell>1936.36</cell></row><row><cell>enduro</cell><cell cols="2">2376.68 276.20</cell><cell>2000</cell><cell>232.42</cell><cell>2367.71</cell><cell cols="5">275.16 14330 1665.31 14300 1661.82</cell></row><row><cell>fishing derby</cell><cell>81.96</cell><cell>328.28</cell><cell>32</cell><cell>233.84</cell><cell>86.97</cell><cell>337.75</cell><cell>59</cell><cell>285.71</cell><cell>65</cell><cell>296.22</cell></row><row><cell>freeway</cell><cell>34</cell><cell>114.86</cell><cell>28.5</cell><cell>96.28</cell><cell>32.59</cell><cell>110.10</cell><cell>34</cell><cell>114.86</cell><cell>34</cell><cell>114.86</cell></row><row><cell>frostbite</cell><cell cols="7">11238.4 261.70 206400 4832.76 541280.88 12676.32 10485</cell><cell>244.05</cell><cell>11330</cell><cell>263.84</cell></row><row><cell>gopher</cell><cell cols="10">122196 5658.66 113400 5250.47 117777.08 5453.59 488830 22672.63 473560 21964.01</cell></row><row><cell>gravitar</cell><cell>6750</cell><cell cols="5">206.93 14200 441/32 19213.96 599.07</cell><cell>5905</cell><cell>180.34</cell><cell>5915</cell><cell>180.66</cell></row><row><cell>hero</cell><cell cols="7">37030.4 120.82 69400 229.44 114736.26 381.58 38330</cell><cell>125.18</cell><cell>38225</cell><cell>124.83</cell></row><row><cell>ice hockey</cell><cell>71.56</cell><cell>683.97</cell><cell>-4.1</cell><cell>58.68</cell><cell>63.64</cell><cell cols="2">618.51 44.94</cell><cell>463.97</cell><cell>47.11</cell><cell>481.90</cell></row><row><cell>jamesbond</cell><cell cols="10">23266 8486.85 26600 9704.53 135784.96 49582.16 594500 217118.70 620780 226716.95</cell></row><row><cell>kangaroo</cell><cell>14112</cell><cell cols="6">471.34 35100 1174.92 24034.16 803.96 14500</cell><cell>484.34</cell><cell>14636</cell><cell>488.90</cell></row><row><cell>krull</cell><cell cols="10">145284.8 13460.12 127400 11784.73 251997.31 23456.61 97575 8990.82 594540 55544.92</cell></row><row><cell>kung fu master</cell><cell cols="10">200176 889.40 212100 942.45 206845.82 919.07 140440 623.64 1666665 7413.57</cell></row><row><cell cols="2">montezuma revenge 2504</cell><cell>52.68</cell><cell cols="2">10400 218.80</cell><cell>9352.01</cell><cell>196.75</cell><cell>3000</cell><cell>63.11</cell><cell>2500</cell><cell>52.60</cell></row><row><cell>ms pacman</cell><cell cols="7">29928.2 445.81 40800 609.44 63994.44 958.52 11536</cell><cell>169.00</cell><cell>11573</cell><cell>169.55</cell></row><row><cell>name this game</cell><cell cols="7">45214.8 745.61 23900 375.35 54386.77 904.94 34434</cell><cell>558.34</cell><cell>36296</cell><cell>590.68</cell></row><row><cell>phoenix</cell><cell cols="10">811621.6 125.11 959100 14786.66 908264.15 14002.29 894460 13789.30 959580 14794.07</cell></row><row><cell>pitfall</cell><cell>0</cell><cell>3.43</cell><cell>7800</cell><cell cols="3">119.97 18756.01 283.66</cell><cell>0</cell><cell>3.43</cell><cell>-4.3</cell><cell>3.36</cell></row><row><cell>pong</cell><cell>21</cell><cell>118.13</cell><cell>19.6</cell><cell>114.16</cell><cell>20.67</cell><cell>117.20</cell><cell>21</cell><cell>118.13</cell><cell>21</cell><cell>118.13</cell></row><row><cell>private eye</cell><cell>300</cell><cell>0.40</cell><cell cols="5">100000 143.75 79716.46 114.59 15100</cell><cell>21.68</cell><cell>15100</cell><cell>21.68</cell></row><row><cell>qbert</cell><cell cols="7">161000 1210.10 451900 3398.79 580328.14 4365.06 27800</cell><cell>207.93</cell><cell>28657</cell><cell>214.38</cell></row><row><cell>riverraid</cell><cell cols="7">34076.4 207.47 36700 224.10 63318.67 392.79 28075</cell><cell>169.44</cell><cell>28349</cell><cell>171.17</cell></row><row><cell>road runner</cell><cell cols="10">498660 6365.59 128600 1641.52 243025.8 3102.24 878600 11215.78 999999 12765.53</cell></row><row><cell>robotank</cell><cell>132.4</cell><cell>1342.27</cell><cell>9.1</cell><cell>71.13</cell><cell>127.32</cell><cell cols="5">1289.90 108.2 1092.78 113.4 1146.39</cell></row><row><cell>seaquest</cell><cell cols="10">999991.84 2381.55 1000000 2381.57 999997.63 2381.56 943910 2247.98 1000000 2381.57</cell></row><row><cell>skiing</cell><cell cols="4">-29970.32 -100.87 -22977.9 -46.08</cell><cell>-4202.6</cell><cell cols="2">101.05 -6774</cell><cell>80.90</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell>4198.4</cell><cell>26.71</cell><cell>4700</cell><cell>31.23</cell><cell cols="3">44199.93 387.39 11074</cell><cell>88.70</cell><cell>9105</cell><cell>70.95</cell></row><row><cell>space invaders</cell><cell cols="10">55889 3665.48 43400 2844.22 48680.86 3191.48 140460 9226.80 154380 10142.17</cell></row><row><cell>star gunner</cell><cell cols="10">521728 5435.68 414600 4318.13 839573.53 8751.40 465750 4851.72 677590 7061.61</cell></row><row><cell>surround</cell><cell>9.96</cell><cell>120.97</cell><cell>-9.6</cell><cell>2.42</cell><cell>9.5</cell><cell>118.18</cell><cell>-7.8</cell><cell>13.33</cell><cell>2.606</cell><cell>76.40</cell></row><row><cell>tennis</cell><cell>24</cell><cell>308.39</cell><cell>10.2</cell><cell>219.35</cell><cell>23.84</cell><cell>307.35</cell><cell>24</cell><cell>308.39</cell><cell>24</cell><cell>308.39</cell></row><row><cell>time pilot</cell><cell cols="10">348932 20791.28 344700 20536.51 405425.31 24192.24 216770 12834.99 450810 26924.45</cell></row><row><cell>tutankham</cell><cell>393.64</cell><cell cols="3">244.71 191.1 115.04</cell><cell cols="3">2354.91 1500.33 423.9</cell><cell>264.08</cell><cell>418.2</cell><cell>260.44</cell></row><row><cell>up n down</cell><cell cols="10">542918.8 4860.17 620100 5551.77 623805.73 5584.98 986440 8834.45 966590 8656.58</cell></row><row><cell>venture</cell><cell>1992</cell><cell>167.75</cell><cell>1700</cell><cell>143.16</cell><cell>2623.71</cell><cell>220.94</cell><cell>2035</cell><cell>171.37</cell><cell>2000</cell><cell>168.42</cell></row><row><cell>video pinball</cell><cell cols="10">483569.72 2737.00 965300 5463.58 992340.74 5616.63 925830 5240.18 978190 5536.54</cell></row><row><cell>wizard of wor</cell><cell cols="10">133264 3164.81 106200 2519.35 157306.41 3738.20 64293 1519.90 63735 1506.59</cell></row><row><cell>yars revenge</cell><cell cols="10">918854.32 1778.73 986000 1909.15 998532.37 1933.49 972000 1881.96 968090 1874.36</cell></row><row><cell>zaxxon</cell><cell cols="10">181372 1983.85 111100 1215.07 249808.9 2732.54 109140 1193.63 216020 2362.89</cell></row><row><cell>MEAN HNS(%)</cell><cell></cell><cell>3374.31</cell><cell></cell><cell>3169.90</cell><cell></cell><cell>4763.69</cell><cell></cell><cell>7810.6</cell><cell></cell><cell>9620.98</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>3.37E-09</cell><cell></cell><cell>9.06E-10</cell><cell></cell><cell>4.76E-10</cell><cell></cell><cell>3.91E-07</cell><cell></cell><cell>4.81E-07</cell></row><row><cell>MEDIAN HNS(%)</cell><cell></cell><cell>1342.27</cell><cell></cell><cell>1208.11</cell><cell></cell><cell>1933.49</cell><cell></cell><cell>832.5</cell><cell></cell><cell>1146.39</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>1.34E-09</cell><cell></cell><cell>3.45E-10</cell><cell></cell><cell>1.93E-10</cell><cell></cell><cell>4.16E-08</cell><cell></cell><cell>5.73E-08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 9 :</head><label>9</label><figDesc>Score table of SOTA model-free algorithms on HNS.J.5.3 Comparison with SOTA Model-Based Algorithms on HNSSimPLe<ref type="bibr" target="#b31">(Kaiser et al., 2019)</ref> andDreamerV2(Hafner et al., 2020)  haven't evaluated all 57 Atari Games in their paper. For fairness, we set the score on those games as N/A, which will not be considered when calculating the median and mean HNS.</figDesc><table><row><cell>Games</cell><cell cols="10">MuZero HNS(%) DreamerV2 HNS(%) SimPLe HNS(%) GDI-I 3 HNS(%) GDI-H 3 HNS(%)</cell></row><row><cell>Scale</cell><cell>20B</cell><cell></cell><cell>200M</cell><cell></cell><cell>1M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell cols="2">741812.63 10747.61</cell><cell>3483</cell><cell>47.18</cell><cell>616.9</cell><cell>5.64</cell><cell>43384</cell><cell>625.45</cell><cell>48735</cell><cell>703.00</cell></row><row><cell>amidar</cell><cell cols="2">28634.39 1670.57</cell><cell>2028</cell><cell>118.00</cell><cell>74.3</cell><cell>4.00</cell><cell>1442</cell><cell>83.81</cell><cell>1065</cell><cell>61.81</cell></row><row><cell>assault</cell><cell cols="2">143972.03 27665.44</cell><cell>7679</cell><cell cols="2">1435.07 527.2</cell><cell>58.66</cell><cell cols="4">63876 12250.50 97155 18655.23</cell></row><row><cell>asterix</cell><cell cols="3">998425 12036.40 25669</cell><cell cols="7">306.98 1128.3 11.07 759910 9160.41 999999 12055.38</cell></row><row><cell>asteroids</cell><cell cols="2">678558.64 1452.42</cell><cell>3064</cell><cell>5.02</cell><cell>793.6</cell><cell>0.16</cell><cell cols="4">751970 1609.72 760005 1626.94</cell></row><row><cell>atlantis</cell><cell cols="10">1674767.2 10272.64 989207 6035.05 20992.5 50.33 3803000 23427.66 3837300 23639.67</cell></row><row><cell>bank heist</cell><cell>1278.98</cell><cell>171.17</cell><cell>1043</cell><cell>139.23</cell><cell>34.2</cell><cell>2.71</cell><cell>1401</cell><cell>187.68</cell><cell>1380</cell><cell>184.84</cell></row><row><cell>battle zone</cell><cell cols="2">848623 2295.95</cell><cell>31225</cell><cell cols="7">83.86 4031.2 10.27 478830 1295.20 824360 2230.29</cell></row><row><cell>beam rider</cell><cell cols="2">454993.53 2744.92</cell><cell>12413</cell><cell>72.75</cell><cell>621.6</cell><cell>1.56</cell><cell cols="4">162100 976.51 422390 2548.07</cell></row><row><cell>berzerk</cell><cell cols="2">85932.6 3423.18</cell><cell>751</cell><cell>25.02</cell><cell>N/A</cell><cell>N/A</cell><cell>7607</cell><cell>298.53</cell><cell>14649</cell><cell>579.46</cell></row><row><cell>bowling</cell><cell>260.13</cell><cell>172.26</cell><cell>48</cell><cell>18.10</cell><cell>30</cell><cell>5.01</cell><cell>202</cell><cell>129.94</cell><cell>205.2</cell><cell>132.34</cell></row><row><cell>boxing</cell><cell>100</cell><cell>832.50</cell><cell>87</cell><cell>724.17</cell><cell>7.8</cell><cell>64.17</cell><cell>100</cell><cell>832.50</cell><cell>100</cell><cell>832.50</cell></row><row><cell>breakout</cell><cell>864</cell><cell>2994.10</cell><cell>350</cell><cell cols="2">1209.38 16.4</cell><cell>51.04</cell><cell>864</cell><cell>2994.10</cell><cell>864</cell><cell>2994.10</cell></row><row><cell>centipede</cell><cell cols="2">1159049.27 11655.72</cell><cell>6601</cell><cell>45.44</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">155830 1548.84 195630 1949.80</cell></row><row><cell cols="3">chopper command 991039.7 15056.39</cell><cell>2833</cell><cell>30.74</cell><cell>979.4</cell><cell>2.56</cell><cell cols="4">999999 15192.62 999999 15192.62</cell></row><row><cell>crazy climber</cell><cell cols="2">458315.4 1786.64</cell><cell>141424</cell><cell cols="7">521.55 62583.6 206.81 201000 759.39 241170 919.76</cell></row><row><cell>defender</cell><cell cols="2">839642.95 5291.18</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">893110 5629.27 970540 6118.89</cell></row><row><cell>demon attack</cell><cell cols="2">143964.26 7906.55</cell><cell>2775</cell><cell cols="2">144.20 208.1</cell><cell>3.08</cell><cell cols="4">675530 37131.12 787985 43313.70</cell></row><row><cell>double dunk</cell><cell>23.94</cell><cell>1933.64</cell><cell>22</cell><cell cols="2">1845.45 N/A</cell><cell>N/A</cell><cell>24</cell><cell>1936.36</cell><cell>24</cell><cell>1936.36</cell></row><row><cell>enduro</cell><cell>2382.44</cell><cell>276.87</cell><cell>2112</cell><cell>245.44</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">14330 1665.31 14300 1661.82</cell></row><row><cell>fishing derby</cell><cell>91.16</cell><cell>345.67</cell><cell>60</cell><cell cols="2">286.77 -90.7</cell><cell>1.89</cell><cell>59</cell><cell>285.71</cell><cell>65</cell><cell>296.22</cell></row><row><cell>freeway</cell><cell>33.03</cell><cell>111.59</cell><cell>34</cell><cell>114.86</cell><cell>16.7</cell><cell>56.42</cell><cell>34</cell><cell>114.86</cell><cell>34</cell><cell>114.86</cell></row><row><cell>frostbite</cell><cell cols="3">631378.53 14786.59 15622</cell><cell cols="2">364.37 236.9</cell><cell>4.02</cell><cell>10485</cell><cell>244.05</cell><cell>11330</cell><cell>263.84</cell></row><row><cell>gopher</cell><cell cols="2">130345.58 6036.85</cell><cell>53853</cell><cell cols="2">2487.14 596.8</cell><cell cols="5">15.74 488830 22672.6 473560 21964.01</cell></row><row><cell>gravitar</cell><cell>6682.7</cell><cell>204.81</cell><cell>3554</cell><cell cols="2">106.37 173.4</cell><cell>0.01</cell><cell>5905</cell><cell>180.34</cell><cell>5915</cell><cell>180.66</cell></row><row><cell>hero</cell><cell cols="2">49244.11 161.81</cell><cell>30287</cell><cell cols="2">98.19 2656.6</cell><cell>5.47</cell><cell>38330</cell><cell>125.18</cell><cell>38225</cell><cell>124.83</cell></row><row><cell>ice hockey</cell><cell>67.04</cell><cell>646.61</cell><cell>29</cell><cell cols="2">332.23 -11.6</cell><cell>-3.31</cell><cell>44.94</cell><cell>463.97</cell><cell>47.11</cell><cell>481.90</cell></row><row><cell>jamesbond</cell><cell cols="2">41063.25 14986.94</cell><cell>9269</cell><cell cols="2">3374.73 100.5</cell><cell cols="5">26.11 594500 217118.70 620780 226716.95</cell></row><row><cell>kangaroo</cell><cell cols="2">16763.6 560.23</cell><cell>11819</cell><cell>394.47</cell><cell>51.2</cell><cell>-0.03</cell><cell>14500</cell><cell>484.34</cell><cell>14636</cell><cell>488.90</cell></row><row><cell>krull</cell><cell cols="2">269358.27 25082.93</cell><cell>9687</cell><cell cols="3">757.75 2204.8 56.84</cell><cell cols="4">97575 8990.82 594540 55544.92</cell></row><row><cell>kung fu master</cell><cell>204824</cell><cell>910.08</cell><cell>66410</cell><cell cols="7">294.30 14862.5 64.97 140440 623.64 1666665 7413.57</cell></row><row><cell>montezuma revenge</cell><cell>0</cell><cell>0.00</cell><cell>1932</cell><cell>40.65</cell><cell>N/A</cell><cell>N/A</cell><cell>3000</cell><cell>63.11</cell><cell>2500</cell><cell>52.60</cell></row><row><cell>ms pacman</cell><cell cols="2">243401.1 3658.68</cell><cell>5651</cell><cell>80.43</cell><cell>1480</cell><cell>17.65</cell><cell>11536</cell><cell>169.00</cell><cell>11573</cell><cell>169.55</cell></row><row><cell cols="3">name this game 157177.85 2690.53</cell><cell>14472</cell><cell cols="2">211.57 2420.7</cell><cell>2.23</cell><cell>34434</cell><cell>558.34</cell><cell>36296</cell><cell>590.68</cell></row><row><cell>phoenix</cell><cell cols="3">955137.84 14725.53 13342</cell><cell>194.11</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">894460 13789.30 959580 14794.07</cell></row><row><cell>pitfall</cell><cell>0</cell><cell>3.43</cell><cell>-1</cell><cell>3.41</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>3.43</cell><cell>-4.3</cell><cell>3.36</cell></row><row><cell>pong</cell><cell>21</cell><cell>118.13</cell><cell>19</cell><cell>112.46</cell><cell>12.8</cell><cell>94.90</cell><cell>21</cell><cell>118.13</cell><cell>21</cell><cell>118.13</cell></row><row><cell>private eye</cell><cell>15299.98</cell><cell>21.96</cell><cell>158</cell><cell>0.19</cell><cell>35</cell><cell>0.01</cell><cell>15100</cell><cell>21.68</cell><cell>15100</cell><cell>21.68</cell></row><row><cell>qbert</cell><cell>72276</cell><cell>542.56</cell><cell cols="3">162023 1217.80 1288.8</cell><cell>8.46</cell><cell>27800</cell><cell>207.93</cell><cell>28657</cell><cell>214.38</cell></row><row><cell>riverraid</cell><cell cols="2">323417.18 2041.12</cell><cell>16249</cell><cell cols="2">94.49 1957.8</cell><cell>3.92</cell><cell>28075</cell><cell>169.44</cell><cell>28349</cell><cell>171.17</cell></row><row><cell>road runner</cell><cell cols="2">613411.8 7830.48</cell><cell>88772</cell><cell cols="7">1133.09 5640.6 71.86 878600 11215.78 999999 12765.53</cell></row><row><cell>robotank</cell><cell>131.13</cell><cell>1329.18</cell><cell>65</cell><cell>647.42</cell><cell>N/A</cell><cell>N/A</cell><cell>108</cell><cell cols="3">1092.78 113.4 1146.39</cell></row><row><cell>seaquest</cell><cell cols="2">999976.52 2381.51</cell><cell>45898</cell><cell cols="2">109.15 683.3</cell><cell>1.46</cell><cell cols="4">943910 2247.98 1000000 2381.57</cell></row><row><cell>skiing</cell><cell cols="2">-29968.36 -100.86</cell><cell>-8187</cell><cell>69.83</cell><cell>N/A</cell><cell>N/A</cell><cell>-6774</cell><cell>80.90</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell>56.62</cell><cell>-10.64</cell><cell>883</cell><cell>-3.19</cell><cell>N/A</cell><cell>N/A</cell><cell>11074</cell><cell>88.70</cell><cell>9105</cell><cell>70.95</cell></row><row><cell>space invaders</cell><cell cols="2">74335.3 4878.50</cell><cell>2611</cell><cell>161.96</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">140460 9226.80 154380 10142.17</cell></row><row><cell>star gunner</cell><cell cols="2">549271.7 5723.01</cell><cell>29219</cell><cell>297.88</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">465750 4851.72 677590 7061.61</cell></row><row><cell>surround</cell><cell>9.99</cell><cell>121.15</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>-7.8</cell><cell>13.33</cell><cell>2.606</cell><cell>76.40</cell></row><row><cell>tennis</cell><cell>0</cell><cell>153.55</cell><cell>23</cell><cell>301.94</cell><cell>N/A</cell><cell>N/A</cell><cell>24</cell><cell>308.39</cell><cell>24</cell><cell>308.39</cell></row><row><cell>time pilot</cell><cell cols="3">476763.9 28486.90 32404</cell><cell cols="2">1735.96 N/A</cell><cell>N/A</cell><cell cols="4">216770 12834.99 450810 26924.45</cell></row><row><cell>tutankham</cell><cell>491.48</cell><cell>307.35</cell><cell>238</cell><cell>145.07</cell><cell>N/A</cell><cell>N/A</cell><cell>424</cell><cell>264.08</cell><cell>418.2</cell><cell>260.44</cell></row><row><cell>up n down</cell><cell cols="10">715545.61 6407.03 648363 5805.03 3350.3 25.24 986440 8834.45 966590 8656.58</cell></row><row><cell>venture</cell><cell>0.4</cell><cell>0.03</cell><cell>0</cell><cell>0.00</cell><cell>N/A</cell><cell>N/A</cell><cell>2035</cell><cell>171.37</cell><cell>2000</cell><cell>168.42</cell></row><row><cell>video pinball</cell><cell cols="2">981791.88 5556.92</cell><cell>22218</cell><cell>125.75</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">925830 5240.18 978190 5536.54</cell></row><row><cell>wizard of wor</cell><cell cols="2">197126 4687.87</cell><cell>14531</cell><cell>333.11</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">64439 1523.38 63735 1506.59</cell></row><row><cell>yars revenge</cell><cell cols="2">553311.46 1068.72</cell><cell>20089</cell><cell cols="2">33.01 5664.3</cell><cell>4.99</cell><cell cols="4">972000 1881.96 968090 1874.36</cell></row><row><cell>zaxxon</cell><cell cols="2">725853.9 7940.46</cell><cell>18295</cell><cell>199.79</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">109140 1193.63 216020 2362.89</cell></row><row><cell>MEAN HNS(%)</cell><cell></cell><cell>4996.20</cell><cell></cell><cell>631.17</cell><cell></cell><cell>25.3</cell><cell></cell><cell>7810.6</cell><cell></cell><cell>9620.98</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>2.50E-09</cell><cell></cell><cell>3.16E-08</cell><cell></cell><cell>2.53E-07</cell><cell></cell><cell>3.91E-07</cell><cell></cell><cell>4.81E-07</cell></row><row><cell>MEDIAN HNS(%)</cell><cell></cell><cell>2041.12</cell><cell></cell><cell>161.96</cell><cell></cell><cell>5.55</cell><cell></cell><cell>832.5</cell><cell></cell><cell>1146.39</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>1.02E-09</cell><cell></cell><cell>8.10E-09</cell><cell></cell><cell>5.55E-08</cell><cell></cell><cell>4.16E-08</cell><cell></cell><cell>5.73E-08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 10 :</head><label>10</label><figDesc>Score table of SOTA model-based algorithms on HNS.J.5.4 Comparison with Other SOTA algorithms on HNSIn this section, we report the performance of our algorithm compared with other SOTA algorithms,Go-Explore (Ecoffet et al., 2019)  andMuesli (Hessel et al., 2021).</figDesc><table><row><cell>Games</cell><cell cols="8">Muesli HNS(%) Go-Explore HNS(%) GDI-I 3 HNS(%) GDI-H 3 HNS(%)</cell></row><row><cell>Scale</cell><cell>200M</cell><cell></cell><cell>10B</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell cols="5">139409 2017.12 959312 13899.77 43384</cell><cell>625.45</cell><cell>48735</cell><cell>703.00</cell></row><row><cell>amidar</cell><cell cols="2">21653 1263.18</cell><cell>19083</cell><cell cols="2">1113.22 1442</cell><cell>83.81</cell><cell>1065</cell><cell>61.81</cell></row><row><cell>assault</cell><cell cols="2">36963 7070.94</cell><cell>30773</cell><cell cols="5">5879.64 63876 12250.50 97155 18655.23</cell></row><row><cell>asterix</cell><cell cols="8">316210 3810.30 999500 12049.37 759910 9160.41 999999 12055.38</cell></row><row><cell>asteroids</cell><cell cols="3">484609 1036.84 112952</cell><cell cols="5">240.48 751970 1609.72 760005 1626.94</cell></row><row><cell>atlantis</cell><cell cols="8">1363427 8348.18 286460 1691.24 3803000 23427.66 3837300 23639.67</cell></row><row><cell>bank heist</cell><cell>1213</cell><cell>162.24</cell><cell>3668</cell><cell>494.49</cell><cell>1401</cell><cell>187.68</cell><cell>1380</cell><cell>184.84</cell></row><row><cell>battle zone</cell><cell cols="8">414107 1120.04 998800 2702.36 478830 1295.20 824360 2230.29</cell></row><row><cell>beam rider</cell><cell cols="8">288870 1741.91 371723 2242.15 162100 976.51 422390 2548.07</cell></row><row><cell>berzerk</cell><cell cols="5">44478 1769.43 131417 5237.69 7607</cell><cell>298.53</cell><cell>14649</cell><cell>579.46</cell></row><row><cell>bowling</cell><cell>191</cell><cell>122.02</cell><cell>247</cell><cell>162.72</cell><cell>202</cell><cell>129.94</cell><cell>205.2</cell><cell>132.34</cell></row><row><cell>boxing</cell><cell>99</cell><cell>824.17</cell><cell>91</cell><cell>757.50</cell><cell>100</cell><cell>832.50</cell><cell>100</cell><cell>832.50</cell></row><row><cell>breakout</cell><cell>791</cell><cell>2740.63</cell><cell>774</cell><cell>2681.60</cell><cell>864</cell><cell>2994.10</cell><cell>864</cell><cell>2994.10</cell></row><row><cell>centipede</cell><cell cols="8">869751 8741.20 613815 6162.78 155830 1548.84 195630 1949.80</cell></row><row><cell cols="9">chopper command 101289 1527.76 996220 15135.16 999999 15192.62 999999 15192.62</cell></row><row><cell>crazy climber</cell><cell cols="2">175322 656.88</cell><cell>235600</cell><cell cols="5">897.52 201000 759.39 241170 919.76</cell></row><row><cell>defender</cell><cell cols="2">629482 3962.26</cell><cell>N/A</cell><cell>N/A</cell><cell cols="4">893110 5629.27 970540 6118.89</cell></row><row><cell>demon attack</cell><cell cols="8">129544 7113.74 239895 13180.65 675530 37131.12 787985 43313.70</cell></row><row><cell>double dunk</cell><cell>-3</cell><cell>709.09</cell><cell>24</cell><cell>1936.36</cell><cell>24</cell><cell>1936.36</cell><cell>24</cell><cell>1936.36</cell></row><row><cell>enduro</cell><cell>2362</cell><cell>274.49</cell><cell>1031</cell><cell cols="5">119.81 14330 1665.31 14300 1661.82</cell></row><row><cell>fishing derby</cell><cell>51</cell><cell>269.75</cell><cell>67</cell><cell>300.00</cell><cell>59</cell><cell>285.71</cell><cell>65</cell><cell>296.22</cell></row><row><cell>freeway</cell><cell>33</cell><cell>111.49</cell><cell>34</cell><cell>114.86</cell><cell>34</cell><cell>114.86</cell><cell>34</cell><cell>114.86</cell></row><row><cell>frostbite</cell><cell cols="5">301694 7064.73 999990 23420.19 10485</cell><cell>244.05</cell><cell>11330</cell><cell>263.84</cell></row><row><cell>gopher</cell><cell cols="8">104441 4834.72 134244 6217.75 488830 22672.63 473560 21964.01</cell></row><row><cell>gravitar</cell><cell cols="2">11660 361.41</cell><cell>13385</cell><cell>415.68</cell><cell>5905</cell><cell>180.34</cell><cell>5915</cell><cell>180.66</cell></row><row><cell>hero</cell><cell cols="2">37161 121.26</cell><cell>37783</cell><cell cols="2">123.34 38330</cell><cell>125.18</cell><cell>38225</cell><cell>124.83</cell></row><row><cell>ice hockey</cell><cell>25</cell><cell>299.17</cell><cell>33</cell><cell cols="2">365.29 44.94</cell><cell>463.97</cell><cell>47.11</cell><cell>481.90</cell></row><row><cell>jamesbond</cell><cell cols="8">19319 7045.29 200810 73331.26 594500 217118.70 620780 226716.95</cell></row><row><cell>kangaroo</cell><cell cols="2">14096 470.80</cell><cell>24300</cell><cell cols="2">812.87 14500</cell><cell>484.34</cell><cell>14636</cell><cell>488.90</cell></row><row><cell>krull</cell><cell cols="2">34221 3056.02</cell><cell>63149</cell><cell cols="5">5765.90 97575 8990.82 594540 55544.92</cell></row><row><cell>kung fu master</cell><cell cols="2">134689 598.06</cell><cell>24320</cell><cell cols="5">107.05 140440 623.64 1666665 7413.57</cell></row><row><cell cols="2">montezuma revenge 2359</cell><cell>49.63</cell><cell>24758</cell><cell>520.86</cell><cell>3000</cell><cell>63.11</cell><cell>2500</cell><cell>52.60</cell></row><row><cell>ms pacman</cell><cell cols="2">65278 977.84</cell><cell cols="3">456123 6860.25 11536</cell><cell>169.00</cell><cell>11573</cell><cell>169.55</cell></row><row><cell cols="6">name this game 105043 1784.89 212824 3657.16 34434</cell><cell>558.34</cell><cell>36296</cell><cell>590.68</cell></row><row><cell>phoenix</cell><cell cols="3">805305 12413.69 19200</cell><cell cols="5">284.50 894460 13789.30 959580 14794.07</cell></row><row><cell>pitfall</cell><cell>0</cell><cell>3.43</cell><cell>7875</cell><cell>121.09</cell><cell>0</cell><cell>3.43</cell><cell>-4.3</cell><cell>3.36</cell></row><row><cell>pong</cell><cell>20</cell><cell>115.30</cell><cell>21</cell><cell>118.13</cell><cell>21</cell><cell>118.13</cell><cell>21</cell><cell>118.13</cell></row><row><cell>private eye</cell><cell>10323</cell><cell>14.81</cell><cell>69976</cell><cell cols="2">100.58 15100</cell><cell>21.68</cell><cell>15100</cell><cell>21.68</cell></row><row><cell>qbert</cell><cell cols="5">157353 1182.66 999975 7522.41 27800</cell><cell>207.93</cell><cell>28657</cell><cell>214.38</cell></row><row><cell>riverraid</cell><cell cols="2">47323 291.42</cell><cell>35588</cell><cell cols="2">217.05 28075</cell><cell>169.44</cell><cell>28349</cell><cell>171.17</cell></row><row><cell>road runner</cell><cell cols="8">327025 4174.55 999900 12764.26 878600 11215.78 999999 12765.53</cell></row><row><cell>robotank</cell><cell>59</cell><cell>585.57</cell><cell>143</cell><cell>1451.55</cell><cell>108</cell><cell>1092.78</cell><cell>113.4</cell><cell>1146.39</cell></row><row><cell>seaquest</cell><cell cols="8">815970 1943.26 539456 1284.68 943910 2247.98 1000000 2381.57</cell></row><row><cell>skiing</cell><cell cols="2">-18407 -10.26</cell><cell>-4185</cell><cell cols="2">101.19 -6774</cell><cell>80.90</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell>3031</cell><cell>16.18</cell><cell>20306</cell><cell cols="2">171.95 11074</cell><cell>88.70</cell><cell>9105</cell><cell>70.95</cell></row><row><cell>space invaders</cell><cell cols="2">59602 3909.65</cell><cell>93147</cell><cell cols="5">6115.54 140460 9226.80 154380 10142.17</cell></row><row><cell>star gunner</cell><cell cols="8">214383 2229.49 609580 6352.14 465750 4851.72 677590 7061.61</cell></row><row><cell>surround</cell><cell>9</cell><cell>115.15</cell><cell>N/A</cell><cell>N/A</cell><cell>-8</cell><cell>13.33</cell><cell>2.606</cell><cell>76.40</cell></row><row><cell>tennis</cell><cell>12</cell><cell>230.97</cell><cell>24</cell><cell>308.39</cell><cell>24</cell><cell>308.39</cell><cell>24</cell><cell>308.39</cell></row><row><cell>time pilot</cell><cell cols="8">359105 21403.71 183620 10839.32 216770 12834.99 450810 26924.45</cell></row><row><cell>tutankham</cell><cell>252</cell><cell>154.03</cell><cell>528</cell><cell>330.73</cell><cell>424</cell><cell>264.08</cell><cell>418.2</cell><cell>260.44</cell></row><row><cell>up n down</cell><cell cols="8">649190 5812.44 553718 4956.94 986440 8834.45 966590 8656.58</cell></row><row><cell>venture</cell><cell>2104</cell><cell>177.18</cell><cell>3074</cell><cell>258.86</cell><cell>2035</cell><cell>171.37</cell><cell>2000</cell><cell>168.42</cell></row><row><cell>video pinball</cell><cell cols="8">685436 3879.56 999999 5659.98 925830 5240.18 978190 5536.54</cell></row><row><cell>wizard of wor</cell><cell cols="8">93291 2211.48 199900 4754.03 64293 1519.90 63735 1506.59</cell></row><row><cell>yars revenge</cell><cell cols="8">557818 1077.47 999998 1936.34 972000 1881.96 968090 1874.36</cell></row><row><cell>zaxxon</cell><cell cols="2">65325 714.30</cell><cell>18340</cell><cell cols="5">200.28 109140 1193.63 216020 2362.89</cell></row><row><cell>MEAN HNS(%)</cell><cell></cell><cell>2538.66</cell><cell></cell><cell>4989.94</cell><cell></cell><cell>7810.6</cell><cell></cell><cell>9620.98</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>1.27E-07</cell><cell></cell><cell>4.99E-09</cell><cell></cell><cell>3.91E-07</cell><cell></cell><cell>4.81E-07</cell></row><row><cell>MEDIAN HNS(%)</cell><cell></cell><cell>1077.47</cell><cell></cell><cell>1451.55</cell><cell></cell><cell>832.5</cell><cell></cell><cell>1146.39</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>5.39E-08</cell><cell></cell><cell>1.45E-09</cell><cell></cell><cell>4.16E-08</cell><cell></cell><cell>5.73E-08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 11 :</head><label>11</label><figDesc>Score table of other SOTA algorithms on HNS. J.6 Atari Games Table of Scores Based on Human World Records</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 12 :</head><label>12</label><figDesc>Score table of SOTA 200M model-free algorithms on HWRNS J.6.2 Comparison with SOTA 10B+ Model-Free Algorithms on HWRNS</figDesc><table><row><cell>Games</cell><cell cols="10">R2D2 HWRNS(%) NGU HWRNS(%) AGENT57 HWRNS(%) GDI-I 3 HWRNS(%) GDI-H 3 HWRNS(%)</cell></row><row><cell>Scale</cell><cell>10B</cell><cell></cell><cell>35B</cell><cell></cell><cell>100B</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell>109038.4</cell><cell>43.23</cell><cell>248100</cell><cell>98.48</cell><cell>297638.17</cell><cell>118.17</cell><cell>43384</cell><cell>17.15</cell><cell>48735</cell><cell>19.27</cell></row><row><cell>amidar</cell><cell>27751.24</cell><cell>26.64</cell><cell>17800</cell><cell>17.08</cell><cell>29660.08</cell><cell>28.47</cell><cell>1442</cell><cell>1.38</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>assault</cell><cell cols="2">90526.44 1071.91</cell><cell>34800</cell><cell>410.44</cell><cell>67212.67</cell><cell>795.17</cell><cell>63876</cell><cell>755.57</cell><cell>97155</cell><cell>1150.59</cell></row><row><cell>asterix</cell><cell>999080</cell><cell>99.91</cell><cell>950700</cell><cell>95.07</cell><cell>991384.42</cell><cell>99.14</cell><cell>759910</cell><cell>75.99</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>asteroids</cell><cell>265861.2</cell><cell>2.52</cell><cell>230500</cell><cell>2.19</cell><cell>150854.61</cell><cell>1.43</cell><cell>751970</cell><cell>7.15</cell><cell>760005</cell><cell>7.23</cell></row><row><cell>atlantis</cell><cell>1576068</cell><cell>14.76</cell><cell>1653600</cell><cell>15.49</cell><cell>1528841.76</cell><cell>14.31</cell><cell>3803000</cell><cell>35.78</cell><cell>3837300</cell><cell>36.11</cell></row><row><cell>bank heist</cell><cell>46285.6</cell><cell>56.40</cell><cell>17400</cell><cell>21.19</cell><cell>23071.5</cell><cell>28.10</cell><cell>1401</cell><cell>1.69</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>battle zone</cell><cell>513360</cell><cell>64.08</cell><cell>691700</cell><cell>86.35</cell><cell>934134.88</cell><cell>116.63</cell><cell>478830</cell><cell>59.77</cell><cell>824360</cell><cell>102.92</cell></row><row><cell>beam rider</cell><cell>128236.08</cell><cell>12.79</cell><cell>63600</cell><cell>6.33</cell><cell>300509.8</cell><cell>30.03</cell><cell>162100</cell><cell>16.18</cell><cell>422390</cell><cell>42.22</cell></row><row><cell>berzerk</cell><cell>34134.8</cell><cell>3.22</cell><cell>36200</cell><cell>3.41</cell><cell>61507.83</cell><cell>5.80</cell><cell>7607</cell><cell>0.71</cell><cell>14649</cell><cell>1.37</cell></row><row><cell>bowling</cell><cell>196.36</cell><cell>62.57</cell><cell>211.9</cell><cell>68.18</cell><cell>251.18</cell><cell>82.37</cell><cell>201.9</cell><cell>64.57</cell><cell>205.2</cell><cell>65.76</cell></row><row><cell>boxing</cell><cell>99.16</cell><cell>99.16</cell><cell>99.7</cell><cell>99.70</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>breakout</cell><cell>795.36</cell><cell>92.04</cell><cell>559.2</cell><cell>64.65</cell><cell>790.4</cell><cell>91.46</cell><cell>864</cell><cell>100.00</cell><cell>864</cell><cell>100.00</cell></row><row><cell>centipede</cell><cell>532921.84</cell><cell>40.85</cell><cell>577800</cell><cell>44.30</cell><cell>412847.86</cell><cell>31.61</cell><cell>155830</cell><cell>11.83</cell><cell>195630</cell><cell>14.89</cell></row><row><cell>chopper command</cell><cell>960648</cell><cell>96.06</cell><cell>999900</cell><cell>99.99</cell><cell>999900</cell><cell>99.99</cell><cell>999999</cell><cell>100.00</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>crazy climber</cell><cell>312768</cell><cell>144.41</cell><cell>313400</cell><cell>144.71</cell><cell>565909.85</cell><cell>265.46</cell><cell>201000</cell><cell>90.96</cell><cell>241170</cell><cell>110.17</cell></row><row><cell>defender</cell><cell>562106</cell><cell>9.31</cell><cell>664100</cell><cell>11.01</cell><cell>677642.78</cell><cell>11.23</cell><cell>893110</cell><cell>14.82</cell><cell>970540</cell><cell>16.11</cell></row><row><cell>demon attack</cell><cell>143664.6</cell><cell>9.22</cell><cell>143500</cell><cell>9.21</cell><cell>143161.44</cell><cell>9.19</cell><cell>675530</cell><cell>43.40</cell><cell>787985</cell><cell>50.63</cell></row><row><cell>double dunk</cell><cell>23.12</cell><cell>105.35</cell><cell>-14.1</cell><cell>11.36</cell><cell>23.93</cell><cell>107.40</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell></row><row><cell>enduro</cell><cell>2376.68</cell><cell>25.02</cell><cell>2000</cell><cell>21.05</cell><cell>2367.71</cell><cell>24.92</cell><cell>14330</cell><cell>150.84</cell><cell>14300</cell><cell>150.53</cell></row><row><cell>fishing derby</cell><cell>81.96</cell><cell>106.74</cell><cell>32</cell><cell>76.03</cell><cell>86.97</cell><cell>109.82</cell><cell>59</cell><cell>92.89</cell><cell>65</cell><cell>96.31</cell></row><row><cell>freeway</cell><cell>34</cell><cell>89.47</cell><cell>28.5</cell><cell>75.00</cell><cell>32.59</cell><cell>85.76</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell></row><row><cell>frostbite</cell><cell>11238.4</cell><cell>2.46</cell><cell>206400</cell><cell>45.37</cell><cell>541280.88</cell><cell>119.01</cell><cell>10485</cell><cell>2.29</cell><cell>11330</cell><cell>2.48</cell></row><row><cell>gopher</cell><cell>122196</cell><cell>34.37</cell><cell>113400</cell><cell>31.89</cell><cell>117777.08</cell><cell>33.12</cell><cell>488830</cell><cell>137.71</cell><cell>473560</cell><cell>133.41</cell></row><row><cell>gravitar</cell><cell>6750</cell><cell>4.04</cell><cell>14200</cell><cell>8.62</cell><cell>19213.96</cell><cell>11.70</cell><cell>5905</cell><cell>3.52</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>hero</cell><cell>37030.4</cell><cell>3.60</cell><cell>69400</cell><cell>6.84</cell><cell>114736.26</cell><cell>11.38</cell><cell>38330</cell><cell>3.73</cell><cell>38225</cell><cell>3.72</cell></row><row><cell>ice hockey</cell><cell>71.56</cell><cell>175.34</cell><cell>-4.1</cell><cell>15.04</cell><cell>63.64</cell><cell>158.56</cell><cell>37.89</cell><cell>118.94</cell><cell>47.11</cell><cell>123.54</cell></row><row><cell>jamesbond</cell><cell>23266</cell><cell>51.05</cell><cell>26600</cell><cell>58.37</cell><cell>135784.96</cell><cell>298.23</cell><cell cols="4">594500 1305.93 620780 1363.66</cell></row><row><cell>kangaroo</cell><cell>14112</cell><cell>0.99</cell><cell>35100</cell><cell>2.46</cell><cell>24034.16</cell><cell>1.68</cell><cell>14500</cell><cell>1.01</cell><cell>14636</cell><cell>1.02</cell></row><row><cell>krull</cell><cell>145284.8</cell><cell>140.18</cell><cell>127400</cell><cell>122.73</cell><cell>251997.31</cell><cell>244.29</cell><cell>97575</cell><cell>93.63</cell><cell>594540</cell><cell>578.47</cell></row><row><cell>kung fu master</cell><cell>200176</cell><cell>20.00</cell><cell>212100</cell><cell>21.19</cell><cell>206845.82</cell><cell>20.66</cell><cell>140440</cell><cell>14.02</cell><cell cols="2">1666665 166.68</cell></row><row><cell>montezuma revenge</cell><cell>2504</cell><cell>0.21</cell><cell>10400</cell><cell>0.85</cell><cell>9352.01</cell><cell>0.77</cell><cell>3000</cell><cell>0.25</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>ms pacman</cell><cell>29928.2</cell><cell>10.22</cell><cell>40800</cell><cell>13.97</cell><cell>63994.44</cell><cell>21.98</cell><cell>11536</cell><cell>3.87</cell><cell>11573</cell><cell>3.89</cell></row><row><cell>name this game</cell><cell>45214.8</cell><cell>187.21</cell><cell>23900</cell><cell>94.24</cell><cell>54386.77</cell><cell>227.21</cell><cell>34434</cell><cell>140.19</cell><cell>36296</cell><cell>148.31</cell></row><row><cell>phoenix</cell><cell>811621.6</cell><cell>20.20</cell><cell>959100</cell><cell>23.88</cell><cell>908264.15</cell><cell>22.61</cell><cell>894460</cell><cell>22.27</cell><cell>959580</cell><cell>23.89</cell></row><row><cell>pitfall</cell><cell>0</cell><cell>0.20</cell><cell>7800</cell><cell>7.03</cell><cell>18756.01</cell><cell>16.62</cell><cell>0</cell><cell>0.20</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>pong</cell><cell>21</cell><cell>100.00</cell><cell>19.6</cell><cell>96.64</cell><cell>20.67</cell><cell>99.21</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>private eye</cell><cell>300</cell><cell>0.27</cell><cell>100000</cell><cell>98.23</cell><cell>79716.46</cell><cell>78.30</cell><cell>15100</cell><cell>14.81</cell><cell>15100</cell><cell>14.81</cell></row><row><cell>qbert</cell><cell>161000</cell><cell>6.70</cell><cell>451900</cell><cell>18.82</cell><cell>580328.14</cell><cell>24.18</cell><cell>27800</cell><cell>1.15</cell><cell>28657</cell><cell>1.19</cell></row><row><cell>riverraid</cell><cell>34076.4</cell><cell>3.28</cell><cell>36700</cell><cell>3.54</cell><cell>63318.67</cell><cell>6.21</cell><cell>28075</cell><cell>2.68</cell><cell>28349</cell><cell>2.70</cell></row><row><cell>road runner</cell><cell>498660</cell><cell>24.47</cell><cell>128600</cell><cell>6.31</cell><cell>243025.8</cell><cell>11.92</cell><cell>878600</cell><cell>43.11</cell><cell>999999</cell><cell>49.06</cell></row><row><cell>robotank</cell><cell>132.4</cell><cell>176.42</cell><cell>9.1</cell><cell>9.35</cell><cell>127.32</cell><cell>169.54</cell><cell>108</cell><cell>143.63</cell><cell>113.4</cell><cell>150.68</cell></row><row><cell>seaquest</cell><cell cols="2">999991.84 100.00</cell><cell>1000000</cell><cell>100.00</cell><cell>999997.63</cell><cell>100.00</cell><cell>943910</cell><cell>94.39</cell><cell cols="2">1000000 100.00</cell></row><row><cell>skiing</cell><cell>-29970.32</cell><cell>-93.10</cell><cell>-22977.9</cell><cell>-42.53</cell><cell>-4202.6</cell><cell>93.27</cell><cell>-6774</cell><cell>74.67</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell>4198.4</cell><cell>2.69</cell><cell>4700</cell><cell>3.14</cell><cell>44199.93</cell><cell>38.99</cell><cell>11074</cell><cell>8.93</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>space invaders</cell><cell>55889</cell><cell>8.97</cell><cell>43400</cell><cell>6.96</cell><cell>48680.86</cell><cell>7.81</cell><cell>140460</cell><cell>22.58</cell><cell>154380</cell><cell>24.82</cell></row><row><cell>star gunner</cell><cell>521728</cell><cell>679.03</cell><cell>414600</cell><cell>539.43</cell><cell cols="3">839573.53 1093.24 465750</cell><cell>606.09</cell><cell>677590</cell><cell>882.15</cell></row><row><cell>surround</cell><cell>9.96</cell><cell>101.84</cell><cell>-9.6</cell><cell>2.04</cell><cell>9.5</cell><cell>99.49</cell><cell>-7.8</cell><cell>11.22</cell><cell>2.606</cell><cell>64.32</cell></row><row><cell>tennis</cell><cell>24</cell><cell>106.70</cell><cell>10.2</cell><cell>75.89</cell><cell>23.84</cell><cell>106.34</cell><cell>24</cell><cell>106.70</cell><cell>24</cell><cell>106.70</cell></row><row><cell>time pilot</cell><cell>348932</cell><cell>559.46</cell><cell>344700</cell><cell>552.60</cell><cell>405425.31</cell><cell>650.97</cell><cell>216770</cell><cell>345.37</cell><cell>450810</cell><cell>724.49</cell></row><row><cell>tutankham</cell><cell>393.64</cell><cell>7.11</cell><cell>191.1</cell><cell>3.34</cell><cell>2354.91</cell><cell>43.62</cell><cell>423.9</cell><cell>7.68</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>up n down</cell><cell>542918.8</cell><cell>658.98</cell><cell>620100</cell><cell>752.75</cell><cell>623805.73</cell><cell>757.26</cell><cell cols="4">986440 1197.85 966590 1173.73</cell></row><row><cell>venture</cell><cell>1992</cell><cell>5.12</cell><cell>1700</cell><cell>4.37</cell><cell>2623.71</cell><cell>6.74</cell><cell>2000</cell><cell>5.23</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>video pinball</cell><cell>483569.72</cell><cell>0.54</cell><cell>965300</cell><cell>1.08</cell><cell>992340.74</cell><cell>1.11</cell><cell>925830</cell><cell>1.04</cell><cell>978190</cell><cell>1.10</cell></row><row><cell>wizard of wor</cell><cell>133264</cell><cell>33.62</cell><cell>106200</cell><cell>26.76</cell><cell>157306.41</cell><cell>39.71</cell><cell>64439</cell><cell>16.14</cell><cell>63735</cell><cell>16.00</cell></row><row><cell>yars revenge</cell><cell>918854.32</cell><cell>6.11</cell><cell>986000</cell><cell>6.55</cell><cell>998532.37</cell><cell>6.64</cell><cell>972000</cell><cell>6.46</cell><cell>968090</cell><cell>6.43</cell></row><row><cell>zaxxon</cell><cell>181372</cell><cell>216.74</cell><cell>111100</cell><cell>132.75</cell><cell>249808.9</cell><cell>298.53</cell><cell>109140</cell><cell>130.41</cell><cell>216020</cell><cell>258.15</cell></row><row><cell>MEAN HWRNS(%)</cell><cell></cell><cell>98.78</cell><cell></cell><cell>76.00</cell><cell></cell><cell>125.92</cell><cell></cell><cell>117.99</cell><cell></cell><cell>154.27</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>9.88E-11</cell><cell></cell><cell>2.17E-11</cell><cell></cell><cell>1.26E-11</cell><cell></cell><cell>5.90E-09</cell><cell></cell><cell>7.71E-09</cell></row><row><cell>MEDIAN HWRNS(%)</cell><cell></cell><cell>33.62</cell><cell></cell><cell>21.19</cell><cell></cell><cell>43.62</cell><cell></cell><cell>35.78</cell><cell></cell><cell>50.63</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>3.36E-11</cell><cell></cell><cell>6.05E-12</cell><cell></cell><cell>4.36E-12</cell><cell></cell><cell>1.79E-09</cell><cell></cell><cell>2.53E-09</cell></row><row><cell>HWRB</cell><cell></cell><cell>15</cell><cell></cell><cell>8</cell><cell></cell><cell>18</cell><cell></cell><cell>17</cell><cell></cell><cell>22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 13 :</head><label>13</label><figDesc>Score table of SOTA 10B+ model-free algorithms on HWRNS. J.6.3 Comparison with SOTA Model-Based Algorithms on HWRNS SimPLe (Kaiser et al.,<ref type="bibr" target="#b32">2019)</ref> andDreamerV2(Hafner et al., 2020)  haven't evaluated all 57 Atari Games in their paper. For fairness, we set the score on those games as N/A, which will not be considered when calculating the median and mean HWRNS and human world record breakthrough (HWRB).</figDesc><table><row><cell>Games</cell><cell cols="10">MuZero HWRNS(%) DreamerV2 HWRNS(%) SimPLe HWRNS(%) GDI-I 3 HWRNS(%) GDI-H 3 HWRNS(%)</cell></row><row><cell>Scale</cell><cell>20B</cell><cell></cell><cell>200M</cell><cell></cell><cell>1M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell>741812.63</cell><cell>294.64</cell><cell>3483</cell><cell>1.29</cell><cell>616.9</cell><cell>0.15</cell><cell>43384</cell><cell>17.15</cell><cell>48735</cell><cell>19.27</cell></row><row><cell>amidar</cell><cell>28634.39</cell><cell>27.49</cell><cell>2028</cell><cell>1.94</cell><cell>74.3</cell><cell>0.07</cell><cell>1442</cell><cell>1.38</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>assault</cell><cell cols="2">143972.03 1706.31</cell><cell>7679</cell><cell>88.51</cell><cell>527.2</cell><cell>3.62</cell><cell>63876</cell><cell>755.57</cell><cell>97155</cell><cell>1150.59</cell></row><row><cell>asterix</cell><cell>998425</cell><cell>99.84</cell><cell>25669</cell><cell>2.55</cell><cell>1128.3</cell><cell>0.09</cell><cell>759910</cell><cell>75.99</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>asteroids</cell><cell>678558.64</cell><cell>6.45</cell><cell>3064</cell><cell>0.02</cell><cell>793.6</cell><cell>0.00</cell><cell>751970</cell><cell>7.15</cell><cell>760005</cell><cell>7.23</cell></row><row><cell>atlantis</cell><cell>1674767.2</cell><cell>15.69</cell><cell>989207</cell><cell>9.22</cell><cell>20992.5</cell><cell>0.08</cell><cell>3803000</cell><cell>35.78</cell><cell>3837300</cell><cell>36.11</cell></row><row><cell>bank heist</cell><cell>1278.98</cell><cell>1.54</cell><cell>1043</cell><cell>1.25</cell><cell>34.2</cell><cell>0.02</cell><cell>1401</cell><cell>1.69</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>battle zone</cell><cell>848623</cell><cell>105.95</cell><cell>31225</cell><cell>3.87</cell><cell>4031.2</cell><cell>0.47</cell><cell>478830</cell><cell>59.77</cell><cell>824360</cell><cell>102.92</cell></row><row><cell>beam rider</cell><cell>454993.53</cell><cell>45.48</cell><cell>12413</cell><cell>1.21</cell><cell>621.6</cell><cell>0.03</cell><cell>162100</cell><cell>16.18</cell><cell>422390</cell><cell>42.22</cell></row><row><cell>berzerk</cell><cell>85932.6</cell><cell>8.11</cell><cell>751</cell><cell>0.06</cell><cell>N/A</cell><cell>N/A</cell><cell>7607</cell><cell>0.71</cell><cell>14649</cell><cell>1.37</cell></row><row><cell>bowling</cell><cell>260.13</cell><cell>85.60</cell><cell>48</cell><cell>8.99</cell><cell>30</cell><cell>2.49</cell><cell>202</cell><cell>64.57</cell><cell>205.2</cell><cell>65.76</cell></row><row><cell>boxing</cell><cell>100</cell><cell>100.00</cell><cell>87</cell><cell>86.99</cell><cell>7.8</cell><cell>7.71</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>breakout</cell><cell>864</cell><cell>100.00</cell><cell>350</cell><cell>40.39</cell><cell>16.4</cell><cell>1.70</cell><cell>864</cell><cell>100.00</cell><cell>864</cell><cell>100.00</cell></row><row><cell>centipede</cell><cell>1159049.27</cell><cell>89.02</cell><cell>6601</cell><cell>0.35</cell><cell>N/A</cell><cell>N/A</cell><cell>155830</cell><cell>11.83</cell><cell>195630</cell><cell>14.89</cell></row><row><cell>chopper command</cell><cell>991039.7</cell><cell>99.10</cell><cell>2833</cell><cell>0.20</cell><cell>979.4</cell><cell>0.02</cell><cell>999999</cell><cell>100.00</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>crazy climber</cell><cell>458315.4</cell><cell>214.01</cell><cell>141424</cell><cell>62.47</cell><cell>62583.6</cell><cell>24.77</cell><cell>201000</cell><cell>90.96</cell><cell>241170</cell><cell>110.17</cell></row><row><cell>defender</cell><cell>839642.95</cell><cell>13.93</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>893110</cell><cell>14.82</cell><cell>970540</cell><cell>16.11</cell></row><row><cell>demon attack</cell><cell>143964.26</cell><cell>9.24</cell><cell>2775</cell><cell>0.17</cell><cell>208.1</cell><cell>0.00</cell><cell>675530</cell><cell>43.40</cell><cell>787985</cell><cell>50.63</cell></row><row><cell>double dunk</cell><cell>23.94</cell><cell>107.42</cell><cell>22</cell><cell>102.53</cell><cell>N/A</cell><cell>N/A</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell></row><row><cell>enduro</cell><cell>2382.44</cell><cell>25.08</cell><cell>2112</cell><cell>22.23</cell><cell>N/A</cell><cell>N/A</cell><cell>14330</cell><cell>150.84</cell><cell>14300</cell><cell>150.53</cell></row><row><cell>fishing derby</cell><cell>91.16</cell><cell>112.39</cell><cell>93.24</cell><cell>286.77</cell><cell>-90.7</cell><cell>0.61</cell><cell>59</cell><cell>92.89</cell><cell>65</cell><cell>96.31</cell></row><row><cell>freeway</cell><cell>33.03</cell><cell>86.92</cell><cell>34</cell><cell>89.47</cell><cell>16.7</cell><cell>43.95</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell></row><row><cell>frostbite</cell><cell>631378.53</cell><cell>138.82</cell><cell>15622</cell><cell>3.42</cell><cell>236.9</cell><cell>0.04</cell><cell>10485</cell><cell>2.29</cell><cell>11330</cell><cell>2.48</cell></row><row><cell>gopher</cell><cell>130345.58</cell><cell>36.67</cell><cell>53853</cell><cell>15.11</cell><cell>596.8</cell><cell>0.10</cell><cell>488830</cell><cell>137.71</cell><cell>473560</cell><cell>133.41</cell></row><row><cell>gravitar</cell><cell>6682.7</cell><cell>4.00</cell><cell>3554</cell><cell>2.08</cell><cell>173.4</cell><cell>0.00</cell><cell>5905</cell><cell>3.52</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>hero</cell><cell>49244.11</cell><cell>4.83</cell><cell>30287</cell><cell>2.93</cell><cell>2656.6</cell><cell>0.16</cell><cell>38330</cell><cell>3.73</cell><cell>38225</cell><cell>3.72</cell></row><row><cell>ice hockey</cell><cell>67.04</cell><cell>165.76</cell><cell>29</cell><cell>85.17</cell><cell>-11.6</cell><cell>-0.85</cell><cell>38</cell><cell>118.94</cell><cell>47.11</cell><cell>123.54</cell></row><row><cell>jamesbond</cell><cell>41063.25</cell><cell>90.14</cell><cell>9269</cell><cell>20.30</cell><cell>100.5</cell><cell>0.16</cell><cell cols="3">594500 1305.93 620780</cell><cell>1363.66</cell></row><row><cell>kangaroo</cell><cell>16763.6</cell><cell>1.17</cell><cell>11819</cell><cell>0.83</cell><cell>51.2</cell><cell>0.00</cell><cell>14500</cell><cell>1.01</cell><cell>14636</cell><cell>1.02</cell></row><row><cell>krull</cell><cell>269358.27</cell><cell>261.22</cell><cell>9687</cell><cell>7.89</cell><cell>2204.8</cell><cell>0.59</cell><cell>97575</cell><cell>93.63</cell><cell>594540</cell><cell>578.47</cell></row><row><cell>kung fu master</cell><cell>204824</cell><cell>20.46</cell><cell>66410</cell><cell>6.62</cell><cell>14862.5</cell><cell>1.46</cell><cell>140440</cell><cell>14.02</cell><cell cols="2">1666665 166.68</cell></row><row><cell>montezuma revenge</cell><cell>0</cell><cell>0.00</cell><cell>1932</cell><cell>0.16</cell><cell>N/A</cell><cell>N/A</cell><cell>3000</cell><cell>0.25</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>ms pacman</cell><cell>243401.1</cell><cell>83.89</cell><cell>5651</cell><cell>1.84</cell><cell>1480</cell><cell>0.40</cell><cell>11536</cell><cell>3.87</cell><cell>11573</cell><cell>3.89</cell></row><row><cell>name this game</cell><cell>157177.85</cell><cell>675.54</cell><cell>14472</cell><cell>53.12</cell><cell>2420.7</cell><cell>0.56</cell><cell>34434</cell><cell>140.19</cell><cell>36296</cell><cell>148.31</cell></row><row><cell>phoenix</cell><cell>955137.84</cell><cell>23.78</cell><cell>13342</cell><cell>0.31</cell><cell>N/A</cell><cell>N/A</cell><cell>894460</cell><cell>22.27</cell><cell>959580</cell><cell>23.89</cell></row><row><cell>pitfall</cell><cell>0</cell><cell>0.20</cell><cell>-1</cell><cell>0.20</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>0.20</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>pong</cell><cell>21</cell><cell>100.00</cell><cell>19</cell><cell>95.20</cell><cell>12.8</cell><cell>80.34</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>private eye</cell><cell>15299.98</cell><cell>15.01</cell><cell>158</cell><cell>0.13</cell><cell>35</cell><cell>0.01</cell><cell>15100</cell><cell>14.81</cell><cell>15100</cell><cell>14.81</cell></row><row><cell>qbert</cell><cell>72276</cell><cell>3.00</cell><cell>162023</cell><cell>6.74</cell><cell>1288.8</cell><cell>0.05</cell><cell>27800</cell><cell>1.15</cell><cell>28657</cell><cell>1.19</cell></row><row><cell>riverraid</cell><cell>323417.18</cell><cell>32.25</cell><cell>16249</cell><cell>1.49</cell><cell>1957.8</cell><cell>0.06</cell><cell>28075</cell><cell>2.68</cell><cell>28349</cell><cell>2.70</cell></row><row><cell>road runner</cell><cell>613411.8</cell><cell>30.10</cell><cell>88772</cell><cell>4.36</cell><cell>5640.6</cell><cell>0.28</cell><cell>878600</cell><cell>43.11</cell><cell>999999</cell><cell>49.06</cell></row><row><cell>robotank</cell><cell>131.13</cell><cell>174.70</cell><cell>65</cell><cell>85.09</cell><cell>N/A</cell><cell>N/A</cell><cell>108</cell><cell>143.63</cell><cell>113.4</cell><cell>150.68</cell></row><row><cell>seaquest</cell><cell>999976.52</cell><cell>100.00</cell><cell>45898</cell><cell>4.58</cell><cell>683.3</cell><cell>0.06</cell><cell>943910</cell><cell>94.39</cell><cell cols="2">1000000 100.00</cell></row><row><cell>skiing</cell><cell>-29968.36</cell><cell>-93.09</cell><cell>-8187</cell><cell>64.45</cell><cell>N/A</cell><cell>N/A</cell><cell>-6774</cell><cell>74.67</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell>56.62</cell><cell>-1.07</cell><cell>883</cell><cell>-0.32</cell><cell>N/A</cell><cell>N/A</cell><cell>11074</cell><cell>8.93</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>space invaders</cell><cell>74335.3</cell><cell>11.94</cell><cell>2611</cell><cell>0.40</cell><cell>N/A</cell><cell>N/A</cell><cell>140460</cell><cell>22.58</cell><cell>154380</cell><cell>24.82</cell></row><row><cell>star gunner</cell><cell>549271.7</cell><cell>714.93</cell><cell>29219</cell><cell>37.21</cell><cell>N/A</cell><cell>N/A</cell><cell>465750</cell><cell>606.09</cell><cell>677590</cell><cell>882.15</cell></row><row><cell>surround</cell><cell>9.99</cell><cell>101.99</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>-8</cell><cell>11.22</cell><cell>2.606</cell><cell>64.32</cell></row><row><cell>tennis</cell><cell>0</cell><cell>53.13</cell><cell>23</cell><cell>104.46</cell><cell>N/A</cell><cell>N/A</cell><cell>24</cell><cell>106.70</cell><cell>24</cell><cell>106.70</cell></row><row><cell>time pilot</cell><cell>476763.9</cell><cell>766.53</cell><cell>32404</cell><cell>46.71</cell><cell>N/A</cell><cell>N/A</cell><cell>216770</cell><cell>345.37</cell><cell>450810</cell><cell>724.49</cell></row><row><cell>tutankham</cell><cell>491.48</cell><cell>8.94</cell><cell>238</cell><cell>4.22</cell><cell>N/A</cell><cell>N/A</cell><cell>424</cell><cell>7.68</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>up n down</cell><cell>715545.61</cell><cell>868.72</cell><cell>648363</cell><cell>787.09</cell><cell>3350.3</cell><cell>3.42</cell><cell cols="4">986440 1197.85 966590 1173.73</cell></row><row><cell>venture</cell><cell>0.4</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>N/A</cell><cell>N/A</cell><cell>2030</cell><cell>5.23</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>video pinball</cell><cell>981791.88</cell><cell>1.10</cell><cell>22218</cell><cell>0.02</cell><cell>N/A</cell><cell>N/A</cell><cell>925830</cell><cell>1.04</cell><cell>978190</cell><cell>1.10</cell></row><row><cell>wizard of wor</cell><cell>197126</cell><cell>49.80</cell><cell>14531</cell><cell>3.54</cell><cell>N/A</cell><cell>N/A</cell><cell>64439</cell><cell>16.14</cell><cell>63735</cell><cell>16.00</cell></row><row><cell>yars revenge</cell><cell>553311.46</cell><cell>3.67</cell><cell>20089</cell><cell>0.11</cell><cell>5664.3</cell><cell>0.02</cell><cell>972000</cell><cell>6.46</cell><cell>968090</cell><cell>6.43</cell></row><row><cell>zaxxon</cell><cell>725853.9</cell><cell>867.51</cell><cell>18295</cell><cell>21.83</cell><cell>N/A</cell><cell>N/A</cell><cell>109140</cell><cell>130.41</cell><cell>216020</cell><cell>258.15</cell></row><row><cell>MEAN HWRNS(%)</cell><cell></cell><cell>152.1</cell><cell></cell><cell>37.9</cell><cell></cell><cell>4.67</cell><cell></cell><cell>117.99</cell><cell></cell><cell>154.27</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>7.61E-11</cell><cell></cell><cell>1.89E-09</cell><cell></cell><cell>4.67E-08</cell><cell></cell><cell>5.90E-09</cell><cell></cell><cell>7.71E-09</cell></row><row><cell>MEDIAN HWRNS(%)</cell><cell></cell><cell>49.8</cell><cell></cell><cell>4.22</cell><cell></cell><cell>0.13</cell><cell></cell><cell>35.78</cell><cell></cell><cell>50.63</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>2.49E-11</cell><cell></cell><cell>2.11E-10</cell><cell></cell><cell>1.25E-09</cell><cell></cell><cell>1.79E-09</cell><cell></cell><cell>2.53E-09</cell></row><row><cell>HWRB</cell><cell></cell><cell>19</cell><cell></cell><cell>3</cell><cell></cell><cell>0</cell><cell></cell><cell>17</cell><cell></cell><cell>22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 14 :</head><label>14</label><figDesc>Score table of SOTA model-based algorithms on HWRNS.J.6.4 Comparison with Other SOTA algorithms on HWRNSIn this section, we report the performance of our algorithm compared with other SOTA algorithms,Go-Explore (Ecoffet et al., 2019)  andMuesli (Hessel et al., 2021).</figDesc><table><row><cell>Games</cell><cell cols="8">Muesli HWRNS(%) Go-Explore HWRNS(%) GDI-I 3 HWRNS(%) GDI-H 3 HWRNS(%)</cell></row><row><cell>Scale</cell><cell></cell><cell>200M</cell><cell>10B</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell>139409</cell><cell>55.30</cell><cell>959312</cell><cell>381.06</cell><cell>43384</cell><cell>17.15</cell><cell>48735</cell><cell>19.27</cell></row><row><cell>amidar</cell><cell>21653</cell><cell>20.78</cell><cell>19083</cell><cell>18.32</cell><cell>1442</cell><cell>1.38</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>assault</cell><cell>36963</cell><cell>436.11</cell><cell>30773</cell><cell>362.64</cell><cell>63876</cell><cell>755.57</cell><cell>97155</cell><cell>1150.59</cell></row><row><cell>asterix</cell><cell>316210</cell><cell>31.61</cell><cell>999500</cell><cell>99.95</cell><cell>759910</cell><cell>75.99</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>asteroids</cell><cell>484609</cell><cell>4.61</cell><cell>112952</cell><cell>1.07</cell><cell>751970</cell><cell>7.15</cell><cell>760005</cell><cell>7.23</cell></row><row><cell>atlantis</cell><cell>1363427</cell><cell>12.75</cell><cell>286460</cell><cell>2.58</cell><cell>3803000</cell><cell>35.78</cell><cell>3837300</cell><cell>36.11</cell></row><row><cell>bank heist</cell><cell>1213</cell><cell>1.46</cell><cell>3668</cell><cell>4.45</cell><cell>1401</cell><cell>1.69</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>battle zone</cell><cell>414107</cell><cell>51.68</cell><cell>998800</cell><cell>124.70</cell><cell>478830</cell><cell>59.77</cell><cell>824360</cell><cell>102.92</cell></row><row><cell>beam rider</cell><cell>288870</cell><cell>28.86</cell><cell>371723</cell><cell>37.15</cell><cell>162100</cell><cell>16.18</cell><cell>422390</cell><cell>42.22</cell></row><row><cell>berzerk</cell><cell>44478</cell><cell>4.19</cell><cell>131417</cell><cell>12.41</cell><cell>7607</cell><cell>0.71</cell><cell>14649</cell><cell>1.37</cell></row><row><cell>bowling</cell><cell>191</cell><cell>60.64</cell><cell>247</cell><cell>80.86</cell><cell>202</cell><cell>64.57</cell><cell>205.2</cell><cell>65.76</cell></row><row><cell>boxing</cell><cell>99</cell><cell>99.00</cell><cell>91</cell><cell>90.99</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>breakout</cell><cell>791</cell><cell>91.53</cell><cell>774</cell><cell>89.56</cell><cell>864</cell><cell>100.00</cell><cell>864</cell><cell>100.00</cell></row><row><cell>centipede</cell><cell>869751</cell><cell>66.76</cell><cell>613815</cell><cell>47.07</cell><cell>155830</cell><cell>11.83</cell><cell>195630</cell><cell>14.89</cell></row><row><cell>chopper command</cell><cell>101289</cell><cell>10.06</cell><cell>996220</cell><cell>99.62</cell><cell>999999</cell><cell>100.00</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>crazy climber</cell><cell>175322</cell><cell>78.68</cell><cell>235600</cell><cell>107.51</cell><cell>201000</cell><cell>90.96</cell><cell>241170</cell><cell>110.17</cell></row><row><cell>defender</cell><cell>629482</cell><cell>10.43</cell><cell>N/A</cell><cell>N/A</cell><cell>893110</cell><cell>14.82</cell><cell>970540</cell><cell>16.11</cell></row><row><cell>demon attack</cell><cell>129544</cell><cell>8.31</cell><cell>239895</cell><cell>15.41</cell><cell>675530</cell><cell>43.40</cell><cell>787985</cell><cell>50.63</cell></row><row><cell>double dunk</cell><cell>-3</cell><cell>39.39</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell></row><row><cell>enduro</cell><cell>2362</cell><cell>24.86</cell><cell>1031</cell><cell>10.85</cell><cell>14330</cell><cell>150.84</cell><cell>14300</cell><cell>150.53</cell></row><row><cell>fishing derby</cell><cell>51</cell><cell>87.71</cell><cell>67</cell><cell>97.54</cell><cell>59</cell><cell>92.89</cell><cell>65</cell><cell>96.31</cell></row><row><cell>freeway</cell><cell>33</cell><cell>86.84</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell></row><row><cell>frostbite</cell><cell>301694</cell><cell>66.33</cell><cell>999990</cell><cell>219.88</cell><cell>10485</cell><cell>2.29</cell><cell>11330</cell><cell>2.48</cell></row><row><cell>gopher</cell><cell>104441</cell><cell>29.37</cell><cell>134244</cell><cell>37.77</cell><cell>488830</cell><cell>137.71</cell><cell>473560</cell><cell>133.41</cell></row><row><cell>gravitar</cell><cell>11660</cell><cell>7.06</cell><cell>13385</cell><cell>8.12</cell><cell>5905</cell><cell>3.52</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>hero</cell><cell>37161</cell><cell>3.62</cell><cell>37783</cell><cell>3.68</cell><cell>38330</cell><cell>3.73</cell><cell>38225</cell><cell>3.72</cell></row><row><cell>ice hockey</cell><cell>25</cell><cell>76.69</cell><cell>33</cell><cell>93.64</cell><cell>45</cell><cell>118.94</cell><cell>47.11</cell><cell>123.54</cell></row><row><cell>jamesbond</cell><cell>19319</cell><cell>42.38</cell><cell>200810</cell><cell>441.07</cell><cell cols="3">594500 1305.93 620780</cell><cell>1363.66</cell></row><row><cell>kangaroo</cell><cell>14096</cell><cell>0.99</cell><cell>24300</cell><cell>1.70</cell><cell>14500</cell><cell>1.01</cell><cell>14636</cell><cell>1.02</cell></row><row><cell>krull</cell><cell>34221</cell><cell>31.83</cell><cell>63149</cell><cell>60.05</cell><cell>97575</cell><cell>93.63</cell><cell>594540</cell><cell>578.47</cell></row><row><cell>kung fu master</cell><cell>134689</cell><cell>13.45</cell><cell>24320</cell><cell>2.41</cell><cell>140440</cell><cell>14.02</cell><cell cols="2">1666665 166.68</cell></row><row><cell>montezuma revenge</cell><cell>2359</cell><cell>0.19</cell><cell>24758</cell><cell>2.03</cell><cell>3000</cell><cell>0.25</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>ms pacman</cell><cell>65278</cell><cell>22.42</cell><cell>456123</cell><cell>157.30</cell><cell>11536</cell><cell>3.87</cell><cell>11573</cell><cell>3.89</cell></row><row><cell>name this game</cell><cell>105043</cell><cell>448.15</cell><cell>212824</cell><cell>918.24</cell><cell>34434</cell><cell>140.19</cell><cell>36296</cell><cell>148.31</cell></row><row><cell>phoenix</cell><cell>805305</cell><cell>20.05</cell><cell>19200</cell><cell>0.46</cell><cell>894460</cell><cell>22.27</cell><cell>959580</cell><cell>23.89</cell></row><row><cell>pitfall</cell><cell>0</cell><cell>0.20</cell><cell>7875</cell><cell>7.09</cell><cell>0</cell><cell>0.2</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>pong</cell><cell>20</cell><cell>97.60</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100</cell><cell>21</cell><cell>100.00</cell></row><row><cell>private eye</cell><cell>10323</cell><cell>10.12</cell><cell>69976</cell><cell>68.73</cell><cell>15100</cell><cell>14.81</cell><cell>15100</cell><cell>14.81</cell></row><row><cell>qbert</cell><cell>157353</cell><cell>6.55</cell><cell>999975</cell><cell>41.66</cell><cell>27800</cell><cell>1.15</cell><cell>28657</cell><cell>1.19</cell></row><row><cell>riverraid</cell><cell>47323</cell><cell>4.60</cell><cell>35588</cell><cell>3.43</cell><cell>28075</cell><cell>2.68</cell><cell>28349</cell><cell>2.70</cell></row><row><cell>road runner</cell><cell>327025</cell><cell>16.05</cell><cell>999900</cell><cell>49.06</cell><cell>878600</cell><cell>43.11</cell><cell>999999</cell><cell>49.06</cell></row><row><cell>robotank</cell><cell>59</cell><cell>76.96</cell><cell>143</cell><cell>190.79</cell><cell>108</cell><cell>143.63</cell><cell>113.4</cell><cell>150.68</cell></row><row><cell>seaquest</cell><cell>815970</cell><cell>81.60</cell><cell>539456</cell><cell>53.94</cell><cell>943910</cell><cell>94.39</cell><cell cols="2">1000000 100.00</cell></row><row><cell>skiing</cell><cell>-18407</cell><cell>-9.47</cell><cell>-4185</cell><cell>93.40</cell><cell>-6774</cell><cell>74.67</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell>3031</cell><cell>1.63</cell><cell>20306</cell><cell>17.31</cell><cell>11074</cell><cell>8.93</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>space invaders</cell><cell>59602</cell><cell>9.57</cell><cell>93147</cell><cell>14.97</cell><cell>140460</cell><cell>22.58</cell><cell>154380</cell><cell>24.82</cell></row><row><cell>star gunner</cell><cell>214383</cell><cell>278.51</cell><cell>609580</cell><cell>793.52</cell><cell>465750</cell><cell>606.09</cell><cell>677590</cell><cell>882.15</cell></row><row><cell>surround</cell><cell>9</cell><cell>96.94</cell><cell>N/A</cell><cell>N/A</cell><cell>-8</cell><cell>11.22</cell><cell>2.606</cell><cell>64.32</cell></row><row><cell>tennis</cell><cell>12</cell><cell>79.91</cell><cell>24</cell><cell>106.7</cell><cell>24</cell><cell>106.70</cell><cell>24</cell><cell>106.70</cell></row><row><cell>time pilot</cell><cell>359105</cell><cell>575.94</cell><cell>183620</cell><cell>291.67</cell><cell>216770</cell><cell>345.37</cell><cell>450810</cell><cell>724.49</cell></row><row><cell>tutankham</cell><cell>252</cell><cell>4.48</cell><cell>528</cell><cell>9.62</cell><cell>424</cell><cell>7.68</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>up n down</cell><cell>649190</cell><cell>788.10</cell><cell>553718</cell><cell>672.10</cell><cell cols="4">986440 1197.85 966590 1173.73</cell></row><row><cell>venture</cell><cell>2104</cell><cell>5.41</cell><cell>3074</cell><cell>7.90</cell><cell>2035</cell><cell>5.23</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>video pinball</cell><cell>685436</cell><cell>0.77</cell><cell>999999</cell><cell>1.12</cell><cell>925830</cell><cell>1.04</cell><cell>978190</cell><cell>1.10</cell></row><row><cell>wizard of wor</cell><cell>93291</cell><cell>23.49</cell><cell>199900</cell><cell>50.50</cell><cell>64293</cell><cell>16.14</cell><cell>63735</cell><cell>16.00</cell></row><row><cell>yars revenge</cell><cell>557818</cell><cell>3.70</cell><cell>999998</cell><cell>6.65</cell><cell>972000</cell><cell>6.46</cell><cell>968090</cell><cell>6.43</cell></row><row><cell>zaxxon</cell><cell>65325</cell><cell>78.04</cell><cell>18340</cell><cell>21.88</cell><cell>109140</cell><cell>130.41</cell><cell>216020</cell><cell>258.15</cell></row><row><cell>MEAN HWRNS(%)</cell><cell></cell><cell>75.52</cell><cell></cell><cell>116.89</cell><cell></cell><cell>117.99</cell><cell></cell><cell>154.27</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>3.78E-09</cell><cell></cell><cell>1.17E-10</cell><cell></cell><cell>5.90E-09</cell><cell></cell><cell>7.71E-09</cell></row><row><cell>MEDIAN HWRNS(%)</cell><cell></cell><cell>24.68</cell><cell></cell><cell>50.5</cell><cell></cell><cell>35.78</cell><cell></cell><cell>50.63</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>1.24E-09</cell><cell></cell><cell>5.05E-11</cell><cell></cell><cell>1.79E-09</cell><cell></cell><cell>2.53E-09</cell></row><row><cell>HWRB</cell><cell></cell><cell>5</cell><cell></cell><cell>15</cell><cell></cell><cell>17</cell><cell></cell><cell>22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 15 :</head><label>15</label><figDesc>Score table of other SOTA algorithms on HWRNS. J.7.1 Comparison with SOTA 200M Algorithms on SABER</figDesc><table><row><cell>Games</cell><cell>RND</cell><cell cols="11">HWR RAINBOW SABER(%) IMPALA SABER(%) LASER SABER(%) GDI-I 3 SABER(%) GDI-H 3 SABER(%)</cell></row><row><cell>Scale</cell><cell></cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell cols="2">227.8 251916</cell><cell>9491.7</cell><cell>3.68</cell><cell>15962.1</cell><cell>6.25</cell><cell>976.51</cell><cell>14.04</cell><cell>43384</cell><cell>17.15</cell><cell>48735</cell><cell>19.27</cell></row><row><cell>amidar</cell><cell>5.8</cell><cell>104159</cell><cell>5131.2</cell><cell>4.92</cell><cell>1554.79</cell><cell>1.49</cell><cell>1829.2</cell><cell>1.75</cell><cell>1442</cell><cell>1.38</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>assault</cell><cell>222.4</cell><cell>8647</cell><cell>14198.5</cell><cell>165.90</cell><cell>19148.47</cell><cell>200.00</cell><cell>21560.4</cell><cell>200.00</cell><cell>63876</cell><cell>200.00</cell><cell>97155</cell><cell>200.00</cell></row><row><cell>asterix</cell><cell cols="3">210 1000000 428200</cell><cell>42.81</cell><cell>300732</cell><cell>30.06</cell><cell>240090</cell><cell>23.99</cell><cell>759910</cell><cell>75.99</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>asteroids</cell><cell cols="3">719 10506650 2712.8</cell><cell>0.02</cell><cell>108590.05</cell><cell>1.03</cell><cell>213025</cell><cell>2.02</cell><cell>751970</cell><cell>7.15</cell><cell>760005</cell><cell>7.23</cell></row><row><cell>atlantis</cell><cell cols="3">12850 10604840 826660</cell><cell>7.68</cell><cell>849967.5</cell><cell>7.90</cell><cell>841200</cell><cell>7.82</cell><cell>3803000</cell><cell>35.78</cell><cell>3837300</cell><cell>36.11</cell></row><row><cell>bank heist</cell><cell>14.2</cell><cell>82058</cell><cell>1358</cell><cell>1.64</cell><cell>1223.15</cell><cell>1.47</cell><cell>569.4</cell><cell>0.68</cell><cell>1401</cell><cell>1.69</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>battle zone</cell><cell>236</cell><cell>801000</cell><cell>62010</cell><cell>7.71</cell><cell>20885</cell><cell>2.58</cell><cell>64953.3</cell><cell>8.08</cell><cell>478830</cell><cell>59.77</cell><cell>824360</cell><cell>102.92</cell></row><row><cell>beam rider</cell><cell cols="2">363.9 999999</cell><cell>16850.2</cell><cell>1.65</cell><cell>32463.47</cell><cell>3.21</cell><cell>90881.6</cell><cell>9.06</cell><cell>162100</cell><cell>16.18</cell><cell>422390</cell><cell>42.22</cell></row><row><cell>berzerk</cell><cell cols="2">123.7 1057940</cell><cell>2545.6</cell><cell>0.23</cell><cell>1852.7</cell><cell>0.16</cell><cell>25579.5</cell><cell>2.41</cell><cell>7607</cell><cell>0.71</cell><cell>14649</cell><cell>1.37</cell></row><row><cell>bowling</cell><cell>23.1</cell><cell>300</cell><cell>30</cell><cell>2.49</cell><cell>59.92</cell><cell>13.30</cell><cell>48.3</cell><cell>9.10</cell><cell>201.9</cell><cell>64.57</cell><cell>205.2</cell><cell>65.76</cell></row><row><cell>boxing</cell><cell>0.1</cell><cell>100</cell><cell>99.6</cell><cell>99.60</cell><cell>99.96</cell><cell>99.96</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>breakout</cell><cell>1.7</cell><cell>864</cell><cell>417.5</cell><cell>48.22</cell><cell>787.34</cell><cell>91.11</cell><cell>747.9</cell><cell>86.54</cell><cell>864</cell><cell>100.00</cell><cell>864</cell><cell>100.00</cell></row><row><cell>centipede</cell><cell cols="2">2090.9 1301709</cell><cell>8167.3</cell><cell>0.47</cell><cell>11049.75</cell><cell>0.69</cell><cell>292792</cell><cell>22.37</cell><cell>155830</cell><cell>11.83</cell><cell>195630</cell><cell>14.89</cell></row><row><cell>chopper command</cell><cell>811</cell><cell>999999</cell><cell>16654</cell><cell>1.59</cell><cell>28255</cell><cell>2.75</cell><cell>761699</cell><cell>76.15</cell><cell cols="2">999999 100.00</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>crazy climber</cell><cell cols="3">10780.5 219900 168788.5</cell><cell>75.56</cell><cell>136950</cell><cell>60.33</cell><cell>167820</cell><cell>75.10</cell><cell>201000</cell><cell>90.96</cell><cell>241170</cell><cell>110.17</cell></row><row><cell>defender</cell><cell cols="2">2874.5 6010500</cell><cell>55105</cell><cell>0.87</cell><cell>185203</cell><cell>3.03</cell><cell>336953</cell><cell>5.56</cell><cell>893110</cell><cell>14.82</cell><cell>970540</cell><cell>16.11</cell></row><row><cell>demon attack</cell><cell cols="3">152.1 1556345 111185</cell><cell>7.13</cell><cell>132826.98</cell><cell>8.53</cell><cell>133530</cell><cell>8.57</cell><cell>675530</cell><cell>43.10</cell><cell>787985</cell><cell>50.63</cell></row><row><cell>double dunk</cell><cell>-18.6</cell><cell>21</cell><cell>-0.3</cell><cell>46.21</cell><cell>-0.33</cell><cell>46.14</cell><cell>14</cell><cell>82.32</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell></row><row><cell>enduro</cell><cell>0</cell><cell>9500</cell><cell>2125.9</cell><cell>22.38</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>14330</cell><cell>150.84</cell><cell>14300</cell><cell>150.53</cell></row><row><cell>fishing derby</cell><cell>-91.7</cell><cell>71</cell><cell>31.3</cell><cell>75.60</cell><cell>44.85</cell><cell>83.93</cell><cell>45.2</cell><cell>84.14</cell><cell>59</cell><cell>95.08</cell><cell>65</cell><cell>96.31</cell></row><row><cell>freeway</cell><cell>0</cell><cell>38</cell><cell>34</cell><cell>89.47</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell></row><row><cell>frostbite</cell><cell>65.2</cell><cell>454830</cell><cell>9590.5</cell><cell>2.09</cell><cell>317.75</cell><cell>0.06</cell><cell>5083.5</cell><cell>1.10</cell><cell>10485</cell><cell>2.29</cell><cell>11330</cell><cell>2.48</cell></row><row><cell>gopher</cell><cell cols="2">257.6 355040</cell><cell>70354.6</cell><cell>19.76</cell><cell>66782.3</cell><cell>18.75</cell><cell>114820.7</cell><cell>32.29</cell><cell cols="2">488830 137.71</cell><cell>473560</cell><cell>133.41</cell></row><row><cell>gravitar</cell><cell>173</cell><cell>162850</cell><cell>1419.3</cell><cell>0.77</cell><cell>359.5</cell><cell>0.11</cell><cell>1106.2</cell><cell>0.57</cell><cell>5905</cell><cell>3.52</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>hero</cell><cell cols="3">1027 1000000 55887.4</cell><cell>5.49</cell><cell>33730.55</cell><cell>3.27</cell><cell>31628.7</cell><cell>3.06</cell><cell>38330</cell><cell>3.73</cell><cell>38225</cell><cell>3.72</cell></row><row><cell>ice hockey</cell><cell>-11.2</cell><cell>36</cell><cell>1.1</cell><cell>26.06</cell><cell>3.48</cell><cell>31.10</cell><cell>17.4</cell><cell>60.59</cell><cell>44.92</cell><cell>118.94</cell><cell>47.11</cell><cell>123.54</cell></row><row><cell>jamesbond</cell><cell>29</cell><cell>45550</cell><cell>19809</cell><cell>43.45</cell><cell>601.5</cell><cell>1.26</cell><cell>37999.8</cell><cell>83.41</cell><cell>594500</cell><cell>200.00</cell><cell>620780</cell><cell>200.00</cell></row><row><cell>kangaroo</cell><cell>52</cell><cell cols="2">1424600 14637.5</cell><cell>1.02</cell><cell>1632</cell><cell>0.11</cell><cell>14308</cell><cell>1.00</cell><cell>14500</cell><cell>1.01</cell><cell>14636</cell><cell>1.02</cell></row><row><cell>krull</cell><cell cols="2">1598 104100</cell><cell>8741.5</cell><cell>6.97</cell><cell>8147.4</cell><cell>6.39</cell><cell>9387.5</cell><cell>7.60</cell><cell>97575</cell><cell>93.63</cell><cell>594540</cell><cell>200.00</cell></row><row><cell>kung fu master</cell><cell cols="2">258.5 1000000</cell><cell>52181</cell><cell>5.19</cell><cell>43375.5</cell><cell>4.31</cell><cell>607443</cell><cell>60.73</cell><cell>140440</cell><cell>14.02</cell><cell cols="2">1666665 166.68</cell></row><row><cell>montezuma revenge</cell><cell>0</cell><cell>1219200</cell><cell>384</cell><cell>0.03</cell><cell>0</cell><cell>0.00</cell><cell>0.3</cell><cell>0.00</cell><cell>3000</cell><cell>0.25</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>ms pacman</cell><cell cols="2">307.3 290090</cell><cell>5380.4</cell><cell>1.75</cell><cell>7342.32</cell><cell>2.43</cell><cell>6565.5</cell><cell>2.16</cell><cell>11536</cell><cell>3.87</cell><cell>11573</cell><cell>3.89</cell></row><row><cell>name this game</cell><cell cols="2">2292.3 25220</cell><cell>13136</cell><cell>47.30</cell><cell>21537.2</cell><cell>83.94</cell><cell>26219.5</cell><cell>104.36</cell><cell>34434</cell><cell>140.19</cell><cell>36296</cell><cell>148.31</cell></row><row><cell>phoenix</cell><cell cols="3">761.5 4014440 108529</cell><cell>2.69</cell><cell>210996.45</cell><cell>5.24</cell><cell>519304</cell><cell>12.92</cell><cell>894460</cell><cell>22.27</cell><cell>959580</cell><cell>23.89</cell></row><row><cell>pitfall</cell><cell cols="2">-229.4 114000</cell><cell>0</cell><cell>0.20</cell><cell>-1.66</cell><cell>0.20</cell><cell>-0.6</cell><cell>0.20</cell><cell>0</cell><cell>0.20</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>pong</cell><cell>-20.7</cell><cell>21</cell><cell>20.9</cell><cell>99.76</cell><cell>20.98</cell><cell>99.95</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>private eye</cell><cell>24.9</cell><cell>101800</cell><cell>4234</cell><cell>4.14</cell><cell>98.5</cell><cell>0.07</cell><cell>96.3</cell><cell>0.07</cell><cell>15100</cell><cell>14.81</cell><cell>15100</cell><cell>14.81</cell></row><row><cell>qbert</cell><cell cols="3">163.9 2400000 33817.5</cell><cell>1.40</cell><cell>351200.12</cell><cell>14.63</cell><cell>21449.6</cell><cell>0.89</cell><cell>27800</cell><cell>1.03</cell><cell>28657</cell><cell>1.19</cell></row><row><cell>riverraid</cell><cell cols="3">1338.5 1000000 22920.8</cell><cell>2.16</cell><cell>29608.05</cell><cell>2.83</cell><cell>40362.7</cell><cell>3.91</cell><cell>28075</cell><cell>2.68</cell><cell>28349</cell><cell>2.70</cell></row><row><cell>road runner</cell><cell cols="2">11.5 2038100</cell><cell>62041</cell><cell>3.04</cell><cell>57121</cell><cell>2.80</cell><cell>45289</cell><cell>2.22</cell><cell>878600</cell><cell>43.11</cell><cell>999999</cell><cell>49.06</cell></row><row><cell>robotank</cell><cell>2.2</cell><cell>76</cell><cell>61.4</cell><cell>80.22</cell><cell>12.96</cell><cell>14.58</cell><cell>62.1</cell><cell>81.17</cell><cell>108.2</cell><cell>143.63</cell><cell>113.4</cell><cell>150.68</cell></row><row><cell>seaquest</cell><cell>68.4</cell><cell>999999</cell><cell>15898.9</cell><cell>1.58</cell><cell>1753.2</cell><cell>0.17</cell><cell>2890.3</cell><cell>0.28</cell><cell>943910</cell><cell>94.39</cell><cell cols="2">1000000 100.00</cell></row><row><cell>skiing</cell><cell cols="2">-17098 -3272</cell><cell>-12957.8</cell><cell>29.95</cell><cell>-10180.38</cell><cell>50.03</cell><cell cols="2">-29968.4 -93.09</cell><cell>-6774</cell><cell>74.67</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell cols="2">1236.3 111420</cell><cell>3560.3</cell><cell>2.11</cell><cell>2365</cell><cell>1.02</cell><cell>2273.5</cell><cell>0.94</cell><cell>11074</cell><cell>8.93</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>space invaders</cell><cell>148</cell><cell>621535</cell><cell>18789</cell><cell>3.00</cell><cell>43595.78</cell><cell>6.99</cell><cell>51037.4</cell><cell>8.19</cell><cell>140460</cell><cell>22.58</cell><cell>154380</cell><cell>24.82</cell></row><row><cell>star gunner</cell><cell>664</cell><cell>77400</cell><cell>127029</cell><cell>164.67</cell><cell>200625</cell><cell>200.00</cell><cell>321528</cell><cell>418.14</cell><cell>465750</cell><cell>200.00</cell><cell>677590</cell><cell>200.00</cell></row><row><cell>surround</cell><cell>-10</cell><cell>9.6</cell><cell>9.7</cell><cell>100.51</cell><cell>7.56</cell><cell>89.59</cell><cell>8.4</cell><cell>93.88</cell><cell>-7.8</cell><cell>11.22</cell><cell>2.606</cell><cell>64.32</cell></row><row><cell>tennis</cell><cell>-23.8</cell><cell>21</cell><cell>0</cell><cell>53.13</cell><cell>0.55</cell><cell>54.35</cell><cell>12.2</cell><cell>80.36</cell><cell>24</cell><cell>106.70</cell><cell>24</cell><cell>106.70</cell></row><row><cell>time pilot</cell><cell>3568</cell><cell>65300</cell><cell>12926</cell><cell>15.16</cell><cell>48481.5</cell><cell>72.76</cell><cell>105316</cell><cell>164.82</cell><cell>216770</cell><cell>200.00</cell><cell>450810</cell><cell>200.00</cell></row><row><cell>tutankham</cell><cell>11.4</cell><cell>5384</cell><cell>241</cell><cell>4.27</cell><cell>292.11</cell><cell>5.22</cell><cell>278.9</cell><cell>4.98</cell><cell>423.9</cell><cell>7.68</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>up n down</cell><cell>533.4</cell><cell>82840</cell><cell>125755</cell><cell cols="3">152.14 332546.75 200.00</cell><cell>345727</cell><cell>200.00</cell><cell cols="2">986440 200.00</cell><cell>966590</cell><cell>200.00</cell></row><row><cell>venture</cell><cell>0</cell><cell>38900</cell><cell>5.5</cell><cell>0.01</cell><cell>0</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>2000</cell><cell>5.14</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>video pinball</cell><cell>0</cell><cell cols="2">89218328 533936.5</cell><cell>0.60</cell><cell>572898.27</cell><cell>0.64</cell><cell>511835</cell><cell>0.57</cell><cell>925830</cell><cell>1.04</cell><cell>978190</cell><cell>1.10</cell></row><row><cell>wizard of wor</cell><cell cols="2">563.5 395300</cell><cell>17862.5</cell><cell>4.38</cell><cell>9157.5</cell><cell>2.18</cell><cell>29059.3</cell><cell>7.22</cell><cell>64439</cell><cell>16.18</cell><cell>63735</cell><cell>16.00</cell></row><row><cell>yars revenge</cell><cell cols="3">3092.9 15000105 102557</cell><cell>0.66</cell><cell>84231.14</cell><cell>0.54</cell><cell>166292.3</cell><cell>1.09</cell><cell>972000</cell><cell>6.46</cell><cell>968090</cell><cell>6.43</cell></row><row><cell>zaxxon</cell><cell>32.5</cell><cell>83700</cell><cell>22209.5</cell><cell>26.51</cell><cell>32935.5</cell><cell>39.33</cell><cell>41118</cell><cell>49.11</cell><cell>109140</cell><cell>130.41</cell><cell>216020</cell><cell>200.00</cell></row><row><cell>MEAN SABER(%)</cell><cell>0.00</cell><cell>100.00</cell><cell></cell><cell>28.39</cell><cell></cell><cell>29.45</cell><cell></cell><cell>36.78</cell><cell></cell><cell>61.66</cell><cell></cell><cell>71.26</cell></row><row><cell>Learning Efficiency</cell><cell>0.00</cell><cell>N/A</cell><cell></cell><cell>1.42E-09</cell><cell></cell><cell>1.47E-09</cell><cell></cell><cell>1.84E-09</cell><cell></cell><cell>3.08E-09</cell><cell></cell><cell>3.56E-09</cell></row><row><cell cols="2">MEDIAN SABER(%) 0.00</cell><cell>100.00</cell><cell></cell><cell>4.92</cell><cell></cell><cell>4.31</cell><cell></cell><cell>8.08</cell><cell></cell><cell>35.78</cell><cell></cell><cell>50.63</cell></row><row><cell>Learning Efficiency</cell><cell>0.00</cell><cell>N/A</cell><cell></cell><cell>2.46E-10</cell><cell></cell><cell>2.16E-10</cell><cell></cell><cell>4.04E-10</cell><cell></cell><cell>2.27E-09</cell><cell></cell><cell>2.53E-09</cell></row><row><cell>HWRB</cell><cell>0</cell><cell>57</cell><cell></cell><cell>4</cell><cell></cell><cell>3</cell><cell></cell><cell>7</cell><cell></cell><cell>17</cell><cell></cell><cell>22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 16 :</head><label>16</label><figDesc>Score table of SOTA 200M model-free algorithms on SABER. J.7.2 Comparison with SOTA 10B+ Model-Free Algorithms on SABER</figDesc><table><row><cell>Games</cell><cell cols="10">R2D2 SABER(%) NGU SABER(%) AGENT57 SABER(%) GDI-I 3 SABER(%) GDI-H 3 SABER(%)</cell></row><row><cell>Scale</cell><cell>10B</cell><cell></cell><cell>35B</cell><cell></cell><cell>100B</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell>109038.4</cell><cell>43.23</cell><cell>248100</cell><cell>98.48</cell><cell>297638.17</cell><cell>118.17</cell><cell>43384</cell><cell>17.15</cell><cell>48735</cell><cell>19.27</cell></row><row><cell>amidar</cell><cell>27751.24</cell><cell>26.64</cell><cell>17800</cell><cell>17.08</cell><cell>29660.08</cell><cell>28.47</cell><cell>1442</cell><cell>1.38</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>assault</cell><cell>90526.44</cell><cell>200.00</cell><cell>34800</cell><cell>200.00</cell><cell>67212.67</cell><cell>200.00</cell><cell>63876</cell><cell>200.00</cell><cell>97155</cell><cell>200.00</cell></row><row><cell>asterix</cell><cell>999080</cell><cell>99.91</cell><cell>950700</cell><cell>95.07</cell><cell>991384.42</cell><cell>99.14</cell><cell>759910</cell><cell>75.99</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>asteroids</cell><cell>265861.2</cell><cell>2.52</cell><cell>230500</cell><cell>2.19</cell><cell>150854.61</cell><cell>1.43</cell><cell>751970</cell><cell>7.15</cell><cell>760005</cell><cell>7.23</cell></row><row><cell>atlantis</cell><cell>1576068</cell><cell>14.76</cell><cell>1653600</cell><cell>15.49</cell><cell>1528841.76</cell><cell>14.31</cell><cell>3803000</cell><cell>35.78</cell><cell>3837300</cell><cell>36.11</cell></row><row><cell>bank heist</cell><cell>46285.6</cell><cell>56.40</cell><cell>17400</cell><cell>21.19</cell><cell>23071.5</cell><cell>28.10</cell><cell>1401</cell><cell>1.69</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>battle zone</cell><cell>513360</cell><cell>64.08</cell><cell>691700</cell><cell>86.35</cell><cell>934134.88</cell><cell>116.63</cell><cell>478830</cell><cell>59.77</cell><cell>824360</cell><cell>102.92</cell></row><row><cell>beam rider</cell><cell>128236.08</cell><cell>12.79</cell><cell>63600</cell><cell>6.33</cell><cell>300509.8</cell><cell>30.03</cell><cell>162100</cell><cell>16.18</cell><cell>422390</cell><cell>42.22</cell></row><row><cell>berzerk</cell><cell>34134.8</cell><cell>3.22</cell><cell>36200</cell><cell>3.41</cell><cell>61507.83</cell><cell>5.80</cell><cell>7607</cell><cell>0.71</cell><cell>14649</cell><cell>1.37</cell></row><row><cell>bowling</cell><cell>196.36</cell><cell>62.57</cell><cell>211.9</cell><cell>68.18</cell><cell>251.18</cell><cell>82.37</cell><cell>201.9</cell><cell>64.57</cell><cell>205.2</cell><cell>65.76</cell></row><row><cell>boxing</cell><cell>99.16</cell><cell>99.16</cell><cell>99.7</cell><cell>99.70</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>breakout</cell><cell>795.36</cell><cell>92.04</cell><cell>559.2</cell><cell>64.65</cell><cell>790.4</cell><cell>91.46</cell><cell>864</cell><cell>100.00</cell><cell>864</cell><cell>100</cell></row><row><cell>centipede</cell><cell>532921.84</cell><cell>40.85</cell><cell>577800</cell><cell>44.30</cell><cell>412847.86</cell><cell>31.61</cell><cell>155830</cell><cell>11.83</cell><cell>195630</cell><cell>14.89</cell></row><row><cell>chopper command</cell><cell>960648</cell><cell>96.06</cell><cell>999900</cell><cell>99.99</cell><cell>999900</cell><cell>99.99</cell><cell>999999</cell><cell>100.00</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>crazy climber</cell><cell>312768</cell><cell>144.41</cell><cell>313400</cell><cell>144.71</cell><cell>565909.85</cell><cell>200.00</cell><cell>201000</cell><cell>90.96</cell><cell>241170</cell><cell>110.17</cell></row><row><cell>defender</cell><cell>562106</cell><cell>9.31</cell><cell>664100</cell><cell>11.01</cell><cell>677642.78</cell><cell>11.23</cell><cell>893110</cell><cell>14.82</cell><cell>970540</cell><cell>16.11</cell></row><row><cell>demon attack</cell><cell>143664.6</cell><cell>9.22</cell><cell>143500</cell><cell>9.21</cell><cell>143161.44</cell><cell>9.19</cell><cell>675530</cell><cell>43.10</cell><cell>787985</cell><cell>50.63</cell></row><row><cell>double dunk</cell><cell>23.12</cell><cell>105.35</cell><cell>-14.1</cell><cell>11.36</cell><cell>23.93</cell><cell>107.40</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell></row><row><cell>enduro</cell><cell>2376.68</cell><cell>25.02</cell><cell>2000</cell><cell>21.05</cell><cell>2367.71</cell><cell>24.92</cell><cell>14330</cell><cell>150.84</cell><cell>14300</cell><cell>150.53</cell></row><row><cell>fishing derby</cell><cell>81.96</cell><cell>106.74</cell><cell>32</cell><cell>76.03</cell><cell>86.97</cell><cell>109.82</cell><cell>59</cell><cell>95.08</cell><cell>65</cell><cell>96.31</cell></row><row><cell>freeway</cell><cell>34</cell><cell>89.47</cell><cell>28.5</cell><cell>75.00</cell><cell>32.59</cell><cell>85.76</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell></row><row><cell>frostbite</cell><cell>11238.4</cell><cell>2.46</cell><cell>206400</cell><cell>45.37</cell><cell>541280.88</cell><cell>119.01</cell><cell>10485</cell><cell>2.29</cell><cell>11330</cell><cell>2.48</cell></row><row><cell>gopher</cell><cell>122196</cell><cell>34.37</cell><cell>113400</cell><cell>31.89</cell><cell>117777.08</cell><cell>33.12</cell><cell>488830</cell><cell>137.71</cell><cell>473560</cell><cell>133.41</cell></row><row><cell>gravitar</cell><cell>6750</cell><cell>4.04</cell><cell>14200</cell><cell>8.62</cell><cell>19213.96</cell><cell>11.70</cell><cell>5905</cell><cell>3.52</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>hero</cell><cell>37030.4</cell><cell>3.60</cell><cell>69400</cell><cell>6.84</cell><cell>114736.26</cell><cell>11.38</cell><cell>38330</cell><cell>3.73</cell><cell>38225</cell><cell>3.72</cell></row><row><cell>ice hockey</cell><cell>71.56</cell><cell>175.34</cell><cell>-4.1</cell><cell>15.04</cell><cell>63.64</cell><cell>158.56</cell><cell>44.92</cell><cell>118.94</cell><cell>47.11</cell><cell>123.54</cell></row><row><cell>jamesbond</cell><cell>23266</cell><cell>51.05</cell><cell>26600</cell><cell>58.37</cell><cell>135784.96</cell><cell>200.00</cell><cell>594500</cell><cell>200.00</cell><cell>620780</cell><cell>200.00</cell></row><row><cell>kangaroo</cell><cell>14112</cell><cell>0.99</cell><cell>35100</cell><cell>2.46</cell><cell>24034.16</cell><cell>1.68</cell><cell>14500</cell><cell>1.01</cell><cell>14636</cell><cell>1.02</cell></row><row><cell>krull</cell><cell>145284.8</cell><cell>140.18</cell><cell>127400</cell><cell>122.73</cell><cell>251997.31</cell><cell>200.00</cell><cell>97575</cell><cell>93.63</cell><cell>594540</cell><cell>200.00</cell></row><row><cell>kung fu master</cell><cell>200176</cell><cell>20.00</cell><cell>212100</cell><cell>21.19</cell><cell>206845.82</cell><cell>20.66</cell><cell>140440</cell><cell>14.02</cell><cell cols="2">1666665 166.68</cell></row><row><cell>montezuma revenge</cell><cell>2504</cell><cell>0.21</cell><cell>10400</cell><cell>0.85</cell><cell>9352.01</cell><cell>0.77</cell><cell>3000</cell><cell>0.25</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>ms pacman</cell><cell>29928.2</cell><cell>10.22</cell><cell>40800</cell><cell>13.97</cell><cell>63994.44</cell><cell>21.98</cell><cell>11536</cell><cell>3.87</cell><cell>11573</cell><cell>3.89</cell></row><row><cell>name this game</cell><cell>45214.8</cell><cell>187.21</cell><cell>23900</cell><cell>94.24</cell><cell>54386.77</cell><cell>200.00</cell><cell>34434</cell><cell>140.19</cell><cell>36296</cell><cell>148.31</cell></row><row><cell>phoenix</cell><cell>811621.6</cell><cell>20.20</cell><cell>959100</cell><cell>23.88</cell><cell>908264.15</cell><cell>22.61</cell><cell>894460</cell><cell>22.27</cell><cell>959580</cell><cell>23.89</cell></row><row><cell>pitfall</cell><cell>0</cell><cell>0.20</cell><cell>7800</cell><cell>7.03</cell><cell>18756.01</cell><cell>16.62</cell><cell>0</cell><cell>0.20</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>pong</cell><cell>21</cell><cell>100.00</cell><cell>19.6</cell><cell>96.64</cell><cell>20.67</cell><cell>99.21</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>private eye</cell><cell>300</cell><cell>0.27</cell><cell>100000</cell><cell>98.23</cell><cell>79716.46</cell><cell>78.30</cell><cell>15100</cell><cell>14.81</cell><cell>15100</cell><cell>14.81</cell></row><row><cell>qbert</cell><cell>161000</cell><cell>6.70</cell><cell>451900</cell><cell>18.82</cell><cell>580328.14</cell><cell>24.18</cell><cell>27800</cell><cell>1.03</cell><cell>28657</cell><cell>1.19</cell></row><row><cell>riverraid</cell><cell>34076.4</cell><cell>3.28</cell><cell>36700</cell><cell>3.54</cell><cell>63318.67</cell><cell>6.21</cell><cell>28075</cell><cell>2.68</cell><cell>28349</cell><cell>2.70</cell></row><row><cell>road runner</cell><cell>498660</cell><cell>24.47</cell><cell>128600</cell><cell>6.31</cell><cell>243025.8</cell><cell>11.92</cell><cell>878600</cell><cell>43.11</cell><cell>999999</cell><cell>49.06</cell></row><row><cell>robotank</cell><cell>132.4</cell><cell>176.42</cell><cell>9.1</cell><cell>9.35</cell><cell>127.32</cell><cell>169.54</cell><cell>108</cell><cell>143.63</cell><cell>113.4</cell><cell>150.68</cell></row><row><cell>seaquest</cell><cell cols="4">999991.84 100.00 1000000 100.00</cell><cell>999997.63</cell><cell>100.00</cell><cell>943910</cell><cell>94.39</cell><cell cols="2">1000000 100.00</cell></row><row><cell>skiing</cell><cell cols="4">-29970.32 -93.10 -22977.9 -42.53</cell><cell>-4202.6</cell><cell>93.27</cell><cell>-6774</cell><cell>74.67</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell>4198.4</cell><cell>2.69</cell><cell>4700</cell><cell>3.14</cell><cell>44199.93</cell><cell>38.99</cell><cell>11074</cell><cell>8.93</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>space invaders</cell><cell>55889</cell><cell>8.97</cell><cell>43400</cell><cell>6.96</cell><cell>48680.86</cell><cell>7.81</cell><cell>140460</cell><cell>22.58</cell><cell>154380</cell><cell>24.82</cell></row><row><cell>star gunner</cell><cell>521728</cell><cell>200.00</cell><cell>414600</cell><cell>200.00</cell><cell>839573.53</cell><cell>200.00</cell><cell>465750</cell><cell>200.00</cell><cell>677590</cell><cell>200.00</cell></row><row><cell>surround</cell><cell>9.96</cell><cell>101.84</cell><cell>-9.6</cell><cell>2.04</cell><cell>9.5</cell><cell>99.49</cell><cell>-7.8</cell><cell>11.22</cell><cell>2.606</cell><cell>64.32</cell></row><row><cell>tennis</cell><cell>24</cell><cell>106.70</cell><cell>10.2</cell><cell>75.89</cell><cell>23.84</cell><cell>106.34</cell><cell>24</cell><cell>106.70</cell><cell>24</cell><cell>106.70</cell></row><row><cell>time pilot</cell><cell>348932</cell><cell>200.00</cell><cell>344700</cell><cell>200.00</cell><cell>405425.31</cell><cell>200.00</cell><cell>216770</cell><cell>200.00</cell><cell>450810</cell><cell>200.00</cell></row><row><cell>tutankham</cell><cell>393.64</cell><cell>7.11</cell><cell>191.1</cell><cell>3.34</cell><cell>2354.91</cell><cell>43.62</cell><cell>423.9</cell><cell>7.68</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>up n down</cell><cell>542918.8</cell><cell>200.00</cell><cell>620100</cell><cell>200.00</cell><cell>623805.73</cell><cell>200.00</cell><cell>986440</cell><cell>200.00</cell><cell>966590</cell><cell>200.00</cell></row><row><cell>venture</cell><cell>1992</cell><cell>5.12</cell><cell>1700</cell><cell>4.37</cell><cell>2623.71</cell><cell>6.74</cell><cell>2000</cell><cell>5.14</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>video pinball</cell><cell>483569.72</cell><cell>0.54</cell><cell>965300</cell><cell>1.08</cell><cell>992340.74</cell><cell>1.11</cell><cell>925830</cell><cell>1.04</cell><cell>978190</cell><cell>1.10</cell></row><row><cell>wizard of wor</cell><cell>133264</cell><cell>33.62</cell><cell>106200</cell><cell>26.76</cell><cell>157306.41</cell><cell>39.71</cell><cell>64439</cell><cell>16.18</cell><cell>63735</cell><cell>16.00</cell></row><row><cell>yars revenge</cell><cell>918854.32</cell><cell>6.11</cell><cell>986000</cell><cell>6.55</cell><cell>998532.37</cell><cell>6.64</cell><cell>972000</cell><cell>6.46</cell><cell>968090</cell><cell>6.43</cell></row><row><cell>zaxxon</cell><cell>181372</cell><cell>200.00</cell><cell>111100</cell><cell>132.75</cell><cell>249808.9</cell><cell>200.00</cell><cell>109140</cell><cell>130.41</cell><cell>216020</cell><cell>200.00</cell></row><row><cell>MEAN SABER(%)</cell><cell></cell><cell>60.43</cell><cell></cell><cell>50.47</cell><cell></cell><cell>76.26</cell><cell></cell><cell>61.66</cell><cell></cell><cell>71.26</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>6.04E-11</cell><cell></cell><cell>1.44E-11</cell><cell></cell><cell>7.63E-12</cell><cell></cell><cell>5.90E-09</cell><cell></cell><cell>3.56E-09</cell></row><row><cell>MEDIAN SABER(%)</cell><cell></cell><cell>33.62</cell><cell></cell><cell>21.19</cell><cell></cell><cell>43.62</cell><cell></cell><cell>35.78</cell><cell></cell><cell>50.63</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>3.36E-11</cell><cell></cell><cell>6.05E-12</cell><cell></cell><cell>4.36E-12</cell><cell></cell><cell>2.27E-09</cell><cell></cell><cell>2.53E-09</cell></row><row><cell>HWRB</cell><cell></cell><cell>15</cell><cell></cell><cell>9</cell><cell></cell><cell>18</cell><cell></cell><cell>17</cell><cell></cell><cell>22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table 17 :</head><label>17</label><figDesc>Score table of SOTA 10B+ model-free algorithms on SABER. J.7.3 Comparison with SOTA Model-Based Algorithms on SABER SimPLe (Kaiser et al.,<ref type="bibr" target="#b32">2019)</ref> andDreamerV2 (Hafner et al., 2020)  haven't evaluated all 57 Atari Games in their paper. For fairness, we set the score on those games as N/A, which will not be considered when calculating the median and mean SABER.</figDesc><table><row><cell>Games</cell><cell cols="10">MuZero SABER(%) DreamerV2 SABER(%) SimPLe SABER(%) GDI-I 3 SABER(%) GDI-H 3 SABER(%)</cell></row><row><cell>Scale</cell><cell>20B</cell><cell></cell><cell>200M</cell><cell></cell><cell>1M</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell cols="2">741812.63 200.00</cell><cell>3483</cell><cell>1.29</cell><cell>616.9</cell><cell>0.15</cell><cell>43384</cell><cell>17.15</cell><cell>48735</cell><cell>19.27</cell></row><row><cell>amidar</cell><cell>28634.39</cell><cell>27.49</cell><cell>2028</cell><cell>1.94</cell><cell>74.3</cell><cell>0.07</cell><cell>1442</cell><cell>1.38</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>assault</cell><cell cols="2">143972.03 200.00</cell><cell>7679</cell><cell>88.51</cell><cell>527.2</cell><cell>3.62</cell><cell>63876</cell><cell>200.00</cell><cell>97155</cell><cell>200.00</cell></row><row><cell>asterix</cell><cell>998425</cell><cell>99.84</cell><cell>25669</cell><cell>2.55</cell><cell>1128.3</cell><cell>0.09</cell><cell>759910</cell><cell>75.99</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>asteroids</cell><cell>678558.64</cell><cell>6.45</cell><cell>3064</cell><cell>0.02</cell><cell>793.6</cell><cell>0.00</cell><cell>751970</cell><cell>7.15</cell><cell>760005</cell><cell>7.23</cell></row><row><cell>atlantis</cell><cell>1674767.2</cell><cell>15.69</cell><cell>989207</cell><cell>9.22</cell><cell>20992.5</cell><cell>0.08</cell><cell>3803000</cell><cell>35.78</cell><cell>3837300</cell><cell>36.11</cell></row><row><cell>bank heist</cell><cell>1278.98</cell><cell>1.54</cell><cell>1043</cell><cell>1.25</cell><cell>34.2</cell><cell>0.02</cell><cell>1401</cell><cell>1.69</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>battle zone</cell><cell>848623</cell><cell>105.95</cell><cell>31225</cell><cell>3.87</cell><cell>4031.2</cell><cell>0.47</cell><cell>478830</cell><cell>59.77</cell><cell>824360</cell><cell>102.92</cell></row><row><cell>beam rider</cell><cell>454993.53</cell><cell>45.48</cell><cell>12413</cell><cell>1.21</cell><cell>621.6</cell><cell>0.03</cell><cell>162100</cell><cell>16.18</cell><cell>422390</cell><cell>42.22</cell></row><row><cell>berzerk</cell><cell>85932.6</cell><cell>8.11</cell><cell>751</cell><cell>0.06</cell><cell>N/A</cell><cell>N/A</cell><cell>7607</cell><cell>0.71</cell><cell>14649</cell><cell>1.37</cell></row><row><cell>bowling</cell><cell>260.13</cell><cell>85.60</cell><cell>48</cell><cell>8.99</cell><cell>30</cell><cell>2.49</cell><cell>202</cell><cell>64.57</cell><cell>205.2</cell><cell>65.76</cell></row><row><cell>boxing</cell><cell>100</cell><cell>100.00</cell><cell>87</cell><cell>86.99</cell><cell>7.8</cell><cell>7.71</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>breakout</cell><cell>864</cell><cell>100.00</cell><cell>350</cell><cell>40.39</cell><cell>16.4</cell><cell>1.70</cell><cell>864</cell><cell>100.00</cell><cell>864</cell><cell>100.00</cell></row><row><cell>centipede</cell><cell>1159049.27</cell><cell>89.02</cell><cell>6601</cell><cell>0.35</cell><cell>N/A</cell><cell>N/A</cell><cell>155830</cell><cell>11.83</cell><cell>195630</cell><cell>14.89</cell></row><row><cell>chopper command</cell><cell>991039.7</cell><cell>99.10</cell><cell>2833</cell><cell>0.20</cell><cell>979.4</cell><cell>0.02</cell><cell>999999</cell><cell>100.00</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>crazy climber</cell><cell>458315.4</cell><cell>200.00</cell><cell>141424</cell><cell>62.47</cell><cell>62583.6</cell><cell>24.77</cell><cell>201000</cell><cell>90.96</cell><cell>241170</cell><cell>110.17</cell></row><row><cell>defender</cell><cell>839642.95</cell><cell>13.93</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>893110</cell><cell>14.82</cell><cell>970540</cell><cell>16.11</cell></row><row><cell>demon attack</cell><cell>143964.26</cell><cell>9.24</cell><cell>2775</cell><cell>0.17</cell><cell>208.1</cell><cell>0.00</cell><cell>675530</cell><cell>43.40</cell><cell>787985</cell><cell>50.63</cell></row><row><cell>double dunk</cell><cell>23.94</cell><cell>107.42</cell><cell>22</cell><cell>102.53</cell><cell>N/A</cell><cell>N/A</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell></row><row><cell>enduro</cell><cell>2382.44</cell><cell>25.08</cell><cell>2112</cell><cell>22.23</cell><cell>N/A</cell><cell>N/A</cell><cell>14330</cell><cell>150.84</cell><cell>14300</cell><cell>150.53</cell></row><row><cell>fishing derby</cell><cell>91.16</cell><cell>112.39</cell><cell>93.24</cell><cell>200.00</cell><cell>-90.7</cell><cell>0.61</cell><cell>59</cell><cell>92.89</cell><cell>65</cell><cell>96.31</cell></row><row><cell>freeway</cell><cell>33.03</cell><cell>86.92</cell><cell>34</cell><cell>89.47</cell><cell>16.7</cell><cell>43.95</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell></row><row><cell>frostbite</cell><cell>631378.53</cell><cell>138.82</cell><cell>15622</cell><cell>3.42</cell><cell>236.9</cell><cell>0.04</cell><cell>10485</cell><cell>2.29</cell><cell>11330</cell><cell>2.48</cell></row><row><cell>gopher</cell><cell>130345.58</cell><cell>36.67</cell><cell>53853</cell><cell>15.11</cell><cell>596.8</cell><cell>0.10</cell><cell>488830</cell><cell>137.71</cell><cell>473560</cell><cell>133.41</cell></row><row><cell>gravitar</cell><cell>6682.7</cell><cell>4.00</cell><cell>3554</cell><cell>2.08</cell><cell>173.4</cell><cell>0.00</cell><cell>5905</cell><cell>3.52</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>hero</cell><cell>49244.11</cell><cell>4.83</cell><cell>30287</cell><cell>2.93</cell><cell>2656.6</cell><cell>0.16</cell><cell>38330</cell><cell>3.73</cell><cell>38225</cell><cell>3.72</cell></row><row><cell>ice hockey</cell><cell>67.04</cell><cell>165.76</cell><cell>29</cell><cell>85.17</cell><cell>-11.6</cell><cell>-0.85</cell><cell>44.92</cell><cell>118.94</cell><cell>47.11</cell><cell>123.54</cell></row><row><cell>jamesbond</cell><cell>41063.25</cell><cell>90.14</cell><cell>9269</cell><cell>20.30</cell><cell>100.5</cell><cell>0.16</cell><cell>594500</cell><cell>200.00</cell><cell>620780</cell><cell>200.00</cell></row><row><cell>kangaroo</cell><cell>16763.6</cell><cell>1.17</cell><cell>11819</cell><cell>0.83</cell><cell>51.2</cell><cell>0.00</cell><cell>14500</cell><cell>1.01</cell><cell>14636</cell><cell>1.02</cell></row><row><cell>krull</cell><cell>269358.27</cell><cell>200.00</cell><cell>9687</cell><cell>7.89</cell><cell>2204.8</cell><cell>0.59</cell><cell>97575</cell><cell>93.63</cell><cell>594540</cell><cell>200.00</cell></row><row><cell>kung fu master</cell><cell>204824</cell><cell>20.46</cell><cell>66410</cell><cell>6.62</cell><cell>14862.5</cell><cell>1.46</cell><cell>140440</cell><cell>14.02</cell><cell cols="2">1666665 166.68</cell></row><row><cell>montezuma revenge</cell><cell>0</cell><cell>0.00</cell><cell>1932</cell><cell>0.16</cell><cell>N/A</cell><cell>N/A</cell><cell>3000</cell><cell>0.25</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>ms pacman</cell><cell>243401.1</cell><cell>83.89</cell><cell>5651</cell><cell>1.84</cell><cell>1480</cell><cell>0.40</cell><cell>11536</cell><cell>3.87</cell><cell>11573</cell><cell>3.89</cell></row><row><cell>name this game</cell><cell cols="2">157177.85 200.00</cell><cell>14472</cell><cell>53.12</cell><cell>2420.7</cell><cell>0.56</cell><cell>34434</cell><cell>140.19</cell><cell>36296</cell><cell>148.31</cell></row><row><cell>phoenix</cell><cell>955137.84</cell><cell>23.78</cell><cell>13342</cell><cell>0.31</cell><cell>N/A</cell><cell>N/A</cell><cell>894460</cell><cell>22.27</cell><cell>959580</cell><cell>23.89</cell></row><row><cell>pitfall</cell><cell>0</cell><cell>0.20</cell><cell>-1</cell><cell>0.20</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>0.20</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>pong</cell><cell>21</cell><cell>100.00</cell><cell>19</cell><cell>95.20</cell><cell>12.8</cell><cell>80.34</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100.00</cell></row><row><cell>private eye</cell><cell>15299.98</cell><cell>15.01</cell><cell>158</cell><cell>0.13</cell><cell>35</cell><cell>0.01</cell><cell>15100</cell><cell>14.81</cell><cell>15100</cell><cell>14.81</cell></row><row><cell>qbert</cell><cell>72276</cell><cell>3.00</cell><cell>162023</cell><cell>6.74</cell><cell>1288.8</cell><cell>0.05</cell><cell>27800</cell><cell>1.15</cell><cell>28657</cell><cell>1.19</cell></row><row><cell>riverraid</cell><cell>323417.18</cell><cell>32.25</cell><cell>16249</cell><cell>1.49</cell><cell>1957.8</cell><cell>0.06</cell><cell>28075</cell><cell>2.68</cell><cell>28349</cell><cell>2.70</cell></row><row><cell>road runner</cell><cell>613411.8</cell><cell>30.10</cell><cell>88772</cell><cell>4.36</cell><cell>5640.6</cell><cell>0.28</cell><cell>878600</cell><cell>43.11</cell><cell>999999</cell><cell>49.06</cell></row><row><cell>robotank</cell><cell>131.13</cell><cell>174.70</cell><cell>65</cell><cell>85.09</cell><cell>N/A</cell><cell>N/A</cell><cell>108</cell><cell>143.63</cell><cell>113.4</cell><cell>150.68</cell></row><row><cell>seaquest</cell><cell>999976.52</cell><cell>100.00</cell><cell>45898</cell><cell>4.58</cell><cell>683.3</cell><cell>0.06</cell><cell>943910</cell><cell>94.39</cell><cell cols="2">1000000 100.00</cell></row><row><cell>skiing</cell><cell>-29968.36</cell><cell>-93.09</cell><cell>-8187</cell><cell>64.45</cell><cell>N/A</cell><cell>N/A</cell><cell>-6774</cell><cell>74.67</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell>56.62</cell><cell>-1.07</cell><cell>883</cell><cell>-0.32</cell><cell>N/A</cell><cell>N/A</cell><cell>11074</cell><cell>8.93</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>space invaders</cell><cell>74335.3</cell><cell>11.94</cell><cell>2611</cell><cell>0.40</cell><cell>N/A</cell><cell>N/A</cell><cell>140460</cell><cell>22.58</cell><cell>154380</cell><cell>24.82</cell></row><row><cell>star gunner</cell><cell>549271.7</cell><cell>200.00</cell><cell>29219</cell><cell>37.21</cell><cell>N/A</cell><cell>N/A</cell><cell>465750</cell><cell>200.00</cell><cell>677590</cell><cell>200.00</cell></row><row><cell>surround</cell><cell>9.99</cell><cell>101.99</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>-8</cell><cell>11.22</cell><cell>2.606</cell><cell>64.32</cell></row><row><cell>tennis</cell><cell>0</cell><cell>53.13</cell><cell>23</cell><cell>104.46</cell><cell>N/A</cell><cell>N/A</cell><cell>24</cell><cell>106.70</cell><cell>24</cell><cell>106.70</cell></row><row><cell>time pilot</cell><cell>476763.9</cell><cell>200.00</cell><cell>32404</cell><cell>46.71</cell><cell>N/A</cell><cell>N/A</cell><cell>216770</cell><cell>200.00</cell><cell>450810</cell><cell>200.00</cell></row><row><cell>tutankham</cell><cell>491.48</cell><cell>8.94</cell><cell>238</cell><cell>4.22</cell><cell>N/A</cell><cell>N/A</cell><cell>424</cell><cell>7.68</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>up n down</cell><cell>715545.61</cell><cell>200.00</cell><cell>648363</cell><cell>200.00</cell><cell>3350.3</cell><cell>3.42</cell><cell>986440</cell><cell>200.00</cell><cell>966590</cell><cell>200.00</cell></row><row><cell>venture</cell><cell>0.4</cell><cell>0.00</cell><cell>0</cell><cell>0.00</cell><cell>N/A</cell><cell>N/A</cell><cell>2000</cell><cell>5.23</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>video pinball</cell><cell>981791.88</cell><cell>1.10</cell><cell>22218</cell><cell>0.02</cell><cell>N/A</cell><cell>N/A</cell><cell>925830</cell><cell>1.04</cell><cell>978190</cell><cell>1.10</cell></row><row><cell>wizard of wor</cell><cell>197126</cell><cell>49.80</cell><cell>14531</cell><cell>3.54</cell><cell>N/A</cell><cell>N/A</cell><cell>64439</cell><cell>16.14</cell><cell>63735</cell><cell>16.00</cell></row><row><cell>yars revenge</cell><cell>553311.46</cell><cell>3.67</cell><cell>20089</cell><cell>0.11</cell><cell>5664.3</cell><cell>0.02</cell><cell>972000</cell><cell>6.46</cell><cell>968090</cell><cell>6.43</cell></row><row><cell>zaxxon</cell><cell>725853.9</cell><cell>200.00</cell><cell>18295</cell><cell>21.83</cell><cell>N/A</cell><cell>N/A</cell><cell>109140</cell><cell>130.41</cell><cell>216020</cell><cell>200.00</cell></row><row><cell>MEAN SABER(%)</cell><cell></cell><cell>71.94</cell><cell></cell><cell>27.22</cell><cell></cell><cell>4.67</cell><cell></cell><cell>61.66</cell><cell></cell><cell>71.26</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>3.60E-11</cell><cell></cell><cell>1.36E-09</cell><cell></cell><cell>4.67E-08</cell><cell></cell><cell>5.90E-09</cell><cell></cell><cell>3.56E-09</cell></row><row><cell>MEDIAN SABER(%)</cell><cell></cell><cell>49.8</cell><cell></cell><cell>4.22</cell><cell></cell><cell>0.13</cell><cell></cell><cell>35.78</cell><cell></cell><cell>50.63</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>2.49E-11</cell><cell></cell><cell>2.11E-10</cell><cell></cell><cell>1.60E-09</cell><cell></cell><cell>2.27E-09</cell><cell></cell><cell>2.53E-09</cell></row><row><cell>HWRB</cell><cell></cell><cell>19</cell><cell></cell><cell>3</cell><cell></cell><cell>0</cell><cell></cell><cell>17</cell><cell></cell><cell>22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head>Table 18 :</head><label>18</label><figDesc>Score table of SOTA model-based algorithms on SABER.J.7.4 Comparison with Other SOTA algorithms on SABERIn this section, we report the performance of our algorithm compared with other SOTA algorithms,Go-Explore (Ecoffet et al., 2019)  andMuesli (Hessel et al., 2021).</figDesc><table><row><cell>Games</cell><cell cols="8">Muesli SABER(%) Go-Explore SABER(%) GDI-I 3 SABER(%) GDI-H 3 SABER(%)</cell></row><row><cell>Scale</cell><cell>200M</cell><cell></cell><cell>10B</cell><cell></cell><cell>200M</cell><cell></cell><cell>200M</cell><cell></cell></row><row><cell>alien</cell><cell>139409</cell><cell>55.30</cell><cell>959312</cell><cell>200.00</cell><cell>43384</cell><cell>17.15</cell><cell>48735</cell><cell>19.27</cell></row><row><cell>amidar</cell><cell>21653</cell><cell>20.78</cell><cell>19083</cell><cell>18.32</cell><cell>1442</cell><cell>1.38</cell><cell>1065</cell><cell>1.02</cell></row><row><cell>assault</cell><cell>36963</cell><cell>200.00</cell><cell>30773</cell><cell>200.00</cell><cell>63876</cell><cell>200.00</cell><cell>97155</cell><cell>200.00</cell></row><row><cell>asterix</cell><cell>316210</cell><cell>31.61</cell><cell>999500</cell><cell>99.95</cell><cell>759910</cell><cell>75.99</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>asteroids</cell><cell>484609</cell><cell>4.61</cell><cell>112952</cell><cell>1.07</cell><cell>751970</cell><cell>7.15</cell><cell>760005</cell><cell>7.23</cell></row><row><cell>atlantis</cell><cell>1363427</cell><cell>12.75</cell><cell>286460</cell><cell>2.58</cell><cell>3803000</cell><cell>35.78</cell><cell>3837300</cell><cell>36.11</cell></row><row><cell>bank heist</cell><cell>1213</cell><cell>1.46</cell><cell>3668</cell><cell>4.45</cell><cell>1401</cell><cell>1.69</cell><cell>1380</cell><cell>1.66</cell></row><row><cell>battle zone</cell><cell>414107</cell><cell>51.68</cell><cell>998800</cell><cell>124.70</cell><cell>478830</cell><cell>59.77</cell><cell>824360</cell><cell>102.92</cell></row><row><cell>beam rider</cell><cell>288870</cell><cell>28.86</cell><cell>371723</cell><cell>37.15</cell><cell>162100</cell><cell>16.18</cell><cell>422390</cell><cell>42.22</cell></row><row><cell>berzerk</cell><cell>44478</cell><cell>4.19</cell><cell>131417</cell><cell>12.41</cell><cell>7607</cell><cell>0.71</cell><cell>14649</cell><cell>1.37</cell></row><row><cell>bowling</cell><cell>191</cell><cell>60.64</cell><cell>247</cell><cell>80.86</cell><cell>202</cell><cell>64.57</cell><cell>205.2</cell><cell>65.76</cell></row><row><cell>boxing</cell><cell>99</cell><cell>99.00</cell><cell>91</cell><cell>90.99</cell><cell>100</cell><cell>100.00</cell><cell>100</cell><cell>100.00</cell></row><row><cell>breakout</cell><cell>791</cell><cell>91.53</cell><cell>774</cell><cell>89.56</cell><cell>864</cell><cell>100.00</cell><cell>864</cell><cell>100.00</cell></row><row><cell>centipede</cell><cell>869751</cell><cell>66.76</cell><cell>613815</cell><cell>47.07</cell><cell>155830</cell><cell>11.83</cell><cell>195630</cell><cell>14.89</cell></row><row><cell cols="2">chopper command 101289</cell><cell>10.06</cell><cell>996220</cell><cell>99.62</cell><cell>999999</cell><cell>100.00</cell><cell>999999</cell><cell>100.00</cell></row><row><cell>crazy climber</cell><cell>175322</cell><cell>78.68</cell><cell>235600</cell><cell>107.51</cell><cell>201000</cell><cell>90.96</cell><cell>241170</cell><cell>110.17</cell></row><row><cell>defender</cell><cell>629482</cell><cell>10.43</cell><cell>N/A</cell><cell>N/A</cell><cell>893110</cell><cell>14.82</cell><cell>970540</cell><cell>16.11</cell></row><row><cell>demon attack</cell><cell>129544</cell><cell>8.31</cell><cell>239895</cell><cell>15.41</cell><cell>675530</cell><cell>43.40</cell><cell>787985</cell><cell>50.63</cell></row><row><cell>double dunk</cell><cell>-3</cell><cell>39.39</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell><cell>24</cell><cell>107.58</cell></row><row><cell>enduro</cell><cell>2362</cell><cell>24.86</cell><cell>1031</cell><cell>10.85</cell><cell>14330</cell><cell>150.84</cell><cell>14300</cell><cell>150.53</cell></row><row><cell>fishing derby</cell><cell>51</cell><cell>87.71</cell><cell>67</cell><cell>97.54</cell><cell>59</cell><cell>92.89</cell><cell>65</cell><cell>96.31</cell></row><row><cell>freeway</cell><cell>33</cell><cell>86.84</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell><cell>34</cell><cell>89.47</cell></row><row><cell>frostbite</cell><cell>301694</cell><cell>66.33</cell><cell>999990</cell><cell>200.00</cell><cell>10485</cell><cell>2.29</cell><cell>11330</cell><cell>2.48</cell></row><row><cell>gopher</cell><cell>104441</cell><cell>29.37</cell><cell>134244</cell><cell>37.77</cell><cell>488830</cell><cell>137.71</cell><cell>473560</cell><cell>133.41</cell></row><row><cell>gravitar</cell><cell>11660</cell><cell>7.06</cell><cell>13385</cell><cell>8.12</cell><cell>5905</cell><cell>3.52</cell><cell>5915</cell><cell>3.53</cell></row><row><cell>hero</cell><cell>37161</cell><cell>3.62</cell><cell>37783</cell><cell>3.68</cell><cell>38330</cell><cell>3.73</cell><cell>38225</cell><cell>3.72</cell></row><row><cell>ice hockey</cell><cell>25</cell><cell>76.69</cell><cell>33</cell><cell>93.64</cell><cell>44.92</cell><cell>118.94</cell><cell>47.11</cell><cell>123.54</cell></row><row><cell>jamesbond</cell><cell>19319</cell><cell>42.38</cell><cell>200810</cell><cell>200.00</cell><cell>594500</cell><cell>200.00</cell><cell>620780</cell><cell>200.00</cell></row><row><cell>kangaroo</cell><cell>14096</cell><cell>0.99</cell><cell>24300</cell><cell>1.70</cell><cell>14500</cell><cell>1.01</cell><cell>14636</cell><cell>1.02</cell></row><row><cell>krull</cell><cell>34221</cell><cell>31.83</cell><cell>63149</cell><cell>60.05</cell><cell>97575</cell><cell>93.63</cell><cell>594540</cell><cell>200.00</cell></row><row><cell>kung fu master</cell><cell>134689</cell><cell>13.45</cell><cell>24320</cell><cell>2.41</cell><cell>140440</cell><cell>14.02</cell><cell cols="2">1666665 166.68</cell></row><row><cell>montezuma revenge</cell><cell>2359</cell><cell>0.19</cell><cell>24758</cell><cell>2.03</cell><cell>3000</cell><cell>0.25</cell><cell>2500</cell><cell>0.21</cell></row><row><cell>ms pacman</cell><cell>65278</cell><cell>22.42</cell><cell>456123</cell><cell>157.30</cell><cell>11536</cell><cell>3.87</cell><cell>11573</cell><cell>3.89</cell></row><row><cell>name this game</cell><cell>105043</cell><cell>200.00</cell><cell>212824</cell><cell>200.00</cell><cell>34434</cell><cell>140.19</cell><cell>36296</cell><cell>148.31</cell></row><row><cell>phoenix</cell><cell>805305</cell><cell>20.05</cell><cell>19200</cell><cell>0.46</cell><cell>894460</cell><cell>22.27</cell><cell>959580</cell><cell>23.89</cell></row><row><cell>pitfall</cell><cell>0</cell><cell>0.20</cell><cell>7875</cell><cell>7.09</cell><cell>0</cell><cell>0.2</cell><cell>-4.3</cell><cell>0.20</cell></row><row><cell>pong</cell><cell>20</cell><cell>97.60</cell><cell>21</cell><cell>100.00</cell><cell>21</cell><cell>100</cell><cell>21</cell><cell>100.00</cell></row><row><cell>private eye</cell><cell>10323</cell><cell>10.12</cell><cell>69976</cell><cell>68.73</cell><cell>15100</cell><cell>14.81</cell><cell>15100</cell><cell>14.81</cell></row><row><cell>qbert</cell><cell>157353</cell><cell>6.55</cell><cell>999975</cell><cell>41.66</cell><cell>27800</cell><cell>1.15</cell><cell>28657</cell><cell>1.19</cell></row><row><cell>riverraid</cell><cell>47323</cell><cell>4.60</cell><cell>35588</cell><cell>3.43</cell><cell>28075</cell><cell>2.68</cell><cell>28349</cell><cell>2.70</cell></row><row><cell>road runner</cell><cell>327025</cell><cell>16.05</cell><cell>999900</cell><cell>49.06</cell><cell>878600</cell><cell>43.11</cell><cell>999999</cell><cell>49.06</cell></row><row><cell>robotank</cell><cell>59</cell><cell>76.96</cell><cell>143</cell><cell>190.79</cell><cell>108</cell><cell>143.63</cell><cell>113.4</cell><cell>150.68</cell></row><row><cell>seaquest</cell><cell>815970</cell><cell>81.60</cell><cell>539456</cell><cell>53.94</cell><cell>943910</cell><cell>94.39</cell><cell cols="2">1000000 100.00</cell></row><row><cell>skiing</cell><cell>-18407</cell><cell>-9.47</cell><cell>-4185</cell><cell>93.40</cell><cell>-6774</cell><cell>74.67</cell><cell>-6025</cell><cell>86.77</cell></row><row><cell>solaris</cell><cell>3031</cell><cell>1.63</cell><cell>20306</cell><cell>17.31</cell><cell>11074</cell><cell>8.93</cell><cell>9105</cell><cell>7.14</cell></row><row><cell>space invaders</cell><cell>59602</cell><cell>9.57</cell><cell>93147</cell><cell>14.97</cell><cell>140460</cell><cell>22.58</cell><cell>154380</cell><cell>24.82</cell></row><row><cell>star gunner</cell><cell>214383</cell><cell>200.00</cell><cell>609580</cell><cell>200.00</cell><cell>465750</cell><cell>200.00</cell><cell>677590</cell><cell>200.00</cell></row><row><cell>surround</cell><cell>9</cell><cell>96.94</cell><cell>N/A</cell><cell>N/A</cell><cell>-8</cell><cell>11.22</cell><cell>2.606</cell><cell>64.32</cell></row><row><cell>tennis</cell><cell>12</cell><cell>79.91</cell><cell>24</cell><cell>106.7</cell><cell>24</cell><cell>106.70</cell><cell>24</cell><cell>106.70</cell></row><row><cell>time pilot</cell><cell>359105</cell><cell>200.00</cell><cell>183620</cell><cell>200.00</cell><cell>216770</cell><cell>200.00</cell><cell>450810</cell><cell>200.00</cell></row><row><cell>tutankham</cell><cell>252</cell><cell>4.48</cell><cell>528</cell><cell>9.62</cell><cell>424</cell><cell>7.68</cell><cell>418.2</cell><cell>7.57</cell></row><row><cell>up n down</cell><cell>649190</cell><cell>200.00</cell><cell>553718</cell><cell>200.00</cell><cell cols="3">986440 11.9785 966590</cell><cell>200.00</cell></row><row><cell>venture</cell><cell>2104</cell><cell>5.41</cell><cell>3074</cell><cell>7.90</cell><cell>2035</cell><cell>5.23</cell><cell>2000</cell><cell>5.14</cell></row><row><cell>video pinball</cell><cell>685436</cell><cell>0.77</cell><cell>999999</cell><cell>1.12</cell><cell>925830</cell><cell>1.04</cell><cell>978190</cell><cell>1.10</cell></row><row><cell>wizard of wor</cell><cell>93291</cell><cell>23.49</cell><cell>199900</cell><cell>50.50</cell><cell>64293</cell><cell>16.14</cell><cell>63735</cell><cell>16.00</cell></row><row><cell>yars revenge</cell><cell>557818</cell><cell>3.70</cell><cell>999998</cell><cell>6.65</cell><cell>972000</cell><cell>6.46</cell><cell>968090</cell><cell>6.43</cell></row><row><cell>zaxxon</cell><cell>65325</cell><cell>78.04</cell><cell>18340</cell><cell>21.88</cell><cell>109140</cell><cell>130.41</cell><cell>216020</cell><cell>200.00</cell></row><row><cell>MEAN SABER(%)</cell><cell></cell><cell>48.74</cell><cell></cell><cell>71.80</cell><cell></cell><cell>61.66</cell><cell></cell><cell>71.26</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>2.43E-09</cell><cell></cell><cell>7.18E-11</cell><cell></cell><cell>3.08E-09</cell><cell></cell><cell>3.56E-09</cell></row><row><cell>MEDIAN SABER(%)</cell><cell></cell><cell>24.86</cell><cell></cell><cell>50.5</cell><cell></cell><cell>35.78</cell><cell></cell><cell>50.63</cell></row><row><cell>Learning Efficiency</cell><cell></cell><cell>1.24E-09</cell><cell></cell><cell>5.05E-11</cell><cell></cell><cell>1.78E-09</cell><cell></cell><cell>2.53E-09</cell></row><row><cell>HWRB</cell><cell></cell><cell>5</cell><cell></cell><cell>15</cell><cell></cell><cell>17</cell><cell></cell><cell>22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 19 :</head><label>19</label><figDesc>Score table of other SOTA algorithms on SABER. J.8 Atari Games Learning Curves J.8.1 Atari Games Learning Curves of GDI-I 3</figDesc><table><row><cell>5.5e+4</cell><cell></cell><cell>1.8e+3</cell><cell></cell></row><row><cell>3e+4 3.5e+4 4e+4 4.5e+4 5e+4</cell><cell></cell><cell>1e+3 1.2e+3 1.4e+3 1.6e+3</cell><cell></cell></row><row><cell>2.5e+4</cell><cell></cell><cell>800</cell><cell></cell></row><row><cell>0 5e+3 1e+4 1.5e+4 2e+4</cell><cell></cell><cell>0 200 400 600</cell><cell></cell></row><row><cell>-5e+3</cell><cell></cell><cell>-200</cell><cell></cell></row><row><cell>-20k 0</cell><cell>20k 40k 60k 80k 100k 120k 14</cell><cell>-20k 0</cell><cell>20k 40k 60k 80k 100k 120k 14</cell></row><row><cell></cell><cell>1. alien</cell><cell></cell><cell>2. amidar</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39"><head></head><label></label><figDesc>Atari Games Learning Curves of GDI-H 3</figDesc><table><row><cell>-1e+5 0 1e+5 2e+5 3e+5 4e+5 5e+5 6e+5 7e+5 8e+5 9e+5 1e+6 1.1e+6 1.2e+6 -20k 0 20k 40k 60k 80k 100k 120k 140k 16 52. up_n_down J.8.2 -1e+4 -5e+3 0 5e+3 1e+4 1.5e+4 2e+4 2.5e+4 3e+4 3.5e+4 4e+4 4.5e+4 5e+4 5.5e+4 6e+4</cell><cell>-400 -200 0 200 400 600 800 1e+3 1.2e+3 1.4e+3 1.6e+3 1.8e+3 2e+3 2.2e+3 2.4e+3 -200 0 200 400 600 800 1e+3 1.2e+3 1.4e+3</cell><cell>0</cell><cell>20k 40k 60k 80k 100k 120k 140k 16 53. venture</cell><cell>-1e+5 0 1e+5 2e+5 3e+5 4e+5 5e+5 6e+5 7e+5 8e+5 9e+5 1e+6 1.1e+6 1.2e+6 -20k 0 20k 40k 60k 80k 100k 120k 140k 16 54. video_pinball -1e+4 0 1e+4 2e+4 3e+4 4e+4 5e+4 6e+4 7e+4 8e+4 9e+4 1e+5 1.1e+5 1.2e+5</cell></row><row><cell>-20k 0 20k 40k 60k 80k 100k120k140k160k18</cell><cell cols="3">-20k 0 20k 40k 60k 80k 100k120k140k160k18</cell><cell>-20k 0 20k 40k 60k 80k 100k 120k 140k 16</cell></row><row><cell>-1e+4 0 1e+4 2e+4 3e+4 4e+4 5e+4 6e+4 7e+4 8e+4 -20k 0 20k 40k 60k 80k 100k 120k 140k 16 55. wizard_of_wor 1. alien -1e+5 0 1e+5 2e+5 3e+5 4e+5 5e+5 6e+5 7e+5 8e+5 9e+5 1e+6 1.1e+6 1.2e+6</cell><cell cols="3">-1e+5 0 1e+5 2e+5 3e+5 4e+5 5e+5 6e+5 7e+5 1.2e+6 1.1e+6 1e+6 9e+5 8e+5 -20k 0 20k 40k 60k 80k 100k 120k 140k 16 2. amidar 56. yars_revenge</cell><cell>3. assault -20k 0 20k 40k 60k 80k 100k 120k 140k 16 1.4e+5 1.2e+5 1e+5 -2e+4 0 2e+4 4e+4 6e+4 8e+4 57. zaxxon</cell></row><row><cell>-20k 0 20k 40k 60k 80k 100k 120k 140k 16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4. asterix</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>51. tutankham</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head>Table 20 :</head><label>20</label><figDesc>Settings of ablation study.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">According to<ref type="bibr" target="#b2">(LaBar, Adami, 2017)</ref>, most mutations are deleterious and cause a reduction in population fitness known as the mutational load. Therefore, excessive and redundant diversity may be harmful.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">200M and 10B+ represent the training scale.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">200M and 10B+ represent the training scale.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">200M and 10B+ represent the training scale.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Games</head><p>Fix Selection HNS(%) Boltzmann Selection HNS(%) <ref type="bibr">GDI</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapturowski</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ostrovski</forename><surname>Georg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munos</forename><surname>Remi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dabney</forename><surname>Will</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Recurrent experience replay in distributed reinforcement learning // International conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Aviral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gupta</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levine</forename><surname>Sergey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07305.2020</idno>
		<title level="m">Discor: Corrective feedback in reinforcement learning via distribution correction</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Evolution of drift robustness in small populations // Nature Communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Labar</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adami</forename><surname>Christoph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyra</forename><surname>Ola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perel</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalibard</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaderberg</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Chenjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budden</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harley</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gupta</forename><surname>Pramod</surname></persName>
		</author>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1791" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<title level="m">Machine learning</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mnih</forename><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirza</forename><surname>Puigdomenech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillicrap</forename><surname>Graves Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harley</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silver</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavukcuoglu</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<title level="m">Asynchronous methods for deep reinforcement learning // International conference on machine learning</title>
		<imprint>
			<date type="published" when="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Human-level control through deep reinforcement learning // nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mnih</forename><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavukcuoglu</forename><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silver</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rusu</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veness</forename><surname>Joel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bellemare</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graves</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riedmiller</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fidjeland</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ostrovski</forename><surname>Georg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Safe and Efficient Off-Policy Reinforcement Learning // Advances in Neural Information Processing Systems 29</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munos</forename><surname>Remi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepleton</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harutyunyan</forename><surname>Anna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bellemare</forename><surname>Marc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Recht</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wright</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hogwild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1106.5730</idno>
		<title level="m">A lock-free approach to parallelizing stochastic gradient descent // arXiv preprint</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Effective diversity in population-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker-Holder</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pacchiano</forename><surname>Aldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choromanski</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberts</forename><surname>Stephen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00632.2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">RE: Human-level Performance in 3D Multiplayer Games with Populationbased Reinforcement Learning // Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedersen Carsten Lund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tracking how humans evolve in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pennisi</forename><surname>Elizabeth</surname></persName>
		</author>
		<idno>2016. 352</idno>
		<imprint>
			<biblScope unit="volume">6288</biblScope>
			<biblScope unit="page" from="876" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schaul</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonoglou</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silver</forename><surname>David</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Prioritized experience replay // arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber Sepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Long short-term memory // Neural Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?rgen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Off-policy actor-critic with shared experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schmitt</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hessel</forename><surname>Matteo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simonyan</forename><surname>Karen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. 2020</title>
		<imprint>
			<biblScope unit="page" from="8545" to="8554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hassabis Demis, Graepel Thore, others . Mastering atari, go, chess and shogi by planning with a learned model // Nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schrittwieser</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonoglou</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simonyan</forename><surname>Karen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifre</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schmitt</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guez</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lockhart</forename><surname>Edward</surname></persName>
		</author>
		<idno>. 2020. 588</idno>
		<imprint>
			<biblScope unit="volume">7839</biblScope>
			<biblScope unit="page" from="604" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schulman</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbeel</forename><surname>Pieter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06440</idno>
		<title level="m">Equivalence between policy gradients and soft q-learning // arXiv preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schulman</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levine</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbeel</forename><surname>Pieter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Philipp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schulman</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolski</forename><surname>Filip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhariwal</forename><surname>Prafulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radford</forename><surname>Alec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klimov</forename><surname>Oleg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to predict by the methods of temporal differences // Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutton</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutton</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barto</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toromanoff</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wirbel</forename><surname>Emilie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moutarde</forename><surname>Fabien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04683</idno>
		<title level="m">Is deep reinforcement learning really superhuman on atari? leveling the playing field // arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guez</forename><surname>Van Hasselt Hado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silver</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ewalds Timo, Georgiev Petko, others . Grandmaster level in StarCraft II using multi-agent reinforcement learning // Nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babuschkin</forename><surname>Igor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Czarnecki</forename><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Micha?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dudzik</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Junyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choi</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Powell</forename><surname>Richard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="page" from="350" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dueling network architectures for deep reinforcement learning // International conference on machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ziyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schaul</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hessel</forename><surname>Matteo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasselt</forename><surname>Hado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanctot</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freitas</forename><surname>Nando</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Q-learning // Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Watkins</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jch</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Peter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Watkins Christopher John Cornish</forename><surname>Hellaby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Explorations in efficient reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wiering</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning // Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SOTA model-based algorithms and other SOTA algorithms. 4 Additionally, we calculate the capped human world records normalized world score (CHWRNS) or called SABER (Toromanoff et al., 2019) of each game with each algorithms. First of all, we demonstrate the sources of the scores that we used. Random scores are from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Badia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Atari Games Table of Scores Based on SABER In this part, we detail the raw score of several representative SOTA algorithms including the SOTA 200M model-free algorithms</title>
		<meeting><address><addrLine>Badia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>NGU&apos;s scores are from. et al., 2020b). Agent57&apos;s scores are from</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Go-Explore&apos;s scores are form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schrittwieser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>DreamerV2&apos;s scores are from</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">In the following we detail the raw scores and SABER of each algorithms on 57 Atari games</title>
		<imprint/>
	</monogr>
	<note>Muesli&apos;s scores are form (Hessel et al., 2021)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">we mark the state generated by GDI-I 3 as A i and mark the state generated by GDI-I 1 as B i</title>
	</analytic>
	<monogr>
		<title level="m">SNE In all the t-SNE</title>
		<imprint/>
	</monogr>
	<note>where i = 1, 2, 3 represents three stages of the training process. WLOG, we choose the Boltzmann Selection as the representative of GDI-I 1 algorithms</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Early stage of GDI-I 3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Early stage of GDI-I 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Middle stage of GDI-I 3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Middle stage of GDI-I 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Later stage of GDI-I 3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Later stage of GDI-I 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">We sample 1k states from each stage of GDI-I 3 and GDI-I 1 . We highlight 1k states of each stage of GDI-I 3 and GDI-I 1 . 1. Early stage of GDI-I 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Sne Of</forename><surname>Seaquest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
	<note>t-SNE is drawn from 6k states</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Early stage of GDI-I 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Middle stage of GDI-I 3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Middle stage of GDI-I 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Later stage of GDI-I 3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Later stage of GDI-I 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">We sample 1k states from each stage of GDI-I 3 and GDI-I 1 . We highlight 1k states of each stage of GDI-I 3 and GDI-I 1 . 1. Early stage of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Sne Of</forename><surname>Choppercommand</surname></persName>
		</author>
		<idno>GDI-H 3</idno>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
	<note>t-SNE is drawn from 6k states</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Early stage of GDI-I 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<title level="m">Middle stage of GDI-H 3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Middle stage of GDI-I 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<title level="m">Later stage of GDI-H 3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Later stage of GDI-I 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">We sample 1k states from each stage of GDI-H 3 and GDI-I 1 . We highlight 1k states of each stage of GDI-H 3 and GDI-I 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Sne Of</forename><surname>Krull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
	<note>t-SNE is drawn from 6k states</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">GDI-I 3 on Seaquest 2. GDI-I 1 on Seaquest</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<title level="m">GDI-I 3 on ChopperCommand 4. GDI-I 1 on ChopperCommand</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">GDI-I 1 on Krull Figure 22: Overview of t-SNE in Atari games. Each t-SNE figure is drawn from 6k states. We highlight 3k states of GDI-I 3 , GDI-H 3 and GDI-I 1</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
