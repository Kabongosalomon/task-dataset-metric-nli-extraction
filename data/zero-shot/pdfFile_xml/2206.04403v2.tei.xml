<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VITA: Video Instance Segmentation via Object Token Association</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miran</forename><surname>Heo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukjun</forename><surname>Hwang</surname></persName>
							<email>sj.hwang@yonsei.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seoung</forename><forename type="middle">Wug</forename><surname>Oh</surname></persName>
							<email>seoh@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
							<email>jolee@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon</forename><forename type="middle">Joo</forename><surname>Kim</surname></persName>
							<email>seonjookim@yonsei.ac.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VITA: Video Instance Segmentation via Object Token Association</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel paradigm for offline Video Instance Segmentation (VIS), based on the hypothesis that explicit object-oriented information can be a strong clue for understanding the context of the entire sequence. To this end, we propose VITA, a simple structure built on top of an off-the-shelf Transformer-based image instance segmentation model. Specifically, we use an image object detector as a means of distilling object-specific contexts into object tokens. VITA accomplishes video-level understanding by associating frame-level object tokens without using spatio-temporal backbone features. By effectively building relationships between objects using the condensed information, VITA achieves the state-of-the-art on VIS benchmarks with a ResNet-50 backbone: 49.8 AP, 45.7 AP on YouTube-VIS 2019 &amp; 2021, and 19.6 AP on OVIS. Moreover, thanks to its object token-based structure that is disjoint from the backbone features, VITA shows several practical advantages that previous offline VIS methods have not explored -handling long and highresolution videos with a common GPU, and freezing a frame-level detector trained on image domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of Video Instance Segmentation (VIS) is to predict both mask trajectories and categories of each object belonging to a set of predefined categories. Numerous studies have attained the goal in a variety of ways, but a notable innovation in terms of accuracy has been achieved by Transformer-based <ref type="bibr" target="#b26">[27]</ref> architectures. Extending DETR <ref type="bibr" target="#b4">[5]</ref> to the video domain, VisTR <ref type="bibr" target="#b27">[28]</ref> made the first attempt to design an end-to-end model that jointly predicts object trajectories with their corresponding segmentation masks. By adopting this paradigm, subsequent studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref> also tackle the problem in a complete-offline manner: video-in and video-out.</p><p>The key message from the follow-up approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref> is to effectively design core interactions between frames. In parallel with recent studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref> that improve the accuracy in various tasks by localizing the attention scope of Transformer layers, the subsequent VIS methods suggest bounding the attention scope in the encoder <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref> or the decoder <ref type="bibr" target="#b29">[30]</ref>. Specifically, they decompose the global attention by iteratively mixing two phases: intra-frame attention and inter-frame communication. Interestingly, the temporal interactions between frames are commonly achieved with only a small number of tokens, e.g., memory tokens <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref>, messenger tokens <ref type="bibr" target="#b33">[34]</ref>, and instance queries <ref type="bibr" target="#b29">[30]</ref>. As a result, the question arises: "what information is important to understand a video?" Pred. w/ Association Trans. Enc. <ref type="figure">Figure 1</ref>: (a) Early-stage VIS methods divide the problem into two components, detection and association. (b) To alleviate the context-limited structure, complete-offline methods jointly track and segment instances in an end-to-end manner by employing dense spatio-temporal features. (c) On the other hand, our VITA is a new paradigm that directly leverages object queries for offline VIS.</p><p>In this paper, we introduce Video Instance Segmentation via Object Token Association (VITA), a new offline VIS paradigm which suggests that a video can be effectively understood from a collection of object-centric tokens. Existing offline methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref>  <ref type="figure">(Fig. 1 (b)</ref>) localize objects in multiple frames by iteratively referring to dense spatio-temporal backbone features. However, such methods show difficulties in handling long sequences as the myriad of dense reference features hinders the Transformer layers from retrieving relevant information. With the motivation to devise an effective method for the long-range understanding, we obtain clues from the traditional trackingby-detection paradigm ( <ref type="figure">Fig. 1 (a)</ref>) and make two hypotheses: 1) an image object detector can fully embody the context of an object into a feature vector (or a token); and 2) a video can be represented by the relationship between the objects.</p><p>In this regard, VITA aims to parse an input video from the collection of object tokens without the necessity of referencing dense spatio-temporal backbone ( <ref type="figure">Fig. 1 (c)</ref>). Given the compactness of the token representation, VITA can collect the object tokens over the whole video and directly analyzes the collection using Transformer layers. This unique design enables the complete-offline inference (i.e., video-in and video-out) even for extremely long videos. This also facilitates building relationships between every detected object and successfully achieves global video understanding. As a result, VITA achieves state-of-the-art performance on various VIS benchmarks.</p><p>We evaluate VITA on three popular VIS benchmarks, YouTube-VIS 2019 &amp; 2021 <ref type="bibr" target="#b31">[32]</ref> and OVIS <ref type="bibr" target="#b23">[24]</ref>. With ResNet-50 <ref type="bibr" target="#b13">[14]</ref> backbone, VITA achieves the new state-of-the-arts of 49.8 AP &amp; 45.7 AP on YouTube-VIS 2019 &amp; 2021, and 19.6 AP on OVIS. Above all, VITA outperforms the previous best approaches by 5.1 AP for YouTube-VIS 2021, which contains more complicated and long sequences than YouTube-VIS 2019. VITA is the first offline method that presents the results on OVIS benchmark that consists of long videos (the longest video has 292 frames) using a single 12GB GPU.</p><p>In addition to the performance, the design of VITA have several practical advantages over the previous offline VIS methods. It can handle long and high-resolution videos so it does not require heuristics for associating clip-level results. VITA can process 1392 frames at once regardless of video resolution using a single 12GB GPU which is 11 times longer than IFC <ref type="bibr" target="#b14">[15]</ref>. Moreover, VITA can be trained on top of a parameter-frozen image object detector without sacrificing the performance much. This property is especially useful for the applications that cannot afford to store separated image and video instance segmentation models. VITA takes only 6% additional parameters to extend the Swin-L detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Online VIS approaches first predict individual tracklets within a local range window consisting of a single or a few frames. After obtaining results from adjacent windows, they associate individual tracklets of same identities by a hand-crafted or a learnable matching algorithm. MaskTrack R-CNN <ref type="bibr" target="#b31">[32]</ref> sets the groundwork for VIS research by proposing a simple tracking branch added on a two-stage image instance segmentation model <ref type="bibr" target="#b12">[13]</ref>. The methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21]</ref> that follow the tracking-by-detection paradigm ( <ref type="figure">Fig. 1 (a)</ref>) measure the similarities between per-frame predictions, then employ an association algorithm.</p><p>To deploy temporal context from multiple frames, per-clip methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> design an architecture of predicting tracklets within a local window and stitching the tracklets sequentially in a near-  <ref type="figure">Figure 2</ref>: VITA takes only mask features and frame queries that are independently decoded by the frame-level detector for entire video sequence. By directly constructing temporal interactions between frame queries that encapsulate rich object-aware knowledge in spatial scenes, VITA yields mask trajectories with corresponding categories in an end-to-end manner.</p><p>online manner. Propagation-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b11">12]</ref> devise a paradigm that conjugates rich previous information stored in memories to facilitate online applications. EfficientVIS <ref type="bibr" target="#b28">[29]</ref> introduces correspondence learning between adjacent tracklet features and successfully runs in a cascaded manner which eliminates the hand-crafted tracklet association.</p><p>Offline VIS architectures are proposed with the motivation of predicting mask trajectories through a whole video sequence at once. VisTR <ref type="bibr" target="#b27">[28]</ref> successfully extends DETR <ref type="bibr" target="#b4">[5]</ref> to the VIS domain, introducing a new paradigm of jointly tracking and segmenting instances. However, its dense selfattention over the spatio-temporal inputs leads to explosive computations and memories. With the motivation of relaxing the heavy computation of VisTR, IFC <ref type="bibr" target="#b14">[15]</ref> adopts memory tokens to the Transformer encoder and decodes clip-level object queries. By setting the frame-level encoder to be independent and adopting the decoder of IFC, Mask2Former-VIS <ref type="bibr" target="#b5">[6]</ref> records considerable performance on benchmarks by taking the advantage of its mask-oriented representation <ref type="bibr" target="#b6">[7]</ref>. TeViT <ref type="bibr" target="#b33">[34]</ref> proposes a new backbone that efficiently exchanges temporal information internally based on Vision Transformers <ref type="bibr" target="#b8">[9]</ref> instead of the frame-wise CNN backbone. SeqFormer <ref type="bibr" target="#b29">[30]</ref> decomposes the decoder to be frame-independent, while building communication between different frames using instance queries that are used for frame-wise detection. All these studies achieve promising performance by referring to dense backbone features ( <ref type="figure">Fig. 1 (b)</ref>). On the other hand, our VITA suggests a new offline VIS paradigm that directly interprets a video from the collection of object tokens ( <ref type="figure">Fig. 1 (c)</ref>).</p><p>Global trackers that aim to associate frame-level predictions across an entire sequence as a whole are studied in the Multiple Object Tracking (MOT) community. Conventional approaches formulate the problem as a graph optimization -interpreting each detection as a node and considering the edges as possible connections between the nodes <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>. Different from existing methods, GTR <ref type="bibr" target="#b35">[36]</ref> introduces a Transformer-based architecture that receives queries, then explicitly searches for the predictions with the same identities. Similarly, a recent method <ref type="bibr" target="#b15">[16]</ref> proposes a set classifier that classifies the category of each tracklet by globally aggregating information from multiple frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first give a brief overview of Mask2Former <ref type="bibr" target="#b6">[7]</ref>, a frame-level detector for VITA. Then, we introduce the architecture of our proposed VITA, which is built on top of Mask2Former. Finally, we describe how VITA handles extremely long videos in a complete-offline manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Frame-level Detector</head><p>In this paper, we adopt Mask2Former <ref type="bibr" target="#b6">[7]</ref> for the frame-level detector which directly localizes instances using masks without the necessity of bounding boxes. Following the set prediction mechanism of DETR <ref type="bibr" target="#b4">[5]</ref>, the frame-level detector parse an input image H ? W using N f object queries, which we call frame queries (f ? R C?N f ) throughout this paper. Having the spatially encoded features to be decoded by the frame queries through a Transformer decoder, each object in the image gets represented as a C-dimensional vector. Then, the frame queries are used for both classifying and segmenting their matched objects where the predictions are also used for auxiliary supervision for VITA. Specifically, the frame-level detector generates two features for the frame-level predictions: 1) dynamic 1?1 convolutional weight from the frame queries; 2) per-pixel embeddings M ? R C? H S ? W S from the pixel decoder, where S is the stride of the feature map. Finally, the detector segments objects by applying a simple dot product between the two embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VITA</head><p>We now propose the novel end-to-end video instance segmentation method VITA, which can be largely divided into three phases <ref type="figure">(Fig. 2)</ref>. First, VITA operates on top of the frame-level detector <ref type="bibr" target="#b6">[7]</ref> in a complete frame-independent manner; no inter-computation between frames is involved. Then, the frame queries that hold object-centric information are collected throughout the whole video and they embed video-level information by building communications between different frames using Object Encoder. Finally, Object Decoder aggregates information from the frame queries to video queries, which are eventually used for predicting categories and masks of objects in videos at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input of VITA.</head><p>Given an input video of T frames, the frame-level detector executes frame-byframe as previously explained. Among a number of intermediate embeddings that are generated by the detector, the only features that are used by VITA are 1) frame queries Object Encoder. After the frame-wise detector distills the object-wise context into the frame queries, Object Encoder aims to build temporal communication by employing self-attention along the temporal axis. First, Object Encoder gathers frame queries from all frames and converts them to object tokens through a linear layer. However, a naive self-attention over the whole T N f object tokens is not applicable when processing long videos due to the quadratic computational overhead of Transformers. Inspired by Swin Transformer <ref type="bibr" target="#b21">[22]</ref>, we adopt window-based self-attention layers that shift along the temporal dimension. As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, Object Encoder initially partitions object tokens {f t } T t=1 to the temporal axis with local windows of size W without an overlap. By alternatively shifting the windows, object tokens from different frames can exchange object-wise information which allows VITA to both effectively and efficiently handle long sequences.</p><formula xml:id="formula_0">{f t } T t=1 ? R C?T ?N f which hold object-centric information; and 2) per-pixel embeddings {M t } T t=1 ? R C?T ? H S ? W S from the pixel decoder.</formula><p>Object Decoder and Output heads. Two limitations of previous offline VIS methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6]</ref> are the ineffectiveness in handling dynamic scenes and the inability of processing long videos. For example, such methods obtain high accuracy when dealing with static and short videos (YouTube-VIS 2019 <ref type="bibr" target="#b31">[32]</ref>), but struggle to track objects or executes end-to-end on benchmarks with dynamic and long videos (YouTube-VIS 2021 <ref type="bibr" target="#b31">[32]</ref> and OVIS <ref type="bibr" target="#b23">[24]</ref>). Both limitations are mainly caused by the decoder, which parses object contexts directly from dense spatio-temporal features. As recent studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37]</ref> suggest, typical Transformer decoders show difficulties in retrieving relevant information from global context. In the video domain, the number of backbone features being referred to proportionally increases with the number of frames. Therefore, when handling extremely long videos, the countless reference tokens result in both imprecise information retrieval and intractable peak memories.</p><p>For the solution to the problem, we suggest Object Decoder which extracts information from the object tokens, not the spatio-temporal backbone features. Implicitly embedding the context of objects, object tokens can provide sufficient instance-specific information without the interference of dense backbone features. Specifically, we employ N v trainable video queries v ? R C?Nv to decode objectwise information from all object tokens {f t } T t=1 that are collected from all T frames. Receiving much condensed input over naively taking dense spatio-temporal features, Object Decoder effectively captures video contexts and aggregates relevant information into the video queries. As a result, Object Decoder shows fast convergence speed while achieving high accuracy. Furthermore, the compact input greatly saves memories, thus facilitates processing long and high-resolution videos.</p><p>From the decoded video queries v, VITA returns final predictions z = {(p i , m i )} Nv i=1 using two output heads similar to IFC <ref type="bibr" target="#b14">[15]</ref>; the class head and the mask head. The class head is a single linear classifier, which directly predicts class probabilities p ? R Nv?(K+1) of each video query, where K + 1 is the number of categories including an auxiliary label "no object" (?). The mask head dynamically generates mask embeddings w v ? R C?Nv per a video query, which corresponds to the tracklet of an instance over all frames. Finally, the predicted mask logits m ? R Nv?T ?H?W can be obtained from a matrix multiplication between w v and {M t } T t=1 . Instance matching. We search for optimal pair indices between the predictions from VITA and G v ground-truth to remove postprocessing heuristics such as NMS. First, we calculate costs from all possible pairs using the cost function of Mask2Former <ref type="bibr" target="#b6">[7]</ref> with a simple extension of mask-related costs to the temporal axis <ref type="bibr" target="#b14">[15]</ref>. Then, from N v ?G v costs of pairs, we follow DETR <ref type="bibr" target="#b4">[5]</ref> and use Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref> for the optimal matching as shown in <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Clip-wise losses</head><p>Similarity loss. Inspired by the initial VIS approach (MaskTrack R-CNN <ref type="bibr" target="#b31">[32]</ref>) where the similarity loss is adopted to track instances at different frames, we train video queries and frame queries to be clustered in the latent space by their identities. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref> (a), our adopted frame-level detector <ref type="bibr" target="#b6">[7]</ref> also searches for paired indices between N f frame-wise predictions and G t f ground-truth objects at each t th frame. The frame queries and the video queries that are matched to ground-truths get collected and we embed the collection through a linear layer. Then, we measure the similarity of all possible pairs using a simple matrix multiplication. Finally, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref> (c), binary cross entropy is used to compute L sim between the predicted similarities and the ground-truth where annotated to 1 for pairs of equal identities and 0 for vice-versa.</p><p>Total loss. We attach the proposed module VITA on top of the frame-level detector, and all components of the model get trained end-to-end. Note that not only video-level outputs from VITA are used for the loss computation, but also per-frame outputs from the frame-level detector get involved. Specifically, we use L f from <ref type="bibr" target="#b6">[7]</ref> to calculate loss from the per-frame outputs to frame-wise ground-truth. Extending the loss function of <ref type="bibr" target="#b6">[7]</ref> to the temporal axis as similar to <ref type="bibr" target="#b14">[15]</ref>, we use outputs from VITA z to calculate the video-level loss L v . Finally, we integrate all losses together as follows:</p><formula xml:id="formula_1">L total = ? v L v + ? f L f + ? sim L sim .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>YouTube-VIS 2019. YouTube-VIS 2019 <ref type="bibr" target="#b31">[32]</ref> is the first dataset proposed for VIS and contains 40 semantic categories. Mostly originated from Video Object Segmentation (VOS) datasets, the VIS benchmark has a small number of unique instances (average 1.7 per video for the train set) and the categories of instances appearing in the same video are different in general. Also, the average length of videos in the valid set is short (27.4 frames), which enables existing complete-offline approaches to load a whole video and infer the benchmark at once. YouTube-VIS 2021. In order to address more difficult scenarios, additional videos are included in YouTube-VIS2021 (794 videos for training and 129 videos for validation). In particular, a greater number of objects with confusing trajectories has been added (average 3.4 per video for the additional videos in the train set). However, the average length of the additional validation videos is still 39.7 frames, which is not significantly increased compared to YouTube-VIS 2019.</p><p>OVIS. Under the same definition as YouTube-VIS, OVIS <ref type="bibr" target="#b23">[24]</ref> specifically aims to tackle objects with heavy occlusions that are belonging to 25 semantic categories. In addition to the heavily occluded situation, OVIS has three challenging characteristics that are distinct from the YouTube-VIS datasets. First, although it has fewer categories than YouTube-VIS, much more instances appear in a single video (average 5.9 per video for the train set). Second, the instances with the same categories in the same video have almost similar appearances, thus approaches that rely heavily on visual cues often struggle to predict accurate trajectories. Finally, the average length of videos for the valid set is 62.7 frames (the longest video has 292 frames) which is much longer than that of YouTube-VIS. Therefore, not only do previous approaches show relatively low accuracy, but all existing complete-offline VIS methods are not feasible to infer OVIS without hand-crafted association algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Our method is implemented on top of detectron2 <ref type="bibr" target="#b30">[31]</ref>. All hyper-parameters regarding the framelevel detector are equal to the defaults of Mask2Former <ref type="bibr" target="#b6">[7]</ref>. The total loss L total is balanced with ? v , ? f , and ? sim where 1.0, 1.0, and 0.5, respectively. By default, Object Encoder is composed of three layers with the window size W = 6, and Object Decoder employs six layers with N v = 100 video queries. Having VITA built on top of Mask2Former, we first train our model on the COCO [20] dataset following Mask2Former. Then, we train our method on the VIS datasets <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24]</ref> simultaneously with pseudo videos generated from images <ref type="bibr" target="#b19">[20]</ref> following the details of SeqFormer <ref type="bibr" target="#b29">[30]</ref>. During inference, each frame is resized to a shorter edge size of 360 and 448 pixels when using ResNet <ref type="bibr" target="#b13">[14]</ref> and Swin <ref type="bibr" target="#b21">[22]</ref> backbones, respectively. Note that all reported scores in main results and ablation studies are the mean of five runs, and we use the standard ResNet-50 <ref type="bibr" target="#b13">[14]</ref> for the backbone unless specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>Using the popular VIS benchmarks -YouTube-VIS 2019 &amp; 2021 <ref type="bibr" target="#b31">[32]</ref> and OVIS [24] -we compare VITA with state-of-the-art approaches following the standard evaluation metric <ref type="bibr" target="#b31">[32]</ref>.</p><p>YouTube-VIS 2019. Tab. 1 shows the comparison on YouTube-VIS 2019 dataset with backbones of both CNN-based (ResNet-50 and 101 <ref type="bibr" target="#b13">[14]</ref>) and Transformer-based (Swin-L <ref type="bibr" target="#b21">[22]</ref>). Offline methods can take two advantages over (near) online approaches: 1) they have a greater receptive field to the temporal axis, and 2) they can avoid error propagation derived from hand-crafted association algorithms. As a result, the tendency of offline methods with higher accuracy is clearly shown in the table. Among the competitive offline models, our VITA sets a new state-of-the-art of 49.8 AP and 51.7 AP using CNN backbones, ResNet-50 and ResNet-101 respectively. In addition, with Swin-L backbone, VITA achieves 63.0 AP outperforming all existing VIS methods.</p><p>YouTube-VIS 2021. We compare VITA with state-of-the-art methods on YouTube-VIS 2021 benchmark in Tab. 2. Note that the longest video in the valid set has 84 frames, thus previous offline methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6]</ref> can infer videos at once with GPUs with large memories. Above all, VITA achieves the highest accuracy, 45.7 AP, which outperforms the previous state-of-the-art approach <ref type="bibr" target="#b5">[6]</ref> with a huge margin of 5.1 AP. Considering the accuracy gap on YouTube-VIS 2019, the results demonstrate that VITA can effectively handle tricky scenarios, e.g., numerous unique instances with confusing trajectories. We hypothesize that the object-oriented design of VITA is more effective than typical dense Transformer decoders in addressing such challenging scenes.</p><p>OVIS. In Tab. 2, we demonstrate the competitiveness of VITA on the challenging OVIS benchmark. Due to the considerable lengths of videos -the longest video has 292 frames -existing offline approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6]</ref> cannot process OVIS benchmark in their original design: video-in and video-out. To the best of our knowledge, VITA is the first complete-offline approach to evaluate on OVIS valid set. Thanks to its object token-based structure which is disjoint from backbone features, VITA can process the benchmark without any hand-crafted association algorithm. Moreover, VITA sets a new state-of-the-art performance of 19.6 AP, demonstrating the potential of the complete-offline pipeline in long and complicated scenes.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>We provide a series of ablation studies using a ResNet-50 <ref type="bibr" target="#b13">[14]</ref> backbone. All experiments are conducted on YouTube-VIS 2019 <ref type="bibr" target="#b31">[32]</ref> valid set except for Tab. 5 with OVIS <ref type="bibr" target="#b23">[24]</ref> valid set.</p><p>Attention window size. Tab. 3 shows the performance of VITA with varying sizes of shifted attention window W in Object Encoder during inference. The larger the window, the greater the receptive field for the temporal axis in Object Encoder. The results suggest that larger window sizes utilize information from multiple frames, which helps Object Encoder understand the context of objects in videos. We set W = 6 considering a trade-off between performance and inference scalability.</p><p>Maximum number of frames. In Tab. 4, we calculate the maximum number of frames that VITA can handle with respect to the various window sizes W , and compare it with existing complete-offline VIS methods. To take into account the general environment, all results are computed using a single 12GB Titan XP GPU. As shown in results, existing methods have limitations in processing long videos in a video-in and video-out manner. Clearly, the bottleneck of VisTR <ref type="bibr" target="#b27">[28]</ref> is the encoder, where the full spatio-temporal self-attention leads to a tremendous memory usage. IFC <ref type="bibr" target="#b14">[15]</ref> alleviates the computation of VisTR <ref type="bibr" target="#b27">[28]</ref>, achieving a higher number of input frames. However, IFC makes use of a typical Transformer decoder that visits all dense spatio-temporal features. Therefore, IFC cannot infer the OVIS <ref type="bibr" target="#b23">[24]</ref> benchmark at once which contains a video of 292 frames. The problem gets aggravated in Mask2Former-VIS <ref type="bibr" target="#b5">[6]</ref> as the scope of the decoder is extended to multiple feature levels <ref type="bibr" target="#b6">[7]</ref>. On the other hand, VITA presents considerable frame numbers that can be inferred completely offline. Furthermore, VITA is independent from input frame resolutions as each frame gets summarized into compact object tokens. With input resolution of 360 ? 640 and W = 6, the maximum length of sequence that VITA is able to process in complete-offline is about 11? longer than IFC <ref type="bibr" target="#b14">[15]</ref>.</p><p>Heuristic clip association. Tab. 5 shows the results on OVIS valid set of splitting a video into shorter clips and associating clip-wise predictions through heuristic matching. The length of the clip is set to be less than the average length of videos of OVIS valid set (62.7). Then, we associate outputs from different clips using mask IoU score as the matching cost. We test with two matching algorithms: Greedy and Hungarian. As shown in Tab. 5, VITA demonstrates the best performance on the complete-offline inference that use all the video frames at once.</p><p>Pruning Tokens. In Tab. 6, we investigate the effects of removing redundant frame queries. From a collection of frame queries, VITA understands the overall context of the given clip. As only a small portion of the collection is matched to foreground objects, the number of total input frame queries  can be reduced. First, for each frame, we sort frame queries in ascending order by the "no object" (?) probability. Then, we keep only top rN f queries from the sorted list where r is the ratio, and discard the rest. The accuracy with respect to the ratio r is as shown in Tab. 6.</p><p>By setting the ratio r = 0.75, the accuracy of VITA shows only a marginal degradation in the accuracy (?0.1 AP). This signifies that VITA focuses more on the foreground contexts that are embedded in the frame queries. Meanwhile, as the quadratic computation in Clip Encoder can be alleviated, VITA can process a much greater number of frames; using the ratio r = 0.75, the maximum frame number increases from 1392 (Tab. 4) to 2635.</p><p>Convergence speed and Similarity loss. <ref type="figure" target="#fig_3">Fig. 5</ref> validates our claim of the faster convergence speed and the effectiveness of the proposed Similarity loss. For a fair comparison, we report average scores and standard deviations of five runs, each trained without pseudo videos, same as Mask2Former-VIS <ref type="bibr" target="#b5">[6]</ref>. Thanks to its object-centric design, VITA shows faster convergence than Mask2Former-VIS. Furthermore, the use of Similarity loss leads to an additional accuracy gain of 1.8 AP. The results demonstrate that the loss mitigates the discrepancies between the embeddings of equal identities, leading to better performance.</p><p>Frozen frame-level detector. In Tab. 7, we demonstrate the performance of VITA where the framelevel detector is completely frozen. Specifically, while VITA gets trained on YouTube-VIS 2019, the frame-level detector <ref type="bibr" target="#b6">[7]</ref> does not get updated from pretrained weights on COCO <ref type="bibr" target="#b19">[20]</ref>. Note that among 40 categories in YouTube-VIS 2019 dataset, only 20 categories overlap with the categories of COCO. Interestingly, though the frame-level detector remains completely frozen, VITA achieves compelling results with various backbones. As shown in Tab. 1 and Tab. 7, VITA presents a huge practicality as it surpasses all online approaches on top of the ResNet-50 backbone. This strategy can be beneficial in various scenarios: 1) when the accuracy of image instance segmentation should be kept while extending the network to the video domain, and 2) when having limited time and GPUs to train models. The strategy can be especially useful in mobile applications that have scarce storage for keeping two separate network parameters for image instance segmentation and video instance segmentation. With additional 6% parameters, VITA successfully extends the frozen Swin-L based frame-level detector to the video domain and it achieves great accuracy.</p><p>We also provide a brief discussion of our understanding for the large gap in AP. Compared to COCO, we observe that YouTube-VIS dataset is annotated with only a few salient objects as foregrounds.</p><p>Having weights of the frame-level detector frozen to COCO, the detector cannot adapt to the YouTube-VIS domain and it embeds and interprets more objects in scenes as foregrounds. Therefore, VITA outputs more predictions as a foreground category even if such predictions are not labeled as groundtruths in YouTube-VIS. As a result, it leads to a lower average precision as it comes out with more false positive predictions. On the contrary, the more false positive predictions only slightly affect AR.</p><p>Qualitative Results. We provide some visualizations of the predictions from VITA and frame-level detector in <ref type="figure" target="#fig_4">Fig. 6</ref>. The qualitative results show that VITA leads to better video instance segmentation qualities compared to the frame-level detector. Specifically, the frame-level detector mistakenly interprets in to recognize either category or mask of instances that have been largely occluded, while our method successfully recovers it by leveraging the temporal information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>VITA has achieved high performance in the complete-offline paradigm while dramatically improving the number of input frames that can be processed at once. However, there are two major limitations for the ultimate long video understanding. First, the current architecture still has limitations in processing an infinite number of frames. In addition, since object tokens do not explicitly utilize temporal information, they may have difficulties in identifying complex behaviors that span over very long sequences. We believe that devising explicit designs to address these issues will be a promising future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed VITA for offline Video Instance Segmentation. VITA is a simple model built on top of the off-the-shelf image instance segmentation model <ref type="bibr" target="#b6">[7]</ref>. Unlike existing offline methods, VITA directly leverages object queries decoded by independent frame-level detectors. We demonstrated that deploying object-oriented information is not only effective in improving performance, but also has robust practicality for processing long and high-resolution videos -setting state-of-the-art on popular VIS benchmarks, e.g., YouTubeVIS-2019 &amp; 2021 and OVIS. Moreover, since VITA is designed to absorb spatial knowledge purely from image object detector, it shows fast convergence and demonstrates competitive performance even if trained on frozen detectors. We hope that our method extends the scope of offline VIS research beyond benchmarks to real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head><p>A.1 Implementation</p><p>We use 4 NVIDIA A100 GPUs with 40GB of memory (8 A100 GPUs when using a Swin <ref type="bibr" target="#b21">[22]</ref> backbone), and activate Automatic Mixed Precision (AMP) provided by PyTorch. Our training pipline is two-stage. We first pretrain the model for image instance segmentation on COCO <ref type="bibr" target="#b19">[20]</ref> train set using the batch size of 16 and by setting the number of input frames to T = 1. Then, we finetune the pretrained model on the VIS train sets (YouTube-VIS 2019 <ref type="bibr" target="#b31">[32]</ref>, YouTube-VIS 2021, and OVIS <ref type="bibr" target="#b23">[24]</ref>) with pseudo-videos augmented from COCO images (see Appendix A.1.1). We use the batch size of 8 and set each input clip to be length of T = 6. Considering the difficulty and varying number of training videos included in each dataset, we set up different training iterations for each VIS dataset -130k, 160k, 110k with decay of learning rates at 75k, 100k, 50k for YouTube-VIS 2019, 2021, and OVIS, respectively. And both Object Encoder and Object Decoder in VITA follow the standard Transformer encoder and decoder architectures suggested in DETR <ref type="bibr" target="#b4">[5]</ref>. However, we just switch the order of self-and cross-attention in Object Decoder to make video queries learnable, and eliminate dropouts to make computation more efficient, as discussed in Mask2Former <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Pseudo-video generation</head><p>During training, we follow SeqFormer <ref type="bibr" target="#b29">[30]</ref> to generate pseudo-videos from a single image. Given a single image, we first resize the short side of an image to one of the 400, 500 and 600 pixels while maintaining its ratio. Then, the image is randomly cropped T times to a size in the range [384, 600] to create a pseudo-video of length T . Finally, the cropped images are resized to a shorter edge to be randomly chosen from [288, 512] pixels with a step of 32 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Loss function</head><p>The final loss function of our frame-level detector <ref type="bibr" target="#b6">[7]</ref>, denoted by L f in the main paper, is largely composed of two terms: mask-related loss and categorical loss. The mask-related loss is again consists of L f ce and L f dice , each representing a binary cross-entropy loss and a dice loss, respectively. Then, the final loss L f is a combination of a categorical loss (the cross entropy) and the mask-related loss L f = ? cls L f cls + ? ce L f ce + ? dice L f dice and we set ? cls = 2, ? ce = 2, and ? dice = 5, respectively. For the L v calculated from video-level results generated by VITA, we employ the same hyperparameters as frame-level losses:</p><formula xml:id="formula_2">L v = ? cls L v cls + ? ce L v ce + ? dice L v dice .</formula><p>Note that, for L v ce and L v dice , we extend the functions of L f ce and L f dice to the temporal axis, just as IFC [15] did.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Building VITA on Mask2Former</head><p>Mask2Former uses 9 decoder layers where output frame queries from each layer can be used as an input for VITA. However, using the outputs from all 9 layers during training leads to the lack of GPU memory. Therefore, we use the outputs from the last 3 layers for training VITA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Inference procedure</head><p>In Tab. 4 in the main paper, we measured the maximum number of frames that each model can infer at once. To further specify the process of measuring the numbers, we provide simplified PyTorch-style inference pseudo-codes of both VITA and Mask2Former-VIS in Tab. 8 and Tab. 9 respectively. For fair comparison, we modified the inference procedure of previous methods to collect backbone features of each frame sequentially. The strategy prevents the methods from a memory explosion until entering each VIS prediction module. The most noticeable difference is that VITA collects only frame_queries and mask_features of each frame from our frame-level detector <ref type="bibr" target="#b6">[7]</ref> denoted by the function mask2former() (line 2-12 in Tab. 8). Then, the frame_queries for the entire video become the input of Object Encoder (line 19 in Tab. 8). On the other hand, previous Transformer-based offline VIS models (e.g., Mask2Former-VIS), first aggregate the backbone features of entire video and takes it as inputs for the VIS model, the function mask2former_vis() (line 3-20 in Tab. 9). After that, both of methods generate their video-level predictions by using their vq (video queries) and mask_features.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of an Object Encoder layer. Blocks with dashed line are local windows, and indicates an object token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Similarity loss. and indicate video query and frame query, respectively. Same color represents same GT instance ID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Train speed comparison with Mask2Former-VIS<ref type="bibr" target="#b5">[6]</ref>. ? indicates the same training setup with VITA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of predictions from the frame-level detector and VITA. Instances with the same identity are displayed in the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons on YouTube-VIS 2019. Method Backbone AP AP 50 AP 75 AR 1 AR 10</figDesc><table><row><cell></cell><cell cols="2">MaskTrack R-CNN [32] ResNet-50</cell><cell>30.3 51.1</cell><cell cols="3">32.6 31.0 35.5</cell></row><row><cell>(Near) Online</cell><cell cols="3">MaskTrack R-CNN [32] ResNet-101 31.8 53.0 CrossVIS [33] ResNet-50 36.3 56.8 CrossVIS [33] ResNet-101 36.6 57.3 PCAN [17] ResNet-50 36.1 54.9 PCAN [17] ResNet-101 37.6 57.2 EfficientVIS [29] ResNet-50 37.9 59.7</cell><cell cols="3">33.6 33.2 37.6 38.9 35.6 40.7 39.7 36.0 42.0 39.4 36.3 41.6 41.3 37.2 43.9 43.0 40.3 46.6</cell></row><row><cell></cell><cell>EfficientVIS [29]</cell><cell cols="2">ResNet-101 39.8 61.8</cell><cell cols="3">44.7 42.1 49.8</cell></row><row><cell></cell><cell>VISOLO [12]</cell><cell>ResNet-50</cell><cell>38.6 56.3</cell><cell cols="3">43.7 35.7 42.5</cell></row><row><cell></cell><cell>VisTR [28]</cell><cell>ResNet-50</cell><cell>35.6 56.8</cell><cell cols="3">37.0 35.2 40.2</cell></row><row><cell></cell><cell>VisTR [28]</cell><cell cols="2">ResNet-101 38.6 61.3</cell><cell cols="3">42.3 37.6 44.2</cell></row><row><cell></cell><cell>IFC [15]</cell><cell>ResNet-50</cell><cell>41.2 65.1</cell><cell cols="3">44.6 42.3 49.6</cell></row><row><cell></cell><cell>IFC [15]</cell><cell cols="2">ResNet-101 42.6 66.6</cell><cell cols="3">46.3 43.5 51.4</cell></row><row><cell></cell><cell>TeViT [34]</cell><cell>MsgShifT</cell><cell>46.6 71.3</cell><cell cols="3">51.6 44.9 54.3</cell></row><row><cell>Offline</cell><cell>SeqFormer [30] SeqFormer [30] SeqFormer [30] Mask2Former-VIS [6]</cell><cell cols="2">ResNet-50 ResNet-101 49.0 71.1 47.4 69.8 Swin-L 59.3 82.1 ResNet-50 46.4 68.0</cell><cell cols="3">51.8 45.5 54.8 55.7 46.8 56.9 66.4 51.7 64.4 50.0 --</cell></row><row><cell></cell><cell>Mask2Former-VIS [6]</cell><cell cols="2">ResNet-101 49.2 72.8</cell><cell>54.2</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Mask2Former-VIS [6]</cell><cell>Swin-L</cell><cell>60.4 84.4</cell><cell>67.0</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>ResNet-50</cell><cell>49.8 72.6</cell><cell cols="3">54.5 49.4 61.0</cell></row><row><cell></cell><cell>VITA (Ours)</cell><cell cols="2">ResNet-101 51.9 75.4</cell><cell cols="3">57.0 49.6 59.1</cell></row><row><cell></cell><cell></cell><cell>Swin-L</cell><cell>63.0 86.9</cell><cell cols="3">67.9 56.3 68.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with ResNet-50 backbone on YouTube-VIS 2021 and OVIS. ? indicates using MsgShifT backbone. ? indicates using Swin-L [22] backbone.MethodYouTube-VIS 2021 OVIS AP AP 50 AP 75 AR 1 AR 10 AP AP 50 AP 75 AR 1 AR 10</figDesc><table><row><cell>MaskTrack R-CNN [32]</cell><cell cols="2">28.6 48.9</cell><cell cols="5">29.6 26.5 33.8 10.8 25.3</cell><cell>8.5</cell><cell>7.9</cell><cell>14.9</cell></row><row><cell>CMaskTrack R-CNN [23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">15.4 33.9</cell><cell>13.1</cell><cell>9.3</cell><cell>20.0</cell></row><row><cell>STMask [19]</cell><cell cols="2">31.1 50.4</cell><cell cols="5">33.5 26.9 35.6 15.4 33.8</cell><cell>12.5</cell><cell>8.9</cell><cell>21.3</cell></row><row><cell>CrossVIS [33]</cell><cell cols="2">34.2 54.4</cell><cell cols="5">37.9 30.4 38.2 14.9 32.7</cell><cell cols="3">12.1 10.3 19.8</cell></row><row><cell>IFC [15]</cell><cell cols="2">35.2 55.9</cell><cell cols="3">37.7 32.6 42.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VISOLO [12]</cell><cell cols="2">36.9 54.7</cell><cell cols="5">40.2 30.6 40.9 15.3 31.0</cell><cell cols="3">13.8 11.1 21.7</cell></row><row><cell>TeViT  ? [34]</cell><cell cols="2">37.9 61.2</cell><cell cols="5">42.1 35.1 44.6 17.4 34.9</cell><cell cols="3">15.0 11.2 21.8</cell></row><row><cell>SeqFormer [30]</cell><cell cols="2">40.5 62.4</cell><cell cols="3">43.7 36.1 48.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mask2Former-VIS [6]</cell><cell cols="2">40.6 60.9</cell><cell>41.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VITA (Ours)</cell><cell cols="2">45.7 67.4</cell><cell cols="5">49.5 40.9 53.6 19.6 41.2</cell><cell cols="3">17.4 11.7 26.0</cell></row><row><cell>SeqFormer  ? [30]</cell><cell cols="2">51.8 74.6</cell><cell cols="3">58.2 42.8 58.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mask2Former-VIS  ? [6]</cell><cell cols="2">52.6 76.4</cell><cell>57.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VITA (Ours)  ?</cell><cell cols="2">57.5 80.6</cell><cell cols="5">61.0 47.7 62.6 27.7 51.9</cell><cell cols="3">24.9 14.9 33.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Impact of local windows of varying sizes in Object Encoder. W AP AP 50 AP 75 AR 1 AR 10</figDesc><table><row><cell>3</cell><cell>49.4 72.2</cell><cell>54.4 48.6 60.9</cell></row><row><cell>6</cell><cell>49.8 72.6</cell><cell>54.5 49.4 61.0</cell></row><row><cell cols="2">12 50.0 73.0</cell><cell>54.7 49.0 60.8</cell></row><row><cell cols="2">All 50.1 72.4</cell><cell>54.7 49.0 60.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Maximum number of frames that can be processed at once using a single Titan XP.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Max Frames 360 ? 640 720 ? 1280</cell></row><row><cell cols="2">VisTR [28]</cell><cell>46</cell><cell>12</cell></row><row><cell cols="2">IFC [15]</cell><cell>123</cell><cell>38</cell></row><row><cell cols="2">Mask2Former-VIS [6]</cell><cell>81</cell><cell>20</cell></row><row><cell>VITA (Ours)</cell><cell>W = 3 W = 6 W = 12</cell><cell>2677 1392 741</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Use of different heuristic association algorithms on OVIS valid set.</figDesc><table><row><cell cols="2">Length Algorithm</cell><cell cols="2">AP AP 50 AP 75</cell></row><row><cell>36</cell><cell cols="2">Greedy Hungarian 18.4 38.9 18.8 39.4</cell><cell>17.1 16.3</cell></row><row><cell>48</cell><cell cols="2">Greedy Hungarian 19.1 39.1 18.8 39.0</cell><cell>17.1 17.4</cell></row><row><cell>All</cell><cell>None</cell><cell>19.6 41.2</cell><cell>17.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Pruning tokens by different ratios r. r AP AP 50 AP 75 AR 1 AR 10</figDesc><table><row><cell>1.0</cell><cell>49.8 72.6</cell><cell>54.5 49.4 61.0</cell></row><row><cell cols="2">0.75 49.7 72.5</cell><cell>54.4 48.7 61.0</cell></row><row><cell>0.5</cell><cell>48.9 72.1</cell><cell>52.0 48.3 60.9</cell></row><row><cell cols="2">0.25 48.1 71.6</cell><cell>51.6 47.4 59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results on YouTube-VIS 2019 with freezing detector pretrained on COCO.BackboneFreeze AP AP 50 AP 75 AR 1 AR 10</figDesc><table><row><cell>ResNet-50</cell><cell>?</cell><cell>49.8 72.6 40.9 61.9</cell><cell>54.5 49.4 61.0 44.6 43.1 53.1</cell></row><row><cell>ResNet-101</cell><cell>?</cell><cell>51.9 75.4 43.2 64.4</cell><cell>57.0 49.6 59.1 48.7 46.1 55.9</cell></row><row><cell>Swin-L</cell><cell>?</cell><cell>63.0 86.9 53.4 75.9</cell><cell>67.9 56.3 68.1 58.7 51.9 64.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>PyTorch-style inference pseudo-code of VITA.</figDesc><table><row><cell cols="2">1 def vita ( video ) :</cell></row><row><cell>2</cell><cell>frame_queries = [ ]</cell></row><row><cell>3</cell><cell>mask_features = [ ]</cell></row><row><cell>4</cell><cell></cell></row><row><cell>5</cell><cell>for frame in video :</cell></row><row><cell>6</cell><cell>feats = backbone ( frame )</cell></row><row><cell>7</cell><cell>fq , mf = mask2former (</cell></row><row><cell>8</cell><cell>feats</cell></row><row><cell>9</cell><cell>)</cell></row><row><cell>10</cell><cell></cell></row><row><cell>11</cell><cell>frame_queries . append ( fq )</cell></row><row><cell>12</cell><cell>mask_features . append ( mf )</cell></row><row><cell>13</cell><cell></cell></row><row><cell>14</cell><cell>"""</cell></row><row><cell>15</cell><cell>VITA only aggregates</cell></row><row><cell>16</cell><cell>frame queries for its</cell></row><row><cell>17</cell><cell>remaining computations .</cell></row><row><cell>18</cell><cell>"""</cell></row><row><cell>19</cell><cell>fq = object_encoder (</cell></row><row><cell>20</cell><cell>frame_queries</cell></row><row><cell>21</cell><cell>)</cell></row><row><cell>22</cell><cell>vq = object_decoder ( fq )</cell></row><row><cell>23</cell><cell></cell></row><row><cell>24</cell><cell>w = mask_head ( vq )</cell></row><row><cell>25</cell><cell>pred_mask = [ ]</cell></row><row><cell>26</cell><cell>for mf in mask_features :</cell></row><row><cell>27</cell><cell># w . shape : ( Nv x C )</cell></row><row><cell>28</cell><cell># mf . shape : ( C x H x W )</cell></row><row><cell>29</cell><cell>_mask = w @ mf</cell></row><row><cell>30</cell><cell></cell></row><row><cell>31</cell><cell>pred_mask . append ( _mask )</cell></row><row><cell>32</cell><cell></cell></row><row><cell>33</cell><cell># Nv x ( K + 1 )</cell></row><row><cell>34</cell><cell>pred_cls = cls_head ( vq )</cell></row><row><cell>35</cell><cell></cell></row><row><cell>36</cell><cell># Nv x T x H x W</cell></row><row><cell>37</cell><cell>pred_mask = torch . stack (</cell></row><row><cell>38</cell><cell>pred_mask , dim = 1</cell></row><row><cell>39</cell><cell>)</cell></row><row><cell>40</cell><cell></cell></row><row><cell>41</cell><cell>return pred_cls , pred_mask</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>PyTorch-style inference pseudo-code of Mask2Former-VIS<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table><row><cell cols="2">1 def previous_methods ( video ) :</cell></row><row><cell>2</cell><cell></cell></row><row><cell>3</cell><cell>frame_features = [ ]</cell></row><row><cell>4</cell><cell></cell></row><row><cell>5</cell><cell>for frame in video :</cell></row><row><cell>6</cell><cell>feats = backbone ( frame )</cell></row><row><cell>7</cell><cell>frame_features . append (</cell></row><row><cell>8</cell><cell>feats</cell></row><row><cell>9</cell><cell>)</cell></row><row><cell>10</cell><cell></cell></row><row><cell>11</cell><cell>"""</cell></row><row><cell>12</cell><cell>Previous approaches receive</cell></row><row><cell>13</cell><cell>either multi or single scale</cell></row><row><cell>14</cell><cell>feature map at once for their</cell></row><row><cell>15</cell><cell>encoder / decoder layers .</cell></row><row><cell>16</cell><cell>"""</cell></row><row><cell>17</cell><cell>vq , mask_features = \</cell></row><row><cell>18</cell><cell>mask2former_vis (</cell></row><row><cell>19</cell><cell>frame_features</cell></row><row><cell>20</cell><cell>)</cell></row><row><cell>21</cell><cell></cell></row><row><cell>22</cell><cell></cell></row><row><cell>23</cell><cell></cell></row><row><cell>24</cell><cell>w = mask_head ( vq )</cell></row><row><cell>25</cell><cell>pred_mask = [ ]</cell></row><row><cell>26</cell><cell>for mf in mask_features :</cell></row><row><cell>27</cell><cell># w . shape : ( Nv x C )</cell></row><row><cell>28</cell><cell># mf . shape : ( C x H x W )</cell></row><row><cell>29</cell><cell>_mask = w @ mf</cell></row><row><cell>30</cell><cell></cell></row><row><cell>31</cell><cell>pred_mask . append ( _mask )</cell></row><row><cell>32</cell><cell></cell></row><row><cell>33</cell><cell># Nv x ( K + 1 )</cell></row><row><cell>34</cell><cell>pred_cls = cls_head ( vq )</cell></row><row><cell>35</cell><cell></cell></row><row><cell>36</cell><cell># Nv x T x H x W</cell></row><row><cell>37</cell><cell>pred_mask = torch . stack (</cell></row><row><cell>38</cell><cell>pred_mask , dim = 1</cell></row><row><cell>39</cell><cell>)</cell></row><row><cell>40</cell><cell></cell></row><row><cell>41</cell><cell>return pred_cls , pred_mask</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>VITA is designed for the VIS task and focuses on processing long and high-resolution videos in an end-to-end manner while achieving the state-of-the-art performance. We hope that VITA can have a positive impact on many industrial areas such as video editing applications. We would like to note that research on VIS must be aware of potential misuse that violates personal privacy. <ref type="bibr" target="#b19">[20]</ref>, YouTube-VIS <ref type="bibr" target="#b31">[32]</ref>, OVIS <ref type="bibr" target="#b23">[24]</ref>, and detectron2 <ref type="bibr" target="#b30">[31]</ref>: Attribution 4.0 International, CC BY 4.0, CC BY-NC-SA 4.0, and Apache-2.0, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Licenses of COCO</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this Appendix, we first provide more training details of VITA (Appendix A). In addition, the inference procedure is explained in Appendix B.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stemseg: Spatio-temporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljo?a</forename><surname>O?ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastianan</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Rao Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mask2former for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anwesa</forename><surname>Choudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10764</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a proposal classifier for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangping</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compfeat: Comprehensive feature aggregation for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visolo: Grid-based space-time aggregation for efficient online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukjun</forename><surname>Su Ho Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeonchool</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Jung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Video instance segmentation using inter-frame communication transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukjun</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miran</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cannot see the forest for the trees: Aggregating multiple viewpoints to better classify objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukjun</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miran</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Prototypical cross-attention networks for multiple object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial feature calibration and temporal fusion for effective one-stage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sg-net: Spatial granularity network for one-stage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Occluded video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01558</idno>
		<editor>Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and Song Bai</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Occluded video instance segmentation: Dataset and iccv 2021 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Track on Datasets and Benchmarks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient video instance segmentation via tracklet query and proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhir</forename><surname>Yarram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayan</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Seqformer: a frustratingly simple model for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crossover learning for fast online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporally efficient vision transformer for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Global tracking transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
