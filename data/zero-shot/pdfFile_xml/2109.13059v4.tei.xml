<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRANS-ENCODER: UNSUPERVISED SENTENCE-PAIR MODELLING THROUGH SELF-AND MUTUAL-DISTILLATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Massiah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serhii</forename><surname>Havrylov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TRANS-ENCODER: UNSUPERVISED SENTENCE-PAIR MODELLING THROUGH SELF-AND MUTUAL-DISTILLATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In NLP, a large volume of tasks involve pairwise comparison between two sequences (e.g., sentence similarity and paraphrase identification). Predominantly, two formulations are used for sentence-pair tasks: bi-encoders and cross-encoders. Bi-encoders produce fixed-dimensional sentence representations and are computationally efficient, however, they usually underperform cross-encoders. Crossencoders can leverage their attention heads to exploit inter-sentence interactions for better performance but they require task finetuning and are computationally more expensive. In this paper, we present a completely unsupervised sentence-pair model termed as TRANS-ENCODER that combines the two learning paradigms into an iterative joint framework to simultaneously learn enhanced bi-and crossencoders. Specifically, on top of a pre-trained language model (PLM), we start with converting it to an unsupervised bi-encoder, and then alternate between the bi-and cross-encoder task formulations. In each alternation, one task formulation will produce pseudo-labels which are used as learning signals for the other task formulation. We then propose an extension to conduct such self-distillation approach on multiple PLMs in parallel and use the average of their pseudo-labels for mutual-distillation. TRANS-ENCODER creates, to the best of our knowledge, the first completely unsupervised cross-encoder and also a state-of-the-art unsupervised bi-encoder for sentence similarity. Both the bi-encoder and cross-encoder formulations of TRANS-ENCODER outperform recently proposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT (Liu et al., 2021)  and SimCSE (Gao et al., 2021)   by up to 5% on the sentence similarity benchmarks. Code and models are released at https://github.com/amzn/trans-encoder.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Comparing pairwise sentences is fundamental to a wide spectrum of tasks in NLP such as information retrieval (IR), natural language inference (NLI), semantic textual similarity (STS) and clustering. Two general architectures usually used for sentence-pair modelling are bi-encoders and cross-encoders.</p><p>In a cross-encoder, two sequences are concatenated and sent into the model (usually deep Transformers like BERT/RoBERTa; <ref type="bibr" target="#b10">Devlin et al. 2019;</ref><ref type="bibr" target="#b21">Liu et al. 2019</ref>) in one pass. The attention heads of Transformers could directly model the inter-sentence interactions and output a classification/relevance score. However, a cross-encoder needs to recompute the encoding for different combinations of sentences in each unique sequence pair, resulting in a heavy computational overhead. It is thus impractical in tasks like IR and clustering where massive pairwise sentence comparisons are involved. Also, task finetuning is always required for converting PLMs to cross-encoders. By contrast, in a bi-encoder, each sequence is encoded separately and mapped to a common embedding space for similarity comparison. The encoded sentences can be cached and reused. A bi-encoder is thus much more efficient. Also, the output of a bi-encoder can be used off-the-shelf as sentence embeddings for other downstream tasks. That said, it is well-known that in supervised learning, bi-encoders underperform cross-encoders <ref type="bibr" target="#b16">(Humeau et al., 2020;</ref><ref type="bibr" target="#b30">Thakur et al., 2021)</ref> since the former could not Self-labelled data (from cross-encoder) Self-labelled data (from bi-encoder) <ref type="figure">Figure 1</ref>: A graphical illustration of the self-distillation learning scheme in TRANS-ENCODER. Notice that the blue boxes represent the same model architecture trained sequentially. explicitly model the interactions between sentences but could only compare them in the embedding space in a post hoc manner.</p><p>In this work, we ask the question: can we leverage the advantages of both bi-and cross-encoders and bootstrap knowledge from them in an unsupervised manner? Our proposed TRANS-ENCODER addresses this question with the following intuition: As a starting point, we can use bi-encoder representations to tune a cross-encoder. With more powerful inter-sentence modelling, the crossencoder should resurface more knowledge from the PLMs than the bi-encoder given the same data. In turn, the more powerful cross-encoder can distil its knowledge back to the bi-encoder. We can repeat this cycle to iteratively bootstrap from both the bi-and cross-encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TRANS-ENCODER</head><p>The general idea of TRANS-ENCODER is simple yet extremely effective. In ?2.1, we first transform an off-the-shelf PLM to a strong bi-encoder, serving as an initialisation point. Then, the bi-encoder produces pseudo-labels and the PLM subsequently learns from these pseudo-labels in a cross-encoder manner ( ?2.2). Consecutively, the cross-encoder further produces more accurate pseudo-labels for bi-encoder learning ( ?2.3). This self-distillation process is visualised in <ref type="figure">Fig. 1</ref>. Then in, ?2.4, we propose a further extension called mutual-distillation that stabilises the training process and boosts the encoder performance even more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TRANSFORM PLMS INTO EFFECTIVE BI-ENCODERS</head><p>Off-the-shelf PLMs are unsatisfactory bi-encoders. <ref type="bibr">1</ref> To have a reasonably good starting point, we leverage a simple contrastive tuning procedure to transform existing PLMs to bi-encoders. This approach is concurrently proposed in both  and <ref type="bibr" target="#b12">(Gao et al., 2021)</ref>.</p><p>Let f (?) denote the encoder model (a Transformer-based model); X be a batch of randomly sampled raw sentences. For any sentence x i ? X , we send it to the encoder twice to create two views of the same data point: f (x i ) and f (x i ). Note that the two encodings slightly differ due to the dropout layers in the encoder. <ref type="bibr">2</ref> We then use the standard InfoNCE loss <ref type="bibr" target="#b26">(Oord et al., 2018)</ref> to cluster the positive pairs together and push away the negative pairs in the mini-batch:</p><formula xml:id="formula_0">L infoNCE = ? |X | i=1 log exp(cos(f (x i ), f (x i ))/? ) xj ?Ni exp(cos(f (x i ), f (x j ))/? ) .<label>(1)</label></formula><p>? denotes a temperature parameter; N i denotes all positive &amp; negatives of x i within the current data batch. Intuitively, the numerator is the similarity of the self-duplicated pair (the positive example) and the denominator is the sum of the similarities between x i and all other negative strings besides x i . The loss encourages the positive pairs to be relatively close compared to the negative ones. In experiments, we use checkpoints released by SimCSE and Mirror-BERT. But in principle, any techniques could be used here as long as the method serves the purpose of transforming BERT to an effective bi-encoder and does not require additional labelled data (see ?6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SELF-DISTILLATION: BI-TO CROSS-ENCODER</head><p>After obtaining a sufficiently good bi-encoder, we leverage it to label sentence pairs sampled from the task of interest. Specifically, for a given sentence-pair (sent1, sent2), we input them to the bi-encoder separately and get two embeddings (we use the embedding of [CLS] from the last layer). The cosine similarity between them is regarded as their relevance score. In this way we have constructed a self-labelled sentence-pair scoring dataset in the format of (sent1, sent2, score).</p><p>We then employ the same model architecture to learn from these score, but with a cross-encoder formulation. The cross-encoder weights are initialised from the original PLM. 3 For the sentence-pair (sent1, sent2), we concatenate them to produce "[CLS] sent1 [SEP] sent2 [SEP]" and input it to the cross-encoder. A linear layer (newly initialised) then map the sequence's encoding (embedding of the [CLS] token) to a scalar. The learning objective of the cross-encoder is minimising the KL divergence between its predictions and the self-labelled scores from the bi-encoder. This is equivalent to optimising the (soft) binary cross-entropy (BCE) loss:</p><formula xml:id="formula_1">L BCE = ? 1 N N n=1 y n ? log(?(x n )) + (1 ? y n ) ? log(1 ? ?(x n ))<label>(2)</label></formula><p>where N is the data-batch size; ?(?) is the sigmoid activation; x n is the prediction of the cross-encoder; y n is the self-labelled ground-truth score from the bi-encoder.</p><p>Note that while the cross-encoder is essentially learning from the data produced by itself (in a bi-encoder form), usually, it outperforms the original bi-encoder on held-out data. The cross-encoder directly discovers the similarity between two sentences through its attention heads, finding more accurate cues to justify the relevance score. The ability of discovering such cues could then generalise to unseen data, resulting in stronger sentence-pair scoring capability than the original bi-encoder. From a knowledge distillation perspective, we can view the bi-and cross-encoder as the teacher and student respectively. In this case the student outperforms the teacher, not because of stronger model capacity, but smarter task formulation. By leveraging this simple yet powerful observation, we are able to design a learning scheme that iteratively boosts the performance of both bi-and cross-encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SELF-DISTILLATION: CROSS-TO BI-ENCODER</head><p>With the more powerful cross-encoder at hand, a natural next step is distilling the extra gained knowledge back to a bi-encoder form, which is more useful for downstream tasks. Besides, and more importantly, a better bi-encoder could produce even more self-labelled data for cross-encoder learning.</p><p>In this way we could repeat ?2.2 and ?2.3, continually bootstrapping the encoder performance.</p><p>We create the self-labelled sentence-scoring dataset in the same way as ?2.2 except that the crossencoder is used for producing the relevance score. The bi-encoder is initialised with the weights after SimCSE training from ?2.1. 4 For every sentence pair, two sentence embeddings are produced separately by the bi-encoder. The cosine similarity between the two embeddings are regarded as their predictions of the relevance score. The aim is to regress the predictions to the self-labelled scores by the cross-encoder. We use a mean square error (MSE) loss:</p><formula xml:id="formula_2">L MSE = ? 1 N N n=1 x n ? y n 2<label>(3)</label></formula><p>PLM-1 bi-encoder-1 crossencoder-1 PLM-2 bi-encoder-2 crossencoder-2 Average predictions of both models <ref type="figure">Figure 2</ref>: A graphical illustration of the mutual-distillation learning scheme in TRANS-ENCODER. Note that, for simplicity, only the bi-to cross-encoder mutual-distillation is shown. We also conduct cross-to bi-encoder mutual-distillation in the same manner.</p><p>where N is the batch size; x n is the cosine similarity between a sentence pair; y n is the self-labelled ground-truth. In experiments, we will show that this resulting bi-encoder is more powerful than the initial bi-encoder. Sometimes, the bi-encoder will even outperform its teacher (i.e., the cross-encoder).</p><p>Choice of loss functions: maintaining student-teacher discrepancy is the key. Intuitively, MSE allows for more accurate distillations since it punishes any numerical discrepancies between predicted scores and labels on the instance level. However, in practice, we found that using MSE for bi-tocross distillation aggravates the overfitting issue for cross-encoder: the cross-encoder, with its strong sentence-pair modelling capability, completely overfits to the mapping between concatenated sentence pairs and the pseudo scores. This elimination of discrepancy between model predictions and pseudo labels harms generalisation and the iterative learning cycle cannot continue (as the predicted scores by the student model will be the same as the teacher model). We thus use BCE loss for bi-to-cross since BCE is a more forgiving loss function compared with MSE. According to our gradient derivation (App. ?A.2), BCE essentially is a temperature-sharpened version of MSE, which is more tolerant towards numerical discrepancies. This prevents cross-encoders from overfitting to the pseudo labels completely. Similar issue does not exist for the cross-to-bi distillation as for bi-encoders, two input sequences are separately encoded and the model does not easily overfit to the labels. We have a more thorough discussion in App. ?A.2 and Tab. 8 to highlight the rationales of the current configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">MUTUAL-DISTILLATION</head><p>The aforementioned self-learning approach has a drawback: since the model regresses to its previous predictions, it tends to reinforce its errors. To mitigate this problem, we design a simple mutualdistillation approach to smooth out the errors/biases originated from PLMs. Specifically, we conduct self-distillation on multiple PLMs in parallel (for brevity, we use two in this paper: BERT and RoBERTa, however the framework itself is compatible with any number of PLMs). Each PLM does not communicate/synchronise with each other except when producing the self-labelled scores. In mutual-distillation, we use the average predictions of all models as the ground-truth for the next round of learning. A graphical illustration is shown in <ref type="figure">Fig. 2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. 5</head><p>In the following, we call all self-distillation models TRANS-ENCODER (or TENC for short); all mutual-distillation models TRANS-ENCODER-mutual (or TENC-mutual for short). Note that one TENC (-mutual) training produces one best bi-encoder, called TENC (-mutual) (bi), and one best cross-encoder, called TENC (-mutual) (cross), based on dev set. We report numbers of both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EVALUATION TASKS</head><p>Evaluation task: semantic textual similarity (STS). Following prior works <ref type="bibr" target="#b29">(Reimers &amp; Gurevych, 2019;</ref><ref type="bibr" target="#b12">Gao et al., 2021)</ref>, we consider seven STS datasets: SemEval STS 2012-2016 (STS12-16, <ref type="bibr" target="#b0">Agirre et al. 2012;</ref><ref type="bibr" target="#b1">2013;</ref>, STS Benchmark (STS-B, <ref type="bibr" target="#b7">Cer et al. 2017)</ref> and SICK-Relatedness (SICK-R, <ref type="bibr" target="#b24">Marelli et al. 2014)</ref>. In all these datasets, sentence pairs are given a human-judged relevance score from 1 to 5. We normalise them to 0 and 1 and models are asked to predict the scores. We report Spearman's ? rank correlation between the two.</p><p>Evaluation task: binary classification. We also test TRANS-ENCODER on sentence-pair binary classification tasks where the model has to decide if a sentence-pair has certain relations. We choose (1) the Quora Question Pair (QQP) dataset, requiring a model to judge whether two questions are duplicated;</p><p>(2) a question-answering entailment dataset QNLI <ref type="bibr" target="#b28">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b31">Wang et al., 2019)</ref> in which given a question and a sentence the model needs to judge if the sentence answers the question; (3) the Microsoft Research Paraphrase Corpus (MRPC) which asks a model to decide if two sentences are paraphrases of each other. The ground truth labels of all datasets are either 0 or 1. Following <ref type="bibr" target="#b19">(Li et al., 2020;</ref>, we compute Area Under Curve (AUC) scores using the binary labels and the relevance scores predicted by models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP</head><p>Training and evaluation details. For each task, we use all available sentence pairs (from train, development and test sets of all datasets combined) without their labels as training data. The original QQP and QNLI datasets are extremely large. We thus downsample QQP to have 10k, 1k and 10k pairs for train, dev and test; QNLI to have 10k train set. QNLI does not have public ground truth labels for testing. So, we use its first 1k examples in the official dev set as our dev data and the rest in the official dev set as test data. The dev set for MRPC is its official dev sets. The dev set for STS12-16, STS-B and SICK-R is the dev set of STS-B. We save one checkpoint for every 200 training steps and at the end of each epoch. We use the dev sets to select the best model for testing. <ref type="bibr">6</ref> Dev sets are also used to tune the hyperprameters in each task.</p><p>For clear comparison with SimCSE and Mirror-BERT, we use their released checkpoints as initialisation points (i.e., we do not train models in ?2.1 ourselves). We consider four SimCSE variants. Two base variants: SimCSE-BERT-base, SimCSE-RoBERTa-base; and two large variants: SimCSE-BERTlarge, SimCSE-RoBERTa-large. We consider two Mirror-BERT variants: Mirror-RoBERTa-base and Mirror-RoBERTa-base-drophead. 7 For brevity, our analysis in the main text focuses on SimCSE models. We list Mirror-BERT results in Appendix. We train TRANS-ENCODER models for 3 cycles on the STS task and 5 cycles on the binary classification tasks. Within each cycle, all bi-and cross-encoders are trained for 10 and 1 epochs respectively for the STS task; 15 and 3 epochs for binary classification. 8 All models use AdamW <ref type="bibr" target="#b23">(Loshchilov &amp; Hutter, 2019)</ref> as the optimiser. In all tasks, unless noted otherwise, we create final representations using <ref type="bibr">[CLS]</ref>. We train our base models on a server with 4 * V100 (16GB) GPUs and large models on a server with 8 * A100 (40GB) GPUs. All main experiments have the same fixed random seed. All other hparams are listed in Appendix.</p><p>Mutual-distillation setup. For the two models used for mutual-distillation, they are either (1) the base variant of SimCSE-BERT and SimCSE-RoBERTa or (2) the large variant of the two. Theoretically, we could mutually distil even more models but we keep the setup simple for fast and clear comparison. Also, since mutual-distillation models use information from both PLMs, we also list (in tables) ensemble results that use the average of the predictions from two TRANS-ENCODERs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MAIN RESULTS</head><p>STS results (Tab. 1). The main results for STS are listed in Tab. 1. Compared with the baseline SimCSE, TRANS-ENCODER has brought significant improvements across the board. With various variants of the SimCSE as the base model, TRANS-ENCODER consistently enhances the average score 6 Note that checkpoints of bi-and cross-encoders are independently saved and evaluated on dev set. In the end, one best bi-encoder and one best cross-encoder are obtained. <ref type="bibr">7</ref> We do not consider BERT-based checkpoints by  since they adopt mean pooling over all tokens. For brevity, we only experiment with encoders trained with <ref type="bibr">[CLS]</ref>. <ref type="bibr">8</ref> We use fewer epochs for cross-encoders since they usually converge much faster than bi-encoders.  <ref type="table" target="#tab_1">Table 1</ref>: English STS. Spearman's ? rank correlations are reported. TENC models use only selfdistillation while TENC-mutual models use mutual-distillation as well. Blue and red denotes mutual-distillation models that are trained in the base and large group respectively. Models without colour are not co-trained with any other models. * Note that for base encoders, our results can slightly differ from numbers reported in <ref type="bibr" target="#b12">(Gao et al., 2021)</ref> since different evaluation packages are used.</p><p>by approximately 4-5%. 9 Self-distillation usually brings an improvement of 2-3% (e.g., compare line 1.1, 1.2 with 1 in Tab. 1). Further, mutual-distillation brings another boost of 1-3% (e.g., compare line 3.3, 3.4 with 3.1, 3.2 in Tab. 1). Besides single-model results, the ensemble of mutual-distilled models are clearly better than the ensemble models of either (1) naively averaging the two initial SimCSE models' scores or (2) averaging predictions of two self-distilled models in a post hoc manner. This demonstrates the benefit of allowing models to communicate/synchronise with each other in all stages of the self-distillation training.</p><p>Binary classification results (Tab. 2). On QQP, QNLI, and MRPC, we observe similar trends as the STS tasks, and the improvement is sometimes even more significant (see full table in Appendix Tab. 11). We conjecture it is due to the fact that TRANS-ENCODER training also adapts models to the task domain (since tasks like QQP and QNLI are very different from Wikipedia data, which is where SimCSE models are trained on). We will discuss the domain adaptation effect in more details in analysis. Another point worth discussing is the benefit of mutual-distillation is more inconsistent on binary classification tasks, compared with STS tasks (again, see full   suspect it is due to the performance imbalance between the BERT and RoBERTa variants on these tasks (a significant performance gap exists in the first place).</p><p>Domain transfer setup (Tab. 3). One of the key questions we are keen to find out is how much of TRANS-ENCODER's success on in-domain tasks generalises/transfers to other tasks/domains. Specifically, does TRANS-ENCODER create universally better bi-and cross-encoders, or is it only task/domain-specific? To answer the question, we directly test models trained with the STS task data on binary classification tasks. For bi-encoders, i.e., TENC (bi), the results are inconsistent across setups. These results hint that training on in-domain data is important for optimal performance gains for unsupervised bi-encoders. This is in line with the finding of . However, for cross-encoders, surprisingly, we see extremely good transfer performance. W/ or w/o mutualdistillation, TENC (cross) models outperform the SimCSE baselines by large margins, despite the fact that tasks like QQP and QNLI are of a very different domain compared with STS. In fact, they are sometimes even better than models tuned on the in-domain task data (c.f. Tab. 11). This finding has deep implications. It hints that the cross-encoder architecture (cross-attention) is by design very suitable for sentence-pair modelling (i.e., strong inductive bias is already in the architecture design), and once its sentence-pair modelling capability is 'activated', it is an extremely powerful universal representation that can be transferred to other tasks. The reason that STS models outperform in-domain models in some tasks hint that more sentence-pairs as raw training data is welcome (since the STS train set is larger than the binary classification ones). As future work, we plan to explore mining sentence pairs for TRANS-ENCODER learning from a general corpus (such as Wikipedia).</p><p>Both lexical-(such as BM25) and semantic-level (neural models such as bi-encoders) IR systems could be leveraged here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DISCUSSIONS AND ANALYSIS</head><p>In this section we discuss some interesting phenomena we observed, justify design choices we made in an empirical way, and also show a more fine-grained understanding of what exactly happens when using TRANS-ENCODER.</p><p>Initialise with the original weights or train sequentially? <ref type="figure">(Fig. 3a)</ref> As mentioned in ?2.2 and ?2.3, we initialise bi-encoders with SimCSE/Mirror-BERT weights and cross-encoders with PLMs' weights (we call this strategy refreshing). We have also tried maintaining the weights of bi-and cross-encoders sequentially. I.e., we initialise the cross-encoder's weights with the bi-encoder which just created the pseudo labels and vice versa (we call this strategy sequential). The benefit of doing so is keeping all the legacy knowledge learned in the process of self-distillation in the weights. As  suggested in <ref type="figure">Fig. 3a</ref> (also in Appendix Tab. 13), refreshing is slightly better than sequential. We suspect it is because initialising with original weights alleviate catastrophic forgetting problem in self-supervised learning (SSL), similar to the moving average and stop-gradient strategy used in contrastive SSL methods such as MoCo .</p><p>Self-distillation without alternating between different task formulations? <ref type="figure">(Fig. 3b</ref>) If we disregard the bi-and cross-encoder alternating training paradigm but using the same bi-encoder architecture all along, the model is then similar to the standard self-distillation model (c.f. <ref type="figure">Figure 1</ref> by <ref type="bibr" target="#b25">Mobahi et al. (2020)</ref>). Theoretically, standard self-distillation could also have helped, especially considering that it adapts the model to in-domain data. However, as seen in <ref type="figure">Fig. 3b</ref> (and also Appendix Tab. 14), standard self-distillation lags behind TRANS-ENCODER by a significant amount, and sometimes underperforms the base model. Standard self-distillation essentially ensembles different checkpoints (i.e., the same model at different training phases provides different 'views' of the data). TRANS-ENCODER can be seen as a type of self-distillation model, but instead of using previous models to provide views of data, we use different task formulations (i.e., bi-and cross-encoder) to provide different views of the data. The fact that standard self-distillation underperforms TRANS-ENCODER by large margins suggests the effectiveness of our proposed bi-and cross-encoder training scheme.</p><p>How many cycles is optimal? <ref type="figure">(Fig. 4)</ref> Our approach iteratively bootstraps the performance of both bi-and cross-encoders. But how many iterations (cycles) are enough? We find that it heavily depends on the model and dataset, and could be unpredictable (since the 'optimality' depends on a relatively small dev set). In <ref type="figure">Fig. 4</ref>, we plot the TENC models' (self-distillation only) performances on dev sets in three tasks: STS, QQP, and QNLI. In general, the patterns are different across datasets and models.</p><p>Does more training data help? <ref type="figure" target="#fig_2">(Fig. 5)</ref> We control the number of training samples drawn and test the TRANS-ENCODER model (using SimCSE-BERT-base) on STS test sets. The average performance on all STS tasks are reported in <ref type="figure" target="#fig_2">Fig. 5</ref>, with three runs of different random seeds. There are in total 37,081 data points from STS. We only draw from these in-domain data until there is none left. It can be seen that for in-domain training, more data points are usually always beneficial. Next, we test with more samples drawn from another task, <ref type="bibr">SNLI (Bowman et al., 2015)</ref>, containing abundant sentence-pairs. The performance of the model further increases till 30k extra data points and starts to gradually decrease after that point. However, it is worth mentioning that there will be discrepancy of such trends across tasks and models. And when needed, one can mine as many sentence-pairs as desired from a general corpus (such as Wikipedia) for TRANS-ENCODER learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Our work is closely related to (1) unsupervised sentence representation learning;</p><p>(2) sentence-pair tasks using bi-and cross-encoders; and (3) self-distillation. Each has a large body of work which we can only provide a highly selected summary below.</p><p>Unsupervised sentence representations. Prior to the emergence of large PLMs such as BERT, popular unsupervised sentence embedding methods included Skip-Thoughts <ref type="bibr" target="#b18">(Kiros et al., 2015)</ref>, FastSent <ref type="bibr" target="#b15">(Hill et al., 2016)</ref>, and Quick-Thoughts <ref type="bibr" target="#b22">(Logeswaran &amp; Lee, 2018)</ref>. They all exploit the co-occurrence statistics of sentences in large corpora (i.e., sentences under similar contexts have similar meanings). A recent paper DeCLUTR <ref type="bibr" target="#b13">(Giorgi et al., 2021)</ref> follows such idea and formulate the training as a contrastive learning task. Very recently, there has been a growing interest in using individual raw sentences for self-supervised contrastive learning on top of PLMs. Contrastive Tension <ref type="bibr" target="#b6">(Carlsson et al., 2021)</ref>, Mirror-BERT , SimCSE <ref type="bibr" target="#b12">(Gao et al., 2021)</ref>, Self-Guided Contrastive Learning <ref type="bibr" target="#b17">(Kim et al., 2021)</ref>, ConSERT , BSL , inter alia, all follow such an idea. Specifically, data augmentations on either input or feature space are used to create two views of the same sentence for contrastive learning. In our experiments, we use such methods as an initial step for creating a reasonably strong bi-encoder.</p><p>Cross-and bi-encoder for pairwise sentence tasks. Sentence-BERT <ref type="bibr" target="#b29">(Reimers &amp; Gurevych, 2019)</ref> points out the speed advantage of bi-encoders and proposes to train BERT with a siamese architecture for sentence-pair tasks. Poly-encoder <ref type="bibr" target="#b16">(Humeau et al., 2020)</ref> proposes a hybrid architecture in which intermediate encodings are cached and a final layer of cross-attention is used to exploit intersentence interactions. Conceptually similar to our work, Di-pair <ref type="bibr" target="#b8">(Chen et al., 2020)</ref>, augmented Sentence-BERT <ref type="bibr" target="#b30">(Thakur et al., 2021)</ref>, RocketQA <ref type="bibr" target="#b27">(Qu et al., 2021)</ref>, and DvBERT <ref type="bibr" target="#b9">(Cheng, 2021)</ref> distil knowledge in supervised cross-encoders to bi-encoders. We show that it is also possible to distil knowledge from bi-to cross-encoders.</p><p>Self-distillation. In standard self-distillation, models of the same architecture train on predictions by previous checkpoint(s) with the same task formulation <ref type="bibr" target="#b11">(Furlanello et al., 2018;</ref><ref type="bibr" target="#b25">Mobahi et al., 2020)</ref>. However, the standard approach does not alter the training paradigm (task formulation) as we do (biand cross-encoder training). In experiments, we have shown that alternating task formulation is the key to the success of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We propose TRANS-ENCODER, an unsupervised approach of training bi-and cross-encoders for sentence-pair tasks. The core idea of TRANS-ENCODER is self-distillation in a smart way: alternatively training a bi-encoder and a cross-encoder (of the same architecture) with pseudo-labels created from the other. We also propose a mutual-distillation extension to mutually bootstrap two self-distillation models trained in parallel. On sentence-pair tasks including sentence similarity, question de-duplication, question-answering entailment, and paraphrase identification, we show strong empirical evidence verifying the effectiveness of TRANS-ENCODER. We also found the surprisingly strong generalisation capability of our trained cross-encoders across domains and tasks. Finally, we conduct thorough ablation studies and analysis to verify our design choices and shed insight on the model mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGEMENTS</head><p>We thank Hanchen Wang, Giorgio Giannone, Arushi Goel for providing useful feedback during internal discussions.   In the main text, we experimented with SimCSE, but in theory TRANS-ENCODER is compatible with any unsupervised contrastive bi-encoder learning models. In this section, we experiment with Mirror-BERT (which is similar to SimCSE) and also two additional unsupervised contrastive models Contrastive-Tension <ref type="bibr" target="#b6">(Carlsson et al., 2021)</ref> and DeCLUTR <ref type="bibr" target="#b13">(Giorgi et al., 2021)</ref> to verify the robustness of TRANS-ENCODER.</p><p>As seen in Tab. 4 and Tab. 5, TRANS-ENCODER brings consistent and significant improvements to all base models, similar to what we have observed on SimCSE models. After TRANS-ENCODER training, the Mirror-BERT-based models perform even slightly better than SimCSE-based models, on both the STS and binary classification tasks. We suspect it is due to that SimCSE checkpoints are trained for more iterations (with the contrastive learning objective) and thus are more prone to overfit the training corpus.   as two examples for showing results of five runs (shown in Tab. 6). In general, the TRANS-ENCODER approach is quite robust and stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MORE DISCUSSIONS AND ANALYSIS</head><p>STS and SICK-R as different tasks (Tab. 7, <ref type="figure">Fig. 6</ref>). It is worth noting that in the STS main results table (Tab. 1), SICK-R is the only dataset where TRANS-ENCODER models lead to performance worse than the baselines in some settings (e.g., on SimCSE-BERT-large). We believe it is due to that SICK-R is essentially a different task from STS2012-2016 and STS-B since SICK-R is labelled with a different aim in mind. It focuses specifically on compositional distributional semantics. 10 To verify our claim, we train SICK-R as a standalone task using its official dev set instead of STS-B. All sentence pairs from SICK-R are used as raw training data. The results are shown in Tab. 7 and <ref type="figure">Fig. 6</ref>. As shown, around 3-4% gain can be obtained by switching to training on SICK-R only, confirming the different nature of the two tasks. This suggests that generalisation is also dependent on the standard of how relevance is defined between sequences in the target dataset, and training with an in-domain dev set maybe crucial when the domain shift between training and testing data is significant.</p><p>Gradient derivation of BCE and MSE loss. We show that BCE is essentially a normalised version of MSE where the gradient of x i is scaled by the sigmoid function (?(?)). Here are the gradients of the two loss functions:</p><formula xml:id="formula_3">?L BCE ?x i = ? 1 N y i (?(x i ) ? 1) + (1 ? y i )?(x i ) = ? 1 N ?(x i ) ? y i ?L MSE ?x i = ? 2 N x i ? y i .<label>(4)</label></formula><p>As can be seen, the only difference lies in whether x i is scaled by ?(?). This gives a greater degree of freedom to L BCE especially at the areas in the two ends (close to 0 or 1): a large change of  in a small change of ?(x i ), making the approximation of y i an easier task. In other words, in L BCE , it is in general easier for the model to push the gradient to 0, while in L MSE , only a strict numerical match leads to 0 gradient.</p><p>Loss function configurations (Tab. 8). We experimented with loss function configurations comprehensively and found that it requires caution when choosing learning objectives for cross-to bi-encoder and bi-to cross-encoder distillation. As mentioned in the main text and the paragraph above, choosing MSE for bi-to-cross distillation causes severe overfitting to the pseudo scores and choosing BCE for cross-to-bi distillation fail to make the model converge. As a result, our configuration of using BCE and MSE for bi-to-cross and cross-to-bi distillations respectively becomes the only plausible solution (see Tab. 8).</p><p>For the regression loss used for cross-to-bi distillation, besides MSE, we also experimented with several other regression loss functions such as Log-Cosh but found no improvement.</p><p>Do sentence pairs leak label information? (Tab. 9) TRANS-ENCODER has leveraged the sentence pairs given by the task. One concern is that the sentence pairs may be positively correlated and the model may have exploited this na?ve bias. We highlight that this work does not have this concern as the tasks in our study test the relative similarity ranking of sentence pairs under the evaluation metrics (both spearman's rank used in STS and AUC used in binary classification are ranking-based). Therefore, the overall correlation distribution of sentence pairs does not reveal what is tested by the tasks in this study. To further verify the model has not learned label distribution from the task, we run a controlled setting where we treat all sentence pairs from the tasks as (pseudo-)positive pairs and apply a standard contrastive finetuning framework (i.e. Mirror-BERT and SimCSE finetuning) with Eq.</p><p>(1) as the loss on these pairs. Performance improvement from this finetuning setup will indicate that the overall positive correlation from the sentence pairs can be exploited by the model.   <ref type="table" target="#tab_1">Table 10</ref>: Running time for all models across different datasets/tasks. We experimented with NVIDIA V100, A100 and 3090 and found the estimated time to be similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 TRAINING TIME</head><p>Overall runtime. We list the runtime of TRANS-ENCODER models on different tasks and configurations in Tab. 10. All base models can be trained under 2.5 hours and large models can be trained within 5 hours. TENC-mutual models usually take twice the amount of time training a self-distillation model. Also, TENC-mutual models require two GPUs while self-distillation models need only one. We have tested running TRANS-ENCODER on three types of Nvidia GPUs: V100 (16 GB), A100 (40 GB) and 3090 (24 GB). The runtimes are similar.</p><p>Cross-encoder vs. bi-encoder. For training one epoch, bi-encoder takes significantly less time than cross-encoders. However, bi-encoders take much longer time to converge (15 epochs vs. 3 epochs on binary classification tasks and 10 epochs vs. 1 epoch on STS; see Tab. 16). As a result, in practice, bi-to-cross and cross-to-bi distillations take similar amount of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 FULL TABLES</head><p>Here, we list the full tables of SimCSE binary classification results (Tab. 11); SimCSE domain transfer results (Tab. 12); and full tables for ablation studies <ref type="bibr">(Tab. 13,</ref><ref type="bibr">Tab. 14)</ref>, which are presented as figures in the main texts in <ref type="figure">Fig. 3</ref>.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>TENC vs.standard self-distillation (on QNLI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Ablation studies of TRANS-ENCODER regarding two design choices. TENC's performance against distillation cycles under different base models and tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>STS performance against number of training samples. 'full' means all STS in-domain data are used. After 'full', the samples are drawn from the SNLI dataset. Note that each point in the graph is an individual run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-bi distillation won't converge () MSE MSE bi-to-cross distillation overfits completely to pseudo labels () MSE BCE bi-to-cross distillation overfits completely to pseudo labels () &amp; cross-to-bi distillation won't converge ()</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>68.64 82.39 74.30 80.69 78.71 76.54 72.22 76.21 1.1 + TENC (bi) 72.17 84.40 76.69 83.28 80.91 81.26 71.84 78.65 83.49 76.45 83.13 81.79 81.51 71.94 78.43 1.1+2.1 TENC-base (bi) ensemble 73.58 85.01 78.35 85.02 83.21 84.07 70.93 80.03 1.2+2.2 TENC-base (cross) ensemble 74.25 84.92 78.57 85.16 83.25 83.52 70.73 80.06 1.3+2.3 TENC-mutual-base (bi) ensemble 75.29 85.34 78.49 85.28 83.43 84.09 72.75 80.67 1.4+2.4 TENC-mutual-base (cross) ensemble 76.60 86.18 79.09 85.49 83.64 84.67 71.96 81.09 3+4 SimCSE-large ensemble 73.22 86.03 78.15 85.95 82.83 83.05 73.86 80.66 3.1+4.1 TENC-large (bi) ensemble 78.49 87.02 80.31 87.85 84.31 86.54 72.39 82.41 3.2+4.2 TENC-large (cross) ensemble 77.93 86.91 79.83 87.82 83.74 85.58 72.02 81.98 3.3+4.3 TENC-mutual-large (bi) ensemble 78.42 88.60 81.91 88.38 85.01 86.52 72.23 83.01 3.4+4.4 TENC-mutual-large (cross) ensemble 78.52 88.70 81.90 88.67 84.96 86.70 72.03 83.07</figDesc><table><row><cell>#</cell><cell>dataset?</cell><cell>STS12 STS13 STS14 STS15 STS16 STS-B SICK-R avg.</cell></row><row><cell></cell><cell></cell><cell>single-model results</cell></row><row><cell cols="2">1 SimCSE-BERT-base  1.2 + TENC (cross)</cell><cell>71.94 84.14 76.39 82.87 80.65 81.06 71.16 78.32</cell></row><row><cell>1.3</cell><cell>+ TENC-mutual (bi)</cell><cell>75.09 85.10 77.90 85.08 83.05 83.90 72.76 80.41</cell></row><row><cell>1.4</cell><cell>+ TENC-mutual (cross)</cell><cell>75.44 85.59 78.03 84.44 82.65 83.61 69.52 79.90</cell></row><row><cell>2</cell><cell>SimCSE-RoBERTa-base</cell><cell>68.34 80.96 73.13 80.86 80.61 80.20 68.62 76.10</cell></row><row><cell>2.1</cell><cell>+ TENC (bi)</cell><cell>73.36 82.47 76.39 83.96 82.67 82.05 67.63 78.36</cell></row><row><cell>2.2</cell><cell>+ TENC (cross)</cell><cell>72.59 83.24 76.83 84.20 82.82 82.85 69.51 78.86</cell></row><row><cell>2.3</cell><cell>+ TENC-mutual (bi)</cell><cell>75.01 85.22 78.26 85.16 83.22 83.88 72.56 80.47</cell></row><row><cell>2.4</cell><cell>+ TENC-mutual (cross)</cell><cell>76.37 85.87 79.03 85.77 83.77 84.65 72.62 81.15</cell></row><row><cell>3</cell><cell>SimCSE-BERT-large</cell><cell>71.30 84.32 76.32 84.28 79.78 79.04 73.88 78.42</cell></row><row><cell>3.1</cell><cell>+ TENC (bi)</cell><cell>75.55 84.08 77.01 85.43 81.37 82.88 71.46 79.68</cell></row><row><cell>3.2</cell><cell>+ TENC (cross)</cell><cell>75.81 84.51 76.50 85.65 82.14 83.47 70.90 79.85</cell></row><row><cell>3.3</cell><cell>+ TENC-mutual (bi)</cell><cell>78.19 88.51 81.37 88.16 84.81 86.16 71.33 82.65</cell></row><row><cell>3.4</cell><cell>+ TENC-mutual (cross)</cell><cell>77.97 88.31 81.02 88.11 84.40 85.95 71.92 82.52</cell></row><row><cell>4</cell><cell>SimCSE-RoBERTa-large</cell><cell>71.40 84.60 75.94 84.36 82.22 82.67 71.23 78.92</cell></row><row><cell>4.1</cell><cell>+ TENC (bi)</cell><cell>77.92 86.69 79.29 87.23 84.22 86.10 68.36 81.40</cell></row><row><cell>4.2</cell><cell>+ TENC (cross)</cell><cell>78.32 86.20 79.61 86.88 82.93 84.48 67.90 80.90</cell></row><row><cell>4.3</cell><cell>+ TENC-mutual (bi)</cell><cell>78.15 88.39 81.76 88.38 84.95 86.55 72.31 82.93</cell></row><row><cell>4.4</cell><cell>+ TENC-mutual (cross)</cell><cell>78.28 88.31 81.94 88.63 85.03 86.70 71.63 82.93</cell></row><row><cell></cell><cell cols="2">ensemble results (average predictions of two models)</cell></row><row><cell>1+2</cell><cell>SimCSE-base ensemble</cell><cell>70.71</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>table Tab</head><label>Tab</label><figDesc></figDesc><table><row><cell>dataset?</cell><cell cols="2">QQP QNLI MRPC avg.</cell></row><row><cell>SimCSE-BERT-base</cell><cell>80.38 71.38</cell><cell>75.02 75.59</cell></row><row><cell>+ TENC (bi)</cell><cell>82.10 75.30</cell><cell>75.71 77.70</cell></row><row><cell>+ TENC (cross)</cell><cell>82.10 75.61</cell><cell>76.21 77.97</cell></row><row><cell>+ TENC-mutual (bi)</cell><cell>84.00 76.93</cell><cell>76.62 79.18</cell></row><row><cell cols="2">+ TENC-mutual (cross) 84.29 77.11</cell><cell>77.77 79.72</cell></row><row><cell></cell><cell></cell><cell>. 11 in Appendix). We</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Binary classification task results. AUC scores are reported. We only demonstrate results for one model for brevity. Full table can be found in Appendix (Tab. 11).</figDesc><table><row><cell>dataset?</cell><cell cols="2">QQP QNLI MRPC avg.</cell></row><row><cell cols="2">SimCSE-RoBERTa-base 81.82 73.54</cell><cell>75.06 76.81</cell></row><row><cell>+ TENC (bi)</cell><cell>82.56 71.67</cell><cell>74.24 76.16</cell></row><row><cell>+ TENC (cross)</cell><cell>83.66 79.38</cell><cell>79.53 80.86</cell></row><row><cell>+ TENC-mutual (bi)</cell><cell>81.71 72.78</cell><cell>75.51 76.67</cell></row><row><cell>+ TENC-mutual (cross)</cell><cell>83.92 79.79</cell><cell>79.96 81.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>A domain transfer setup: testing TRANS-ENCODER models trained with STS data directly on binary classification tasks. We only demonstrate results for one model for brevity. Full table can be found in Appendix (Tab. 12).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>81.53 73.05 79.78 78.16 78.36 70.03 75.08 + TENC (bi) 71.99 81.85 75.73 83.32 79.97 81.19 69.47 77.64 + TENC (cross) 73.10 82.31 76.49 83.71 79.64 81.70 69.70 78.09</figDesc><table><row><cell>dataset?</cell><cell cols="2">STS12 STS13 STS14 STS15 STS16 STS-B SICK-R avg.</cell></row><row><cell cols="2">Mirror-RoBERTa-base 64.67 Mirror-RoBERTa-base-drophead 68.15 82.55 73.47 82.26 79.44 79.63</cell><cell>71.58 76.72</cell></row><row><cell>+ TENC (bi)</cell><cell>74.95 83.86 77.50 85.80 83.22 83.94</cell><cell>72.56 80.26</cell></row><row><cell>+ TENC (cross)</cell><cell>75.70 84.58 78.35 86.49 83.96 84.26</cell><cell>72.76 80.87</cell></row><row><cell cols="2">Contrastive-Tension-BERT-base 61.98 76.82 68.35 77.38 76.56 73.99</cell><cell>69.20 72.04</cell></row><row><cell>+ TENC (bi)</cell><cell>69.58 79.38 69.32 77.59 77.96 78.23</cell><cell>70.70 74.68</cell></row><row><cell>+ TENC (cross)</cell><cell>70.67 79.27 69.26 76.77 78.78 79.08</cell><cell>70.66 74.93</cell></row><row><cell>DeCLUTR-RoBERTa-base</cell><cell>45.56 73.38 63.39 72.72 76.15 66.40</cell><cell>68.99 66.66</cell></row><row><cell>+ TENC (bi)</cell><cell>62.40 77.55 70.05 80.28 81.81 77.31</cell><cell>72.60 74.57</cell></row><row><cell>+ TENC (cross)</cell><cell>60.68 78.11 69.23 80.48 81.03 78.47</cell><cell>70.07 74.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>English STS (models beyond SimCSE). Spearman's ? scores are reported.</figDesc><table><row><cell>dataset?</cell><cell cols="2">QQP QNLI MRPC</cell><cell>avg.</cell></row><row><cell>Mirror-RoBERTa-base</cell><cell>78.89 73.73</cell><cell>75.44</cell><cell>76.02</cell></row><row><cell>+ TENC (bi)</cell><cell>82.34 79.57</cell><cell>77.08</cell><cell>79.66</cell></row><row><cell>+ TENC (cross)</cell><cell>82.00 79.87</cell><cell>78.40</cell><cell>80.09</cell></row><row><cell cols="2">Mirror-RoBERTa-base-drophead 78.36 75.56</cell><cell>77.18</cell><cell>77.03</cell></row><row><cell>+ TENC (bi)</cell><cell>82.04 82.12</cell><cell>79.63</cell><cell>81.26</cell></row><row><cell>+ TENC (cross)</cell><cell>82.90 82.52</cell><cell>81.38</cell><cell>82.27</cell></row><row><cell>Contrastive-Tension-BERT-base</cell><cell>78.95 69.73</cell><cell>72.86</cell><cell>73.85</cell></row><row><cell>+ TENC (bi)</cell><cell>80.96 68.31</cell><cell>72.54</cell><cell>73.94</cell></row><row><cell>+ TENC (cross)</cell><cell>81.22 74.17</cell><cell>71.33</cell><cell>75.57</cell></row><row><cell>DeCLUTR-RoBERTa-base</cell><cell>78.68 74.77</cell><cell>72.86</cell><cell>75.44</cell></row><row><cell>+ TENC (bi)</cell><cell>83.64 82.87</cell><cell>74.55</cell><cell>80.35</cell></row><row><cell>+ TENC (cross)</cell><cell>84.02 83.41</cell><cell>75.55</cell><cell>80.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Binary classification (models beyond SimCSE). AUC scores are reported.</figDesc><table /><note>A APPENDIXA.1 RESULTS WITH OTHER CONTRASTIVE LEARNING METHODS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Mean and standard deviation (S.D.) of five runs on STS.</figDesc><table><row><cell>Data?</cell><cell></cell><cell>STS</cell><cell></cell><cell>SICK-R</cell><cell></cell></row><row><cell>model?</cell><cell cols="3">off-the-shelf +TENC (bi) +TENC (cross)</cell><cell cols="2">+TENC (bi) +TENC (cross)</cell></row><row><cell>SimCSE-BERT-base</cell><cell>72.22</cell><cell>71.84</cell><cell>71.16</cell><cell>74.13</cell><cell>74.43</cell></row><row><cell>SimCSE-RoBERTa-base</cell><cell>68.62</cell><cell>67.63</cell><cell>69.51</cell><cell>68.39</cell><cell>70.38</cell></row><row><cell>SimCSE-BERT-large</cell><cell>73.88</cell><cell>71.46</cell><cell>70.90</cell><cell>74.92</cell><cell>74.98</cell></row><row><cell>SimCSE-RoBERTa-large</cell><cell>71.23</cell><cell>68.36</cell><cell>67.90</cell><cell>72.63</cell><cell>73.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Compare TRANS-ENCODER models trained with all STS data (using STS-B's dev set) and</cell></row><row><cell>SICK-R data only (using SICK-R's dev set). Large performance gains can be obtained when treating</cell></row><row><cell>SICK-R as a standalone task.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Compare different loss function configurations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Results are shown in Tab. 9. Contrastive methods on pseudo-positive pairs almost always 11 underperform the original SimCSE and Mirror-BERT checkpoints significantly while TRANS-ENCODER models improve the checkpoints by large margins.</figDesc><table><row><cell>dataset?</cell><cell cols="3">STS (avg.) QQP QNLI MRPC</cell><cell>avg.</cell></row><row><cell>SimCSE-RoBERTa-base</cell><cell>76.10</cell><cell>81.82 73.54</cell><cell>75.06</cell><cell>76.63</cell></row><row><cell>+ TENC (bi)</cell><cell>78.36</cell><cell>84.13 77.08</cell><cell>75.29</cell><cell>78.72</cell></row><row><cell>+ TENC (cross)</cell><cell>78.86</cell><cell>85.16 80.49</cell><cell>76.09</cell><cell>80.15</cell></row><row><cell>+ contrastive tuning</cell><cell>73.93</cell><cell>74.00 77.56</cell><cell>75.25</cell><cell>75.19</cell></row><row><cell>Mirror-RoBERTa-base-drophead</cell><cell>76.72</cell><cell>78.36 75.56</cell><cell>77.18</cell><cell>77.00</cell></row><row><cell>+ TENC (bi)</cell><cell>80.26</cell><cell>82.04 82.12</cell><cell>79.63</cell><cell>81.01</cell></row><row><cell>+ TENC (cross)</cell><cell>80.87</cell><cell>82.90 82.52</cell><cell>81.38</cell><cell>82.27</cell></row><row><cell>+ contrastive tuning</cell><cell>72.21</cell><cell>73.51 77.50</cell><cell>75.63</cell><cell>74.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Compare TRANS-ENCODER with SimCSE/Mirror-BERT-style contrastive tuning on sentence pairs given by the task.</figDesc><table><row><cell>model size</cell><cell cols="2">dataset # GPU required</cell><cell>time</cell></row><row><cell>TENC (base)</cell><cell>STS</cell><cell>1</cell><cell>?0.5 hrs</cell></row><row><cell>TENC (large)</cell><cell>STS</cell><cell>1</cell><cell>?1.5 hrs</cell></row><row><cell>TENC-mutual (base)</cell><cell>STS</cell><cell>2</cell><cell>?1.25 hrs</cell></row><row><cell>TENC-mutual (large)</cell><cell>STS</cell><cell>2</cell><cell>?3 hrs</cell></row><row><cell>TENC (base)</cell><cell>QQP</cell><cell>1</cell><cell>?1 hrs</cell></row><row><cell>TENC (large)</cell><cell>QQP</cell><cell>1</cell><cell>?2.5 hrs</cell></row><row><cell>TENC-mutual (base)</cell><cell>QQP</cell><cell>2</cell><cell>?2.25 hrs</cell></row><row><cell>TENC-mutual (large)</cell><cell>QQP</cell><cell>2</cell><cell>?5 hrs</cell></row><row><cell>TENC (base)</cell><cell>QNLI</cell><cell>1</cell><cell>?0.5 hrs</cell></row><row><cell>TENC (large)</cell><cell>QNLI</cell><cell>1</cell><cell>?1.75 hrs</cell></row><row><cell>TENC-mutual (base)</cell><cell>QNLI</cell><cell>2</cell><cell>?1.25 hrs</cell></row><row><cell>TENC-mutual (large)</cell><cell>QNLI</cell><cell>2</cell><cell>?3.5 hrs</cell></row><row><cell>TENC (base)</cell><cell>MRPC</cell><cell>1</cell><cell>?0.25 hrs</cell></row><row><cell>TENC (large)</cell><cell>MRPC</cell><cell>1</cell><cell>?0.75 hrs</cell></row><row><cell>TENC-mutual (base)</cell><cell>MRPC</cell><cell>2</cell><cell>?0.5 hrs</cell></row><row><cell cols="2">TENC-mutual (large) MRPC</cell><cell>2</cell><cell>?1.5 hrs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Binary classification task results (SimCSE models; full table). AUC scores are reported.A.5 DATA STATISTICSA complete listing of train/dev/test stats of all used datasets can be found in Tab. 15. Note that for STS 2012-2016, we dropped all sentence-pairs without a valid score. And the train sets include all sentence pairs (w/o labels) regardless of split in each task.A.6 HYPERPARAMETERSThe full listing of hyperparameters is shown in Tab. 16. Since performing a grid-search for all these hparams is ultra-expensive, we mainly used the default hparams from the sentence-BERT library.12   The hparams for TENC and TENC-mutual are the same.A.7 PRE-TRAINED ENCODERSA complete listing of URLs for all used PLMs is provided in Tab. 17. 12 www.sbert.net dataset? QQP QNLI MRPC avg. SimCSE-BERT-base 80.38 71.38 75.02 75.59 + TENC (bi) 80.80 72.35 74.53 75.89 + TENC (cross) 80.84 78.81 79.42 79.69 + TENC-mutual (bi) 83.24 72.88 75.86 77.33 + TENC-mutual (cross) 82.40 78.30 78.72 79.81 SimCSE-RoBERTa-base 81.82 73.54 75.06 76.81 + TENC (bi) 82.56 71.67 74.24 76.16 + TENC (cross) 83.66 79.38 79.53 80.86 + TENC-mutual (bi) 81.71 72.78 75.51 76.67 + TENC-mutual (cross) 83.92 79.79 79.96 81.22</figDesc><table><row><cell>SimCSE-BERT-large</cell><cell>82.42 72.46 75.93 76.94</cell></row><row><cell>+ TENC (bi)</cell><cell>81.86 71.99 74.99 76.28</cell></row><row><cell>+ TENC (cross)</cell><cell>83.31 79.62 79.93 80.95</cell></row><row><cell>+ TENC-mutual (bi)</cell><cell>82.30 72.47 76.74 77.17</cell></row><row><cell cols="2">+ TENC-mutual (cross) 83.04 79.58 81.18 81.27</cell></row><row><cell cols="2">SimCSE-RoBERTa-large 82.99 75.76 77.24 78.66</cell></row><row><cell>+ TENC (bi)</cell><cell>82.34 70.88 76.05 76.42</cell></row><row><cell>+ TENC (cross)</cell><cell>85.98 80.07 81.20 82.42</cell></row><row><cell>+ TENC-mutual (bi)</cell><cell>85.28 71.56 76.81 77.88</cell></row><row><cell cols="2">+ TENC-mutual (cross) 86.31 81.77 81.86 83.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Full table for domain transfer setup: testing TRANS-ENCODER (SimCSE models) trained with STS data directly on binary classification tasks.</figDesc><table><row><cell>dataset?</cell><cell>STS (avg.)</cell></row><row><cell>SimCSE-BERT-base</cell><cell>76.21</cell></row><row><cell>+ TENC (bi) sequential</cell><cell>78.08</cell></row><row><cell>+ TENC (cross) sequential</cell><cell>77.92</cell></row><row><cell>+ TENC (bi) refreshing</cell><cell>78.65</cell></row><row><cell>+ TENC (cross) refreshing</cell><cell>78.32</cell></row><row><cell>SimCSE-RoBERTa-base</cell><cell>76.10</cell></row><row><cell>+ TENC (bi) sequential</cell><cell>78.32</cell></row><row><cell>+ TENC (cross) sequential</cell><cell>78.72</cell></row><row><cell>+ TENC (bi) refreshing</cell><cell>78.36</cell></row><row><cell>+ TENC (cross) refreshing</cell><cell>78.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Ablation: sequential training with the same set of weights vs. refreshing weights for all models.</figDesc><table><row><cell>dataset?</cell><cell cols="2">STS (avg.) QNLI</cell></row><row><cell>SimCSE-BERT-base</cell><cell>76.21</cell><cell>71.38</cell></row><row><cell>+ standard-self-distillation</cell><cell>77.16</cell><cell>70.95</cell></row><row><cell>+ TENC (bi)</cell><cell>78.65</cell><cell>75.30</cell></row><row><cell>+ TENC (cross)</cell><cell>78.32</cell><cell>75.61</cell></row><row><cell>SimCSE-RoBERTa-base</cell><cell>76.10</cell><cell>73.54</cell></row><row><cell>+ standard-self-distillation</cell><cell>76.25</cell><cell>73.40</cell></row><row><cell>+ TENC (bi)</cell><cell>78.36</cell><cell>77.04</cell></row><row><cell>+ TENC (cross)</cell><cell>78.86</cell><cell>80.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Ablation: compare TRANS-ENCODER with standard self-distillation.</figDesc><table><row><cell>Dataset</cell><cell cols="2">|Train| |Dev|</cell><cell>|Test|</cell></row><row><cell>STS 2012</cell><cell>-</cell><cell>-</cell><cell>3,108</cell></row><row><cell>STS 2013</cell><cell>-</cell><cell>-</cell><cell>1,500</cell></row><row><cell>STS 2014</cell><cell>-</cell><cell>-</cell><cell>3,750</cell></row><row><cell>STS 2015</cell><cell>-</cell><cell>-</cell><cell>3,000</cell></row><row><cell>STS 2016</cell><cell>-</cell><cell>-</cell><cell>1,186</cell></row><row><cell>STS-B</cell><cell>-</cell><cell>1,500</cell><cell>1,379</cell></row><row><cell>SICK-R</cell><cell>-</cell><cell>495</cell><cell>4,906</cell></row><row><cell>STS train (full)</cell><cell>37,081</cell><cell>-</cell><cell>-</cell></row><row><cell>QQP</cell><cell cols="3">21,000 1,000 10,000</cell></row><row><cell>QNLI</cell><cell cols="2">15,463 1,000</cell><cell>4,463</cell></row><row><cell>MRPC</cell><cell>5,801</cell><cell>408</cell><cell>1,725</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>A listing of train/dev/test stats of all used datasets. : a collection of all individual sentence-pairs from all STS tasks.</figDesc><table><row><cell>task</cell><cell>direction</cell><cell cols="5">learning rate batch size epoch max token length cycles</cell></row><row><cell></cell><cell></cell><cell></cell><cell>base models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>STS</cell><cell>bi ? cross</cell><cell>2e-5</cell><cell>32</cell><cell>1</cell><cell>64</cell><cell>3</cell></row><row><cell>STS</cell><cell>cross ? bi</cell><cell>5e-5</cell><cell>128</cell><cell>10</cell><cell>32</cell><cell>3</cell></row><row><cell cols="2">binary bi ? cross</cell><cell>2e-5</cell><cell>32</cell><cell>3</cell><cell>64</cell><cell>5</cell></row><row><cell cols="2">binary cross ? bi</cell><cell>5e-5</cell><cell>128</cell><cell>15</cell><cell>32</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>large models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>STS</cell><cell>bi ? cross</cell><cell>2e-5</cell><cell>32</cell><cell>1</cell><cell>64</cell><cell>3</cell></row><row><cell>STS</cell><cell>cross ? bi</cell><cell>5e-5</cell><cell>64</cell><cell>10</cell><cell>32</cell><cell>3</cell></row><row><cell cols="2">binary bi ? cross</cell><cell>2e-5</cell><cell>32</cell><cell>3</cell><cell>64</cell><cell>5</cell></row><row><cell cols="2">binary cross ? bi</cell><cell>5e-5</cell><cell>64</cell><cell>15</cell><cell>32</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>A listing of hyperpamters used for all TRANS-ENCODER models.</figDesc><table /><note>model URL</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this work, a bi-encoder is the same as a universal text encoder. When embedding a pair of sentences under the bi-encoder architecture, the same encoder is used (i.e., the two-branches of bi-encoder share weights).2 Mirror-BERT has the option to also randomly mask a span in xi and using drophead to replace dropout as hidden states augmentation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We have a design choice to make here, we could either (1) directly use the weights of the bi-encoder or (2) use the weights of the original PLM. We will show in experiments ( ?5.2) that (2) is slightly better than (1).4  Again, we have the choice of using the cross-encoder weights from ?2.2 (the extra linear layer is disregarded). We will discuss more in experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We also experimented with a sequential ensemble strategy: initialising the cross-and bi-encoder with BERT/RoBERTa alternatively (1 cycle of BERT then 1 cycle of RoBERTa) but found the performance worse than mutual-distillation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Similar trends observed on Mirror-BERT, see Appendix (App. ?A.1).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">SICK-R "includes a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena that Compositional Distributional Semantic Models (CDSMs) are expected to account for (e.g., contextual synonymy and other lexical variation phenomena, active/passive and other syntactic alternations, impact of negation, determiners and other grammatical elements), but do not require dealing with other aspects of existing sentential data sets (e.g., STS, RTE) that are not within the scope of compositional distributional semantics." (see http://marcobaroni.org/composes/sick.html).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">The only exception is QNLI. We suspect it is because QNLI is from the QA domain and contrastive tuning on QNLI sentence pairs has some domain adaptation effect. According to, contrastive tuning on randomly sampled sentences from the target domain can also achieve domain adaptation, and sentence pairs from the task are thus not a must.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><forename type="middle">*</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sem</surname></persName>
		</author>
		<title level="m">Semantic textual similarity. In *SEM 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SemEval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval-2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Erik Ylip?? Hellqvist, and Magnus Sahlgren. Semantic re-tuning with contrastive tension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelia</forename><surname>Amaru Cuba Gyllensten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gogoulou</surname></persName>
		</author>
		<idno>ICLR 2021</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DiPair: Fast and accurate distillation for trillion-scale text matching and pair modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Jung</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Emadzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-findings 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2925" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dual-view distilled bert for sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2151" to="2155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6894" to="6910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeCLUTR: Deep contrastive learning for unsupervised textual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvald</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="879" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>ICLR 2020</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-guided contrastive learning for BERT sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2528" to="2540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1442" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-distillation amplifies regularization in hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05715</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5835" to="5847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
			<pubPlace>Austin, Texas</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="296" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ConSERT: A contrastive framework for self-supervised sentence representation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanmeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5065" to="5075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bootstrapped unsupervised sentence representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5168" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scheduled DropHead: A regularization method for transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m"># dataset? QQP QNLI MRPC avg</title>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
	<note>EMNLP-findings 2020</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc</surname></persName>
		</author>
		<idno>bi) 82.10 75.30 75.71 77.70</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc</surname></persName>
		</author>
		<idno>cross) 82.10 75.61 76.21 77.97</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc-Mutual</surname></persName>
		</author>
		<idno>bi) 84.00 76.93 76.62 79.18</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc-Mutual</surname></persName>
		</author>
		<idno>cross) 84.29 77.11 77.77 79.72</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc</surname></persName>
		</author>
		<idno>bi) 84.13 77.08 75.29 78.83</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc</surname></persName>
		</author>
		<idno>cross) 85.16 80.49 76.09 80.58</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc-Mutual</surname></persName>
		</author>
		<idno>bi) 84.36 77.29 76.90 79.52</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc-Mutual</surname></persName>
		</author>
		<idno>cross) 84.73 77.32 78.47 80.17</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc</surname></persName>
		</author>
		<idno>bi) 84.53 78.22 78.42 80.39</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc</surname></persName>
		</author>
		<idno>cross) 84.18 79.71 79.43 81.11</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc-Mutual</surname></persName>
		</author>
		<idno>bi) 85.72 81.61 79.59 82.31</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc-Mutual</surname></persName>
		</author>
		<idno>cross) 86.55 81.55 79.69 82.60</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simcse-Roberta</surname></persName>
		</author>
		<idno>large 82.99 75.76 77.24 78.66</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc</surname></persName>
		</author>
		<idno>bi) 85.65 84.49 79.34 83.16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc</surname></persName>
		</author>
		<idno>cross) 86.16 84.69 81.00 83.95</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc-Mutual</surname></persName>
		</author>
		<idno>bi) 85.65 81.87 79.53 82.35</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Tenc-Mutual</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>cross</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenc-Base</surname></persName>
		</author>
		<idno>cross) ensemble 84.15 78.55 78.90 80.53</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenc-Large</surname></persName>
		</author>
		<idno>bi) ensemble 85.09 81.35 78.88 81.77</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenc-Large</surname></persName>
		</author>
		<idno>cross) ensemble 86.22 83.78 81.06 83.69</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Table 17: A listing of HuggingFace URLs of all PLMs used in this work</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
