<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">P-STMO: Pre-Trained Spatial Temporal Many-to-One Model for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Visual Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Visual Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab 4 Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfeng</forename><surname>Zhang</surname></persName>
							<email>xfzhang@ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshe</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Visual Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Visual Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Visual Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">P-STMO: Pre-Trained Spatial Temporal Many-to-One Model for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D human pose estimation</term>
					<term>Transformer</term>
					<term>pre-training</term>
					<term>masked pose modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a novel Pre-trained Spatial Temporal Many-to-One (P-STMO) model for 2D-to-3D human pose estimation task. To reduce the difficulty of capturing spatial and temporal information, we divide this task into two stages: pre-training (Stage I) and fine-tuning (Stage II). In Stage I, a self-supervised pre-training sub-task, termed masked pose modeling, is proposed. The human joints in the input sequence are randomly masked in both spatial and temporal domains. A general form of denoising auto-encoder is exploited to recover the original 2D poses and the encoder is capable of capturing spatial and temporal dependencies in this way. In Stage II, the pre-trained encoder is loaded to STMO model and fine-tuned. The encoder is followed by a many-to-one frame aggregator to predict the 3D pose in the current frame. Especially, an MLP block is utilized as the spatial feature extractor in STMO, which yields better performance than other methods. In addition, a temporal downsampling strategy is proposed to diminish data redundancy. Extensive experiments on two benchmarks show that our method outperforms state-of-the-art methods with fewer parameters and less computational overhead. For example, our P-STMO model achieves 42.1mm MPJPE on Human3.6M dataset when using 2D poses from CPN as inputs. Meanwhile, it brings a 1.5-7.1? speedup to state-of-the-art methods. Code is available at https://github.com/paTRICK-swk/P-STMO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monocular 3D human pose estimation in videos is a long-standing computer vision task with extensive applications, such as virtual reality, medical assistance, and self-driving. Two-step estimation methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref> first detect 2D human keypoints, and then regress the 3D position of each joint. They use an arXiv:2203.07628v2 [cs.CV] 29 Jul 2022 off-the-shelf 2D keypoint detector (e.g., <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b7">8]</ref>) to obtain 2D poses and mainly focus on lifting 3D poses from these 2D keypoints. Despite considerable success achieved, this task remains an ill-posed problem due to depth ambiguity.</p><p>Existing works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b53">54]</ref> rely on fully-connected (fc) layers, graph convolutions, or 1D convolutions to integrate information over spatial and temporal domains. Recently, some works <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref> introduce Transformer <ref type="bibr" target="#b44">[45]</ref> to 3D human pose estimation. The self-attention mechanism of Transformer is exploited to depict spatial dependencies between joints in each frame or temporal dependencies between frames in a sequence. However, there are two drawbacks to these methods: (1) They directly learn 2D-to-3D spatial and temporal correlations, which is a challenging task. This might make it difficult to optimize the model. (2) Previous works <ref type="bibr" target="#b11">[12]</ref> show that Transformer requires more training data than convolutional neural networks.</p><p>For these two issues, self-supervised pre-training of Transformer, which has proven to be effective in natural language processing (NLP) and computer vision (CV), is a promising solution. Previous approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref> randomly mask a portion of input data and then recover the masked content. In this way, the model is enabled to represent the inherent characteristics within the data. Thus, we are motivated to exploit the self-supervised pre-training method for 3D human pose estimation.</p><p>In this paper, we propose a Pre-trained Spatial Temporal Many-to-One (P-STMO) model for 2D-to-3D human pose estimation. The whole process is split into two stages: pre-training (Stage I) and fine-tuning (Stage II). In Stage I, a self-supervised spatial temporal pre-training task, called masked pose modeling (MPM), is constructed. We randomly mask some frames 5 (temporally) as well as some 2D joints (spatially) in the remaining frames. The model in this stage, a general form of denoising auto-encoder <ref type="bibr" target="#b45">[46]</ref>, is intended to reconstruct the corrupted 2D poses. This gives the network a favorable initialization. In Stage II, the pre-trained encoder, combined with a many-to-one frame aggregator, is retrained to predict the 3D pose of the current (middle) frame by using a sequence of 2D poses as input. With this two-stage strategy, the encoder is supposed to capture 2D spatial temporal dependencies in Stage I and extract 3D spatial and temporal features in Stage II. Experimental results show that this strategy can reduce the optimization difficulty of STMO and improve the prediction performance. The proposed STMO model consists of three modules: (i) spatial encoding module (SEM): capturing spatial information within a single frame, (ii) temporal encoding module (TEM): capturing temporal dependencies between different frames, (iii) many-to-one frame aggregator (MOFA): aggregating information from multiple frames to assist in the prediction of the current pose. These three modules play different roles and are organically linked to each other, which improves the overall performance. Herein, we propose to use an MLP block as the backbone network of SEM. Compared with fc <ref type="bibr" target="#b24">[25]</ref> and Transformer <ref type="bibr" target="#b54">[55]</ref>, it achieves better performance while having moderate computational complexity.</p><p>In addition, a temporal downsampling strategy (TDS) is introduced on the input side to reduce data redundancy while enlarging the temporal receptive field.</p><p>Our contributions can be summarized as follows:</p><p>-To the best of our knowledge, P-STMO is the first approach that introduces the pre-training technique to 3D human pose estimation. A pre-training task, namely MPM, is proposed in a self-supervised manner to better capture both spatial and temporal dependencies. -The proposed STMO model simplifies the responsibility of each module and therefore significantly reduces the optimization difficulty. An MLP block is utilized as an effective spatial feature extractor for SEM. In addition, a temporal downsampling strategy is employed to mitigate the data redundancy problem for TEM. -Compared with other approaches, our method achieves state-of-the-art performance on two benchmarks with fewer parameters and smaller computational budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Human Pose Estimation</head><p>Recently, there is a research trend that uses 2D keypoints to regress corresponding 3D joint positions. The advantage is that it is compatible with any existing 2D pose estimation method. Our approach falls under this category. Extensive works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref> have been carried out around an important issue in videos, which is how to exploit the information in spatial and temporal domains. Some works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b13">14]</ref> only focus on 3D single-frame pose estimation and ignore temporal dependencies. Other recent approaches <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17]</ref> explore the way of integrating spatio-temporal information. The following are the shortcomings of these methods. Approaches relying on recurrent neural network (RNN) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref> suffer from high computational complexity. Graph convolutional network (GCN)-based methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b18">19]</ref> perform graph convolutions on the spatial temporal graph, and predict all poses in the sequence. This diminishes the capability of the network to model the 3D pose in a particular frame. The Transformer-based method <ref type="bibr" target="#b54">[55]</ref> predicts the 3D pose of the current (middle) frame by performing a weighted average at the last layer over the features of all frames in the sequence. It ignores the importance of the current pose and its near neighbors. On the other hand, some works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6]</ref> focus on the process of manyto-one frame aggregation. Pavllo et al. <ref type="bibr" target="#b36">[37]</ref> first propose a temporal convolutional network (TCN) that uses multiple frames to aid the modeling of one frame by progressively reducing the temporal dimension. Since then many methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29]</ref> have been proposed based on TCN. However, these methods do not explicitly extract spatial and temporal features. Li et al. <ref type="bibr" target="#b24">[25]</ref> alleviate this problem by using a vanilla Transformer to capture long-range temporal dependencies in the sequence. But they acquire spatial information by a single fc layer, which has insufficient representation capability.</p><p>In contrast, we propose a Spatial Temporal Many-to-One (STMO) model. The most important steps in 3D human pose estimation are represented as three modules in STMO: SEM, TEM, and MOFA. Our method clearly delineates the responsibility of each module and promotes the modeling of intrinsic properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-Training of Transformer</head><p>Transformer <ref type="bibr" target="#b44">[45]</ref> has become the de facto backbone in NLP <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">39]</ref> and CV <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref>. Transformer owes its widespread success to the pre-training technique <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50]</ref>. In NLP, Devlin et al. <ref type="bibr" target="#b10">[11]</ref> propose a masked language modeling (MLM) task, which triggers a wave of research on pre-training. They randomly mask some words and aim to predict these masked words based only on their context. Subsequent developments in CV have followed a similar trajectory to NLP. Some works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref> transfer self-supervised pre-trained models to image-based CV tasks, such as classification, object detection, semantic segmentation, etc. They replace the words to be reconstructed in MLM with pixels or discrete visual tokens. The pre-training task in CV is termed as masked image modeling (MIM).</p><p>Inspired by these works, we apply the pre-training technique to 3D human pose estimation and propose a masked pose modeling (MPM) task similarly. We randomly mask some joints in spatial and temporal domains and try to recover the original 2D poses. Note that METRO <ref type="bibr" target="#b26">[27]</ref> also masks the input 2D poses, but the goal is to directly regress 3D joint positions rather than recovering the input. This approach is essentially a data augmentation method and therefore different from the proposed pre-training task. <ref type="figure" target="#fig_0">Fig. 1</ref> depicts an overview of the proposed P-STMO method, which divides the optimization process into two stages: pre-training and fine-tuning. Firstly, a part of STMO model is pre-trained by solving the masked pose modeling (MPM) task in a self-supervised manner. The goal of this stage is to recover the input sequence from the corrupted 2D poses. Secondly, STMO model is fine-tuned to predict the 3D pose in the current (middle) frame given a sequence of 2D poses obtained by an off-the-shelf 2D pose detector. For both stages, we take a sequence of 2D poses as input, denoted as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><formula xml:id="formula_0">X = {x n } (N ?1)/2 n=?(N ?1)/2 , x n = {p i } J i=1 ,<label>(1)</label></formula><p>where p i ? R 2 is the 2D position of i th joint. N, J are the number of frames and human joints in each frame, respectively. Usually, N is an odd number, which means that the current frame and (N ? 1)/2 frames to the left and right of it are used as inputs.  Stage I: As shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>, a proportion of frames as well as some joints in the remaining frames are randomly masked. This spatially and temporally masked input is denoted as X ST . The whole network architecture consists of a spatial temporal encoder (SEM+TEM) that maps the masked input X ST to the latent space, and a decoder that recovers the original 2D poses X from latent representations. To predict the complete 2D poses, the model has to seek relevant unmasked joints for help. In this way, SEM and TEM are enabled to learn 2D spatial and temporal relationships. Since monocular motion contributes to depth estimation <ref type="bibr" target="#b40">[41]</ref>, acquiring 2D spatio-temporal relationships is beneficial for 3D human pose estimation.</p><p>Stage II: <ref type="figure" target="#fig_0">Fig. 1b</ref> shows the proposed STMO model. The pre-trained encoder is loaded and fine-tuned in this stage to obtain knowledge about the 3D space. This encoder is followed by MOFA that aggregates multiple frames to estimate the 3D pose in the middle frame of the sequence, denoted as Y = {y 0 }, y 0 ? R J?3 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-Training of STMO</head><p>The spatial and temporal dependencies of the pose sequence are mainly captured by SEM and TEM in our STMO model. To improve the overall estimation accuracy, we propose a self-supervised spatial temporal pre-training task, namely MPM. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we use three masking strategies to mask the input 2D poses, which are described in detail below.</p><p>Temporal Masking. A portion of input frames are randomly masked, which is called temporal masking. The masked frame is replaced with a temporal padding embedding e T which is a shared learnable vector. Similar to <ref type="bibr" target="#b15">[16]</ref>, to improve the efficiency of the model, we only use the unmasked frames as inputs to the encoder, excluding the temporal padding embeddings. Instead, the decoder takes the temporal padding embeddings as well as the encoded unmasked embeddings as inputs and reconstructs the original 2D poses. In this way, the encoder models temporal dependencies between two unmasked frames that are not adjacent to each other in the original sequence, and then the decoder fills in the missing 2D poses between these two frames. We denote the indices of the masked frames as a set:</p><formula xml:id="formula_1">M T ? {? N ?1 2 , ? N ?1 2 + 1, . . . , N ?1 2 }, | M T |= q T ? N ,</formula><p>where q T ? R is the temporal masking ratio. We use a large temporal masking ratio (e.g., 90%), so this task cannot be solved easily by interpolation. The input X to the encoder (eq. 1) is modified to</p><formula xml:id="formula_2">X T = {x T n : n / ? M T } (N ?1)/2 n=?(N ?1)/2 , x T n = {p i } J i=1 .<label>(2)</label></formula><p>After that, the input to the decoder is denoted as</p><formula xml:id="formula_3">{h n : n / ? M T } (N ?1)/2 n=?(N ?1)/2 {e T : n ? M T } (N ?1)/2 n=?(N ?1)/2 ,<label>(3)</label></formula><p>where h n , e T ? R d are the encoded unmasked embedding and temporal padding embedding respectively. d is the dimension of the latent features. The output is Y = {y n } (N ?1)/2 n=?(N ?1)/2 , where y n ? R J?2 is the recovered 2D pose in frame n.</p><p>Spatial Masking. A fixed number of joints in each frame are randomly masked, which is called spatial masking. The masked joint is replaced with a spatial padding joint e S that is a shared learnable vector. The spatial padding joints, together with other unmasked joints, are sent to the encoder. Since the pose sequence is not masked temporally, no temporal padding embedding is used at the decoder side. In such manner, the encoder models spatial dependencies between joints, and then the decoder is capable of recovering the contaminated joints. The indices of the masked joints in frame n are denoted as a set:</p><formula xml:id="formula_4">M S n ? {? N ?1 2 , ? N ?1 2 + 1, . . . , N ?1 2 }, | M S n |= m S , where m S ? N</formula><p>is the number of masked joints. The spatial masking ratio is q S = m S /J. Although the number of masked joints is the same for each frame, the indices of the masked joints are various in different frames to increase the diversity. The input X to the encoder (eq. 1) is modified to</p><formula xml:id="formula_5">X S = {x S n } (N ?1)/2 n=?(N ?1)/2 , x S n = {p i : i / ? M S n } J i=1 {e S : i ? M S n } J i=1 ,<label>(4)</label></formula><p>where e S ? R 2 is the spatial padding joint.</p><p>Spatial Temporal Masking. To integrate the information on the spatiotemporal domain, we propose a spatial temporal masking strategy, which is a combination of the above two masking methods. Specifically, the temporal masking is implemented on the input pose sequence, followed by the spatial masking on the unmasked frames. This strategy is utilized in the proposed P-STMO model. The total spatial temporal masking ratio is calculated by</p><formula xml:id="formula_6">q ST = q T + (1 ? q T ) ? q S .</formula><p>The input X to the encoder (eq. 1) is modified to</p><formula xml:id="formula_7">X ST = {x ST n : n / ? M T } (N ?1)/2 n=?(N ?1)/2 ,<label>(5)</label></formula><formula xml:id="formula_8">x ST n = {p i : i / ? M S n } J i=1 {e S : i ? M S n } J i=1 .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spatial Temporal Many-to-One (STMO) Model</head><p>In Stage II, the pre-trained encoder is loaded to the proposed STMO model and fine-tuned on 3D poses. The detailed architecture of STMO is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. The input sequence will go through SEM, TEM, and MOFA to obtain the final output. The role of each module is described in detail below.</p><p>Spatial Encoding Module (SEM). SEM aims to capture the characteristics of each frame in the spatial domain. Zheng et al. <ref type="bibr" target="#b54">[55]</ref> propose to use a Transformer <ref type="bibr" target="#b44">[45]</ref> as the backbone network of SEM to integrate information across all joints in a single frame. However, the self-attention operation brings a great computational overhead, which limits the scalability of the network in the case of using multiple frames as inputs. Therefore, we propose to use a simple MLP block as the backbone network to establish spatial relationships between joints. Compared to <ref type="bibr" target="#b54">[55]</ref>, this lightweight design allows the network to accommodate more frames with the same computational budget. Each 2D pose in the input sequence is independently sent to the MLP block whose weights are shared across all frames.</p><p>Temporal Encoding Module (TEM). As 2D keypoints are used as inputs to the network, the amount of data per frame is small. Thus, we can take advantage of the extra-long 2D input pose sequence (e.g., 243 frames). Since the objective of this module is to exploit temporal information from the changes of human posture within this sequence, it is inefficient to focus on the relationships between highly redundant frames in a local region. Therefore convolution operations, which introduce the inductive bias of locality, can be discarded. Instead, the self-attention mechanism is exploited, which allows the network to easily focus on correlations between frames that are far apart. We use a standard Transformer architecture as the backbone network of TEM. It is capable of capturing nonlocal self-attending associations. TEM takes a sequence of latent features from SEM as input and treats the features in each frame as an individual token. As the inputs are already embedded, the token embedding process in Transformer is skipped. Previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b5">6]</ref> show that the performance of 3D human pose estimation can be improved with more frames as inputs. However, there are two drawbacks when increasing the number of frames. (1) The computational complexity of Transformer is O(N 2 ), which constrains the network from utilizing a larger scale of temporal information. <ref type="formula" target="#formula_2">(2)</ref> The input data is highly redundant. For example, the video frame rate of Human3.6M <ref type="bibr" target="#b20">[21]</ref> is 50 fps, which means that there is only a slight change between adjacent frames. To alleviate these two problems, we propose to use a temporal downsampling strategy (TDS), which uniformly downsamples the input pose sequence. In this way, the network can accommodate a longer time span of information with the same number of input frames. In other words, the receptive field in the temporal domain can be expanded without increasing the number of parameters and computational complexity. Meanwhile, TDS removes highly similar neighboring frames, thus promoting data diversity. After TDS is applied, the input X is modified to X TDS = {x n?s } (N ?1)/2 n=?(N ?1)/2 , where s ? N + is the downsampling rate. The output Y remains unchanged. Some works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b55">56]</ref> also downsample the data, but they reduce the size of the dataset. Since the scale of the dataset is preserved by TDS, the proposed strategy is more effective than these works.</p><p>Many-to-One Frame Aggregator (MOFA). TEM mainly concerns the understanding of the temporal dependencies in the overall sequence, while MOFA aims to aggregate information from multiple frames and extract the 3D pose of the current frame. Therefore, it is effective to leverage local information around the current pose, which allows convolution to shine in this module. Li et al. <ref type="bibr" target="#b24">[25]</ref> propose a Strided Transformer Encoder (STE), which is a combination of Transformer and convolution. Specifically, STE replaces the MLP block in Transformer with strided 1D temporal convolutions proposed by TCN <ref type="bibr" target="#b36">[37]</ref>. We use STE as the backbone network of MOFA. For the MLP block in vanilla Transformer, the fc layer is formulated as l</p><formula xml:id="formula_9">(k) i,n = d k?1 j=0 w i,j * l (k?1) j,n , where l (k)</formula><p>i,n is the i th channel of n th token (frame) in k th layer, w i,j is the weight shared across frames, and d k?1 is the number of channels in (k ? 1) th layer. In the case that 1D tempo-  ral convolution is used instead of the fc layer, the formulation is modified to l</p><formula xml:id="formula_10">(k) i,n = d k?1 j=0 (M ?1)/2 m=?(M ?1)/2 w i,j,m * l (k?1)</formula><p>j,n+m , where M is the kernel size. It aggregates adjacent M frames in the sequence into a single frame. The stride is set to M so that there will be no overlap between any two convolution operations. The stacking of multiple layers of 1D temporal convolution eventually enables the mapping from N frames to one frame (current frame).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>Stage I. The objective of the pre-training stage is to minimize the negative loglikelihood of the correct 2D pose x n = {p i } J i=1 in each frame given the corrupted input sequence:</p><formula xml:id="formula_11">L pretrain = x?D E n ? ? (N ?1)/2 n=?(N ?1)/2 ? log p x n | X ST ? ? ,<label>(7)</label></formula><p>where D is the training corpus, and p(? | ?) is the conditional probability.</p><p>Stage II. The final estimated 3D pose of the middle frame in the sequence is obtained via a linear projection after MOFA. To optimize the network, we use L2 loss to minimize the errors between predictions and ground truths :</p><formula xml:id="formula_12">L single = 1 J J i=1 ?y i ? y i ? 2 ,<label>(8)</label></formula><p>where y i and y i are the ground truth and estimated 3D positions of i th joint in the current pose respectively. In addition, to better supervise the optimization, we append a multi-frame loss <ref type="bibr" target="#b24">[25]</ref> after TEM. To be specific, a linear projection is added to obtain the 3D poses of all frames in the sequence. In this way, we force TEM to exploit the temporal relationships between frames to predict 3D poses in all frames. Since the output features of TEM are quite close to 3D poses after applying the multiframe loss, the role of MOFA is to aggregate these 3D pose-related features of all frames into a single representation. The multi-frame loss is formulated as:</p><formula xml:id="formula_13">L multiple = 1 J ? N J i=1 (N ?1)/2 n=?(N ?1)/2 ?y i,n ? y i,n ? 2 .<label>(9)</label></formula><p>The final loss function of the whole network is L = L single + ?L multiple , where ? is the balance factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>Human3.6M <ref type="bibr" target="#b20">[21]</ref> is the most commonly used indoor dataset, which consists of 3.6 million frames captured by four 50 Hz cameras. The dataset is divided into 15 daily activities (e.g., walking and sitting) performed by 11 subjects. Following <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b2">3]</ref>, we use 5 subjects for training (S1, S5, S6, S7, S8), and 2 subjects for testing (S9, S11). We report the mean per joint position error (MPJPE) and Procrustes MPJPE (P-MPJPE) for Human3.6M dataset. The former computes the Euclidean distance between the predicted joint positions and the ground truth positions. The latter is the MPJPE after the predicted results align to the ground truth via a rigid transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP [32]</head><p>is a more challenging dataset with both indoor and outdoor scenes. The training set comprises of 8 subjects, covering 8 activities. The test set covers 7 activities, containing three scenes: green screen, non-green screen, and outdoor. Following <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b5">6]</ref>, we train the proposed method using all activities from 8 camera views in the training set and evaluate on valid frames in the test set.</p><p>We report MPJPE, percentage of correct keypoints (PCK) within 150mm range, and area under curve (AUC) as the evaluation metrics for MPI-INF-3DHP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-Art Methods</head><p>Results on Human3.6M. We compare our P-STMO with existing state-ofthe-art methods on Human3.6M dataset. Following <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29]</ref>, we use CPN <ref type="bibr" target="#b7">[8]</ref> as the 2D keypoint detector, and then train our networks on the detected 2D pose sequence. As shown in <ref type="table" target="#tab_2">Table 1</ref> (top and middle), our method achieves promising results under both MPJPE (42.8mm) and P-MPJPE (34.4mm) when  using 243 frames as inputs. We also train our model using the same refining module as <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b47">48]</ref>. It achieves 42.1mm under MPJPE, which surpasses all other methods. Our method yields better performance on hard poses (such as Photo and SittingDown) than the previous works. This demonstrates the robustness of our model in the case of depth ambiguity and severe self-occlusion. Additionally, we propose a smaller model P-STMO-Small. The only difference between P-STMO and P-STMO-S lies in the number of Transformer layers. For more details, please refer to the supplementary materials. Compared with P-STMO, P-STMO-S achieves 43.0mm MPJPE with a smaller number of FLOPs and parameters. To explore the lower bound of the proposed method, we utilize the ground truth of 2D poses as inputs. In this way, the input noise is removed. As shown in <ref type="table" target="#tab_2">Table 1</ref> (bottom), P-STMO outperforms other methods with 29.3mm under MPJPE.</p><formula xml:id="formula_14">N =50) - - - - - - - - - - - - - - - 32.</formula><p>Results on MPI-INF-3DHP. <ref type="table" target="#tab_3">Table 2</ref> reports the performance of state-ofthe-art methods and the proposed method on MPI-INF-3DHP dataset. Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref>, we adopt the ground truth of 2D poses as inputs. Since the se- quence length in this dataset is shorter than that in Human3.6M dataset, we set the number of input frames to 81. Our method achieves significant improvements in terms of AUC (9.1%) and MPJPE (24.2%). The PCK metric is at a competitive level compared to <ref type="bibr" target="#b18">[19]</ref>. The results suggest that our method has a strong generalization ability.</p><p>Qualitative Results. We provide some qualitative results in <ref type="figure">Fig. 4</ref>. We compare the proposed method with Poseformer [55] on Human3.6M and MPI-INF-3DHP datasets. Our method achieves good qualitative performance in both indoor scenes and complex outdoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To verify the effectiveness of each component, we conduct ablation experiments on Human3.6M dataset using our P-STMO-S model. We utilize 2D keypoints from CPN as inputs. MPJPE is reported for analysis.    1.0mm and 0.2mm respectively. Previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b28">29]</ref> observe decreasing returns when increasing the temporal receptive field, so a small gain (0.2mm) from TDS is to be expected in the case of 243 input frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Each Component. As shown in</head><p>Analysis on Masking Ratio of Pre-Training. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the influence of the masking ratio of three masking strategies in the pre-training stage. As shown in <ref type="figure" target="#fig_4">Fig. 5a</ref>, the optimal ratio is q T = 90% when only temporal masking is used. This ratio is higher than MAE <ref type="bibr" target="#b15">[16]</ref> in CV and BERT <ref type="bibr" target="#b10">[11]</ref> in NLP, whose masking ratios are 75% and 15%, respectively. This is because 2D pose sequences are more redundant than images and sentences. Since two adjacent poses are very similar, we need a high masking ratio to increase the difficulty of the task. We observe a negative correlation between the loss of Stage I and the error of Stage II. This implies that increasing the difficulty of the MPM task in Stage I does promote the fitting ability of the encoder, thus aiding the training of Stage II.</p><p>The way of spatial masking is different from that of temporal masking. In the case of temporal masking, the decoder perceives the indices of the masked frames through positional embeddings. In the case of spatial masking, only the latent features in each frame are visible to the decoder, so the indices of the masked joints cannot be inferred directly. For the above reasons, it is clear that the spatial masking task is more troublesome than the temporal masking task. Therefore, the optimal masking ratio of the former should be smaller than that of the latter. <ref type="figure" target="#fig_4">Fig. 5b</ref> shows that the optimal number is m S = 7 (q S = 41.1%) when only spatial masking is used. When the masking number is greater than 7, continuing to increase the difficulty of the task will result in too much noise in the input data. Subsequently, the encoder is unable to obtain useful information.</p><p>We illustrate the effect of combining temporal masking and spatial masking in <ref type="figure" target="#fig_4">Fig. 5c</ref>. Since this hybrid masking strategy is more challenging, the spatial  masking ratio and temporal masking ratio should be slightly reduced. The best performance is achieved when q T = 80% and m S = 2 (q S = 11.7%). The optimal spatial temporal masking ratio is q ST = 82.3%. The performance improvement of spatial temporal masking over spatial masking and temporal masking is 0.33mm and 0.3mm, respectively.</p><p>Analysis on Different Designs of SEM. To verify the superiority of using an MLP block as the backbone network of SEM, we evaluate different architecture designs in <ref type="table" target="#tab_6">Table 4</ref>. The results show that MLP outperforms fc and Transformer by 3.7mm and 2.9mm respectively. The structure of a single fc layer is so simple that it does not have sufficient capability to represent spatial characteristics. Meanwhile, Transformer is too complex and thus difficult to optimize. Therefore, they are not as effective as an MLP block. MLP achieves better performance than the other two methods, while its parameters and FLOPs are in between. Additionally, we explore the influence of the number of sub-blocks L in MLP. The best performance is achieved when L = 2. We choose L = 1 in other experiments because it is less computationally intensive and achieves similar performance compared to the best one.</p><p>Analysis on Temporal Downsampling Rate of TDS. As shown in <ref type="table" target="#tab_7">Table 5</ref>, we investigate the impact of temporal downsampling rate s for different numbers of input frames N . TDS brings up to 2.1mm, 1.5mm, 0.2mm performance improvement in the case of N = 27, 81, 243 respectively. The results show that when N is fixed, increasing s can enlarge the temporal receptive field (RF) and reduce data redundancy, thus improving the prediction performance. In other words, when RF is fixed, TDS is an effective approach to reduce computational overhead without much performance sacrifice. Besides, the gain from TDS gradually decreases as N becomes larger. When N = 27 and s = 3, TDS improves the performance by 1.4mm. However, when N = 243 and s = 2, TDS only brings 0.2mm improvement. This can be explained by the fact that the farther the two frames are separated in the time domain, the less they are correlated. Therefore, the farther the added frames are from the middle frame, the less help it brings to the 3D pose estimation of the middle frame.</p><p>Computational Complexity. We report the number of parameters of our framework, the number of output frames per second (FPS), floating-point operations (FLOPs) per frame at inference time, and the performance. We conduct experiments on a single GeForce GTX 3080Ti GPU. For our method, we do not include the number of parameters and FLOPs of Stage I in the calculation results, because we only care about the speed of inference. Once the training is completed, the decoder in Stage I is discarded, which does not impose any burden on the inference process. As shown in <ref type="table" target="#tab_8">Table 6</ref>, our 243-frame P-STMO-S has fewer parameters and FLOPs than 81-frame Poseformer. P-STMO outperforms PoseFormer and anatomy-aware model [6] by 1.5/1.3mm while bringing a ?1.5/7.1? speedup. This reveals the effectiveness and efficiency of the proposed method. Note that we only calculate the time for the input data to pass through the model, which does not include data pre-processing time. Therefore, the FPS we report for Poseformer is larger than the result in the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present P-STMO, a Pre-trained Spatial Temporal Many-to-One model for 3D human pose estimation. A pre-training task, called MPM, is proposed to enhance the representation capability of SEM and TEM. For SEM, an MLP block is used as the backbone, which is more effective than Transformer and fc. For TEM, a temporal downsampling strategy is introduced to mitigate input data redundancy and increase the temporal receptive field. Comprehensive experiments on two datasets demonstrate that our method achieves superior performance over state-of-the-art methods with smaller computational budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Other Related Work</head><p>Hu et al. <ref type="bibr" target="#b17">[18]</ref> also explores the spatial temporal masking strategy in the field of sign language recognition. In addition to being applied in different fields, our work differs from theirs in the following ways: 1) They set the coordinates of the masked joints to (0,0), which is a very common position (center of the image). Thus, this approach will lead to confusion between the masked and unmasked joints. In contrast, we replace the masked joints with learnable vectors. These special identifiers enable the model to distinguish whether a joint is masked or not. 2) They take all 2D poses as inputs, while we only take 2D unmasked poses as inputs, which greatly reduces the computational complexity of the encoder. Liu et al. <ref type="bibr" target="#b29">[30]</ref> learn 2D human pose embeddings for downstream tasks, which is similar to our work. Our method differs from theirs in these ways: 1) They aim to learn view-invariant embeddings by metric learning, while we aim to learn spatial temporal embeddings by solving the MPM task. 2) The masking strategy is utilized to increase the difficulty of pre-training in our method, while they use it to simulate the real-world occlusion situation. 3) We use Transformer, whose token design is more suitable for masking. 4) We explore the spatial temporal masking strategy, while they do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>All experiments are carried out on a single GeForce GTX 3080Ti GPU. The proposed method is implemented using PyTorch <ref type="bibr" target="#b34">[35]</ref>. For both stages, we train our model using the Adam <ref type="bibr" target="#b21">[22]</ref> optimizer for 80 epochs. The initial learning rate is 1e ?4 for Stage I and 7e ?4 for Stage II. The learning rate decays by 3% after each epoch. The batch size is set to 160. We use horizontal flipping as the data augmentation approach during training and testing. The balance factor ? in the loss function is set to 1 empirically.</p><p>For SEM, we utilize an MLP block as the backbone. The number of subblocks is L = 1. The dimension of the latent features is set to 256. For TEM as well as the decoder in Stage I, we utilize a vanilla Transformer <ref type="bibr" target="#b44">[45]</ref> as the backbone. The depth of Transformer for these two modules is set to 3,2 (for P-STMO-S) or 4,3 (for P-STMO), respectively. Besides, the dimension of the QKV matrices is 256. The number of heads in self-attention is 8. For MOFA, STE <ref type="bibr" target="#b24">[25]</ref> is used as the backbone. The basic settings are the same as TEM. For N = 27, 81, 243, the depth of Transformer is set to 3, 4, 5, respectively. The stride of 1D convolution is set to 3 for each Transformer layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Analysis on Model Hyperparameters</head><p>As shown in <ref type="table" target="#tab_9">Table 7</ref>, we mainly investigate three hyperparameters in our method: the depth of TEM (L 1 ), the depth of the decoder in Stage I (L D ), and the dimension of latent features of all Transformers (d). Following <ref type="bibr" target="#b15">[16]</ref>, we adopt an asymmetric encoder-decoder design. Unlike classical auto-encoders, the depth of the decoder can be different from that of the encoder. Since we only care about the speed of inference, the number of parameters and FLOPs of the pretraining stage are not included in the calculation results. Thus, these results are not affected by different settings of L D . The best performance is achieved when</p><formula xml:id="formula_15">L 1 = 4, L D = 3, d = 256.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Reconstruction Results in the Pre-Training Stage</head><p>In <ref type="figure" target="#fig_5">Fig. 6</ref>, we give some qualitative results of the 2D pose reconstruction in the pre-training stage. The 2D input sequence is masked with q T = 0.8, m S = 2, which means only 18.7% of the joints are visible to the network. It can be seen that the network is able to perform a rough recovery of the 2D poses in the original sequence by virtue of only a small number of visible joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Visualization of Multi-Head Self-Attention</head><p>We perform a subjective analysis of the self-attention mechanism in Transformer in both stages. The results are shown in <ref type="figure" target="#fig_0">Fig. 7-10</ref>. The x-axis (horizontal) and y-axis (vertical) correspond to the index of key (K) and query (Q) in the multihead self-attention respectively. The output is the normalized attention weight. We set the input frame number to N = 243.</p><p>Stage I. We set the temporal masking ratio to q T = 0.8 and spatial masking number to m S = 2. Therefore, the number of unmasked frames is a = (1 ? q T ) ? N = ?48.6? = 48. The number of masked frames is b = N ? a = 195. The unmasked frames are fed to the encoder (SEM+TEM). The attention maps of TEM are shown in <ref type="figure">Fig. 7</ref>. Since the input is randomly masked, two adjacent frames in the unmasked frame sequence are most likely to be non-adjacent in the original sequence. Our goal is to recover all the masked frames between two adjacent frames in the unmasked frame sequence. Therefore, most heads of TEM focus on aggregating neighbouring information around the query frame. Others (e.g., head 6) capture long-term dependencies. The attention maps of the decoder are shown in <ref type="figure" target="#fig_3">Fig. 8</ref>. An obvious cross can be seen in each attention map because we utilize the same implementation as <ref type="bibr" target="#b15">[16]</ref>. Specifically, to improve the efficiency, we append a list of temporal padding embeddings after the encoded unmasked embeddings at the decoder side. Then, we add positional embeddings to these padding embeddings to restore their original positions. Therefore, the first half of the sequence (before frame 48) behaves differently from the second half (after frame 48). Each attention map can be divided into four matrices as follows. 1) The matrix D 1 ? R a?a in the upper left corner measures the self-attention of unmasked frames. The patterns are very similar to those in <ref type="figure">Fig. 7.</ref> 2) The matrix D 2 ? R a?b in the upper right corner measures the degree of attention paid to the masked frames by the unmasked frames. The values of most heads are 0, which means that the masked frames provide little help to the unmasked ones. A small number of heads behave differently (e.g., head 0), which can be explained by the need to interpret the continuity of the entire pose sequence.</p><p>3) The matrix D 3 ? R b?a in the bottom left corner measures the degree of attention paid to the unmasked frames by the masked frames. For a particular masked frame, it resorts to the unmasked frames that are close to it. As a result, some heads (e.g., head 4) exhibit a locally relevant pattern. In addition, non-local patterns can also be observed in other heads (e.g., head 3). 4) The matrix D 4 ? R b?b in the bottom right corner measures the self-attention of masked frames. Since the masked frames mainly obtain information from the unmasked frames, the self-attention pattern of masked frames is not pronounced. Nevertheless, we can still find some patterns learned by a small number of heads, such as capturing long-term dependencies (e.g., head 0) and short-term dependencies (e.g., head 1).</p><p>Stage II. After the pre-trained model is loaded into STMO, TEM is tuned to acquire long-and short-term information that contributes to the overall 3D pose sequence prediction. As illustrated in <ref type="figure">Fig. 9</ref>, Transformer learns a variety of patterns. The local patterns (e.g., head 0) focus on a small region around the query frame, while the global patterns (e.g., head 5) exploit relevant features over a longer time span.</p><p>As we use STE <ref type="bibr" target="#b24">[25]</ref>, a Transformer-based method, as the backbone network of MOFA, its attention maps can also be visualized in <ref type="figure" target="#fig_0">Fig. 10</ref>. For N = 243, the depth of Transformer is 5. Only the first 4 layers are shown. STE uses 1D convolution to reduce the temporal dimension layer by layer. As we set the stride to 3, the number of frames in each layer is reduced to 1/3 of that in the previous layer. A vertical line can be seen in all heads because MOFA is designed to predict the 3D pose in the current frame. Transformer forces all frames to attend to the current frame and its neighbours. Besides, the multi-head attention maps of MOFA and TEM show different patterns. The divergence of the functionality of these two modules is attributed to the presence of multi-frame loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Qualitative Results on in-the-wild Videos</head><p>We train our method on Human3.6M dataset and evaluate on in-the-wild videos. We use AlphaPose <ref type="bibr" target="#b12">[13]</ref> as the 2D keypoint detector to generate 2D poses. As shown in <ref type="figure" target="#fig_0">Fig. 11</ref>, our method generalizes well to in-the-wild videos that often contain rare or unseen poses in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Generalization Ability and Robustness</head><p>To validate the generalization ability of the proposed pre-training method, we conduct experiments on Human3.6M, where three camera views (cam 0,1,2) are used in Stage I and the other camera view (cam 3) is used in Stage II. The results show that P-STMO-S (w/ pre-training) and STMO-S (w/o pre-training) achieve 44.4mm and 45.2mm MPJPE respectively, which demonstrates that pre-training on several cameras can help the network generalize to a different camera view.</p><p>To validate the robustness of the proposed pre-training method, we utilize the ground truth (GT) of 2D keypoints as noiseless inputs, to which Gaussian noise with different ? is added. Note that CPN can be regarded as adding complex noise introduced by the 2D detector to the ground truth. Besides, the random shuffle (RS) strategy is tested. This strategy randomly disrupts the order of the input frames while leaving the order of the output frames unchanged. <ref type="table" target="#tab_10">Table 8</ref> shows pre-training still delivers performance gains in the case of input noise and RS, which verifies its robustness.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) The pre-training procedure for STMO. The 2D pose sequence is randomly masked and fed to the encoder. The encoded unmasked embeddings as well as the temporal padding embeddings are sent to the decoder to reconstruct the original 2D poses in the input sequence. (b) Overview of our STMO model, which consists of SEM, TEM, and MOFA in series.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of three masking strategies. (a) Temporal masking. (b) Spatial masking. (c) Spatial temporal masking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Detailed architecture of the proposed STMO model in Stage II. For N = 243, we choose 485 frames and downsample them using TDS with the temporal downsampling rate s set to 2. The tensor sizes are shown in parentheses. For example, (243,34) denotes 243 frames and 34 channels, meaning that there are J = 17 joints in each frame, and each joint has 2 coordinates (x, y). (243/3 i ,256) means the temporal dimension is reduced by 3 i as 1D temporal convolution is used and the stride is the same as the kernel size M = 3. i = 1, 2, ..., L2, where L2 = 5 is the number of layers of MOFA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>8</head><label>8</label><figDesc>Liu et al. [29] CVPR'20 (N =243) 34.5 37.1 33.6 34.2 32.9 37.1 39.6 35.8 40.7 41.4 33.0 33.8 33.0 26.6 26.9 34.7 Zeng et al. [52] ECCV'20 (N =243) 34.8 32.1 28.5 30.7 31.4 36.9 35.6 30.5 38.9 40.5 32.5 31.0 29.9 22.5 24.5 32.0 Chen et al. [6] TCSVT'21 (et al. [55] ICCV'21 (N =81) 30.0 33.6 29.9 31.0 30.2 33.3 34.8 31.4 37.8 38.6 31.7 31.5 29.0 23.3 23.1 31.3 Shan et al. [42] MM'21 (N =243) 29.5 30.8 28.8 29.1 30.7 35.2 31.7 27.8 34.5 36.0 30.3 29.4 28.9 24.1 24.7 30.1 P-STMO (N =243) 28.5 30.1 28.6 27.9 29.8 33.2 31.3 27.8 36.0 37.4 29.7 29.5 28.1 21.0 21.0 29.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Performance of three different masking strategies. (a) Temporal masking. (b) Spatial masking. (c) Spatial temporal masking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative results of the 2D pose reconstruction in Stage I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Visualization of multi-head attention maps of TEM in Stage I. Visualization of multi-head attention maps of the decoder in Stage I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 . 8 (d) Layer 3 Fig. 10 .</head><label>98310</label><figDesc>Visualization of multi-head attention maps of TEM in Stage II. Visualization of multi-head attention maps of MOFA in Stage II. We only show the first two heads in each Transformer layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Qualitative results of the proposed method on in-the-wild videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Results on Human3.6M in millimeter under MPJPE and P-MPJPE. Top&amp;Middle table: 2D poses detected by CPN are used as inputs. Bottom table: the ground truth of 2D poses are used as inputs. N is the number of input frames. (*) -uses the refining module proposed in<ref type="bibr" target="#b2">[3]</ref>. The best result is shown in bold, and the second-best result is underlined.MPJPE  (CPN) Dir. Disc. Eat Greet Phone Photo Pose Pur. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg Pavllo et al. [37] CVPR'19 (N =243) 45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8 Lin et al. [26] BMVC'19 (N =50) 42.5 44.8 42.6 44.2 48.5 57.1 42.6 41.4 56.5 64.5 47.4 43.0 48.1 33.0 35.1 46.6 Xu et al. [49] CVPR'20 (N =9) 37.4 43.5 42.7 42.7 46.6 59.7 41.3 45.1 52.7 60.2 45.8 43.1 47.7 33.7 37.1 45.6 Wang et al. [48] ECCV'20 (N =96) 41.3 43.9 44.0 42.2 48.0 57.1 42.2 43.2 57.3 61.3 47.0 43.5 47.0 32.6 31.8 45.6 Liu et al. [29] CVPR'20 (N =243) 41.8 44.8 41.1 44.9 47.4 54.1 43.4 42.2 56.2 63.6 45.3 43.5 45.3 31.3 32.2 45.1 Zeng et al. [52] ECCV'20 (N =243) 46.6 47.1 43.9 41.6 45.8 49.6 46.5 40.0 53.4 61.1 46.1 42.6 43.1 31.5 32.6 44.8 Zeng et al. Zheng et al. [55] ICCV'21 (N =81) 41.5 44.8 39.8 42.5 46.5 51.6 42.1 42.0 53.3 60.7 45.5 43.3 46.1 31.8 32.2 44.3 Shan et al. [42] MM'21 (N =243) 40.8 44.5 41.4 42.7 46.3 55.6 41.8 41.9 53.7 60.8 45.0 41.5 44.8 30.8 31.9 44.3 Chen et al. [6] TCSVT'21 (N =243) 41.4 43.5 40.1 42.9 46.6 51.9 41.7 42.3 53.9 60.2 45.4 41.7 46.0 31.5 32.7 44.1 Hu et al. [19] MM'21 (N =96) 38.0 43.3 39.1 39.4 45.8 53.6 41.4 41.4 55.5 61.9 44.6 41.9 44.5 31.6 29.4 43.4 Li et al. [25] TMM'22 (N =351)(*) 39.9 43.4 40.0 40.9 46.4 50.6 42.1 39.8 55.8 61.6 44.9 43.3 44.9 29.9 30.3 43.6 Pavllo et al. [37] CVPR'19 (N =243) 34.1 36.1 34.4 37.2 36.4 42.2 34.4 33.6 45.0 52.5 37.4 33.8 37.8 25.6 27.3 36.Liu et al. [29] CVPR'20 (N =243) 32.3 35.2 33.3 35.8 35.9 41.5 33.2 32.7 44.6 50.9 37.0 32.4 37.0 25.2 27.2 35.6 Wang et al. [48] ECCV'20 (N =96) 32.9 35.2 35.6 34.4 36.4 42.7 31.2 32.5 45.6 50.2 37.3 32.8 36.3 26.0 23.9 35.5 Chen et al. [6] TCSVT'21 (N =243) 33.1 35.3 33.4 35.9 36.1 41.7 32.8 33.3 42.6 49.4 37.0 32.7 36.5 25.5 27.9 35.</figDesc><table><row><cell>[53] ICCV'21 (N =9)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.7</cell></row><row><cell>P-STMO-S (N =81)</cell><cell cols="16">41.7 44.5 41.0 42.9 46.0 51.3 42.8 41.3 54.9 61.8 45.1 42.8 43.8 30.8 30.7 44.1</cell></row><row><cell>P-STMO-S (N =243)</cell><cell cols="16">40.0 42.5 38.3 41.5 45.8 50.8 41.6 40.9 54.2 59.3 44.4 41.9 43.6 30.3 30.1 43.0</cell></row><row><cell>P-STMO (N =243)</cell><cell cols="16">38.9 42.7 40.4 41.1 45.6 49.7 40.9 39.9 55.5 59.4 44.9 42.2 42.7 29.4 29.4 42.8</cell></row><row><cell>P-STMO (N =243)(*)</cell><cell>38.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4 42.1 39.8 40.2 45.2 48.9 40.4 38.3 53.8 57.3 43.9 41.6 42.2 29.3 29.3 42.1 P-MPJPE (CPN) Dir. Disc. Eat Greet Phone Photo Pose Pur. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg Lin et al. [26] BMVC'19 (N =50) 32.5 35.3 34.3 36.2 37.8 43.0 33.0 32.2 45.7 51.8 38.4 32.8 37.5 25.8 28.9 36.85 Xu et al. [49] CVPR'20 (N =9) 31.0 34.8 34.7 34.4 36.2 43.9 31.6 33.5 42.3 49.0 37.1 33.0 39.1 26.9 31.9 36.26 Shan et al. [42] MM'21 (N =243) 32.5 36.2 33.2 35.3 35.6 42.1 32.6 31.9 42.6 47.9 36.6 32.1 34.8 24.2 25.8 35.0 Zheng et al. [55] ICCV'21 (N =81) 32.5 34.8 32.6 34.6 35.3 39.5 32.1 32.0 42.8 48.5 34.8 32.4 35.3 24.5 26.0 34.6 P-STMO (N =243) 31.3 35.2 32.9 33.9 35.4 39.3 32.5 31.5 44.6 48.2 36.3 32.9 34.4 23.8 23.9 34.4 MPJPE (GT) Dir. Disc. Eat Greet Phone Photo Pose Pur. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg Pavllo et al. [37] CVPR'19 (N =243) 35.2 40.2 32.7 35.7 38.2 45.5 40.6 36.1 48.8 47.3 37.8 39.7 38.7 27.8 29.5 37.8 Lin et al. [26] BMVC'19 (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Results on MPI-INF-3DHP under three evaluation metrics.</figDesc><table><row><cell>Method</cell><cell cols="3">PCK? AUC? MPJPE?</cell></row><row><cell>Mehta et al. [32] 3DV'17 (N =1)</cell><cell>75.7</cell><cell>39.3</cell><cell>117.6</cell></row><row><cell cols="2">Pavllo et al. [37] CVPR'19 (N =81) 86.0</cell><cell>51.9</cell><cell>84.0</cell></row><row><cell>Lin et al. [26] BMVC'19 (N =25)</cell><cell>83.6</cell><cell>51.4</cell><cell>79.8</cell></row><row><cell>Zeng et al. [52] ECCV'20 (N =1)</cell><cell>77.6</cell><cell>43.8</cell><cell>-</cell></row><row><cell cols="2">Wang et al. [48] ECCV'20 (N =96) 86.9</cell><cell>62.1</cell><cell>68.1</cell></row><row><cell>Zheng et al. [55] ICCV'21 (N =9)</cell><cell>88.6</cell><cell>56.4</cell><cell>77.1</cell></row><row><cell cols="2">Chen et al. [6] TCSVT'21 (N =81) 87.9</cell><cell>54.0</cell><cell>78.8</cell></row><row><cell>Hu et al. [19] MM'21 (N =96)</cell><cell cols="2">97.9 69.5</cell><cell>42.5</cell></row><row><cell>P-STMO (N =81)</cell><cell cols="2">97.9 75.8</cell><cell>32.2</cell></row></table><note>Fig. 4. Qualitative comparison between our P-STMO method and Poseformer [55] on Human3.6M and MPI-INF-3DHP datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table /><note>, we validate the contri- butions of different modules in STMO model and the overall performance gain brought by the proposed MPM and TDS methods. A network consisting of only SEM achieves 51.0 mm under MPJPE. To evaluate the effect of TEM, we use a vanilla Transformer as the backbone and yield a result of 49.6mm. After com- bining SEM and TEM, MPJPE drops to 46.0mm. Finally, we end up with the proposed STMO model by assembling SEM, TEM as well as MOFA, and achieve 44.2mm under MPJPE. Furthermore, MPM and TDS improve upon STMO by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>The effectiveness of different components.</figDesc><table><row><cell>SEM TEM MOFA MPM TDS</cell><cell cols="3">Params(M) FLOPs(M) MPJPE?</cell></row><row><cell></cell><cell>1.1</cell><cell>536</cell><cell>51.0</cell></row><row><cell></cell><cell>1.6</cell><cell>769</cell><cell>49.6</cell></row><row><cell></cell><cell>2.2</cell><cell>1094</cell><cell>46.0</cell></row><row><cell></cell><cell>6.2</cell><cell>1482</cell><cell>44.2</cell></row><row><cell></cell><cell>6.2</cell><cell>1482</cell><cell>43.2</cell></row><row><cell></cell><cell>6.2</cell><cell>1482</cell><cell>43.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Analysis on designs of SEM. L is the number of sub-blocks in MLP.</figDesc><table><row><cell>Backbone</cell><cell cols="4">N Params(M) FLOPs(M) MPJPE?</cell></row><row><cell>fc</cell><cell>243</cell><cell>1.6</cell><cell>769</cell><cell>49.6</cell></row><row><cell cols="2">Transformer 81</cell><cell>7.2</cell><cell>1218</cell><cell>48.8</cell></row><row><cell cols="2">MLP (L = 1) 243</cell><cell>2.3</cell><cell>1094</cell><cell>46.0</cell></row><row><cell cols="2">MLP (L = 2) 243</cell><cell>2.8</cell><cell>1350</cell><cell>45.9</cell></row><row><cell cols="2">MLP (L = 3) 243</cell><cell>3.3</cell><cell>1608</cell><cell>46.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Analysis on temporal downsampling rate s. RF is the temporal receptive field.</figDesc><table><row><cell>N s RF</cell><cell cols="3">Params(M) FLOPs(M) MPJPE?</cell></row><row><cell>27 1 27</cell><cell>4.6</cell><cell>163</cell><cell>48.2</cell></row><row><cell>27 3 81</cell><cell>4.6</cell><cell>163</cell><cell>46.8</cell></row><row><cell>27 9 243</cell><cell>4.6</cell><cell>163</cell><cell>46.1</cell></row><row><cell>81 1 81</cell><cell>5.4</cell><cell>493</cell><cell>45.6</cell></row><row><cell>81 3 243</cell><cell>5.4</cell><cell>493</cell><cell>44.1</cell></row><row><cell>243 1 243</cell><cell>6.2</cell><cell>1482</cell><cell>43.2</cell></row><row><cell>243 2 486</cell><cell>6.2</cell><cell>1482</cell><cell>43.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Analysis on computational complexity. Top table: taken from [55]. Bottom table: our implementation.</figDesc><table><row><cell>Method</cell><cell cols="4">Params(M) FLOPs(M) FPS MPJPE?</cell></row><row><cell>Pavllo et al. [37] CVPR'19 (N =243)</cell><cell>16.9</cell><cell>33</cell><cell>863</cell><cell>46.8</cell></row><row><cell>Chen et al. [6] TCSVT'21 (N =243)</cell><cell>58.1</cell><cell>116</cell><cell>264</cell><cell>44.1</cell></row><row><cell>Zheng et al. [55] ICCV'21 (N =81)</cell><cell>9.5</cell><cell>1358</cell><cell>269</cell><cell>44.3</cell></row><row><cell>Chen et al. [6] TCSVT'21 (N =243)</cell><cell>58.1</cell><cell>656</cell><cell>429</cell><cell>44.1</cell></row><row><cell>Zheng et al. [55] ICCV'21 (N =81)</cell><cell>9.5</cell><cell>1624</cell><cell>1952</cell><cell>44.3</cell></row><row><cell>P-STMO-S (N =81)</cell><cell>5.4</cell><cell>493</cell><cell>7824</cell><cell>44.1</cell></row><row><cell>P-STMO-S (N =243)</cell><cell>6.2</cell><cell>1482</cell><cell>3504</cell><cell>43.0</cell></row><row><cell>P-STMO (N =243)</cell><cell>6.7</cell><cell>1737</cell><cell>3040</cell><cell>42.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Analysis on model hyperparameters. Only the number of parameters and FLOPS of Stage II are included in the calculation results. L1, LD are the depth of TEM and the decoder in Stage I respectively. d is the dimension of latent features of all Transformers.</figDesc><table><row><cell>L1 LD d</cell><cell cols="3">Params(M) FLOPs(M) MPJPE?</cell></row><row><cell>4 2 256</cell><cell>6.7</cell><cell>868.5</cell><cell>43.0</cell></row><row><cell>4 3 256</cell><cell>6.7</cell><cell>868.5</cell><cell>42.8</cell></row><row><cell>4 4 256</cell><cell>6.7</cell><cell>868.5</cell><cell>42.9</cell></row><row><cell>4 5 256</cell><cell>6.7</cell><cell>868.5</cell><cell>43.3</cell></row><row><cell>2 3 256</cell><cell>5.7</cell><cell>613.7</cell><cell>44.4</cell></row><row><cell>3 3 256</cell><cell>6.2</cell><cell>741.4</cell><cell>43.1</cell></row><row><cell>4 3 256</cell><cell>6.7</cell><cell>868.5</cell><cell>42.8</cell></row><row><cell>5 3 256</cell><cell>7.3</cell><cell>995.9</cell><cell>43.4</cell></row><row><cell>4 3 64</cell><cell>1.1</cell><cell>121.8</cell><cell>44.9</cell></row><row><cell>4 3 128</cell><cell>2.6</cell><cell>307.0</cell><cell>43.7</cell></row><row><cell>4 3 256</cell><cell>6.7</cell><cell>868.5</cell><cell>42.8</cell></row><row><cell>4 3 512</cell><cell>19.5</cell><cell>2755.1</cell><cell>45.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Analysis on the robustness in terms of MPJPE?. ?: w/o pre-training. RS: random shuffle. STMO-S 29.3(?3.3%) 33.7(?3.2%) 46.9(?2.7%) 69.5(?2.0%) 57.7(?2.9%) 43.0(?2.1%)</figDesc><table><row><cell>Method</cell><cell>GT</cell><cell cols="4">GT+N (0,0.01) GT+N (0,0.03) GT+N (0,0.05) GT+RS</cell><cell>CPN</cell></row><row><cell>STMO-S ?</cell><cell>30.3</cell><cell>34.8</cell><cell>48.2</cell><cell>70.9</cell><cell>59.4</cell><cell>43.9</cell></row><row><cell>P-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">If not specified, "frame" in the following text refers to the 2D pose in this frame.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BEit: BERT pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anatomy-aware 3d human pose estimation with bone-based pose decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7059" to="7069" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Signbert: Pre-training of handmodel-aware representation for sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11087" to="11096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional directed graph convolution for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="602" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1661" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3d human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9887" to="9895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting temporal contexts with strided transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Trajectory space factorization for deep video-based 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning global pose features in graph convolutional networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">View-invariant, occlusion-robust probabilistic embedding for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="135" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Motion parallax as an independent cue for depth perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="134" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving robustness and accuracy via relative information encoding in 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3446" to="3454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="764" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Crossloc: Scalable aerial localization assisted by multimodal synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Doytchinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="17358" to="17368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning skeletal graph neural networks for hard 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11436" to="11445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">3d human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11656" to="11665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
