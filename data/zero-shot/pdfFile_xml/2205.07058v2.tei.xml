<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-25">25 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustafa</forename><surname>Meshry</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Evans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Keller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Loop</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Morrical</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-25">25 Oct 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We present RTMV, a large-scale high-fidelity ray-traced synthetic dataset for novel view synthesis. RTMV is composed of nearly 2000 scenes from 4 different environments exhibiting large varieties in view positions, lighting, object shapes, materials, and textures. Each quadrant shows a single scene from each environment, captured from multiple viewpoints.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a large-scale synthetic dataset for novel view synthesis consisting of ?300k images rendered from nearly 2000 complex scenes using high-quality ray tracing at high resolution (1600 ? 1600 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis, thus providing a large unified benchmark for both training and evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of our dataset exhibit challenging variations in camera views, lighting, shape, materials, and textures. Because our dataset is too large for existing methods to process, we propose Sparse Voxel Light Field (SVLF), an efficient voxel-based light field approach for novel view synthesis that achieves comparable performance to NeRF <ref type="bibr" target="#b49">[49]</ref> on synthetic data, while being * Equal contribution. Second author was an NVIDIA intern.</p><p>an order of magnitude faster to train and two orders of magnitude faster to render. SVLF achieves this speed by relying on a sparse voxel octree, careful voxel sampling (requiring only a handful of queries per ray), and reduced network structure; as well as ground truth depth maps at training time. Our dataset is generated by NViSII <ref type="bibr" target="#b50">[50]</ref>, a Pythonbased ray tracing renderer, which is designed to be simple for non-experts to use and share, flexible and powerful through its use of scripting, and able to create high-quality and physically-based rendered images. Experiments with a subset of our dataset allow us to compare standard methods like NeRF <ref type="bibr" target="#b49">[49]</ref> and mip-NeRF <ref type="bibr" target="#b0">[1]</ref> for single-scene modeling, and pixelNeRF <ref type="bibr" target="#b90">[89]</ref> for category-level modeling, pointing toward the need for future improvements in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the publication of NeRF <ref type="bibr" target="#b49">[49]</ref>, there has been an explosion of interest in neural volume rendering for novel view synthesis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b90">89,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b73">73]</ref>. This heightened attention of the research community is due to the impressive high-fidelity results that had long remained elusive but which are now possible. Further, these techniques are able to capture the 3D geometry of the scene with precision <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b80">80,</ref><ref type="bibr" target="#b71">71]</ref>, thus opening up an entirely new family of algorithms for 3D scene reconstruction.</p><p>Despite this progress, it is not yet clear what are the limitations of such techniques. Because of the difficulty of acquiring multi-view images of complex scenes with ground truth, only a handful of scenes has been available to date. As a result, since the existing methods have only been tested on a small number of scenes, it is unclear how they will perform on a wider variety of scenes. For example, can these methods simultaneously handle multiple objects, diverse materials and textures, challenging lighting conditions, and free camera poses? Can they generalize to new complex scenes from a few views? Questions such as these point to the need for a large-scale dataset to evaluate algorithms for novel view synthesis.</p><p>In this paper we propose to address this problem by sharing a large-scale dataset for novel view synthesis. Comparison to existing datasets is shown in <ref type="table">Table 1</ref>. Our dataset consists of nearly 2000 scenes composed of multiple objects, with complex materials and textures illuminated by various light sources (see <ref type="figure">Fig. 1</ref>). We selected these objects from 4 distinct sources of large collections, thus providing significant variety to the dataset. Approximately 300k high-resolution (1600 ? 1600) synthetic images were created using ray tracing to ensure high fidelity, with the virtual camera placed either on a hemisphere or freely within the environment (see <ref type="figure">Fig. 2</ref>). Using synthetic data enables the following advantages: 1) noise-free annotations, 2) rich metadata that otherwise would not be possible, and 3) full control over variations. Moreover, recent efforts at real-time ray tracing have made tremendous progress in reducing the sim-to-real gap for different computer vision applications, e.g., pose estimation <ref type="bibr" target="#b38">[38]</ref> or facial analysis <ref type="bibr" target="#b83">[82]</ref>. As a result, synthetic data is becoming increasingly important for training and evaluating deep networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b77">77]</ref>.</p><p>We use this dataset to benchmark several existing algorithms, thus highlighting existing limitations and pointing out the need for future research. However, we are only able to evaluate available methods on a small subset of our dataset due to their high computational demands. To reduce the computational cost, we propose a novel algorithm called SVLF (Sparse Voxel Light Field) that is an order of magnitude faster than NeRF to train on synthetic scenes, and nearly two orders of magnitude faster to render. This speedup is achieved by a novel combination of an octree  <ref type="table" target="#tab_3">scenes  8  15  502  124  3511  24  20k  2000  resolution  800  1080 1536 1200  128  480  1088  1600  multi-object       free camera       full GT       HDR export          Table 1</ref>: Comparison of novel view synthesis datasets. From top to bottom: the number of scenes, image resolution (smallest dimension), whether the scene is composed of multiple objects, whether the camera can freely move about the scene (as opposed to front-facing, hemisphere, etc.), whether ground truth (GT) information is available (e.g., depth, lighting, object material), and whether high dynamic range (HDR) is exported.</p><p>representation, querying each voxel only once, and using a much smaller network, as well as leveraging depth maps for training. We show that SVLF achieves results comparable to NeRF while being much more computationally efficient.</p><p>Our dataset has many uses beyond evaluation. First, a large dataset like ours is useful for training neural networks by exposing the network to more variety than is possible with existing datasets. Second, the size of the dataset allows for investigating few-show view synthesis, where an algorithm trained on a larger set of scenes is then able to process new scenes without training from scratch. Third, because the synthetic data allows for rich metadata such as ground truth depth, geometry, camera poses, object positions, and so forth, it facilitates research on networks that require these inputs for training (e.g., novel view synthesis algorithms that train on ground truth depth). Finally, the dataset can be used for problems other than novel view synthesis, such as 3D reconstruction and pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Novel view synthesis. Novel view synthesis is a longstanding problem in computer vision and graphics. Imagebased rendering (IBR) techniques <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b63">63]</ref> synthesize a novel view from a set of reference images by computing blend weights of nearby reference views. Another class of work builds a volumetric representation of the scene such as voxel grids <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b79">79,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">42]</ref> or multi-plane images (MPI) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b94">93,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b48">48]</ref>. To render such representations, techniques like alpha compositing or volume rendering are typically used to synthesize novel views.</p><p>Neural implicit representations. More recently, neural implicit representations <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b7">8]</ref> have shown great potential for novel view synthesis by storing scene properties as the weights of a fully connected network. Most notably, NeRF <ref type="bibr" target="#b49">[49]</ref> optimizes a 5D radiance field that maps a point and viewing direction to its density and color values, which can then be integrated for rendering. Mip-NeRF <ref type="bibr" target="#b0">[1]</ref> ABC Bricks Amz. Ber. <ref type="figure">Figure 2</ref>: Camera distributions for a single scene per environment, for each scene we present 2 example view points. uses integrated positional encoding over a cone frustum to render anti-aliased multi-scale images. NeRF has opened the door to many exciting research directions <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b91">90,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b85">84,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b3">4]</ref>. However, NeRF (and closely-related variants) are slow to train and render, as rendering a single pixel requires evaluating hundreds of samples along a ray, each constituting an expensive neural network query. To increase speed, some works <ref type="bibr" target="#b89">[88,</ref><ref type="bibr" target="#b41">41]</ref> utilize an octree to prune empty space and only sample points close to the surface. Another alternative, DONeRF <ref type="bibr" target="#b52">[52]</ref>, utilizes depth images to train a depth prediction network, thus achieving high-quality synthesis by evaluating only a few point samples around the estimated depth. Instant-NGP <ref type="bibr" target="#b51">[51]</ref>, which was developed concurrently with this work, is yet another alternative that uses multiple tricks to achieve orders of magnitude speedup over NeRF.</p><p>While NeRF focuses on fitting single scenes, several works aim to generalize across multiple scenes <ref type="bibr" target="#b78">[78,</ref><ref type="bibr" target="#b90">89,</ref><ref type="bibr" target="#b81">81,</ref><ref type="bibr" target="#b6">7]</ref>. First, a prior is learned over a feature volume by training on a dataset of multiple scenes. Then, presented only with few-shot images of a new scene, the learned prior is utilized to render novel views of the new scene.</p><p>Data synthesizer. Given the high demand for large anno-tated datasets for deep learning, there has been an increase in both synthetic datasets <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b93">92,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b19">20]</ref> and in tools for generating such data <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b86">85]</ref>. The Cycles renderer included in Blender has been widely used in the research community for generating synthetic data because of its ray tracing ability <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b25">26]</ref>. In an attempt to more easily generate synthetic images, Denninger et al. <ref type="bibr" target="#b14">[15]</ref> introduced an extension to Blender that renders objects falling onto a plane with randomized camera poses. Other tools, like SAPIEN <ref type="bibr" target="#b86">[85]</ref>, have also used the OptiX backend <ref type="bibr" target="#b57">[57]</ref> to optimize ray tracing performance. We believe that our proposed Python ray tracer adds to this suite by providing scripting capabilities, along with the ease of installation and development.</p><p>Datasets for novel view synthesis. The original NeRF paper <ref type="bibr" target="#b49">[49]</ref> introduced 8 synthetic scenes with 360-degree views. Since their release, the accessibility of these scenes has allowed for rapid progress in novel view synthesis, yet, the small number of such scenes makes it difficult to assess performance at scale. Wang et al. <ref type="bibr" target="#b81">[81]</ref> introduced a synthetic dataset including Google Scanned Objects. Other datasets for training multi-view algorithms include DTU <ref type="bibr" target="#b32">[32]</ref>, LLFF <ref type="bibr" target="#b48">[48]</ref>, Tanks and Temples <ref type="bibr" target="#b35">[35]</ref>, Spaces <ref type="bibr" target="#b17">[18]</ref>, RealEstate10K <ref type="bibr" target="#b94">[93]</ref>, SRN <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b90">89]</ref>, Transparent Objects <ref type="bibr" target="#b29">[29]</ref>, ROBI <ref type="bibr" target="#b87">[86]</ref>, CO3D <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b27">28]</ref>, SAPIEN <ref type="bibr" target="#b86">[85]</ref>, and BlendedMVS <ref type="bibr" target="#b88">[87]</ref>. See <ref type="table">Table 1</ref> for further information. During the preparation of this manuscript, Reizenstein et al. <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b27">28]</ref> introduced a large real dataset for multi-view synthesis, using camera views around the object. We believe our proposed dataset nevertheless provides unprecedented scale, variety, and quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Ray-Traced Multi-View Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">NViSII</head><p>Recent advances in ray tracing have allowed researchers to generate photo-realistic images with unprecedented speed and quality. Such advancements are accessible via tools integrated within existing rendering frameworks with complex interfaces (e.g., <ref type="bibr" target="#b14">[15]</ref>). In contrast, we propose an accelerated standalone Python-enabled ray tracing / path tracing renderer built on NVIDIA OptiX <ref type="bibr" target="#b57">[57]</ref> with a C++/CUDA backend. The Python API allows for nongraphics experts to easily install the renderer, quickly create 3D scenes with the full power that scripting provides, and is simply installed through pip package management. <ref type="bibr" target="#b0">1</ref> NViSII allows for complex visual material definitions using Disney's principled BSDF <ref type="bibr" target="#b2">[3]</ref> model. This model takes sixteen parameters, whose combinations result in physically-plausible, well-behaved results across their en-  <ref type="table">Table 2</ref>: High-level parameters for the four environments.</p><p>tire range. These parameters, which include color, metallic, transmission, and roughness components, have a perceptually linear effect on the appearance of the BSDF. The linearity property is well-suited to uniform random sampling in order to create a variety of different materials, such as smooth and rough plastics, metals, and dielectrics (e.g., glass and water). We drive these parameters using either scalar values or textures to create a diverse dataset. Our solution also supports advanced rendering capabilities, such as multi-GPU ray tracing, physically-based light controls, accurate camera models (including defocus blur and motion blur), native headless rendering, and exporting various metadata (e.g., segmentation, motion vectors, optical flow, depth, surface normals, and albedo).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data generation</head><p>We use NViSII <ref type="bibr" target="#b50">[50]</ref> to generate 1927 unique scenes in total from 4 different environments, each environment being a collection of 3D models drawn from a different data source (see Tab. 2). For each scene, we normalize it to be contained within a unit cube and generate 150 unique image frames. Ground truth annotations include a depth/distance map, background alpha mask, segmentation mask, scene point cloud, and various metadata (e.g., camera poses, entity poses, material properties, and light positions). Images are exported in the 'EXR' format as it allows for storing colors in a high dynamic range without tonemapping and/or gamma correction pre-applied. Each image is rendered at 4000 samples per pixel. We use 2 types of camera placements: hemisphere and free (e.g., <ref type="figure">Fig. 2</ref>). The former places the camera on the hemisphere surrounding the scene, while the latter moves the camera freely within a cube avoiding collisions with the objects (please refer to the supp. material for the distribution of camera azimuth and elevation over the entire dataset). Overall the dataset is orders of magnitude larger than what is typically used for novel view synthesis. Since it is time consuming for an algorithm like NeRF <ref type="bibr" target="#b49">[49]</ref> to process the entire dataset, we also provide a smaller subset composed of 40 scenes, 10 from each environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Environments</head><p>These 4 different environments are as follows.</p><p>Google Scanned Objects. All the ?1000 Google scanned models <ref type="bibr" target="#b21">[22]</ref> were used to generate 300 falling objects scenes. In each scene, we randomly selected 20 3D models, added collision shapes, and let the objects fall onto each other and onto a table using the PyBullet physics simulator <ref type="bibr" target="#b10">[11]</ref>. We applied a random material to the texture since BRDF materials are not provided with the models (they are mostly plastic-like). Note that the provided textures from these 3D models have light baked into the textures, which can cause the object to look unrealistic, e.g., having highlights where it should be dark. The camera remains at a constant distance from the center and always points toward the center of the scene. The scenes are lit by a single white dome light. This dataset is the most similar to NeRF's synthetic data <ref type="bibr" target="#b49">[49]</ref>. This is also the easiest of our 4 environments: it has a small number of objects and views distributed on the hemisphere.</p><p>ABC. We used all the 3D models (?1.5M) from the ABC dataset <ref type="bibr" target="#b36">[36]</ref>. In each scene, we randomly selected 50 3D models and scaled them so that they are all of a similar size. Similar to the previous environment, we constructed 300 scenes of falling objects using physical simulation. Each object was also given a random BRDF material (plastic, metallic, rough metallic, etc.), and a random bright color, so that this environment has multiple view-dependent materials. The scenes were illuminated by a uniform dome light, and by a bright point light simulating a directional light to produce hard shadows. The camera was allowed to freely move within the unit cube and look at any of the objects, thus producing both close and far images. As a result, this environment has the most challenging viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bricks.</head><p>We downloaded 1027 unique models from Mecabricks. For each model, we generated a single scene where the model was placed in the middle of the scene. The camera was randomly placed on a hemisphere at a fixed distance from the center. The camera was aimed at random locations within 1/10 of the unit volume used to scale the object, thus producing images that are not centered on the model. Each scene was illuminated by a white dome light and a warm sun placed randomly on the horizon. These detailed scenes are challenging due to the variety of textures and presence of self-reflections. This is our largest environment, since it has 1027 scenes.</p><p>Amazon-Berkeley. Similar to ABC and Google Scanned Objects, we loaded 40 3D models from the Amazon Berkeley Objects (ABO) dataset <ref type="bibr" target="#b9">[10]</ref>, scaled them, and let them fall onto a plane. Since the 3D models are accompanied with high-quality textures and material definitions, we light the scene with a full HDRI map and apply a random texture on the floor, both from Poly Haven. Similar to ABC scenes, the camera was allowed to move freely within the unit cube and to look at any object. Also similar to ABC, the materials are quite challenging, such as metallic (mirror-like) objects. We believe this is the most challenging environment of the dataset offering interesting views, materials, textures, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Sparse Voxel Light Field (SVLF)</head><p>Given the size and complexity of our dataset, in this section we present an efficient and lightweight multi-view voxel-based algorithm for novel view synthesis. At a high level, our method learns a voxel-based function that maps a ray r to the optical thickness ? vi 2 and color c vi associated with a particular voxel v i . This mapping is realized by evaluating two small decoder networks. Rendering a single pixel consists of a volumetric integration that amounts to only a handful of network queries, determined by the number of voxels intersected along the ray. This is in contrast to evaluating hundreds of queries along the ray as in NeRF <ref type="bibr" target="#b49">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Representation</head><p>Scene volume. Given depth maps provided with our dataset, we partition the scene into a voxel grid V and represent the space using an octree <ref type="bibr" target="#b71">[71]</ref>. This allows for efficient memory utilization as the octree only represents voxels that contain a surface. We use Z to represent a collection of feature vectors over the feature volume. Each voxel v in the octree stores learnable feature embeddings z v,j ? Z at each of its eight corners (j ? {1, 2, . . . 8}), where feature embeddings are shared between spatially adjacent voxels. In other words, each vertex in the octree is associated with one feature embedding. The feature embedding z p is defined for any point p ? R 3 in the space as z p = ?(p; Z), where ?(.) is the tri-linear interpolation between corner features of the voxel in which p is contained. The collection of feature embeddings Z is learned to encode local properties of each voxel, while the tri-linear interpolation and shared features between adjacent voxels ensure a degree of continuity in the feature space.</p><p>Ray parameterization. Since we aim at learning voxelbased functions, we parameterize a given ray r in the local coordinates of each voxel. Specifically, as shown in <ref type="figure" target="#fig_0">Fig. 3 (a)</ref>, we compute the intersection points p 1 , p 2 ? R 3 of r with the minimum bounding sphere around the voxel, define vectorsp i from the voxel origin to these intersections, then scale them to unit norm:p i =p i / p i , i = 1, 2. This gives us the overparameterized representation r = [p 1 |p 2 ] ? R 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Voxel-based light field function</head><p>The collection Z is split into feature embeddings Z T for optical thickness, and feature embeddings Z C for color. Each of these sub-collections has an associated small decoder network: F T for optical thickness, and F C for color.</p><p>Given a voxel v and a ray r, we first compute the rayvoxel intersection points x 1 , x 2 (see <ref type="figure" target="#fig_0">Fig. 3 (b)</ref>) using an efficient ray-AABB intersection algorithm <ref type="bibr" target="#b43">[43]</ref>. We then estimate the optical thickness and color of the ray in two steps. The first step estimates whether the ray hits a surface within v, and produces an estimate for the location of the hit surface pointx s as a convex combination of x 1 and x 2 . Specifically, we have:</p><formula xml:id="formula_0">?, ? = F T (r, z T ),<label>(1)</label></formula><p>where ? is the estimated optical thickness, ? ? [0, 1] is the mixing factor interpolating between x 1 , x 2 , and</p><formula xml:id="formula_1">z T = [?(x 1 ; Z T ) | ?(x 2 ; Z T )]</formula><p>is the combined interpolated feature embedding at the two intersection points. If ? = 0, the surface pointx s is estimated asx</p><formula xml:id="formula_2">s = ?x 1 + (1 ? ?)x 2 ,</formula><p>where ? can be seen as a within-voxel depth. Otherwise, if ? = 0, the ray does not hit any surface. We estimate surface points to encourage continuous sampling of color features between adjacent voxels. The second step samples color features Z C at the estimated surface pointx s and computes the color value c as where z C = ?(x s ; Z C ) is the interpolated feature embedding atx s (see <ref type="figure" target="#fig_0">Fig. 3 (c)</ref>).</p><formula xml:id="formula_3">c = F C (r, z C ),<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Rendering</head><p>To render a pixel from an SVLF, we perform volume rendering. We first evaluate the color c and optical thickness ? for all voxels intersected by the ray, then we aggregate the color values using alpha compositing.</p><p>Evaluating intersected voxels. We first compute a list of all voxels intersected by the ray. We use the sparse rayoctree intersection algorithm of <ref type="bibr" target="#b71">[71]</ref> that leverages the hierarchical octree structure and parallel scan kernels <ref type="bibr" target="#b46">[46]</ref> for accelerated computation. This step yields x 1 and x 2 for each intersected voxel. We then evaluate the optical thickness ? and color c at each intersected voxel as described in Sec. 4.2. Each voxel is evaluated only once by querying two lightweight decoder networks F T and F C to compute the optical thickness and color, respectively.</p><p>Aggregating color values along the ray. We use alpha compositing to compute the ray color c(r) as:</p><formula xml:id="formula_4">c(r) = N i=1 T vi (1 ? e ??v i )c vi , T vi = j&lt;i e ??v j<label>(3)</label></formula><p>where N is the number of intersected voxels, T vi is the transmittance up until voxel v i , and voxels are indexed by the order of their intersection along the ray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Multi-stage training</head><p>We train SVLF in three stages:</p><p>1. Voxel training: We begin our training using surface rendering. For each ray, we use the corresponding depth to find the voxel containing the surface hit by the ray. We learn local features for each voxel to produce the correct color c, within-voxel depth ? and optical thickness ? using the following loss:</p><formula xml:id="formula_5">L = v c v ? c gt 2 + ? ? ? v ? ? gt 2 + ? ? e ??v 2 (4)</formula><p>where the last term encourages predicting a zero transparency for the ray segment through v.</p><p>2. Volumetric training of optical thickness: Next, we freeze the color features Z C and decoder F C (Eq. 2), and train the optical thickness network to predict the correct integral weights for volume rendering (Eq. 3). We use the same loss function (Eq. 4) except that we compute the photometric loss with the aggregated ray color, c(r), and we set ? ? to zero to turn off direct supervision on the optical thickness ? . We observe that freezing the color network is important for convergence, otherwise the network gets stuck in poor local minima.</p><p>3. Fine-tuning: Finally, we unfreeze the color features Z C and decoder F C , and fine-tune the full model using the same loss function, but with a lowered learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Implementation details</head><p>Architecture. We represent a scene using a sparse octree, constructed at 128 3 voxel resolution. The octree implementation comes from the Kaolin library <ref type="bibr" target="#b31">[31]</ref>. Specifically, we leverage the Structured Point Cloud (SPC) representation    which supports efficient CUDA kernels for ray tracing. We have additionally implemented CUDA kernels for volumetric rendering using efficient scan primitives from CUB <ref type="bibr" target="#b46">[46]</ref>, which we leverage for fast and efficient rendering of our SVLF model. Both the optical thickness and color decoders F T , F C are ReLU MLP networks. We use a single hidden layer for F T and 3 hidden layers for F C , all with size 128. We apply a ReLU activation to the output optical thickness ? , and a sigmoid activation to both the mixing ratio ? and color c outputs. We set the embedding size of the feature collections Z T and Z C to 64 and 32, respectively.</p><p>Training. We run our staged training (Sec. 4.4) for 300 epochs, which takes ?160 minutes on a single NVIDIA V100 GPU. We use the Adam optimizer <ref type="bibr" target="#b34">[34]</ref> with a learning rate of 1e-3 and a batch size of a single image down-scaled to a 400 ? 400 resolution. We train using surface rendering for 100 epochs. We then freeze the weights of the color features Z C and decoder F C and run the volumetric training stage for 150 epochs. Finally, we unfreeze the color weights (Z C , F C ), and continue volumetric training for another 50 epochs with a learning rate of 2e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we benchmark different baselines against our dataset on two main tasks: single scene view synthesis and few-shot view synthesis. We evaluate the view reconstruction quality of baselines using PSNR and SSIM (higher is better) and LPIPS <ref type="bibr" target="#b92">[91]</ref> (lower is better). We also evaluate the reconstructed depth maps using RMSE and MAE (lower is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Single scene view synthesis</head><p>In our dataset, each scene is composed of 150 unique views. We propose to use 100 views for training, 5 views for validation, and 45 views for testing. For this task we evaluate four baselines: NeRF <ref type="bibr" target="#b49">[49]</ref>, mip-NeRF <ref type="bibr" target="#b0">[1]</ref>, the concurrent work of Instant-NGP <ref type="bibr" target="#b51">[51]</ref> and our proposed SVLF. We use the author-provided implementations for NeRF 3 , mip-NeRF 4 , and Instant-NGP 5 and train all methods until convergence. The number of training steps was determined by the saturation of the mean PSNR value over the validation set for a few scenes and then was fixed for all scenes. As NeRF and mip-NeRF have quite large training time, we propose a smaller subset composed of 10 scenes per environment, for a total of 40 scenes. Finally, we mask out the background in the Amazon Berkeley environment as the baselines are not designed to handle complex backgrounds.  <ref type="table">Table 6</ref>: Training and inference time comparison on the single-scene novel view synthesis task. <ref type="table" target="#tab_3">Table 3</ref> presents high-level results for image and depthbased metrics, and <ref type="figure" target="#fig_1">Figure 4</ref> presents qualitative comparisons. All methods perform reasonably well on the Google Scanned Objects and the Bricks environments which have a setup similar to the NeRF Synthetic dataset <ref type="bibr" target="#b49">[49]</ref>. NeRF and SVLF performance degrade on the ABC environment, and none of the baselines performs well on our most challenging environment, Amazon Berkeley, with Instant-NGP being the most robust of the baselines, followed by mip-NeRF as it is trained on multi-scale images. SVLF benefits from depth supervision during training and therefore constructs better depth maps than NeRF and mip-NeRF. In our experiments we have observed that SVLF does not always capture view dependent effects due to a lack of global context in our voxel-based formulation and the absence of an explicit view direction input.</p><p>Finally, we show results using Instant-NGP for all 1927 scenes of the dataset at full resolution in <ref type="table" target="#tab_4">Table 4</ref>. Please refer to the supp. material for more qualitative results and depth map visualizations.</p><p>Runtime comparison. We compare the training and rendering speed of SVLF to the baselines in <ref type="table">Table 6</ref>. Training is done on a single V100 GPU, while inference is done on a single RTX 3090 GPU. SVLF trains an order of magnitude faster than the other baselines as it utilizes depth maps during training. This observation is also consistent with <ref type="bibr" target="#b52">[52]</ref>, a depth-supervised NeRF.</p><p>To compare inference time, we compute the average rendering time over the 45 test images of the first scene of the Google Scanned Objects environment. SVLF is over 80x faster to render than NeRF, and approximately 40x faster than mip-NeRF on our test scene. There are two main reasons for this speedup. First, rendering SVLF requires significantly fewer network queries per ray compared to NeRF. The number of queries is determined by the number of voxels intersected along the ray, which is efficiently computed using the ray-octree intersection algorithm of <ref type="bibr" target="#b71">[71]</ref>. On average, we have less than 10 voxel evaluations per ray on our scenes. Second, the cost per query is much cheaper for SVLF. This is due to moving much of the network capacity to the learned local features, and thus enabling using a much smaller decoder architecture (?16x smaller) than the NeRF network. input views GT PixelNeRF <ref type="bibr" target="#b90">[89]</ref> Goog. Scan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bricks ABC</head><p>Amz. Ber. <ref type="figure">Figure 5</ref>: Sample results of PixelNeRF <ref type="bibr" target="#b90">[89]</ref> on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Few-shot view synthesis</head><p>Given that each environment includes multiple scenes with shared characteristics, camera distributions, object types, and/or lighting, we can use any environment to train few-shot view synthesis algorithms. Different methods <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b90">89]</ref> have shown that a single or few views (&lt; 5) can be used to predict new views. We propose to partition our dataset as follows: 280 scenes for training, 2 scenes for validation, and 18 scenes for testing. Since the Bricks environment is bigger, we propose to use 900 scenes for training, 30 for validation, and 100 for testing. We use the freely available training code <ref type="bibr" target="#b5">6</ref> for PixelNeRF <ref type="bibr" target="#b90">[89]</ref>. We found the algorithm unstable to train, especially when having random camera poses, and eventually diverging after a few hundred epochs. We terminate the training when the model diverges and use the latest proper checkpoint for evaluation. We show qualitative results for a 3-shot input experiment in <ref type="figure">Fig. 5</ref> and report quantitative metrics in <ref type="table" target="#tab_5">Table 5</ref>. Even though the results are encouraging, it is clear that future work is needed in the area of few-shot view synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed RTMV, a large-scale, high quality and ray-traced synthetic dataset for novel view synthesis. Our dataset is orders of magnitude larger than currently used datasets and offers a challenging variety in terms of camera poses, lighting conditions, object material and textures. Thus, RTMV is suitable as a demanding benchmark to evaluate view synthesis algorithms that will help advance novel view synthesis research. We also proposed SVLF, a voxel-based accelerated algorithm for novel view synthesis. SVLF is an order of magnitude faster to train on synthetic images, and two orders of magnitude faster to render than NeRF, while retaining output quality. Finally, we propose a Python-based ray tracing renderer that is easy to use by non-experts and suitable for fast scripting and rendering high-quality physically-based images.  Our RTMV dataset is designed to be more challenging than existing novel view synthesis datasets. In addition to its diversity in terms of scene lighting and objects materials and textures (see <ref type="figure" target="#fig_2">Figure 6</ref>), it contains two types of camera placements: hemisphere and free. The 'hemisphere' camera placement is an easy setup where cameras are placed at an equal distance from the scene (i.e., on a hemisphere). Therefore, trained multi view synthesis models do not need to reason about multi-scale. In the 'free' camera placement, the camera is allowed to roam freely within a cube surrounding the scene, thus providing a variety of multi-scale images of the scene. <ref type="figure" target="#fig_3">Figure 7</ref> shows the distribution of camera azimuth and elevation for each of our four environments.</p><p>Python-based ray tracer. Our Python-based ray-tracing renderer is built on NVIDIA OptiX <ref type="bibr" target="#b57">[57]</ref> with a C++/CUDA backend. It uses path tracing to render high-quality and physically-based images. In addition, it appeals to nongraphics experts through its simple Python API. The scripting capability makes it easy to install, use and share. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Additional qualitative results</head><p>We show more qualitative results on each of our four environments in <ref type="figure" target="#fig_0">Figures 11, 13, 15, 17</ref>, and the corresponding depth maps in <ref type="figure" target="#fig_1">Figures 12, 14, 16, 18</ref>. SVLF trains with depth supervision, and thus constructs better depth maps compared to NeRF and mip-NeRF. Instant-NGP gives the best results but suffers from floating artifacts which can also be seen in the recovered depth maps. All baselines perform reasonably well on the Google Scanned Objects and the Bricks environments. However, the performance of both NeRF <ref type="bibr" target="#b49">[49]</ref> and SVLF drops on the ABC environment due its challenging camera poses and lighting conditions. And eventually, all baselines struggle on our most challenging environment (Amazon Berkeley). The multi-scale training of mip-NeRF <ref type="bibr" target="#b0">[1]</ref> makes it more robust to random camera poses compared to the other baselines, especially on the Amazon Berkeley environment. Instant-NGP suffers from floating artifacts especially with random camera poses (e.g., ABC and Amazon Berkeley environments). But overall, Instant-NGP recovers fine details, renders sharp results, and achieves the best performance on our dataset.</p><p>The voxel-based formulation of SVLF allows for a better representation of the scene that accommodates free-moving cameras. This is because SVLF focuses on rays intersecting voxels, rather than where rays originate or end. In contrast, the near and far render planes parametrization of NeRF and mip-NeRF poses a limitation. On one hand, setting near and far planes to tightly bound the scene leads to better point sampling along the ray during training, but allows for less flexibility for moving the camera at render time. On the other hand, setting the near and far plane to loosely enclose the scene allows for more flexible camera movement at inference time, but leads to poor training quality as NeRF and mip-NeRF are sensitive to how densely points are sampled along the ray. <ref type="figure">Figure 9</ref> highlights this limitation by showing example results for when the camera is placed at very close (first two rows) or far (third row) distance from the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. SVLF limitations</head><p>In this section we discuss some of the limitations of the proposed SVLF approach.</p><p>Voxel resolution. SVLF uses a sparse octree to represent a voxel grid of size 128 3 . While this voxel resolution allows for high quality results over the test set of our scenes, it can produce voxel boundary artifacts if the camera zooms in too close to the scene. Specifically, the seams between voxels become more visible as the camera zooms in (see <ref type="figure">Figure 10</ref>).</p><p>Sub-optimal training of optical thickness. SVLF learns voxel-based functions. Although using a shared decoder as well as the tri-linear interpolation between voxel features provide a degree of continuity between adjacent voxels, voxel features could still suffer from a degree of independence. This especially happens to the optical thickness features, Z T , where the tri-linear interpolations happens at the intersection points, (x 1 , x 2 ), between the ray and the voxel, rather than at the estimated surface pointx s . This could lead to a lack of global context between voxels along the ray, thus resulting in errors in the learned optical thickness ? . <ref type="figure">Figure 10</ref> show example failures where incorrect predictions of the optical thickness ? lead to either floaters or holes in the rendered image. Note the holes inside and beside the red shoe in <ref type="figure">Figure 10</ref>-(a), and random floaters in <ref type="figure">Figure 10</ref>-(a), (b).</p><p>Translucent surfaces. SVLF evaluates the ray color within a voxel at a single point location (the estimated surface hitx s ). Therefore, we assume solid surfaces, and bootstrap the training using surface rendering (Sec. 4.4 of the main paper). While SVLF utilizes a volumetric training stage to learn non-binary optical thicknesses/densities, the support for translucent surfaces remains limited due to sampling a single point within each voxel.</p><p>View-dependent effects. We observe that SVLF does not capture view dependent effects very well (e.g., specular highlight on the vase in <ref type="figure">Figure 10</ref>-column 3.) We hypothesize this is due to the absence of an explicit view direction input to our network. Adding the view direction as an extra input besides the estimated surface hitx s could help mitigate the problem. SVLF mip-NeRF NeRF <ref type="figure">Figure 9</ref>: The render planes parameterization of the baselines can lead to sub-optimal results when the camera moves freely in the space. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>SVLF overview for a given voxel v. (a) Rays are parameterized by the normalized intersection points with the bounding sphere, r = [p 1 |p 2 ], wherep i =p i / p i . (b) SVLF first estimates the optical thickness ? and a within-voxel depth ? to the surface hit, if any. (c) Color features are sampled at the estimated surface pointx s and a color decoder predicts the final ray color c.lighting, and photo-realistic backgrounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Single-scene novel view synthesis results of the baselines on a sample scene from each environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Examples of multiple views from different environments: Google Scanned (1 st row), ABC (2 nd row), Bricks (3 rd row), and Amazon-Berkeley (4 th row). (Best viewed when zoomed.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Camera azimuth and elevation distributions for our different environments A. Appendix A.1. Data generation Camera pose distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 8 shows a simple code example highlighting the simplicity of our Python API.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>A minimal Python script that renders the inset image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :</head><label>101112131415161718</label><figDesc>Example limitations of SVLF. (a) and (b) show occasional floater/hole artifacts due to sub-optimal training of the optical thickness ? . (c) shows seams between voxel boundaries when the camera is too close to the scene, and a lack of specular highlights (figure best seen in zoom). More qualitative results on the Google Scanned Objects environment (figure best seen in zoom). Depth qualitative results on the Google Scanned Objects environment (figure best seen in zoom). More qualitative results on the ABC environment (figure best seen in zoom). Depth qualitative results on the ABC environment (figure best seen in zoom). More qualitative results on the Bricks environment (figure best seen in zoom). Depth qualitative results on the Bricks environment (figure best seen in zoom). More qualitative results on the Amazon Berkeley environment (figure best seen in zoom). Depth qualitative results on the Amazon Berkeley environment (figure best seen in zoom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>NeRFGoog. Scan.<ref type="bibr" target="#b29">29</ref>.627 ? 1.863 0.937 ? 0.026 0.046 ? 0.015 24.101 ? 14.234 43.854 ? 7.523 mip-NeRF Goog. Scan. 31.727 ? 1.681 0.952 ? 0.019 0.032 ? 0.010 18.726 ? 9.791 36.932 ? 7.464 Ins.-NGP Goog. Scan. 32.772 ? 1.985 0.963 ? 0.012 0.017 ? 0.007 1.920 ? 1.542 7.515 ? 2.784 SVLF Goog. Scan. 30.868 ? 1.941 0.954 ? 0.020 0.019 ? 0.009 9.200 ? 4.341 16.844 ? 12.567</figDesc><table><row><cell cols="2">Method</cell><cell>Env.</cell><cell></cell><cell>Image-based metrics</cell><cell></cell><cell>Depth-based metrics</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>LPIPS ?</cell><cell>RMSE (10 3 ) ?</cell><cell>MAE (10 3 ) ?</cell></row><row><cell cols="2">NeRF</cell><cell>ABC</cell><cell cols="4">29.956 ? 4.267 0.947 ? 0.042 0.041 ? 0.043 66.312 ? 146.052 167.650 ? 330.674</cell></row><row><cell cols="2">mip-NeRF</cell><cell>ABC</cell><cell cols="2">32.191 ? 3.328 0.956 ? 0.037</cell><cell cols="2">0.024 ? 0.027 22.497 ? 11.307</cell><cell>41.285 ? 10.008</cell></row><row><cell cols="2">Ins.-NGP</cell><cell>ABC</cell><cell>34.250 ? 4.564</cell><cell cols="2">0.971 ? 0.027 0.025 ? 0.024</cell><cell>2.121 ? 2.798</cell><cell>14.230 ? 5.016</cell></row><row><cell cols="2">SVLF</cell><cell>ABC</cell><cell cols="4">29.998 ? 3.035 0.949 ? 0.044 0.029 ? 0.032 10.238 ? 7.008</cell><cell>18.816 ? 11.844</cell></row><row><cell cols="2">NeRF</cell><cell>Bricks</cell><cell cols="4">28.279 ? 3.363 0.940 ? 0.036 0.041 ? 0.023 60.762 ? 43.947</cell><cell>34.122 ? 30.115</cell></row><row><cell cols="2">mip-NeRF</cell><cell>Bricks</cell><cell cols="4">31.329 ? 3.361 0.956 ? 0.030 0.023 ? 0.014 40.941 ? 29.233</cell><cell>23.717 ? 6.869</cell></row><row><cell cols="2">Ins.-NGP</cell><cell>Bricks</cell><cell>32.255 ? 3.277</cell><cell cols="2">0.970 ? 0.019 0.023 ? 0.013</cell><cell>4.491 ? 3.299</cell><cell>7.977 ? 3.023</cell></row><row><cell cols="2">SVLF</cell><cell>Bricks</cell><cell cols="2">29.244 ? 3.265 0.949 ? 0.032</cell><cell cols="2">0.022 ? 0.014 20.300 ? 13.938</cell><cell>13.869 ? 9.300</cell></row><row><cell cols="2">NeRF</cell><cell>Amz. Ber.</cell><cell cols="4">25.778 ? 3.704 0.796 ? 0.092 0.184 ? 0.108 28.490 ? 192.051</cell><cell>74.041 ? 21.488</cell></row><row><cell cols="2">mip-NeRF</cell><cell>Amz. Ber.</cell><cell cols="2">26.859 ? 3.231 0.784 ? 0.088</cell><cell cols="2">0.141 ? 0.077 14.919 ? 67.881</cell><cell>66.027 ? 17.342</cell></row><row><cell cols="2">Ins.-NGP</cell><cell>Amz. Ber.</cell><cell>25.405 ? 5.657</cell><cell cols="2">0.836 ? 0.089 0.161 ? 0.103</cell><cell>12.693 ? 112.870</cell><cell>36.202 ? 33.896</cell></row><row><cell cols="2">SVLF</cell><cell>Amz. Ber.</cell><cell cols="4">25.197 ? 3.936 0.795 ? 0.103 0.161 ? 0.106 15.305 ? 49.771</cell><cell>38.119 ? 48.792</cell></row><row><cell>NeRF</cell><cell>(0.15 fps)</cell><cell>all</cell><cell cols="4">28.363 ? 3.819 0.905 ? 0.049 0.078 ? 0.046 44.916 ? 99.071</cell><cell>79.917 ? 97.450</cell></row><row><cell cols="2">mip-NeRF (0.3 fps)</cell><cell>all</cell><cell cols="2">30.526 ? 2.900 0.912 ? 0.043</cell><cell cols="2">0.055 ? 0.032 24.271 ? 29.090</cell><cell>41.990 ? 10.421</cell></row><row><cell cols="2">Ins.-NGP (60 fps)</cell><cell>all</cell><cell>31.170 ? 3.871</cell><cell cols="2">0.935 ? 0.037 0.056 ? 0.037</cell><cell>5.306 ? 30.127</cell><cell>16.481 ? 11.180</cell></row><row><cell>SVLF</cell><cell>(12.10 fps)</cell><cell>all</cell><cell cols="4">28.827 ? 3.044 0.912 ? 0.050 0.058 ? 0.040 13.761 ? 18.764</cell><cell>21.912 ? 20.626</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of baseline methods on the 40-scene subset of our dataset at 400 ? 400 resolution. ? 4.882 0.851 ? 0.078 0.254 ? 0.097 17.766 ? 164.130 47.861 ? 53.498 all 30.358 ? 3.334 1.030 ? 0.027 0.077 ? 0.036 4.412 ? 30.355 14.854 ? 13.878</figDesc><table><row><cell>Env.</cell><cell></cell><cell>Image-based metrics</cell><cell></cell><cell cols="2">Depth-based metrics</cell></row><row><cell></cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>LPIPS ?</cell><cell>RMSE (10 3 ) ?</cell><cell>MAE (10 3 ) ?</cell></row><row><cell>Goog. Scan.</cell><cell cols="3">31.269 ? 1.461 1.486 ? 0.014 0.054 ? 0.013</cell><cell>0.634 ? 0.591</cell><cell>6.790 ? 2.829</cell></row><row><cell>ABC</cell><cell cols="3">33.888 ? 3.842 0.975 ? 0.017 0.034 ? 0.024</cell><cell cols="2">1.311 ? 2.075 13.913 ? 5.104</cell></row><row><cell>Bricks</cell><cell cols="3">31.170 ? 3.281 0.966 ? 0.020 0.045 ? 0.029</cell><cell>2.526 ? 8.298</cell><cell>7.863 ? 8.112</cell></row><row><cell>Amz. Ber.</cell><cell>23.129</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results for the full dataset at 1600 ? 1600 res. with Instant-NGP<ref type="bibr" target="#b51">[51]</ref>.</figDesc><table><row><cell>env.</cell><cell>PSNR ?</cell><cell>SSIM ?</cell></row><row><cell>Goog. Scan.</cell><cell>14.588</cell><cell>0.483</cell></row><row><cell>ABC</cell><cell>12.149</cell><cell>0.629</cell></row><row><cell>Bricks</cell><cell>12.149</cell><cell>0.523</cell></row><row><cell>Amz. Ber.</cell><cell>12.126</cell><cell>0.318</cell></row><row><cell>all</cell><cell>12.753</cell><cell>0.488</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Pixel-NeRF results.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">pip install nvisii, see https://nvisii.com/ for extended documentation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Optical thickness is defined, as in<ref type="bibr" target="#b53">[53]</ref>, as the integral of the density along a segment of a ray, so that ? = (1 ? e ??v i ) is the opacity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/bmild/nerf 4 https://github.com/google/mipnerf 5 https://github.com/NVlabs/instant-ngp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/sxyu/pixel-nerf</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Chen-Hsuan Lin for his valuable feedback. MM was partially funded by DARPA SemaFor (HR001119S0085).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13415</idno>
		<title level="m">Mip-NeRF: A multiscale representation for anti-aliasing neural radiance fields</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unstructured Lumigraph rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Extending the Disney BRDF to a BSDF with integrated subsurface scattering. Physically Based Shading in Theory and Practice SIGGRAPH Course</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Burley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pi-Gan</surname></persName>
		</author>
		<idno>arXiv, 2020. 3</idno>
		<title level="m">Periodic implicit generative adversarial networks for 3D-aware image synthesis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth synthesis and local warps for plausible image-based navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Duchene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Drettakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Silhouette-aware warping for image-based rendering. Computer Graphics Forum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Drettakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1223" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MVSNeRF: Fast generalizable radiance field reconstruction from multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anpei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanbo</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5939" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extreme view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inchang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Troccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7781" to="7790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achleshwar</forename><surname>Luthra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas F Yago</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dideriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06199</idno>
		<title level="m">Abo: Dataset and benchmarks for real-world 3d object understanding</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">PyBullet, a Python module for physics simulation for games, robotics and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Bai</surname></persName>
		</author>
		<ptr target="http://pybullet.org" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unity perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Crespi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Annambhotla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hogins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Thaman</surname></persName>
		</author>
		<ptr target="https://blogs.unity3d.com/2020/06/10/.3" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 23rd Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yen-Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05204,2020.3</idno>
		<title level="m">Neural volume rendering: NeRF and beyond</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Denninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Winkelbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Zidan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01911</idno>
		<title level="m">Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam. Blender-Proc</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canaer</forename><surname>Haz?rba?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural radiance flow for 4D view synthesis and video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepview: View synthesis with learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Broxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Du-Vall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Overbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic neural radiance fields for monocular 4D facial avatar reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="8649" to="8658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichang</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Kai</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05903,2020.3</idno>
		<title level="m">Portrait neural radiance fields from a single image</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Google Scanned Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Googleresearch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Open Robotics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radek</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">The</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lumigraph</surname></persName>
		</author>
		<title level="m">SIGGRAPH&apos;96: Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic facial analysis: From Bayesian filtering to recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">SceneNet: Understanding real world indoor scenes with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Viorica P?tr?ucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
		<idno>arXiv 1511.07041</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to predict robot keypoints using artificially generated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Heindl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Zambal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Scharinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1536" to="1539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a neural 3D texture space from 2D exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8356" to="8364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<imprint>
			<pubPlace>Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea Vedaldi, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3D object categories from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="4700" to="4709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dex-NeRF: Using a neural radiance field to grasp transparent objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ichnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahav</forename><surname>Avigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Blender Cycles: Lighting and Rendering Cookbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Iraci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Packt Publishing Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Murthy</forename><surname>Jatavallabhula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Lafleche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><forename type="middle">Fuji</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommy</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05063</idno>
		<title level="m">Rev Lebaredian, and Sanja Fidler. Kaolin: A PyTorch library for accelerating 3D deep learning research</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large scale multi-view stereopsis evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ABC: A big CAD model dataset for geometric deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Matveev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongshi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Alexa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Panozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winson</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Vanderbilt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Herrasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05474</idno>
		<title level="m">AI2-THOR: An interactive 3D environment for visual AI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CosyPose: Consistent multi-view multi-object 6D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural scene flow fields for space-time view synthesis of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6498" to="6508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Crowdsampling the plenoptic function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="178" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyaw Zaw</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Schwartz</surname></persName>
		</author>
		<idno>65:1-65:14</idno>
	</analytic>
	<monogr>
		<title level="m">Learning dynamic renderable volumes from images</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
	<note>ACM Trans. Graph.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A ray-box intersection algorithm and efficient dynamic voxel rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Majercik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Crassin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Mcguire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Graphics Techniques</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="66" to="81" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the wild: Neural radiance fields for unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noha</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">CUB: A library of warp-wide, block-wide, and device-wide GPU parallel primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duane</forename><surname>Merrill</surname></persName>
		</author>
		<ptr target="https://nvlabs.github.io/cub/.6,7" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3D reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Local light field fusion: Practical view synthesis with prescriptive sampling guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><forename type="middle">Khademi</forename><surname>Ortiz-Cayon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nerf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08934</idno>
		<title level="m">Representing scenes as neural radiance fields for view syn</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">thesis. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">NViSII: A scriptable tool for photorealistic image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Morrical</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Synthetic Data Generation</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Instant neural graphics primitives with a multiresolution hash encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2022-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Stadlbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Parger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kurz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joerg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Alla</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><forename type="middle">S</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kaplanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monte Carlo methods for volumetric light transport simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Nov?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iliyan</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hanika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Jarosz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="551" to="576" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">UNISURF: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">DeepSDF: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<title level="m">Nerfies: Deformable neural radiance fields. ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Op-tiX: A General Purpose Ray Tracing Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Hoberock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luebke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Robison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Soft 3D reconstruction for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Penner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Neural Radiance Fields for Dynamic Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enric</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. D-Nerf</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">DeRF: Decomposed radiance fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rebain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14153" to="14161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Common objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Sbordone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Free view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="623" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">EXPO-HD: Exact object perception using high distraction synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Elbaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14354,2020.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Clear-Grasp: 3D shape estimation of transparent objects for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shreeyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Sajjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">GRAF: Generative radiance fields for 3D-aware image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Piecewise planar stereo for image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipta</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Steedly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Scene representation networks: Continuous 3D-structure-aware neural scene representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of view extrapolation with multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Neural geometric level of detail: Real-time rendering with implicit 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Litalien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangxue</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Loop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learned initializations for optimizing coordinate-based neural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divi</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2846" to="2855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Goldman, and Michael Zollh?fer. Advances in neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><forename type="middle">K</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2021 Courses</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">NDDS: NVIDIA deep learning dataset synthesizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukie</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirby</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Balanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/Dataset_Synthesizer.3" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Training deep networks with synthetic data: Bridging the reality gap by domain randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Brophy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Autonomous Driving (WAD)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Falling things: A synthetic dataset for 3D object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakumar</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CoRL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">GRF: Learning a general radiance field for 3D scene representation and rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Trevithick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04595,2020.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2626" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<pubPlace>Jonathan T. Barron, Ricardo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Ibrnet: Learning multi-view image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Fake it till you make it: Face analysis in the wild using synthetic data alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erroll</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Dziadzio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Estellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Rendering of eyes for eye-shape registration and gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erroll</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Space-time neural irradiance fields for free-viewpoint video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changil</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9421" to="9431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">SAPIEN: A simulated part-based interactive environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanbo</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhe</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikuan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11097" to="11107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04112</idno>
		<title level="m">Robi: A multi-view dataset for reflective objects in robotic bin-picking</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">BlendedMVS: A large-scale dataset for generalized multi-view stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">PlenOctrees for real-time rendering of neural radiance fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vickie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<title level="m">Neural radiance fields from one or few images</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07492,2020.3</idno>
		<title level="m">NeRF++: Analyzing and improving neural radiance fields</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">UnrealStereo: A synthetic dataset for analyzing stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04647</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
