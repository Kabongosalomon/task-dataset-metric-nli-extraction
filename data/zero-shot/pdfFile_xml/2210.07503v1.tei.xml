<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasom</forename><surname>Ahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering</orgName>
								<orgName type="institution">Keimyung University</orgName>
								<address>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwon</forename><surname>Kim</surname></persName>
							<email>eddiesangwonkim@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering</orgName>
								<orgName type="institution">Keimyung University</orgName>
								<address>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsu</forename><surname>Hong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Difine</orgName>
								<address>
									<settlement>Seongnam</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung</forename><forename type="middle">Chul</forename><surname>Ko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering</orgName>
								<orgName type="institution">Keimyung University</orgName>
								<address>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In action recognition, although the combination of spatio-temporal videos and skeleton features can improve the recognition performance, a separate model and balancing feature representation for cross-modal data are required. To solve these problems, we propose Spatio-TemporAl cRoss (STAR)-transformer, which can effectively represent two cross-modal features as a recognizable vector. First, from the input video and skeleton sequence, video frames are output as global grid tokens and skeletons are output as joint map tokens, respectively. These tokens are then aggregated into multi-class tokens and input into STAR-transformer. The STAR-transformer encoder consists of a full spatio-temporal attention (FAttn) module and a proposed zigzag spatio-temporal attention (ZAttn) module. Similarly, the continuous decoder consists of a FAttn module and a proposed binary spatio-temporal attention (BAttn) module. STAR-transformer learns an efficient multi-feature representation of the spatio-temporal features by properly arranging pairings of the FAttn, ZAttn, and BAttn modules. Experimental results on the Penn-Action, NTU-RGB+D 60, and 120 datasets show that the proposed method achieves a promising improvement in performance in comparison to previous state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition is a traditional research topic that classifies human actions using video frames and has been applied in various applications, including human-robot interaction <ref type="bibr" target="#b2">[3]</ref>, healthcare <ref type="bibr" target="#b35">[36]</ref>, and video surveillance <ref type="bibr" target="#b17">[18]</ref>. With the recent development of deep learning, action recognition research trends have been divided into three approaches. First, in a video-based approach <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21]</ref>, deep learning models use only video frames to <ref type="bibr">Figure 1</ref>. The proposed STAR-transformer takes a multi-class token as input and transforms it into a class-separable multi-feature representation. The multi-class token is an aggregation of global grid and joint map tokens obtained by feeding video and pose sequences into a shared CNN. recognize the action. This approach results in a significant degradation in performance owing to various noises from the wild, such as differences in the camera angles and sizes of the human targets and complex backgrounds. The second is a skeleton-based approach <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19]</ref>. Actions are recognized using human skeletons and joint trajectories in different time zones as inputs into the deep learning model. However, an additional deep learning model is required to extract the human skeleton from an image. In addition, the action recognition is largely dependent on the accuracy of the skeleton extractor and the degree of overlap of the skeleton. The third approach is the use of cross-modal data, video, and skeletons together <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39]</ref>. A deep learning model learns the RGB of the video frames and human skeletal features together; thus, it generally shows a high recognition performance. However, combining video and skeleton data is an ambiguous process and requires a separate submodel for cross-modal learning.</p><p>As a new learning paradigm in the deep learning field, Vision Transformer (ViT) <ref type="bibr" target="#b15">[16]</ref> has recently attracted attention owing to its excellent performance in various computer vision fields such as image classification <ref type="bibr" target="#b25">[26]</ref>, image segmentation <ref type="bibr" target="#b47">[48]</ref>, object tracking <ref type="bibr" target="#b51">[52]</ref>, and action recognition <ref type="bibr" target="#b8">[9]</ref>. The self-attention mechanism, which is a key element of ViT, is specialized for determining the spatial relation-arXiv:2210.07503v1 [cs.CV] <ref type="bibr" target="#b13">14</ref> Oct 2022 ship of each image and is effectively applied to image classification. However, in action recognition, the features of long-range frames and multiple features that change over time must both be considered; therefore, ViT based on the existing multi-head attention mechanism has limitations in terms of a high computational cost <ref type="bibr" target="#b26">[27]</ref>.</p><p>In this study, we propose a multi-feature representation method based on cross-modal learning and a Spatio-TemporAl cRoss transformer (STAR-transformer) attention mechanism. For cross-modal learning, we propose a method of aggregating the cross-modal data of spatiotemporal video and a skeleton into a multi-class token to solve the problem of combining cross-modal action data. STAR-transformer consists of a new cross-attention module that replaces the multi-head attention of a simple ViT. The proposed STAR-transformer has demonstrated an excellent performance through various experiments. <ref type="figure">Figure 1</ref> shows the overall operational structure of STAR-transformer. Two cross-modal features are fed into the shared convolutional neural network (CNN) model and separated into multi-class tokens. STAR-transformer consists of an L-layer encoder-decoder output separable multiclass feature, which is used as input for the downstream action recognition network.</p><p>The contributions of this paper can be summarized as follows.</p><p>? Cross-modal learning: It is possible to flexibly aggregate spatio-temporal skeleton features as well as video frames and effectively learn cross-modal data to create multi-class tokens.</p><p>? STAR-transformer: The existing self-attention mechanism is limited to the application of action recognition because it focuses on the relation between spatial features. We therefore propose a STAR attention mechanism that can learn cross-modal features. The encoder and decoder of STAR-transformer are composed of zigzag and binary skip STAR attention.</p><p>? Various performance evaluation experiments: A performance evaluation was conducted based on several benchmark datasets, and the proposed model showed a better performance than existing state-of-the-art (SoTA) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video and image-based action recognition : It aims to recognize actions using only sequential <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19]</ref> or still images <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>. The general process of videobased action recognition involves breaking the action into smaller semantic components and understanding the importance of each component in action recognition <ref type="bibr" target="#b20">[21]</ref>. Because this method uses video frames, it can be processed using a simple single model. However, if the video is long, the recognition speed will be slow and the performance will be significantly affected by the various noises from the wild.</p><p>Skeleton-based action recognition: It aims to recognize actions by applying a list of spatio-temporal joint coordinates of video frames extracted from a pose estimator to a graph convolutional network (GCN) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7]</ref>, 3D-CNN <ref type="bibr" target="#b16">[17]</ref>, and CNN <ref type="bibr" target="#b5">[6]</ref>. A skeleton sequence has an advantage of being unaffected by contextual disturbances such as changes in background and lighting <ref type="bibr" target="#b16">[17]</ref> but has a disadvantage in that the recognition performance is largely dependent on the pose extractor and requires an extra classifier for recognition.</p><p>Video and skeleton-based action recognition: It aims to achieve a high action recognition performance by fusing multi-modal (cross-modal) information into an integrated set of discriminative features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref>. The videopose network (VPN) action recognition mechanism <ref type="bibr" target="#b13">[14]</ref>, which uses cross-modal features and knowledge distillation to infuse poses into RGB streams, has proven that crossmodal features can achieve a better performance than unimodal features. Despite its relatively high recognition performance, this method still has problems in the design of a subnetwork for cross-modal learning and methods for combining cross-modal data.</p><p>Transformer-based action recognition: Because a transformer is a powerful tool in terms of long-range temporal modeling when using a self-attention module <ref type="bibr" target="#b1">[2]</ref>, an increasing number of studies in this area, particularly action recognition, have been conducted <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42]</ref>. Most action recognition approaches using a transformer apply video frames as input tokens <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b32">33]</ref>, and relatively few methods use the skeleton <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33]</ref> of the transformer. However, transformer-based action recognition often suffers from high computational costs owing to self-attention given to the large number of 3D tokens in a video <ref type="bibr" target="#b26">[27]</ref>. Moreover, an approach to coupling cross-modal information using a transformer has yet to be developed. Therefore, this study is the first attempt at using spatiotemporal cross-modal data as input tokens for ViTs without applying separate sub-models. <ref type="figure" target="#fig_0">Figure 2</ref> shows the overall structure of the proposed action-recognition model based on the STAR-transformer module. Sixteen video frames and the corresponding skeleton sequences were received as input. Each frame goes through the pre-trained ResNet mixed convolution 18 (MC18) <ref type="bibr" target="#b42">[43]</ref> to extract local and global feature maps. ResNet MC18 models are unsuitable for the proposed zigzag and binary operations because they reduce the video frame size after an operation. The global feature map, which is the output of the last layer of ResNet MC18, is transformed into a global grid token (GG-token) that represents the visual features of the images <ref type="figure" target="#fig_0">(Fig. 2 (a)</ref>). The local feature map, the output of the middle layer of ResNet MC18, is combined with the joint heat map ( <ref type="figure" target="#fig_0">Fig. 2 (b)</ref>) and then transformed into a joint map token (JM-token), as shown in <ref type="figure" target="#fig_0">Fig. 2</ref> (c). The JM tokens represent the local features of each skeleton joint. The two tokens are aggregated into a multi-class token and then fed into STARtransformer, as shown in <ref type="figure" target="#fig_0">Fig. 2 (d)</ref>, to infer the final action label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cross-Modal Learning</head><p>We first propose a cross-modal learning method that can combine video frames and skeleton features. The video frames are fed to ResNet MC18, and two feature maps are extracted from the middle and last layers. Because the feature map of the middle layer contains more detailed local features than the last layer, it is used for JM-token extraction, and the last layer is applied for GG-token extraction.</p><p>Global grid token (GG-token): Let a GG-token T g consisting of P tokens be T t g = {g t 1 , ..., g t P } in video frame t. To extract the element of GG-token g t p from the t-th frame, the input frame is adjusted to a size of 224?224, and the global feature map generated through ResNet MC18 has a size of h ? w . The global feature map is again flattened into a vector of size hw (P ), which becomes the number of elements of T t g . Because a global feature map consists of C channels, the number of dimensions of each element of T t g is g t P ? R C . This process continues for every video frame, and thus we can obtain T temporal GG-tokens, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a).</head><p>Joint map token (JM-token): In each t-th pose corresponding to the t-th video frame, we obtain N joint heat maps emphasizing joints for each frame and JM-token set  </p><formula xml:id="formula_0">maps F ? R C ? ?h ? ?w ? of ResNet MC18 are obtained. The n-th joint heat map h n ? R h ? ?w ?</formula><p>is the result of projecting the n-th joint onto a temporary map with a size of h ? ? w ? and applying Gaussian blurring at a scale of ?. Because a local feature map consists of C ? channels, the number of dimensions of each joint element of T t j is j t N ? R C ? . The joint element j t n on the t-th pose is obtained through the concatenation (?) of the local feature map F and the n-th joint heat map h t n , as shown in the following equation:</p><formula xml:id="formula_1">T t j,n = ? C ? c ? =1 ( h ? i w ? j F c ? (i, j) ? h t n (i, j)).<label>(1)</label></formula><p>This process continues for every pose sequence, and thus we can obtain T temporal JM-tokens, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref> (c).  Multi-class token: To aggregate the GG-and JM-tokens generated using cross-modal data, we propose a multi-class token aggregation, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Pure ViT <ref type="bibr" target="#b15">[16]</ref>, shown in <ref type="figure" target="#fig_2">Fig. 3</ref> (a), focuses on learning the global relationships between the input tokens using single-class tokens. However, the proposed action recognition model must cooperatively learn the multi-class tokens generated from the crossdomain data. The proposed aggregation method of multiclass tokens therefore effectively learns the characteristics of different feature representations, as shown in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>.</p><p>Multi-class token Z is created by concatenating (?) the class tokens for a GG-token (CLS glob ) and JM-token (CLS joint ) as follows:</p><formula xml:id="formula_2">T g = CLS glob ? T g ,<label>(2)</label></formula><formula xml:id="formula_3">T j = CLS joint ? (T j + pos),<label>(3)</label></formula><formula xml:id="formula_4">Z = T g ? T j ? CLS total<label>(4)</label></formula><p>where CLS total is the class token for all tokens. Unlike a GG-token, with a JM-token, the joint position information pos is important, and thus pos is only added to the JMtoken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatio-temporal cross attention</head><p>Inspired by <ref type="bibr" target="#b1">[2]</ref>, we first propose full spatio-temporal attention (FAttn), as shown in <ref type="figure" target="#fig_4">Fig. 4 (a)</ref>, which applies the attention mechanism for all tokens within the spatiotemporal dimension. When FAttn is applied to all tokens of time dimension T and spatial dimension S, the complexity increases to O(T 2 S 2 ). However, because FAttn alone is insufficient to handle spatio-temporal features, we propose two additional cross-attention mechanisms, i.e., zigzag spatio-temporal attention (ZAttn), as shown in <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>, and binary spatio-temporal attention (BAttn), as shown in <ref type="figure" target="#fig_4">Fig. 4 (c)</ref>. There is no need to pay attention to all tokens of time dimension T . Instead, all tokens are divided into two token groups for ZAttn and BAttn. When ZAttn and BAttn are applied to all tokens of the time dimension T and spatial dimension S, the computational complexity is reduced by 0.25-fold in comparison to FAttn with O( 1 4 T 2 S 2 ) because the tokens in the time dimension are divided into two groups.</p><p>We first obtain the same-sized query (Q), key (K ), and value (V ) ? R S ?T matrices from the multi-class token Z and compute the FAttn outputs as follows:</p><formula xml:id="formula_5">z = FAttn(Q, K, V )<label>(5)</label></formula><formula xml:id="formula_6">FAttn(Q, K, V ) = T t S s Softmax Q s,t ? K s,t ? d h V s,t<label>(6)</label></formula><p>ZAttn learns the detailed process of changing actions. To calculate ZAttn, the odd-numbered vectors in Z are divided into ZQ ? ? R S?T /2 , and the even-numbered vectors in Z are divided into ZK ? and ZV ? ? R S?T /2 in a zigzag manner, as shown in <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>. By contrast, the odd-numbered tokens in Z are divided into ZK ?? and ZV ?? ? R S?T /2 , and the even-numbered vectors in Z are divided into ZQ ?? ? R S?T /2 .</p><p>We calculate a ? ? R S?T /2 and a ?? ? R S?T /2 individually using the two types of matrices extracted in a zigzag manner using the following formulas, and then concatenate the outputs a ? and a ?? as the result of ZAttn.</p><formula xml:id="formula_7">a ? = T /2 t S s Softmax ZQ ? s,t ? ZK ? s,t ? d h ZV ? s,t<label>(7)</label></formula><formula xml:id="formula_8">a ?? = T /2 t S s Softmax ZQ ?? s,t ? ZK ?? s,t ? d h ZV ?? s,t<label>(8)</label></formula><formula xml:id="formula_9">ZAttn(Q, K, V ) = a ? ? a ??<label>(9)</label></formula><p>BAttn is also generated into two groups by dividing the time-dimensional tokens back and forth, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref> (c). Through this process, it is possible to learn the change at the beginning and end of the action. In the case of BAttn, after dividing Z into two groups in a binary manner, the front and rear vectors, BQ ? ? R S?T /2 and BK ? , respectively, and BV ? ? R S?T /2 matrices are calculated. By contrast, the front vectors in Z are divided into BK ?? and BV ?? ? R S?T /2 , and the rear vectors in Z are divided into BQ ?? ? R S?T /2 . We calculate the individual b ? ? R S?T /2 and b ?? ? R S?T /2 using the two types of matrices with the same formula of ZAttn, and concatenate the output b ? and b ?? as the result of BAttn.</p><formula xml:id="formula_10">BAttn(Q, K, V ) = b ? ? b ??<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">STAR-transformer encoder and decoder</head><p>The proposed STAR-transformer follows a encoder-decoder structure of pure transformer <ref type="bibr" target="#b43">[44]</ref> than pure ViT <ref type="bibr" target="#b15">[16]</ref>, as shown in <ref type="figure" target="#fig_0">Fig. 2 (e)</ref>. However, the encoder is composed of a series of FAttn (self-attention) and ZAttn L layers, and the decoder is composed of a series of FAttn and BAttn layers. The encoder uses ZAttn to focus on the learning relationships for detailed changes in action, and the decoders use BAttn to learn the relationships for large changes in action.</p><p>The structure of the STAR-transformer layer is as follows.</p><formula xml:id="formula_11">z l = LN{FSTA(z l?1 ) + z l?1 }, l ? {1, 2, ..., L} (11) z ? l , z ?? l = Decoupling(z l )<label>(12)</label></formula><formula xml:id="formula_12">z l = LN{(STA(z ? l ) + z ? l ) ? (STA(z ?? l ) + z ?? l )} (13) z l = LN{MLP(z l ) +z l }<label>(14)</label></formula><p>Here, l is the number of transformer layers, LN is the layer normalization, and FSTA is the multi-head selfattention for FAttn. Decoupling refers to zigzag or binary grouping. STA represents spatio-temporal attention for ZAttn and BAttn, and MLP is a multi-layer perceptron.</p><p>The multi-class tokens output by STAR-transformer are combined into a single class token by averaging and feeding into the MLP to infer the final action label. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we describe the implementation details, including the dataset and training hyperparameters applied. After conducting a quantitative analysis based on SoTA approaches, ablation studies and a qualitative analysis were applied on the effectiveness of multi-expression learning, the number of transformer layers, and spatio-temporal cross attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Dataset Description: The experiment was conducted using the representative action recognition datasets, Penn-Action <ref type="bibr" target="#b53">[54]</ref>, NTU-RGB+D 60 <ref type="bibr" target="#b36">[37]</ref>, and 120 <ref type="bibr" target="#b27">[28]</ref>. The Penn-Action dataset includes 15 different action classes, such as baseball swings, jumping jacks, and pushups, for a total of 2,326 RGB video sequences. The NTU-RGB+D 60 dataset is a large dataset used for human action recognition containing 56,880 samples of 60 action classes collected from 40 subjects. Actions are divided into three categories having 40 daily actions (e.g., drinking, eating, and reading), 9 health-related actions (e.g., sneezing, staggering, and falling), and 11 mutual actions (e.g., punching, kicking, and hugging), respectively, based on multi-modal information of the action characterization, including depth maps, 3D skeletal joint positions, RGB frames, and infrared sequences. NTU-RGB+D 60 has two evaluation protocols, cross-subject (XSub) and cross-view (XView). NTU-RGB+D 120 extends this version of NTU-RGB+D 60 by adding another 60 classes and containing 114,480 samples in total. NTU-RGB+D 120 has two evaluation protocols, XSub and cross-setup (XSet).</p><p>Implementation details:</p><p>The proposed STARtransformer was implemented using PyTorch, and ResNet MC18 pre-trained with Kinetics-400 was applied as the backbone network. When training the model, the Penn-Action and NTU-RGB+D datasets used 16 fixed frames. For all datasets, we utilized a batch size of 4, 300 epochs, an stochastic gradient descent (SGD) optimizer, a learning rate of 2e-4, and a momentum of 0.9. The experiments were conducted in an environment configured with four NVIDIA Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art Methods</head><p>Penn-Action Dataset: <ref type="table">Table 1</ref> shows the results of the comparison experiments with other SoTA action recognition technologies for the Penn-Action dataset: 1) body joint guided 3D deep convolutional descriptors (3D Deep) <ref type="bibr" target="#b7">[8]</ref>, 2) evolution of pose estimation maps (EV-Pose) <ref type="bibr" target="#b28">[29]</ref>, 3) multitask CNN <ref type="bibr" target="#b31">[32]</ref>, 4) Bayesian hierarchical dynamic model (HDM-BG) <ref type="bibr" target="#b54">[55]</ref>, 5) view-invariant probabilistic embedding (Pr-VIPE) <ref type="bibr" target="#b39">[40]</ref>, and 6) a unified framework for skeletonbased action recognition (UNIK) <ref type="bibr" target="#b48">[49]</ref>. UNIK <ref type="bibr" target="#b48">[49]</ref> was pretrained using the Posetics dataset reconstructed from the Kinect-400 <ref type="bibr" target="#b24">[25]</ref> dataset, and Pr-VIPE <ref type="bibr" target="#b39">[40]</ref> was pretrained using the Human3.6M dataset <ref type="bibr" target="#b23">[24]</ref>. STAR-transformer and the other methods were trained and tested only on the given data, without any pre-training.</p><p>The pre-trained UNIK <ref type="bibr" target="#b48">[49]</ref> model showed a 0.8% lower accuracy than the proposed model at 97.9%, and the Pr-VIPE [40] model showed 97.5% accuracy, which is 1.2% lower than that of the proposed model.</p><p>During the experiment, STAR-transformer and the three methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref> using the RGB of the video frames and the pose (skeleton) feature together showed a high overall performance of 98% or higher. However, the three methods <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref> using only the pose feature showed a relatively low performance of 93% to 97%. As the results in <ref type="table">Table 1</ref> indicate, we can confirm that the action recognition performance can be improved when the RGB of the video frames and pose features are used together. Although STAR-transformer did not use any pre-training, the highest accuracy was derived through the proposed cross-attention using the cross-modal features together. <ref type="table">Table 2</ref> shows the results of the comparison experiments with SoTA action recognition technologies when applying the NTU-RGB+D dataset: 1) long-term localization using 3D LiDARs (PoseMap) <ref type="bibr" target="#b28">[29]</ref>, 2) multimodal transfer module (MMTM) <ref type="bibr" target="#b34">[35]</ref>, 3) videopose embedding (VPN) <ref type="bibr" target="#b13">[14]</ref>, 4) multi-granular spatiotemporal graph network (DualHead-Net) <ref type="bibr" target="#b9">[10]</ref>, 5) skeletal graph neural networks (Skeletal GNN) <ref type="bibr" target="#b50">[51]</ref>, 6) channelwise topology refinement GCN (CTR-GCN) <ref type="bibr" target="#b10">[11]</ref>, 7) information bottleneck-based GCN (InfoGCN) <ref type="bibr" target="#b12">[13]</ref>, 8) contrastive learning (3s-AimCLR) <ref type="bibr" target="#b22">[23]</ref>, 9) 3D skeleton and heatmap stack (PoseC3D) <ref type="bibr" target="#b16">[17]</ref>, and 10) kernel attention adaptive graph transformer network (KA-AGTN) <ref type="bibr" target="#b30">[31]</ref>. Because a transformer-based action recognition method that uses RGB of the video frames and cross-modal features of skeleton together has not yet been published, we compare the performance with KA-AGTN, a SoTA for skeleton and transformer-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTU-RGB+D Dataset:</head><p>During this experiment, the accuracy was measured separately for the NTU-RGB+D 60 and NTU-RGB+D 120 datasets, and the cross-subject (XSub), cross-view (XView), and cross-setup (XSet) were measured separately for each dataset. The performances of four methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29]</ref> using the RGB of the video frames and pose together, and six methods using only the pose <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref>, were compared with STAR-transformer. Pre-training was conducted using only 3s-AimCLR <ref type="bibr" target="#b22">[23]</ref>. As shown in Table 2, the accuracy was higher for NTU60 and NTU120 when the cross-modal features of the RGB and pose were used together than when a unimodal feature was applied. PoseC3D <ref type="bibr" target="#b16">[17]</ref> performed 5% better on NTU60 XSub and 3.7% better on NTU120 XSet than the proposed STARtransformer because PoseC3D did not use annotated poses but applied a separate pre-trained poseConv3D model for 3D pose estimation to achieve better action recognition. As Multi-Class token Accuracy(%) ? 97.3 ? 98.7 <ref type="table">Table 3</ref>. Effectiveness of multi-class token.</p><p>the results indicate, PoseC3D achieved a relatively high accuracy because it extracted the optimal pose features suitable for its own model and used them for learning. However, this method still has certain disadvantages in that it requires a pre-trained model for additional pose detection, and the pose detection and action recognition models cannot be trained end-to-end as a single model. KA-AGTN <ref type="bibr" target="#b30">[31]</ref> used a transformer structure as in our method. However, because it uses only skeleton information and the transformer is used only for spatial information processing between joints, the performance is inferior to the proposed method by up to 1.6% for NTU60 and up to 4.7% for NTU120.</p><p>Although transformers need to be pre-trained using a large dataset, the proposed STAR-transformer combines the RGB and annotated poses without any pre-training, achieving a promising accuracy even on a larger class dataset NTU120. In particular, NTU120 XSub and XSet showed the second highest performance with accuracy rates of 90.3% and 92.7%, respectively. This indicates that STARtransformer is capable of an excellent action recognition, although the action class is increased or the cross view is changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, the detailed performance of the modules constituting the proposed STAR transformer model is verified based on several experiments. All experiments were conducted using the Penn-Action dataset.</p><p>Effectiveness of multi-expression learning: To confirm the effect of the proposed multi-expression learning, <ref type="table">Table 3</ref> presents a comparison experiment conducted with a single-class token used in pure ViT <ref type="bibr" target="#b15">[16]</ref> and the multi-class token proposed in this study. The proposed multi-class token performed 1.4% higher than a single-class token. Although the existing single-class tokens did not effectively conduct learning between the cross-modal tokens, it was confirmed that the proposed multi-class token can effectively increase the performance of the model under the same cross-modal condition.</p><p>Effectiveness of the number of transformer layers: <ref type="figure" target="#fig_5">Figure 5</ref> shows the difference in performance according to the number of transformer layers for the proposed spatiotemporal cross-attention module structure. As shown in <ref type="figure" target="#fig_5">Fig. 5</ref>, the overall performance improves as the number of layers increases; however, when there are more than four Attention Struture Accuracy (%) Encoder Decoder layers, the model is easily overfitted. Therefore, based on the experimental results, we set the number of transformer layers to three.</p><formula xml:id="formula_13">F-F F-F 96.1 F-Z F-Z 97.3 F-B F-B 97.8 F-B F-Z 97.3 F-Z F-B 98.7</formula><p>Effectiveness of spatio-temporal cross attention: <ref type="figure" target="#fig_7">Figure 6</ref> shows the relative frame importance score for the spatio-temporal cross-attention mechanism proposed in this study. The scores were calculated using an attention rollout <ref type="bibr" target="#b0">[1]</ref> to calculate the relative concentration for each frame. The attention rollout recursively receives the embedding attention as the input for each layer of the transformer model and computes the token attention.</p><p>In <ref type="figure" target="#fig_7">Fig. 6 (a)</ref>, FAttn, which has a structure similar to that of ViTs, shows a high score in the last frame, indicating that the front frames are barely considered in action recognition. Here, only the final top-three frames of after throwing the ball significantly contributed to the performance, and thus we can see that FAttn does not consider the overall temporal characteristics.</p><p>In the case of ZAttn and BAttn, which are spatiotemporal cross-attention mechanisms, the importance scores are equally high in all frames, as shown in <ref type="figure" target="#fig_7">Fig. 6 (b)</ref> and (c). When checking the top-three frames of ZAttn and BAttn, sequentially varied frames such as before throwing the ball, while throwing the ball, and after throwing the  When only full spatio-temporal attention is used, the attention score appears high at the end of the action. In the case of zigzag spatiotemporal attention, high attention scores were obtained in the middle and last frames when the action was large. In the case of binary spatio-temporal attention, a high attention score appears in the entire frame of an action.</p><p>ball are considered for a performance improvement.</p><p>Difference in accuracy of cross attention modules: <ref type="table" target="#tab_2">Table 4</ref> shows the differences in accuracy according to the structure of the spatio-temporal cross-attention module. Based on the experimental results, we can see that when ZAttn (Z) and BAttn (B) are used together, the performance is higher than when FAttn is used alone. When FAttn and BAttn were used equally for the encoder and decoder (F-B, F-B), the second highest accuracy was achieved at 97.8%; however, the importance of the entire frame was still not accurately reflected, and thus the performance was slightly lower than that of the F-Z and F-B combinations. The combination of F-B and F-Z, in which BAttn is applied to the encoder and ZAttn is applied to the decoder, showed the second-lowest performance at 97.3%. Based on the experimental results, we used F-Z as the encoder and F-B as the decoder.</p><p>Through these experimental results, we can see that for an accurate action recognition, it is necessary to learn the frame characteristics evenly in all frames through the proposed spatio-temporal cross attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed STAR-transformer, an algorithm based on a spatial-temporal cross-attention module that simultaneously uses video frames and skeleton-based features for action recognition. In addition, the proposed multi-feature representation learning approach was able to flexibly combine the RGB of the video frames, skeleton, and joint trajectories using multi-class tokens. As a result of testing the proposed algorithm using the Penn-Action and NTU-RGB+D action datasets, it was confirmed that the proposed STAR-transformer model achieved substantial improvements in comparison to previous SoTA methods. In a future study, we plan to develop an algorithm that can efficiently learn a model without an overfitting, even with a small number of data. In addition, by extending the proposed STAR-transformer to a model that combines a pose estimation rather than annotated poses, we will modify the STAR-transformer into an end-to-end model that can simultaneously apply pose feature estimation and action recognition optimized for action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of the proposed action recognition model. (a) global grid token, (b) joint heat map, (c) joint map token, (d) STAR-transformer module, and (e) encoder and decoder structure of STAR transformer. An encoder and a decoder have the same structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>T t j = {j t 1 , ..., j t N } based on such maps. First, local feature Transformer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Multi-class token aggregation. (a) single class token generation of pure ViT and (b) proposed multi-token aggregation based on cross-modal learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Proposed spatio-temporal cross attention modules. (a) full spatio-temporal attention (FAttn), (b) zigzag spatio-temporal attention (ZAttn), and (c) binary spatio-temporal attention (BAttn) modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Variation in accuracy according to the number of spatiotemporal cross attention layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>The relative importance score of 16 input frames of a validation video. The bar graph shows the attention score for each frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Difference in accuracy between spatio-temporal crossattention mechanisms. F represents FAttn, Z indicates ZAttn, and B represents the BAttn.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="4190" to="4197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition for human-robot interaction using self-attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Glimpse clouds: Human activity recognition from unstructured feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Skelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Br?mond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Advanced Video and Signal based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jolo-gcn: mining joint-centered light-weight information for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2735" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Body joint guided 3-d deep convolutional descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1095" to="1108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mm-vit: Multi-modal video transformer for compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1910" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning multi-granular spatio-temporal graph network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4334" to="4342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Channel-wise topology refinement graph convolution for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13359" to="13368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Infogcn: Representation learning for human skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="20186" to="20196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vpn: Learning video-pose embedding for activities of daily living</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="72" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical action classification with network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davoodikakhki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="291" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="72" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2969" to="2978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A combined multiple action recognition and summarization for surveillance video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Elharrouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Almaadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouridane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beghdadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="690" to="712" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding action recognition in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Girish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ralescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="370" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey on still image based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3343" to="3361" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="762" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hu-man3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vit-net: Interpretable vision transformers with neural tree decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dualformer: Local-global stratified transformer for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04674</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2684" to="2701" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">No frame left behind: Full video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">K</forename><surname>Nejadasl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van G</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14892" to="14901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph transformer network with temporal kernel attention for skeleton-based action recognition. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="page">108146</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action transformer: A self-attention model for shorttime pose-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mazzia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Angarano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Salvetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiaberge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">108487</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition via spatial and temporal transformer networks. The Computer Vision and Image Understanding (CVIU)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plizzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page">103219</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mmtm: Multimodal transfer module for cnn fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Reza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iuzzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13289" to="13299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wearable sensor-based human activity recognition in the smart healthcare system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Serpush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Menhaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Masoumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karasfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoupled spatialtemporal attention network for skeleton-based action-gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Msaf: Multimodal split attention fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07175</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">View-invariant probabilistic embedding for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="53" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Part-based graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12602</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Oadtr: Online action detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7565" to="7575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Action-net: Multipath excitation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13214" to="13223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Long short-term transformer for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1086" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">After-unet: Axial fusion transformer unet for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="3971" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unik: A unified framework for real-world skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Asformer: Transformer for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning skeletal graph neural networks for hard 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11436" to="11445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03247</idno>
		<title level="m">Motr: End-to-end multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07175</idno>
		<title level="m">Co-training transformer with videos and images improves action recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bayesian hierarchical dynamic model for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7733" to="7742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Action machine: Rethinking action recognition in trimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05770</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
