<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HAT: Hardware-Aware Transformers for Efficient Natural Language Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
							<email>hanrui@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
							<email>zhwu@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
							<email>zhijian@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
							<email>hancai@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
							<email>ligeng@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<email>chuangg@mit.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
							<email>songhan@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HAT: Hardware-Aware Transformers for Efficient Natural Language Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT'14 translation task on Raspberry Pi-4, HAT can achieve 3? speedup, 3.7? smaller size over baseline Transformer; 2.7? speedup, 3.6? smaller size over Evolved Transformer with 12,041? less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-hanlab/hardware-aware-transformers.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> has been widely used in natural language processing tasks. By stacking multiple identical encoder/decoder layers with attention modules, it provides a significant performance improvement over previous convolutional or recurrent neural network models <ref type="bibr" target="#b18">(Kim, 2014)</ref>.</p><p>Nevertheless, it is challenging to deploy Transformers on mobile devices due to the high computation cost. For instance, in order to translate a sentence with only 30 words, a Transformer-Big model needs to execute 13G FLOPs and takes 20  <ref type="figure">Figure 1</ref>: Framework for searching Hardware-Aware Transformers. We first train a SuperTransformer that contains numerous sub-networks, then conduct an evolutionary search with hardware latency feedback to find one specialized SubTransformer for each hardware.</p><p>seconds on a Raspberry Pi. Such long latency will hurt the user experience on edge devices. Thus we need hardware-efficient Transformers <ref type="figure">(Figure 1</ref>). There are two common pitfalls when evaluating the efficiency of a Transformer. (1) FLOPs does not reflect the measured latency. Although FLOPs is used as an metric for efficiency in prior arts <ref type="bibr" target="#b14">(Howard et al., 2017;</ref><ref type="bibr" target="#b44">Wu et al., 2020)</ref>, it is not a good latency proxy. As in <ref type="figure">Figure 2</ref> (Right), models with the same FLOPs can result in very different measured latencies; (2) different hardware prefers different Transformer architecture. As in <ref type="table" target="#tab_5">Table 1</ref>, the Transformer model optimized on one hardware is sub-optimal for another because latency is influenced by different factors on different hardware platforms. For example, the embedding dimension has significant impact on the Raspberry Pi latency but hardly influences the GPU latency <ref type="figure">(Figure 2</ref>).</p><p>Inspired by the success of Neural Architecture Search (NAS) <ref type="bibr" target="#b2">(Bender et al., 2018;</ref><ref type="bibr" target="#b11">Guo et al., 2019;</ref><ref type="bibr"></ref>       <ref type="bibr" target="#b27">Pham et al., 2018;</ref><ref type="bibr" target="#b3">Cai et al., 2019a)</ref>, we propose to search for Hardware-Aware Transformers (HAT) by directly involving the latency feedback into the design loop. In this way, we do not need FLOPs as the latency proxy and can search specialized models for various hardware.</p><p>We first construct a large search space with arbitrary encoder-decoder attention and heterogeneous Transformer layers. Traditional Transformer has an information bottleneck between the encoder and decoder. Arbitrary encoder-decoder attention breaks the bottleneck, allowing all decoder layers to attend to multiple and different encoder layers instead of only the last one. Thus low-level information from the encoder can also be used by the decoder. Motivated by <ref type="figure">Figure 2</ref>, we introduce heterogeneous Transformer layers to allow different layers to have different architecture adapting various hardware.</p><p>To perform a low-cost search in such a large design space, we first train a Transformer supernet -SuperTransformer, which contains many Sub-Transformers sharing the weights. We train all SubTransformers simultaneously by optimizing the uniformly sampled SubTransformers from the Su-perTransformer. The performance of a SubTransformer with inherited weights from the SuperTransformer can provide a good relative performance approximation for different architectures trained from-scratch. Unlike conventional NAS, we only need to pay the SuperTransformer training cost for once and can evaluate all the models in the design space with it. Finally, we conduct an evolutionary search to find the best SubTransformer under the hardware latency constraint. Experiments show that HAT can be naturally incorporated with model compression techniques such as quantization and knowledge distillation.</p><p>We evaluate HAT with WMT'14 En-De, <ref type="bibr">and IWSLT'14</ref> De-En tasks on Raspberry Pi ARM CPU, Intel Xeon CPU, and Nvidia TITAN Xp GPU. Compared with previous work <ref type="bibr" target="#b35">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b29">So et al., 2019;</ref><ref type="bibr" target="#b10">Gu et al., 2019;</ref><ref type="bibr" target="#b44">Wu et al., 2020)</ref>, HAT achieves up to 3? speedup, 3.7? smaller size over Transformer-Big without loss of accuracy. With 12,041? less search cost, HAT outperforms the Evolved Transformer with 2.7? speedup and 3.6? smaller size. It also achieves up to 1.9? speedup over Levenshtein and Lite Transformers with no BLEU score loss. With 4-bit quantization, HAT can further reach 25? model size reduction.</p><p>HAT has three contributions: (1) Hardware-Aware and Specialization. To our best knowledge, we are the first to directly involve the hardware feedback in the model design, to reduce NLP model latency for target hardware, instead of relying on proxy signals (FLOPs). For different hardware platforms, specialized models for low-latency inference are explored. (2) Low-cost Neural Architecture Search with a Large Design Space. We propose arbitrary encoder-decoder attention to break the information bottleneck; and heterogeneous layer to let different layers alter its capacity. A weight-shared SuperTransformer is trained to search for efficient models at a low cost. (3) Design Insights. Based on the search results, we reveal some design insights: Attending to multiple encoder layers is beneficial for the decoder; GPU prefers shallow and wide models while ARM CPU prefers deep and thin ones.  <ref type="figure">Figure 4</ref>: Arbitrary Encoder-Decoder Attention. Each encoder-decoder attention in one decoder layer can attend to the outputs from multiple encoder layers, fully leveraging the features extracted by the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Approaches</head><p>An overview of the HAT framework is shown in <ref type="figure">Figure 3</ref>. We firstly train a SuperTransformer with a large design space. Then, for a given hardware platform, we collect a dataset of (SubTransformer architecture, measured latency) pairs for different models, and train a latency predictor. Finally, we conduct an evolutionary search with a latency constraint to find an efficient model specialized for the target hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Design Space</head><p>We construct a large design space by breaking two conventions in the Transformer design: (1) All decoder layers only attend to the last encoder layer;</p><p>(2) All the layers are identical.</p><p>Arbitrary Encoder-Decoder Attention. Different encoder layers extract features on different abstraction levels. Conventionally, all the decoder lay-ers only attend to the last encoder layer. It forms an information bottleneck that forces all the decoder layers to learn solely from the high abstraction level and ignore the low-level information. To break the bottleneck, we propose Arbitrary Encoder-Decoder Attention to learn the most suitable connections between the encoder and the decoder. Each decoder layer can choose multiple encoder layers to attend. The key and value vectors from encoder layers are concatenated in the sentence length dimension ( <ref type="figure">Figure 4</ref>) and fed to the encoder-decoder cross attention module. The mechanism is efficient because it introduces no additional parameters. The latency overhead is also negligible. For example, with each decoder layer attending to two encoder layers, the latency of Transformer-Base on Nvidia TITAN Xp GPU barely increases by 0.4%. It improves the model capacity by allowing attention to different abstraction levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heterogeneous Transformer Layers. Previous</head><p>Transformers repeat one architecture for all layers. In HAT, instead, different layers are heterogeneous, with different numbers of heads, hidden dim, and embedding dim. In attention layers, different heads are used to capture various dependencies. However, <ref type="bibr" target="#b36">Voita et al. (2019)</ref> shows that many heads are redundant. We thereby make attention head number elastic so that each attention module can decide its necessary number of heads.</p><p>In the FFN layer, the input features are cast to a higher dimension (hidden dim), followed by an  <ref type="figure">Figure 5</ref>: Weight Sharing of the SuperTransformer. All SubTransformers share the front portion of word embeddings, and weights in the fully-connected layers. activation layer. Traditionally, the hidden dim is set as 2? or 4? of the embedding dim, but this is sub-optimal since different layers need different capacities depending on the feature extraction difficulty. We hence make the hidden dim elastic.</p><p>Moreover, we also support elastic embedding dim of encoder and decoder, but it is consistent inside encoder/decoder. The number of encoder &amp; decoder layers are also elastic to learn the proper level of feature encoding and decoding. Other design choices such as the length of Q, K, V vectors in attention modules can be naturally incorporated in our framework, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SuperTransformer</head><p>It is critical to have a large design space in order to find high-performance models. However, training all the models and comparing their BLEU scores is infeasible. We thus propose SuperTransformer, a supernet for performance approximation, which can judge the performance of a model without fully training it. The SuperTransformer is the largest model in the search space with weight sharing <ref type="bibr" target="#b27">(Pham et al., 2018;</ref><ref type="bibr" target="#b3">Cai et al., 2019a)</ref>. Every model in the search space (a SubTransformer) is a part of the SuperTransformer. All SubTransformers share the weights of their common parts. For elastic embedding dim, all SubTransformers share the front portion of the longest word embedding and corresponding FC layer weights. As in <ref type="figure">Figure 5</ref>, for elastic FFN hidden dim, the front part of the FC weights is shared. For elastic head number in attention modules, the whole Q, K, V vectors (the lengths are fixed in our design space) are shared by dividing into head number parts. Elastic layer numbers let all SubTransformers share the first several layers. In the SuperTransformer training, all possible SubTransformers are uniformly sampled, and the corresponding weights are updated. In practice, the SuperTransformer only needs to be trained for the same steps as a baseline Transformer model, which is fast and low-cost. After training, we can get the performance proxy of sampled models in the design space by evaluating the corresponding Sub-Transformers on the validation set without training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evolutionary Search for SubTransformer</head><p>Given a latency requirement, we perform an evolutionary search to find a satisfactory SubTransformer. There are two ways to evaluate the hardware latency of a SubTransformer: (1) Online measurement in which we measure the models during the search process.</p><p>(2) Offline, where we train a latency predictor to provide the latency. We apply the offline method here because it is fast and accurate. For the online method, a single sampled SubTransformer requires hundreds of inferences to get an accurate latency, which lasts for minutes and slows down the searching. For the offline method, we encode the architecture of a SubTransformer into a feature vector, and predict its latency instantly with a multi-layer perceptron (MLP). Trained with thousands of real latency data points, the predictor yields high accuracy ( <ref type="figure" target="#fig_0">Figure 6</ref>). Note that the predicted latency is only used in the search process, and we report real measured latency in the experiment section. Compared with deducing a closed-form latency model for each hardware, the latency predictor method is more general and faster.</p><p>We use an evolutionary algorithm to conduct the search process. As in <ref type="figure">Figure 3</ref>, the search engine queries the latency predictor for SubTransformer latency, and validates the loss on the validation set. The engine only adds SubTransformers with latency smaller than the hardware constraint to the population. We then train the searched models from scratch to obtain the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We conduct experiments on four machine trans-  <ref type="bibr" target="#b8">(Grave et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Setups</head><p>Baselines. Our baseline models are Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>, Levenshtein Transformer <ref type="bibr" target="#b10">(Gu et al., 2019)</ref>, both with the  implementation, Evolved Transformer <ref type="bibr" target="#b29">(So et al., 2019)</ref> and Lite Transformer <ref type="bibr" target="#b44">(Wu et al., 2020)</ref>.</p><p>Evaluation Metrics. For evaluation, we use beam four and length penalty 0.6 for WMT, and beam five for IWSLT <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>. All BLEUs are calculated with case-sensitive tokenization 1 , but we also apply the compound splitting BLEU 2 for WMT, the same as <ref type="bibr" target="#b35">Vaswani et al. (2017)</ref>. We test the model with the lowest validation set loss for WMT and the last ten checkpoints averaged for IWSLT.</p><p>We test the latency of the models by measuring translation from a source sentence to a target sentence with the same length. The length is the average output length on the test set -30 for WMT and 23 for IWSLT. For each model, we measure the latency for 300 times, remove the fastest and slowest 10% and then take the average of the rest 80%. We conduct experiments on three representative hardware platforms: Raspberry Pi-4 with an ARM Cortex-A72 CPU, Intel Xeon E5-2640 CPU, and Nvidia TITAN Xp GPU.</p><p>3.3 Implementation Details SuperTransformer Setups. The SuperTransformer for WMT has the following design space: <ref type="bibr">[512,</ref><ref type="bibr">640]</ref> for embedding dim, <ref type="bibr">[1024,</ref><ref type="bibr">2048,</ref><ref type="bibr">3072]</ref> for hidden dim, <ref type="bibr">[4,</ref><ref type="bibr">8]</ref> for the head number in all attention modules, <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref> for decoder layer number. Due to decoder auto-regression, encoder only accounts for less than 5% of the measured latency; thereby, we set the encoder layer number fixed as 6. For arbitrary encoder-decoder attention, each decoder can choose to attend to the last one, two, or three encoder layers. The Super-Transformer design space for IWSLT is the same as WMT except for <ref type="bibr">[2048,</ref><ref type="bibr">1024,</ref><ref type="bibr">512]</ref> for hidden dim and [4, 2] for head number. We set the Q, K, V vector dim fixed as 512. The design space contains around 10 15 possible SubTransformers and covers a wide range of model size and latency (largest = 6?smallest). We train the SuperTransformers of WMT for 40K steps and 50K steps for IWSLT.</p><p>Hardware-Aware Evolutionary Search Setups. The input of the latency predictor is a feature vector of SubTransformer architecture with ten elements: layer number, embed dim, average hidden dim, average self-attention heads, of both encoder and decoder; plus average encoder-decoder attention heads, and the average number of encoder layers each decoder layer attends. A dataset of 2000 (SubTransformer architecture, measured latency) samples for each hardware is collected, and split into train:valid:test=8:1:1. We normalize the features and latency, and train a three-layer MLP with 400 hidden dim and ReLU activation. We choose three-layer because it is more accurate than the one-layer model, and over three layers do not improve accuracy anymore. With the predictor, we conduct an evolutionary search for 30 iterations in the SuperTransformer, with population 125, parents population 25, mutation population 50 with 0.3 probability and crossover population 50.</p><p>Training Settings. Our training settings are in line with <ref type="bibr" target="#b43">Wu et al. (2019b)</ref> and <ref type="bibr" target="#b44">Wu et al. (2020)</ref>. For WMT, we train for 40K steps with Adam optimizer and a cosine learning rate (LR) scheduler <ref type="bibr" target="#b21">(Kingma and Ba, 2015;</ref><ref type="bibr" target="#b23">Loshchilov and Hutter, 2017)</ref>, where the LR is linearly warmed up from 10 ?7 to 10 ?3 , and then cosine annealed. For IWSLT, we train for 50K steps with inverse square root LR scheduler. The baseline Transformers are trained with the same settings as the searched Sub-Transformers for fair comparisons.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">HAT Performance Comparisons</head><p>In <ref type="figure">Figure 7</ref>, 8 and Appendix  <ref type="bibr" target="#b43">(Wu et al., 2019b)</ref>. We obtain a series of baseline models with layer number scaling (yellow) and dimension scaling (blue). We set different latency constraints on three hardware to get a series of HAT models. HAT consistently outperforms baselines with a large gap under different latency constraints. On ARM CPU, HAT is 3? faster and 3.7? smaller than Transformer-Big with the same BLEU. On Intel CPU, HAT achieves over 2? speedup. On Nvidia GPU, the blue dash line is nearly vertical, indicating that dimension scaling can hardly reduce the latency. In this case, HAT can still find models with low latency and high performance.</p><p>We further compare various aspects of HAT with Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> and Evolved Transformer <ref type="bibr" target="#b29">(So et al., 2019)</ref> in <ref type="table" target="#tab_1">Table 2</ref>. HAT achieves up to 1.6?, 3?, and 3.4? speedup with up to 1.4?, 3.7?, and 4? smaller size than baselines. We report FLOPs for translating a 23-token sentence for IWSLT and 30 for WMT. We show the overall GPU hours for training the SuperTransformer and the searched SubTransformer. We also calculate the cloud computing costs with different modes: "preemptable" is cheaper ($0.74/h) than "on-demand" ($2.48/h) <ref type="bibr" target="#b30">(Strubell et al., 2019)</ref>. HAT is highly affordable since the total GPU-hour is over 12000? smaller than the Evolved Transformer, and is even smaller than Transformer-Big by virtue of the compact model size.</p><p>In <ref type="table" target="#tab_13">Table 3</ref>, we compare HAT with other latest models. We scale down all models to have similar BLEU scores with Levenshtein for fair comparisons. We adopt the average iteration time of 2.88 for decoding <ref type="bibr" target="#b10">(Gu et al., 2019)</ref>, without limiting the length of the output sentence (12 tokens after   <ref type="bibr" target="#b30">Strubell et al. (2019)</ref>. The training time is for one Nvidia V100 GPU, and the latency is measured on the Raspberry Pi ARM CPU. The cloud computing cost is based on AWS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latency BLEU</head><p>Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> 4.3s 25.85 Levenshtein <ref type="bibr" target="#b10">(Gu et al., 2019)</ref> 6.5s 25.20 Evolved Transformer <ref type="bibr" target="#b29">(So et al., 2019)</ref> 3.7s 25.40 Lite Transformer <ref type="bibr" target="#b44">(Wu et al., 2020)</ref> 3.4s 25.79</p><p>HAT (Ours) 3.4s 25.92  three encoder layers, 40% attend to two encoder layers. That demonstrates the necessity of arbitrary encoder-decoder attentions.</p><p>In Appendix <ref type="figure" target="#fig_2">Figure 12</ref>, we visualize the models specialized for different hardware mentioned in <ref type="table" target="#tab_5">Table 1</ref>. We find that the GPU model is wide but shallow; the Raspberry Pi model is deep but thin. The phenomenon echos with our latency profiling ( <ref type="figure">Figure 2)</ref> as GPU latency is insensitive to embedding and hidden dim, but Raspberry Pi is highly sensitive. It guides manual designs: on GPU, we can reduce the layer number and increase dimension to reduce latency and keep high performance.</p><p>Ablation Study. HAT achieves higher BLEU with 1.5? lower latency and 1.5? smaller size compared with the largest SubTransformer <ref type="table" target="#tab_14">(Table 4</ref>). This suggests that larger models do not always provide better performance, and demonstrates the effectiveness of HAT. We also compare the evolutionary search with random search <ref type="figure">(Figure 9</ref>). Evolutionary search can find models with lower losses than random search.  <ref type="table">Table 5</ref>: The performance of SubTransformers with inherited weights are close to those trained from-scratch, and have the same relative performance order. CO2 Emission (lbs) <ref type="figure">Figure 10</ref>: The search cost measured in pounds of CO 2 emission. Our framework for searching HAT reduces the cost by four orders of magnitude than the Evolved Transformer <ref type="bibr" target="#b29">(So et al., 2019)</ref>.</p><p>SubTransformer Performance Proxy. All Sub-Transformers inside the SuperTransformer are uniformly sampled and thus equally trained, so the performance order is well-preserved during training. We conduct experiments to show the effectiveness of the SubTransformer performance proxy as in <ref type="table">Table 5</ref> and Appendix <ref type="figure">Figure 11</ref>. The BLEUs of SubTransformers with inherited weights and weights trained from-scratch are very close. More importantly, they also have the same relative performance order. Therefore, we can rely on the proxy to search high-performance model architecture, significantly reducing the search cost.</p><p>Low Search Cost. As shown in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="figure">Figure 10</ref>, the search cost of HAT is 12,041? lower than the Evolved Transformer. Although both are using Evolutionary Search, the key difference is that Evolved Transformer needs to train all individual models and sort their final performance to pick top ones; on the contrary, HAT trains all models together inside SuperTransformer and sorts their performance proxy to pick top ones. The superior performance of HAT proves that the performance proxy is accurate enough to find good models.</p><p>Finetuning Inherited SubTransformers In section 4.1, we trained each searched SubTransformer  from-scratch in order to conduct fair comparisons with baselines. In practice, we can also directly finetune the SubTransformers with the inherited weights from the SuperTransformer to further reduce the training cost. With 10K finetuning steps (1/4 of from-scratch training), the inherited Sub-Transformers can achieve similar or better performance than trained from-scratch ones (  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Transformer. Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> has prevailed in sequence modeling <ref type="bibr" target="#b15">Junczys-Dowmunt, 2018)</ref>. By stacking identical blocks, the model obtains a large capacity but incurs high latency. Recently, a research trend is to modify the Transformer to improve the performance <ref type="bibr" target="#b43">Wu et al., 2019b;</ref><ref type="bibr" target="#b32">Sukhbaatar et al., 2019;</ref>. Among them, <ref type="bibr" target="#b43">Wu et al. (2019b)</ref> introduced a convolutionbased module to replace the attention;  proposed to train deep Transformers by propagating multiple layers together in the encoder.  and <ref type="bibr" target="#b20">Kim et al. (2019)</ref> also proposed AAN and SSRU to replace the attention mechanism. HAT is orthogonal to them and can be combined to search for efficient architecture with those new modules. Another trend is to apply non-or partially-autoregressive models to cut down the iteration number for decoding <ref type="bibr" target="#b10">(Gu et al., 2019;</ref><ref type="bibr" target="#b0">Akoury et al., 2019;</ref><ref type="bibr" target="#b9">Gu et al., 2018)</ref>. Although reducing latency, they sometimes suffer from low performance.  explored using learned linear combinations of encoder outputs as decoder inputs, while HAT concatenates the outputs without linear combinations, thus better preserving the low-level information. <ref type="bibr" target="#b44">Wu et al. (2020)</ref> investigated mobile settings for NLP tasks and proposed a multi-branch Lite Transformer. However, it relied on FLOPs for efficient model design, which is an inaccurate proxy for hardware latency <ref type="figure">(Figure 2</ref>). There are also works <ref type="bibr" target="#b19">(Kim and Rush, 2016;</ref><ref type="bibr" target="#b20">Kim et al., 2019;</ref><ref type="bibr" target="#b45">Yan et al., 2020)</ref> using Knowledge Distillation (KD) to obtain small student models. Our method is orthogonal to KD and can be combined with it to improve the efficiency further. There are also hardware accelerators <ref type="bibr" target="#b12">(Ham et al., 2020;</ref><ref type="bibr" target="#b47">Zhang et al., 2020)</ref> for attention and fully-connected layers in the Transformer to achieve efficient processing.</p><p>Neural Architecture Search. In the computer vision community, there has been an increasing interest in automating efficient model design with Neural Architecture Search (NAS) <ref type="bibr" target="#b48">(Zoph and Le, 2017;</ref><ref type="bibr" target="#b27">Pham et al., 2018;</ref><ref type="bibr" target="#b13">He et al., 2018)</ref>. Some applied black-box optimization such as evolutionary search <ref type="bibr" target="#b40">(Wang et al., 2020b)</ref> and reinforcement learning <ref type="bibr" target="#b4">(Cai et al., 2019b;</ref><ref type="bibr" target="#b13">He et al., 2018;</ref><ref type="bibr" target="#b37">Wang et al., , 2020a</ref><ref type="bibr" target="#b24">Mao et al., 2019)</ref>; Some leveraged backpropagation with differentiable architecture search . Some also involved hardware constraints into optimizations such as MNasNet <ref type="bibr">(Tan et al., 2019)</ref>, ProxylessNAS <ref type="bibr" target="#b4">(Cai et al., 2019b)</ref>, FBNet <ref type="bibr" target="#b42">(Wu et al., 2019a)</ref> and APQ <ref type="bibr" target="#b40">(Wang et al., 2020b)</ref>. To reduce the NAS cost, supernet based methods <ref type="bibr" target="#b27">(Pham et al., 2018;</ref><ref type="bibr" target="#b2">Bender et al., 2018;</ref><ref type="bibr" target="#b11">Guo et al., 2019)</ref>  Since different hardware has distinct architecture and features <ref type="bibr" target="#b6">(Cong et al., 2018)</ref>, feedback from hardware is critical for efficient NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose Hardware-Aware Transformers (HAT) framework to solve the challenge of efficient deployments of Transformer models on various hardware platforms. We conduct hardware-aware neural architecture search in an ample design space with an efficient weight-shared SuperTransformer, consuming four orders of magnitude less cost than the prior Evolved Transformer, and discover highperformance low-latency models. We hope HAT can open up an avenue towards efficient Transformer deployments for real-world applications. The larger the inherited val loss, the lower the trained from-scratch BLEU.</p><p>The larger the inherited val loss, the lower the trained from-scratch BLEU. SubTransformer trained from-scratch BLEU (Target) <ref type="figure">Figure 11</ref>: The validation loss of SubTransformers is a good performance proxy for BLEU of from-scratch trained SubTransformers. The larger the validation loss, the lower the BLEU score.</p><p>A Appendix for "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"</p><p>A.1 SubTransformer Performance Proxy</p><p>In <ref type="figure">Figure 11</ref>, we show the relationship between the validation loss of SubTransformers directly inherited from the SuperTransformer, and the BLEU score of the SubTransformers trained from-scratch. We can observe that the larger the validation loss, the lower the BLEU score. Therefore the validation loss can be a good performance proxy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Visualizations of Searched Models on WMT'14 En-De Task</head><p>We show the HAT models searched for Raspberry Pi ARM Cortex-A72 CPU and Nvidia TITAN Xp GPU in <ref type="figure" target="#fig_2">Figure 12</ref>. The searched model for Raspberry Pi is deep and thin, while that for GPU is shallow and wide. The BLEU scores of the two models are similar: 28.10 for Raspberry Pi CPU, and 28.15 for Nvidia GPU.  A.3 Latency, BLEU and SacreBLEU of searched HAT models.</p><p>In <ref type="table" target="#tab_10">Table 8</ref>, we show the specific latency numbers, BLEU and SacreBLEU <ref type="bibr" target="#b28">(Post, 2018)</ref> scores for searched HAT models in <ref type="figure">Figure 7</ref> and <ref type="figure" target="#fig_1">Figure 8</ref>.  <ref type="table" target="#tab_10">Table 8</ref>: Specific latency numbers, BLEU and Sacre-BLEU scores for searched HAT models in <ref type="figure">Figure 7</ref> and <ref type="figure" target="#fig_1">Figure 8</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 6 :</head><label>6</label><figDesc>The latency predictor is very accurate, with an average prediction error (RMSE) of 0.1s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 8 :</head><label>8</label><figDesc>Inference latency and BLEU trade-offs of WMT'19 and IWSLT'14 tasks on Nvidia GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 12 :</head><label>12</label><figDesc>SubTransformers optimized for Raspberry Pi ARM CPU and Nvidia GPU on WMT'14 En-De task are different. The CPU model has BLEU 28.10, and GPU model has BLEU 28.15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 Change Hidden Dim Change Hidden Dim Change Embed Dim Change Embed Dim Change #Layers Change #Layers</head><label>2</label><figDesc></figDesc><table><row><cell>159.17185</cell><cell>52.4</cell><cell>159.17185</cell><cell>52.4</cell><cell>159.17185</cell><cell>52</cell></row><row><cell>194.56129</cell><cell>50.9</cell><cell>194.56417</cell><cell>51.4</cell><cell>192.51458</cell><cell>96</cell></row><row><cell>226.01857</cell><cell>51.3</cell><cell>229.95649</cell><cell>54.2</cell><cell>225.85731</cell><cell>13</cell></row><row><cell>257.47585</cell><cell>51.7</cell><cell>265.34881</cell><cell>51.5</cell><cell>259.20004</cell><cell>17</cell></row><row><cell>288.93313</cell><cell>50.4</cell><cell>300.74113</cell><cell>51.7</cell><cell>292.54277</cell><cell>21</cell></row><row><cell>320.39041</cell><cell>52.5</cell><cell>336.13345</cell><cell>51.5</cell><cell>325.8855</cell><cell>26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>-1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hidden Dim</cell><cell>Change Hidden</cell><cell>Embed Dim</cell><cell>Change Embed</cell><cell>Layer Number</cell><cell>Change #Layers</cell></row><row><cell>Scaling</cell><cell>Dim</cell><cell>Scaling</cell><cell>Dim</cell><cell>Scaling</cell><cell></cell></row><row><cell>159.17185</cell><cell>1012.378561</cell><cell>159.17185</cell><cell>1012.378561</cell><cell>159.17185</cell><cell>1012.378561</cell></row><row><cell>194.56129</cell><cell>1147.812423</cell><cell>194.56417</cell><cell>1114.87281</cell><cell>192.51458</cell><cell>1321.4497</cell></row><row><cell>226.01857</cell><cell>1281.604218</cell><cell>229.95649</cell><cell>1270.467544</cell><cell>225.85731</cell><cell>1630.328098</cell></row><row><cell>257.47585</cell><cell>1451.090652</cell><cell>265.34881</cell><cell>1403.119045</cell><cell>259.20004</cell><cell>1925.809944</cell></row><row><cell>288.93313</cell><cell>1585.657468</cell><cell>300.74113</cell><cell>1635.061976</cell><cell>292.54277</cell><cell>2266.552657</cell></row><row><cell>320.39041</cell><cell>1730.037206</cell><cell>336.13345</cell><cell>1831.49392</cell><cell>325.8855</cell><cell>2580.540898</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>-2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Change Hidden</cell><cell>Change Hidden</cell><cell>Change Embed</cell><cell>Change Embed</cell><cell>Change #Layers</cell><cell>Change #Layers</cell></row><row><cell>Dim</cell><cell>Dim</cell><cell>Dim</cell><cell>Dim</cell><cell></cell><cell></cell></row><row><cell>159.17185</cell><cell>57.81710545</cell><cell>159.17185</cell><cell>57.81710545</cell><cell>159.17185</cell><cell>57.81710545</cell></row><row><cell>194.56129</cell><cell>73.918281</cell><cell>194.56417</cell><cell>62.03208764</cell><cell>192.51458</cell><cell>100.9847422</cell></row><row><cell>226.01857</cell><cell>80.10061979</cell><cell>229.95649</cell><cell>63.28665018</cell><cell>225.85731</cell><cell>150.8454541</cell></row><row><cell>257.47585</cell><cell>83.99975101</cell><cell>265.34881</cell><cell>69.9605157</cell><cell>259.20004</cell><cell>205.2778184</cell></row><row><cell>288.93313</cell><cell>89.46600159</cell><cell>300.74113</cell><cell>70.42247256</cell><cell>292.54277</cell><cell>268.6143021</cell></row><row><cell>320.39041</cell><cell>96.02826834</cell><cell>336.13345</cell><cell>77.27889021</cell><cell>325.8855</cell><cell>295.1284428</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>-1-1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dim</cell><cell>Change Hidden</cell><cell>Embed Dim</cell><cell>Change Embed</cell><cell>Layer Number</cell><cell>Change #Layers</cell></row><row><cell></cell><cell>Dim</cell><cell>Scaling</cell><cell>Dim</cell><cell>Scaling</cell><cell></cell></row><row><cell>159.17185</cell><cell>1012.378561</cell><cell>159.17185</cell><cell>1012.378561</cell><cell>159.17185</cell><cell>1012.378561</cell></row><row><cell>194.56129</cell><cell>1147.812423</cell><cell>194.56417</cell><cell>1114.87281</cell><cell>192.51458</cell><cell>1321.4497</cell></row><row><cell>226.01857</cell><cell>1281.604218</cell><cell>229.95649</cell><cell>1270.467544</cell><cell>225.85731</cell><cell>1630.328098</cell></row><row><cell>257.47585</cell><cell>1451.090652</cell><cell>265.34881</cell><cell>1403.119045</cell><cell>259.20004</cell><cell>1925.809944</cell></row><row><cell>288.93313</cell><cell>1585.657468</cell><cell>300.74113</cell><cell>1635.061976</cell><cell>292.54277</cell><cell>2266.552657</cell></row><row><cell>320.39041</cell><cell>1730.037206</cell><cell>336.13345</cell><cell>1831.49392</cell><cell>325.8855</cell><cell>2580.540898</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>BLEU score and measured inference latency of HAT on WMT'14 En-De task. The efficient model for GPU is not efficient for ARM CPU and vice versa.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Finally, the searched model is trained from scratch to get the final performance.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? Evolutionary Search</cell></row><row><cell></cell><cell>Encoder Layer m</cell><cell></cell><cell></cell><cell>Decoder Layer n</cell><cell></cell><cell cols="3">SubTransformer</cell></row><row><cell cols="2">Elastic Layer Num in Encoder</cell><cell></cell><cell></cell><cell cols="2">Num in Decoder Elastic Layer</cell><cell cols="2">Val Loss Architecture</cell><cell cols="2">Engine Evolutionary Search</cell><cell>12/3/2019</cell><cell>_ c a _2500412.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>C ea ed b A ice De ig f he N P jec</cell></row><row><cell>Encoder Layer 1</cell><cell>Elastic Head Num (Self Attention) Elastic Hidden Dim in FFN Encoder Layer 2 e.g. 4 heads</cell><cell cols="2">Arbitrary Encoder-Decoder concat</cell><cell>Elastic Head Num (Self Attention) Elastic Hidden Dim in FFN Elastic Head Num (En-Decoder Attention) e.g. 2 heads</cell><cell>Decoder Layer 1</cell><cell cols="4">12/3/2019 Layer Num Embed Dim Heads Num ? Train a Latency Predictor Latency Predictor for each Hardware SubTransformer Architecture</cell><cell>_g _1132940. g</cell><cell>:///U Latency / a a /D ad / _ c a _2500412. Latency 1/1</cell></row><row><cell></cell><cell>Elastic Embedding Dim</cell><cell cols="2">Attention</cell><cell>Elastic Embedding Dim</cell><cell></cell><cell></cell><cell>e:///U e / a a /D ad / _ e _1895988.</cell><cell>C ea ed b Ca lina Cani f m he N n P jec</cell><cell>12/3/2019 _ e _1895988.</cell><cell>12/3/2019</cell><cell>Electronic De ice</cell><cell>C ea ed b Mi ha Pe i hche f m he N n P jec</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1/1</cell><cell></cell></row><row><cell></cell><cell cols="4">SuperTransformer</cell><cell></cell><cell></cell><cell></cell><cell>IoT</cell><cell>le:///Users/hanrui ang/Do nloads/noun_CPU_2880768.s g C ea ed b M ha f he N P jec ad Kh i CPU</cell><cell>1/1 A</cell><cell>e:///U e / a</cell><cell>a g/D</cell><cell>ad / GPU _g _1132940. g</cell><cell>1/1</cell></row><row><cell cols="7">? Train a SuperTransformer by uniformly sampling SubTransformers with weight sharing</cell><cell cols="3">? Collect Hardware Latency Datasets</cell></row><row><cell cols="10">Figure 3: HAT Overview. A large design space is constructed with Arbitrary Encoder-Decoder Attention and</cell></row><row><cell cols="10">Heterogeneous Layers. (1) Train a weight-shared SuperTransformer by iteratively optimizing randomly sampled</cell></row><row><cell cols="10">SubTransformers. It can provide a performance proxy for SubTransformers. (2) Collect (SubTransformer architec-</cell></row><row><cell cols="10">ture, latency) data pairs on the target hardware. (3) Train a latency predictor for each hardware to provide fast and</cell></row><row><cell cols="10">accurate latency feedback. (4) Perform an evolutionary search with hardware latency constraint to find the model</cell></row><row><cell cols="3">Kj Vj with the lowest validation loss. (5) Encoder Layer j</cell><cell cols="2">En-Decoder Attention Q V K</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">concat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Encoder Layer i</cell><cell></cell><cell cols="2">Self Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ki Vi</cell><cell></cell><cell cols="2">Decoder Layer m</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>lation tasks: WMT'14 En-De, WMT'14 En-Fr, WMT'19 En-De, and IWSLT'14 De-En, consisting of 4.5M, 36.3M, 43.0M, and 160K pairs of training sentences, respectively. For WMT'14 En-De, we apply 32K source-target BPE vocabulary, train on WMT'16, validate on newstest2013 and test on newstest2014, replicating Wu et al. (2019b); For WMT'14 En-Fr, we use 40K source-target BPE vocabulary, validate on newstest2012&amp;2013, and test on newstest2014, replicating Gehring et al. (2017). WMT'19 En-De adopts 49.6K source-target BPE vocabulary, validates on newstest2017, and tests on newstest2018, the same as Junczys-Dowmunt (2019). We use 10K joint BPE vocabulary in lower case for IWSLT'14 De-En</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>, we com-</cell></row><row><cell>pare HAT with Transformer baselines on four tasks.</cell></row><row><cell>The embedding dims are 512 and 1024 for the</cell></row><row><cell>Transformer-Base and Big, respectively. The hid-</cell></row><row><cell>den dims are 4? and 2? of the embedding dim for</cell></row><row><cell>WMT and IWSLT. The IWSLT models are smaller</cell></row><row><cell>to prevent overfitting</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of latency, model size, FLOPs, BLEU and training cost in terms of CO 2 emissions (lbs) and cloud computing cost (USD) for Transformer, the Evolved Transformer and HAT. The training cost estimation is adapted from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 3 :</head><label>3</label><figDesc>Raspberry Pi ARM CPU latency and BLEU comparisons with different models on WMT'14 En-De. HAT has the lowest latency with the highest BLEU.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SubTransformer Latency #Params BLEU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>WMT'14</cell><cell>Largest</cell><cell>10.1s</cell><cell>71M</cell><cell>28.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>En-De</cell><cell>Searched HAT</cell><cell>6.9s</cell><cell>48M</cell><cell>28.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>WMT'14</cell><cell>Largest</cell><cell>10.1s</cell><cell>71M</cell><cell>41.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>En-Fr</cell><cell>Searched HAT</cell><cell>9.1s</cell><cell>57M</cell><cell>41.8</cell></row><row><cell></cell><cell>4.24</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Evolutionary Search</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Random Search</cell></row><row><cell>Validation Loss</cell><cell>4.20 4.22</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4.18</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>6</cell><cell>11 Search Iterations</cell><cell>15</cell><cell>20</cell></row><row><cell cols="6">Figure 9: Evolutionary search can find better SubTrans-</cell></row><row><cell cols="6">formers in the SuperTransformer than random search.</cell></row><row><cell cols="6">decoding). HAT runs 1.3? faster than Transformer</cell></row><row><cell cols="6">with higher BLEU; 1.9? faster than Levenshtein</cell></row><row><cell cols="6">with 0.7 higher BLEU. Under similar latency, HAT</cell></row><row><cell cols="6">also outperforms Lite Transformer. These results</cell></row><row><cell cols="6">demonstrate HAT's effectiveness in lower latency</cell></row><row><cell cols="6">scenarios. Our framework can also be adopted to</cell></row><row><cell cols="4">speedup those models.</cell><cell></cell></row><row><cell cols="2">4.2 Analysis</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Design Insights. For all HAT WMT models</cell></row><row><cell cols="6">in Figure 7, 10% of all decoder layers attend to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 4 :</head><label>4</label><figDesc>The searched HAT compared with the largest SubTransformer in the design space. Larger models do not necessarily have better performance. HAT models have lower latency, smaller size, and higher BLEU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Human Life</cell><cell></cell><cell>11023</cell><cell></cell></row><row><cell></cell><cell cols="2">American Life</cell><cell>36156</cell><cell></cell></row><row><cell></cell><cell cols="2">US car including</cell><cell>126000</cell><cell></cell></row><row><cell></cell><cell>fuel</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Evolved</cell><cell></cell><cell>626155</cell><cell></cell></row><row><cell></cell><cell>Transformer</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>HAT (Ours)</cell><cell></cell><cell>6000</cell><cell></cell></row><row><cell>Human Life</cell><cell>11,023</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Avg. 1 year)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>American Life</cell><cell>36,156</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Avg. 1 year)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>US Car w/ Fuel</cell><cell>126,000</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Avg. 1 lifetime)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Evolved</cell><cell></cell><cell></cell><cell cols="2">626,155</cell></row><row><cell>Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HAT (Ours)</cell><cell>52</cell><cell>12041?</cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>175K</cell><cell>350K</cell><cell>525K</cell><cell>700K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: The SubTransformer inherited from the Super-</cell></row><row><cell>Transformer can achieve similar or better performance</cell></row><row><cell>than the same SubTransformer trained from-scratch.</cell></row><row><cell>Training steps are saved by 4?.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: K-means quantization of HAT models on</cell></row><row><cell>WMT'14 En-Fr. 4-bit quantization reduces the model</cell></row><row><cell>size by 25? with only 0.1 BLEU loss compared with</cell></row><row><cell>the transformer baseline. 8-bit quantization even has</cell></row><row><cell>0.1 higher BLEU than its full precision version.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/moses-smt/mosesdecoder 2 https://github.com/tensorflow/tensor2tensor</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank NSF Career Award #1943349, MIT-IBM Watson AI Lab, Semi-conductor Research Corporation (SRC), Intel, and Facebook for supporting this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Syntactically supervised transformers for faster neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nader</forename><surname>Akoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1269" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training deeper neural machine translation models with transparent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1338</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3028" to="3033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="550" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Proxy-lessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1008</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers; Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding performance differences of fpgas and gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenman</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaochong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="93" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia. PMLR</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Levenshtein Transformer. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A?3: Accelerating attention mechanisms in neural networks with approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae Jun</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Jun</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghak</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeonhong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Hun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghee</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="328" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Microsoft&apos;s submission to the wmt2018 news translation task: How i learned to stop worrying and love the data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00196</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Microsoft translator at wmt 2019: Towards large-scale documentlevel neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06170</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Marian: Cost-effective high-quality neural machine translation in C++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Aue</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2716</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="129" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07947</idno>
		<title level="m">Sequencelevel knowledge distillation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From research to production and back: Ludicrously fast neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5632</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic Gradient Descent with Warm Restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Park: An open platform for learning-augmented computer systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parimarjan</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><forename type="middle">Khani</forename><surname>Shirkoohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Nathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2490" to="2502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facebook FAIR&apos;s WMT19 news translation task submission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyra</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5333</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="314" to="319" />
		</imprint>
	</monogr>
	<note>Task Papers, Day 1)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08771</idno>
		<title level="m">A call for clarity in reporting bleu scores</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5877" to="5886" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1355</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th</title>
		<meeting>the 57th</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1032</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="331" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1580</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5797" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gcn-rl circuit designer: Transferable transistor sizing with graph neural networks and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Seung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 57th Design Automation Conference (DAC)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to design circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Seung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2018 Machine Learning for Systems Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1176</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1810" to="1822" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Apq: Joint search for network architecture, pruning and quantization policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imitation learning for nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1304" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lite transformer with long-short range attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Micronet for efficient language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Accelerating neural transformer via an average attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00631</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sparch: Efficient architecture for sparse matrix multiplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="261" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural Architecture Search with Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Transferable Architectures for Scalable Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
