<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">More Embeddings, Better Sequence Labelers?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<email>yongjiang.jy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
							<email>nguyen.bach@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
							<email>z.huang@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>f.huang@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">More Embeddings, Better Sequence Labelers?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work proposes a family of contextual embeddings that significantly improves the accuracy of sequence labelers over noncontextual embeddings. However, there is no definite conclusion on whether we can build better sequence labelers by combining different kinds of embeddings in various settings. In this paper, we conduct extensive experiments on 3 tasks over 18 datasets and 8 languages to study the accuracy of sequence labeling with various embedding concatenations and make three observations: (1) concatenating more embedding variants leads to better accuracy in rich-resource and cross-domain settings and some conditions of low-resource settings;</p><p>(2) concatenating contextual sub-word embeddings with contextual character embeddings hurts the accuracy in extremely lowresource settings; (3) based on the conclusion of (1), concatenating additional similar contextual embeddings cannot lead to further improvements. We hope these conclusions can help people build stronger sequence labelers in various settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, sequence labelers equipped with contextual embeddings have achieved significant accuracy improvement <ref type="figure">(</ref> . Different types of embeddings have different inductive biases to guide the learning process. However, little work has been done to study how to concatenate these contextual embeddings and non-contextual embeddings to build better sequence labelers in * Yong Jiang and Kewei Tu are the corresponding authors. ? : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. multilingual, low-resource, or cross-domain settings over various sequence labeling tasks. In this paper, we empirically investigate the effectiveness of concatenating various kinds of embeddings for multilingual sequence labeling and try to answer the following questions:</p><p>1. In rich-resources settings, does combining different kinds of contextual embeddings result in a better sequence labeler? Are noncontextual embeddings helpful when the models are equipped with contextual embeddings?</p><p>2. When we train models in low-resource and cross-domain settings, do the conclusions from the rich-resource settings still hold?</p><p>3. Can sequence labelers automatically learn the importance of each kind of embeddings when they are concatenated?</p><p>2 Model Architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence Labeling</head><p>We use the BiLSTM structure for all the sequence labeling tasks, which is one of the most popular approaches to sequence labeling (Huang et al., 2015; Ma and Hovy, 2016). Given a n word sentence x = {x 1 , ? ? ? , x n } and L kinds of embeddings, we feed the sentence to generate the l-th kind of word embeddings {e l 1 , ? ? ? , e l n }:</p><formula xml:id="formula_0">e l i = embed l (x)</formula><p>We concatenate these embeddings to generate the word representations {r 1 , ? ? ? , r n } as the input of the BiLSTM layer:</p><formula xml:id="formula_1">r i = e 1 i ? ? ? ? ? e L i</formula><p>where ? represents the vector concatenation operation. We feed the word representations into a singlelayer BiLSTM to generate the contextual hidden layer of each word. Then we use either a Softmax layer (the MaxEnt approach) or a Conditional Random Field layer (the CRF approach) (Lafferty et al., <ref type="bibr">2001</ref>; Lample et al., 2016; Ma and Hovy, 2016) fed with the hidden layers to generate the conditional probability p(y|x). Given the corresponding sequence of gold labels y * = {y * 1 , ? ? ? , y * n } for the input sentence, the loss function for a model with parameters ? is:</p><formula xml:id="formula_2">L ? = ? log p(y * |x; ?)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embeddings</head><p>There are mainly four kinds of embeddings that have been proved effective on the sequence labeling task: contextual sub-word embeddings, contextual character embeddings, non-contextual word embeddings and non-contextual character embeddings 1 . As we conduct our experiments in multilingual settings, we need to select suitable embeddings from each category for the concatenation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Character Embeddings (CCEs)</head><p>Liu et al. <ref type="bibr">(2018)</ref> proposed a character language model by applying the BiLSTM over the sentence and trained jointly with the sequence labeling task. (Pooled) Contextual string embeddings (Flair) (Akbik et al., 2018, 2019) are pretrained on a large amount of unlabeled data and result in significant improvements for sequence labeling tasks. We use the Flair embeddings due to their high accuracy for sequence labeling task 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-contextual Word Embeddings (NWEs)</head><p>The most common approach to the NWEs is Word2vec <ref type="bibr">(Mikolov et al., 2013)</ref>, which is a skipgram model learning word representations by predicting neighboring words. Based on this approach, GloVe (Pennington et al., 2014) creates a co-occurrence matrix for global information and fastText (Bojanowski et al., 2017) represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-contextual Character Embeddings (NCEs)</head><p>Using character information to represent the embeddings of word is proposed by Santos and Zadrozny (2014) with a lot of following work using a CNN structure to encode character representation (dos Santos and Guimar?es, 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016). Lample et al.</p><p>(2016) utilized BiLSTM on the character sequence of each word. We follow this approach as it usually results in better accuracy (Yang et al., 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>For simplicity, we use M to represent M-BERT embeddings, F to represent Flair embeddings, W to represent fastText embeddings, C to represent non-contextual character embeddings, All to represents the concatenation of all types of embeddings and the operator "+" to represent the concatenation operation. We use the MaxEnt approach for all experiments 3 . Due to the space limit, some detailed experiment settings, extra experiments and discussions are included in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Settings</head><p>Datasets We use datasets from three multilingual    (4) which one is the best concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rich-resource and Low-resource Settings</head><p>How to build better sequence labelers through embedding concatenations in both rich-resource and low-resource settings is the most important concern for users. We report the results of various concatenations of embeddings for the tasks in <ref type="table" target="#tab_1">Table 1</ref> for rich-resource settings and in <ref type="figure" target="#fig_3">Figure 1</ref> for low-resource settings. From the results, we have the following observations. Observation #1. Concatenating more embedding variants results in better sequence labelers: In rich-resource settings, concatenating more embedding variants (M+F+W and All) results in best scores in most of the cases, which indicates that the inductive biases in various kind of embeddings are helpful to train a better sequence labeler. In low-resource settings, M+F+W and All performs inferior to the F+W when the number of sentences are lower than 100. However, when the training set gets larger, the gap between these concatenations becomes smaller and reverses when the training set becomes larger than 100 for NER and POS tagging and the gap also disappears for Chunking. A possible reason is that using CSEs makes the model sample inefficient so that CSEs requires more training samples to improve accuracy than CCEs. The observation suggests that concatenating more embedding variants performs better if the training set is not extremely small. Observation #2. NCEs become less effective when concatenated with CSEs and CCEs: Concatenating NCEs with CSEs only marginally improves the accuracy. There is almost no improvement when concatenated with both CSEs and CCEs but the NCEs does not hurt the accuracy as well. A possible reason is that the CSEs and CCEs largely contain the information in NCEs 4 . Observation #3. NWEs are significantly helpful on top of contextual embeddings: Although models based on contextual embeddings have proved to be stronger than models based on NWEs for sequence labeling, concatenating NWEs with contextual embeddings can still improve the accuracy significantly. The results imply that the contextual embeddings contain more contextual information over the input but lack static word information. From these observations, we find that in most of rich-resource and low-resource settings, concatenating all embeddings variants or all embeddings variants except NCEs is the simplest choice for a better sequence labeler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-domain Settings</head><p>Another concern for users is that we want to build better sequence labelers not only in in-domain set-   <ref type="table" target="#tab_3">Table 2)</ref> are almost consistent with rich-resource settings, suggesting that concatenating more embedding variants results in better sequence labelers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Importance of Embeddings</head><p>To study the effectiveness of concatenating embeddings from another perspective, we preserve only one kind of embedding in All and mask out the other embeddings as 0 to study how the models rely upon each kind of embeddings. To avoid the impact of embedding dimensions, we train the model by linearly projecting each kind of embeddings into the same dimension of 4096. The results ( <ref type="figure">Figure  2)</ref> show that the accuracy of preserved embeddings has a positive correlation with the results in <ref type="table" target="#tab_1">Table  1</ref>. For example, M gets higher accuracy than other embeddings in NER and <ref type="table" target="#tab_1">Table 1</ref> also shows that the model with F performs inferior to the model with M only. The models with concatenated embeddings almost do not rely on NCEs and relies mostly on CSEs or CCEs depending on the task. These results show that models with concatenated embeddings can extract helpful information from each kind of embeddings to improve accuracy.   <ref type="table">Table 4</ref>: Comparisons of F+W, All, and F+W+proj (F+W with linearly projecting the hidden size into the hidden size of All) in three tasks with 10-sentence lowresource settings. The accuracy is averaged over tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">On Concatenating Similar Embeddings</head><p>Since concatenating more embeddings variants results in better sequence labelers, we additionally concatenate multilingual Flair embeddings (M-Flair) or English BERT embeddings (En-BERT) with All embeddings to show whether concatenating the same category of embeddings can further improve the accuracy. We evaluate the addition of En-BERT on English and M-Flair on all languages in each task. The results are shown in <ref type="table" target="#tab_5">Table  3</ref>. It can be seen that additionally concatenating the same category of embeddings does not further improve the accuracy in most cases except for concatenating En-BERT on English WikiAnn NER. A possible reason is that the BERT models are trained on the same domain as WikiAnn and hence the inductive biases of BERT embeddings help improve the accuracy. We also find that concatenating En-BERT with All only improves the accuracy of WikiAnn English NER. We think the possible reason for the improvement is that the BERT and the training data have the same domain of Wikipedia. We conduct the same concatenation on the CoNLL English NER dataset for comparison. The results in <ref type="table" target="#tab_9">Table  7</ref> show that concatenating En-BERT with All does not further improve the accuracy on CoNLL English NER.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">English BERT vs. M-BERT</head><p>We use English BERT embeddings instead of M-BERT embeddings to see whether the languagespecific CSEs impact the observations. The results <ref type="table" target="#tab_6">(Table 5)</ref> show that our observations do not change in both rich-resource and low-resource settings. Using a language-specific BERT embedding can even get better sequence labelers for the POS tagging and chunking tasks in rich-resource settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Hidden Sizes and Accuracy</head><p>In low-resource settings with 10 sentences, we find that models with All perform inferior to the models with F+W. One possible concern is that whether the larger hidden size of All introduces more parameters in the model and makes the model over-fits  the training set. We linearly project the hidden size of F+W (4396) to the same hidden size as All (5214). <ref type="table">Table 4</ref> shows that with linear projection, F+W performs even better. Therefore, the cause for over-fitting is not the inferior accuracy of All but possibly the sample inefficiency for CSEs.</p><p>Another concern is whether we can project each embedding to a larger hidden size to improve the accuracy. Since we try a projection to 4096 for each embedding in F+W+proj (Section 3.4), we further project each embedding variants to see how the projection affect the accuracy in rich-resource settings. The results <ref type="table" target="#tab_7">(Table 6)</ref> show that the linear projection for each embedding significantly decreases the accuracy of the models.</p><p>From the two experiments, we find that the hidden sizes of concatenated embeddings do not impact the observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we analyze how to get a better sequence labeler by concatenating various kinds of embeddings. We make several empirical observations that we hope can guide future work to build better sequence labelers: (1) in most settings, concatenating more embedding variants leads to better results, while in extremely low-resource settings, only using CSEs and NWEs performs better; (2) NCEs become less effective when concatenated with contextual embeddings, while NWEs are still beneficial; (3) neural models can automatically learn which embeddings are beneficial to the task; (4) additionally concatenating similar contextual embeddings with the best concatenations from (1) cannot further improve the accuracy in most cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In this appendix, we use ISO 639-1 codes 5 to represent each language for simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Settings</head><p>Datasets We use the following datasets for experiments:</p><p>? . We run our experiments on a GPU server with NVIDIA Tesla V100 GPU. For model training, we set the mini-batch size to 2,000 tokens for better GPU utilization. Following the official release of Flair, we use an SGD optimizer with a learning rate of 0.1 for training all models and set the hidden size of BiLSTM to 256. We anneal the learning rate by 0.5 if there is no improvement on the development sets for 10 and 100 epochs when training rich-resource and low-resource datasets respectively. We fix these hyper-parameters for all experiments because we find that tuning these hyper-parameters does not impact the observation and usually results in lower accuracy. We average over 5 runs for each experiment and report the macro-average score over all languages for each task.</p><p>Pre-processing and Evaluation We evaluate the NER and chunking by the F1 score and POS tagging by the accuracy. We use the evaluation script in the official release of Flair. We convert the BIO format into BIOES format for all NER and chunking datasets. <ref type="bibr">9</ref> Details of Flair embeddings https://github.com/flairNLP/ flair/blob/master/resources/docs/embeddings/FLAIR_ EMBEDDINGS.md <ref type="bibr">10</ref> We did not observe further gains when increasing the dimension size.  <ref type="table">Treebank  ar  PADT  cs  FicTree  de  GSD  en  EWT  es  GSD  fr  Sequoia  nl</ref> LassySmall ta TTB </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Detailed Results</head><p>For the models using the CRF layer, similar to the main paper, we plot our results in the rich-resource and low-resource settings in <ref type="figure" target="#fig_6">Figure 3</ref>. The figures have similar trends as the MaxEnt models, showing that output structures do not impact the observations. <ref type="table" target="#tab_1">Table 10</ref> shows the importance of each kind of embeddings for each language and task (Section 3.4 in the main paper). <ref type="table" target="#tab_1">Table 11</ref>, 13 and 15 show average scores over each language for each task in the rich-resource and low-resource settings (Section 3.2). <ref type="table" target="#tab_1">Table 12</ref>, 14 and 16 show average scores over each language for each task in the rich-resource and low-resource settings. <ref type="table" target="#tab_14">Table 9</ref> shows the average scores for each language in our cross-domain experiments (Section 3.3). <ref type="table" target="#tab_1">Table 17</ref> show the detailed comparison for additionally concatenating M-Flair embeddings with All for all datasets (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of Sentences</head><p>Relative Score 10 50 100 500 1000 All        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2019; Martin et al., 2019) over approaches that use static non-contextual word embeddings (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Contextual Sub-word Embeddings (CSEs) CSEs such as OpenAI GPT (Radford et al.) and BERT (Devlin et al., 2019) are based on transformer (Vaswani et al., 2017) and use WordPiece embeddings (Sennrich et al., 2016; Wu et al., 2016) as input. Much research has focused on improving BERT model's performance such as better masking strategy (Liu et al., 2019) and cross-lingual training (Conneau and Lample, 2019). Since we focus on the multilingual settings of sequence labeling tasks, we use multilingual BERT (M-BERT), as recent researches shows its strong generalizability over various languages and tasks (Pires et al., 2019; Karthikeyan et al., 2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>sequence labeling tasks over 8 languages in our experiments: WikiAnn NER datasets (Pan et al., 2017), UD Part-Of-Speech (POS) tagging datasets (Nivre et al., 2016), and CoNLL 2003 chunking datasets (Tjong Kim Sang and De Meulder, 2003).We use language-specific fastText and Flair embeddings depending on the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>EmbeddingFigure 1 :</head><label>1</label><figDesc>Concatenation Since experimenting on all 15 concatenation combinations of the four embeddings is not essential for evaluating the effectiveness of each kind of embeddings, we experiment on the following 7 concatenations: F, F+W, Relative score improvements against models with M-BERT embeddings for three tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>11 https://github.com/flairNLP/flair Language</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Relative score improvements against models with M-BERT embeddings for three tasks. Models are equipped with the CRF layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>M+F+W and M+W vs. M+F+W);</figDesc><table><row><cell>: Averaged F1 scores over languages for each</cell></row><row><cell>task with different embedding concatenations.</cell></row><row><cell>M, M+W, M+W+C, M+F+W, All. Through</cell></row><row><cell>these concatenations, we can answer the following</cell></row><row><cell>questions: (1) whether NWEs are still helpful (F</cell></row><row><cell>vs. F+W and M vs. M+W); (2) whether NCEs are</cell></row><row><cell>still helpful (M+W vs. M+W+C and M+F+W vs.</cell></row><row><cell>All); (3) whether concatenating different contex-</cell></row><row><cell>tual embeddings results in a better sequence labeler</cell></row><row><cell>(F+W vs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Cross-domain transfer from the Wikipedia domain to the news domain on the NER task.</figDesc><table><row><cell>Score Percentage</cell><cell>0 0.5 1</cell><cell>0.0</cell></row><row><cell></cell><cell>NER</cell><cell>POS</cell><cell>Chunk</cell></row><row><cell></cell><cell cols="3">M-BERT Flair Word Char</cell></row></table><note>Figure 2: Importance of each embedding over the con- catenation of All embeddings. The score percentage represents the average score preserving only one kind of embeddings divided by the score without masking.tings but in out-of-domain settings as well. We con- duct experiments in cross-domain settings to show how the embedding concatenations impact the ac- curacy when the distribution of training data and test data are different. We evaluate our Wikipedia NER models on CoNLL 2002/2003 NER (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) datasets from the news domain. The results (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of the effectiveness for additionally concatenating the same category of embeddings. B represents the En-BERT embeddings and MF represents the M-Flair embeddings.</figDesc><table><row><cell>EMBEDDINGS</cell><cell>TASKS</cell><cell></cell></row><row><cell></cell><cell cols="2">NER POS CHUNK</cell></row><row><cell>F+W</cell><cell>32.7 81.7</cell><cell>78.2</cell></row><row><cell>F+W+Proj.</cell><cell>33.2 82.3</cell><cell>79.0</cell></row><row><cell>All</cell><cell>27.5 80.4</cell><cell>76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of using English BERT instead of M-BERT in English datasets. B represents the En-BERT embeddings. We also provide the concatenation of Flair and pretrained word embeddings for reference.</figDesc><table><row><cell>EMBEDDINGS</cell><cell>TASKS</cell><cell></cell></row><row><cell></cell><cell cols="2">NER POS CHUNK</cell></row><row><cell>All</cell><cell>86.8 96.7</cell><cell>92.9</cell></row><row><cell>All+50d Proj.</cell><cell>83.8 96.3</cell><cell>92.0</cell></row><row><cell cols="2">All+1024d Proj. 84.8 96.5</cell><cell>92.2</cell></row><row><cell cols="2">All+4096d Proj. 85.1 96.5</cell><cell>92.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Comparisons of All with different linear pro- jection size in three tasks with rich-resource settings. The accuracy is averaged over tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparisons of concatenating En-BERT withAll on CoNLL NER. B represents the En-BERT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Huan Gui, Jian Peng, and Jiawei Han. 2018. Empower sequence labeling with task-aware neural language model. In Thirty-Second AAAI Conference on Artificial Intelligence. Alch?-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024-8035. Curran Associates, Inc. Llion Jones, Aidan N Gomez, ?ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</figDesc><table><row><cell>Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638-1649, Santa Fe, New Mexico, USA. Associ-A. Beygelzimer, F. d Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep-resentation. In Proceedings of the 2014 conference on empirical methods in natural language process-ing (EMNLP), pages 1532-1543. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep-resentations. In Proceedings of the 2018 Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa-ation for Computational Linguistics: Human Lan-guage Technologies, Volume 1 (Long Papers), pages 2227-2237, New Orleans, Louisiana. Association for Computational Linguistics.</cell><cell cols="2">pages 260-270, San Diego, California. Association for Computational Linguistics. Liyuan Liu, Jingbo Shang, Xiang Ren, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between hu-Frank Fangzheng Xu, Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-man and machine translation. arXiv preprint dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, arXiv:1609.08144. Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap-Jie Yang, Shuailong Liang, and Yue Zhang. 2018. De-proach. arXiv preprint arXiv:1907.11692. sign challenges and misconceptions in neural se-quence labeling. In Proceedings of the 27th Inter-Xuezhe Ma and Eduard Hovy. 2016. End-to-end national Conference on Computational Linguistics, sequence labeling via bi-directional LSTM-CNNs-pages 3879-3889, Santa Fe, New Mexico, USA. As-CRF. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol-sociation for Computational Linguistics.</cell></row><row><cell>tion for Computational Linguistics, 5:135-146. Jason P.C. Chiu and Eric Nichols. 2016. Named entity Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Pro-</cell><cell cols="2">ume 1: Long Papers), pages 1064-1074, Berlin, Ger-many. Association for Computational Linguistics.</cell></row><row><cell>recognition with bidirectional LSTM-CNNs. Trans-actions of the Association for Computational Lin-guistics, 4:357-370. ceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 4996-5001, Florence, Italy. Association for Computa-tional Linguistics. Alexis Conneau and Guillaume Lample. 2019. Cross-lingual language model pretraining. In Advances Alec Radford, Karthik Narasimhan, Tim Salimans, and</cell><cell cols="2">Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su?rez, Yoann Dupont, Laurent Romary, ?ric Ville-monte de la Clergerie, Djam? Seddah, and Beno?t Sagot. 2019. Camembert: a tasty french language model. arXiv preprint arXiv:1911.03894.</cell></row><row><cell>in Neural Information Processing Systems, pages 7057-7067. Ilya Sutskever. Improving language understanding by generative pre-training.</cell><cell cols="2">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean. 2013. Distributed representa-</cell></row><row><cell>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under-C?cero dos Santos and Victor Guimar?es. 2015. Boost-ing named entity recognition with neural character embeddings. In Proceedings of the Fifth Named En-standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language tity Workshop, pages 25-33, Beijing, China. Associ-ation for Computational Linguistics.</cell><cell cols="2">tions of words and phrases and their compositional-ity. In Advances in neural information processing systems, pages 3111-3119. Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-ter, Yoav Goldberg, Jan Haji?, Christopher D. Man-</cell></row><row><cell>Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ-ation for Computational Linguistics. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-tional lstm-crf models for sequence tagging. arXiv Cicero D Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st international conference on machine learning (ICML-14), pages 1818-1826. preprint arXiv:1508.01991. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words K Karthikeyan, Zihan Wang, Stephen Mayhew, and with subword units. In Proceedings of the 54th An-Dan Roth. 2020. Cross-lingual ability of multilin-nual Meeting of the Association for Computational gual bert: An empirical study. In International Con-Linguistics (Volume 1: Long Papers), pages 1715-ference on Learning Representations. 1725, Berlin, Germany. Association for Computa-</cell><cell cols="2">ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel Zeman. 2016. Universal dependencies v1: A multilingual treebank collection. In Proceedings of the Tenth In-ternational Conference on Language Resources and Evaluation (LREC 2016), pages 1659-1666, Por-toro?, Slovenia. European Language Resources As-sociation (ELRA). Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Cross-lingual name tagging and linking for 282 languages.</cell></row><row><cell>tional Linguistics. John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Erik F. Tjong Kim Sang. 2002. Introduction to the Probabilistic models for segmenting and labeling se-quence data. In Proceedings of the Eighteenth Inter-CoNLL-2002 shared task: Language-independent</cell><cell cols="2">In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1946-1958, Vancouver, Canada. Association for Computational Linguistics.</cell></row><row><cell>national Conference on Machine Learning, ICML named entity recognition. In COLING-02: The</cell><cell></cell><cell></cell></row><row><cell>'01, page 282-289, San Francisco, CA, USA. Mor-6th Conference on Natural Language Learning 2002</cell><cell cols="2">Adam Paszke, Sam Gross, Francisco Massa, Adam</cell></row><row><cell>gan Kaufmann Publishers Inc. (CoNLL-2002).</cell><cell cols="2">Lerer, James Bradbury, Gregory Chanan, Trevor</cell></row><row><cell></cell><cell cols="2">Killeen, Zeming Lin, Natalia Gimelshein, Luca</cell></row><row><cell>Erik F. Tjong Kim Sang and Fien De Meulder.</cell><cell cols="2">Antiga, Alban Desmaison, Andreas Kopf, Edward</cell></row><row><cell>2003. Introduction to the CoNLL-2003 shared task:</cell><cell cols="2">Yang, Zachary DeVito, Martin Raison, Alykhan Te-</cell></row><row><cell>Language-independent named entity recognition. In</cell><cell cols="2">jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,</cell></row><row><cell>Proceedings of the Seventh Conference on Natu-</cell><cell>Junjie Bai, and Soumith Chintala. 2019.</cell><cell>Py-</cell></row><row><cell>ral Language Learning at HLT-NAACL 2003, pages</cell><cell cols="2">torch: An imperative style, high-performance deep</cell></row><row><cell>142-147.</cell><cell cols="2">learning library. In H. Wallach, H. Larochelle,</cell></row></table><note>Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>The list of treebank that we used in UD POS tagging.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>52.3?0.5 47.9?0.3 54.7?0.3 50.4</figDesc><table><row><cell>EMBEDDINGS</cell><cell></cell><cell cols="3">MaxEnt models on WikiAnn NER</cell><cell></cell></row><row><cell>M F W C</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>nl</cell><cell>avg</cell></row><row><cell>1.</cell><cell cols="5">41.0?0.7 46.9?1.4 47.4?1.7 49.8?0.6 46.3</cell></row><row><cell>2.</cell><cell cols="5">43.2?0.6 48.1?1.0 50.1?0.3 52.9?0.3 48.6</cell></row><row><cell>3.</cell><cell cols="5">45.2?0.9 49.2?0.7 45.2?2.8 49.9?0.7 47.4</cell></row><row><cell>4.</cell><cell cols="5">43.5?0.1 50.9?0.7 46.9?1.3 52.5?0.6 48.4</cell></row><row><cell>5.</cell><cell cols="5">44.4?1.2 50.6?0.6 47.1?0.1 52.9?0.6 48.7</cell></row><row><cell>6.</cell><cell cols="5">46.7?0.4 51.1?0.7 48.0?2.1 53.9?0.9 49.9</cell></row><row><cell>7.</cell><cell>46.6?0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Detailed results of cross-domain transfer from the Wikipedia domain to the news domain on the NER task. We use the ISO 639 language code to represent each language. 0?1.4 43.7?1.1 44.9?2.6 0.0?0.0 82.6?0.1 cs 71.6?0.4 54.5?2.4 45.9?2.1 0.0?0.0 88.0?0.1 de 67.0?0.8 58.1?2.5 33.6?1.9 0.0?0.0 85.6?0.2 en 67.7?0.9 46.5?0.9 22.8?0.5 0.0?0.0 88.4?0.7 es 74.9?1.0 54.6?2.5 35.0?2.0 0.0?0.0 79.9?0.6 fr 75.9?1.5 48.0?1.3 35.2?3.9 0.0?0.0 84.2?0.1 nl 63.4?3.7 59.0?1.6 40.3?0.9 0.0?0.0 86.7?0.4 ta 41.3?0.9 51.5?1.5 43.5?1.4 0.0?0.0 83.4?0.2 5?1.7 88.3?0.7 79.3?0.6 27.4?0.9 92.0?0.6 cs 84.0?0.6 90.8?0.1 68.5?0.6 25.6?3.2 96.5?0.1 de 78.3?2.9 88.3?0.2 71.3?0.8 26.3?3.6 98.8?0.0 en 85.1?0.9 85.8?0.2 65.3?1.8 32.5?1.1 97.2?0.0 es 83.2?1.6 92.4?0.5 80.9?1.6 20.4?5.9 96.6?0.0 fr 92.6?0.7 85.2?0.6 63.7?0.2 15.8?3.0 95.2?0.0 nl 80.9?0.3 89.9?0.1 70.9?1.9 18.9?1.7 98.7?0.0 ta 76.8?1.8 76.6?0.3 62.0?0.0 33.1?4.8 96.7?0.0</figDesc><table><row><cell></cell><cell>M-BERT</cell><cell>Flair</cell><cell>Word</cell><cell>Char</cell><cell>All</cell></row><row><cell></cell><cell></cell><cell cols="2">WikiAnn NER</cell><cell></cell><cell></cell></row><row><cell cols="2">ar 53.Avg. 64.3</cell><cell>52.0</cell><cell>37.6</cell><cell>0.0</cell><cell>84.8</cell></row><row><cell></cell><cell></cell><cell cols="2">UD POS tagging</cell><cell></cell><cell></cell></row><row><cell cols="2">ar 84.Avg. 83.2</cell><cell>87.2</cell><cell>70.2</cell><cell>25.0</cell><cell>96.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Chunking</cell><cell></cell><cell></cell></row><row><cell>de</cell><cell cols="5">66.3?4.0 90.2?0.3 52.9?2.9 28.0?0.5 93.5?0.1</cell></row><row><cell>en</cell><cell cols="5">46.6?2.1 73.0?0.8 70.7?0.6 19.8?0.3 90.8?0.1</cell></row><row><cell>Avg.</cell><cell>56.4</cell><cell>81.6</cell><cell>61.8</cell><cell>23.9</cell><cell>92.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Detailed results on importance of embeddings for each language.</figDesc><table><row><cell>EMBEDDINGS</cell><cell></cell><cell></cell><cell></cell><cell cols="3">MaxEnt models on WikiAnn NER</cell><cell></cell><cell></cell><cell></cell></row><row><cell>M F W C</cell><cell>ar</cell><cell>cs</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>fr</cell><cell>nl</cell><cell>ta</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell cols="4">LOW-RESOURCE: 10 SENTENCES</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Averaged F1 scores over 8 languages for WikiAnn NER.</figDesc><table><row><cell>EMBEDDINGS</cell><cell></cell><cell></cell><cell></cell><cell cols="3">CRF models on WikiAnn NER</cell><cell></cell><cell></cell><cell></cell></row><row><cell>M F W C</cell><cell>ar</cell><cell>cs</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>fr</cell><cell>nl</cell><cell>ta</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell cols="4">LOW-RESOURCE: 10 SENTENCES</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>Averaged F1 scores over 8 languages for WikiAnn NER with the CRF layer.</figDesc><table><row><cell>EMBEDDINGS</cell><cell></cell><cell></cell><cell></cell><cell cols="3">MaxEnt models on UD POS tagging</cell><cell></cell><cell></cell><cell></cell></row><row><cell>M F W C</cell><cell>ar</cell><cell>cs</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>fr</cell><cell>nl</cell><cell>ta</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell cols="4">LOW-RESOURCE: 10 SENTENCES</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Averaged F1 scores over 2 languages for chunking with the CRF layer. 6?0.2 88.4?0.2 85.5?0.3 81.7?0.2 90.6?0.2 88.4?0.5 90.1?0.3 85.5?0.3 86.8 83.6?0.8 87.7?0.3 84.2?0.3 81.4?0.1 90.0?0.1 87.9?0.1 89.9?0.4 84.2?0.2 86.1 84.8?0.5 88.6?0.2 85.1?0.3 82.0?0.2 90.3?0.4 88.1?0.3 90.1?0.2 85.3?0.3 86.8 0?0.1 98.8?0.0 95.4?0.1 97.0?0.1 97.3?0.1 99.1?0.1 96.7?0.1 92.5?0.4 96.7 96.8?0.0 98.7?0.0 95.2?0.1 96.6?0.1 97.2?0.0 99.0?0.0 96.5?0.1 91.2?0.3 96.4 97.0?0.0 98.8?0.0 95.3?0.0 96.9?0.1 97.3?0.1 99.1?0.0 96.7?0.1 92.7?0.5 96.7</figDesc><table><row><cell>EMBEDDINGS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Languages</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>M F W C MF</cell><cell>ar</cell><cell>cs</cell><cell>de</cell><cell>en</cell><cell>NER es</cell><cell>fr</cell><cell>nl</cell><cell>ta</cell><cell>Avg.</cell></row><row><cell></cell><cell cols="5">84.POS TAGGING</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ar</cell><cell>cs</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>fr</cell><cell>nl</cell><cell>ta</cell><cell>Avg.</cell></row><row><cell></cell><cell cols="5">97.CHUNKING</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>de</cell><cell>en</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg.</cell></row><row><cell></cell><cell cols="2">94.0?0.0 91.5?0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92.8</cell></row><row><cell></cell><cell cols="2">94.1?0.1 91.6?0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92.9</cell></row><row><cell></cell><cell cols="2">94.2?0.1 91.7?0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 :</head><label>17</label><figDesc>Detailed comparison for additionally concatenating MF with All. MF represents the M-Flair embeddings.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We do not use contextual word embeddings such as ELMo (Peters et al., 2018) since Akbik et al. (2018)showed that concatenating Flair embeddings with ELMo embeddings cannot further improve the accuracy.2  We do not use the pooled version of Flair due to its slower speed in training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We find that the observations from the MaxEnt experiments do not change in all experiments with the CRF approach.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The observation is consistent with the observation of Akbik et al. (2018), but we experimented on more languages and tasks with the M-BERT embeddings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes 6 https://elisa-ie.github.io/wikiann/ 7 https://www.clips.uantwerpen.be/conll2003/ner/ 8 https://lindat.mff.cuni.cz/repository/xmlui/handle/ 11234/1-2837</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Natural Science Foundation of China (61976139). This work also was supported by Alibaba Group through Alibaba Innovative Research Program. The authors wish to thank Chao Lou for his helpful comments and suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pooled contextualized embeddings for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>References Alan Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vollgraf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1078</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="724" to="728" />
		</imprint>
		<respStmt>
			<orgName>Long and Short Papers</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
