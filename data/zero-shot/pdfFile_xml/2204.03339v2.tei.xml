<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Self-Supervised Embeddings for Speech Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo-Hsuan</forename><surname>Hung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Wei</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan-Hsin</forename><surname>Tseng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Tien</forename><surname>Chiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Academia Sinica</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
							<email>yu.tsao@citi.sinica.edu.twcwlinx@ntu.edu.tw</email>
							<affiliation key="aff2">
								<orgName type="department">Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chii-Wann</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Self-Supervised Embeddings for Speech Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Self-supervised learning</term>
					<term>cross-domain feature</term>
					<term>noise robustness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning (SSL) representation for speech has achieved state-of-the-art (SOTA) performance on several downstream tasks. However, there remains room for improvement in speech enhancement (SE) tasks. In this study, we used a crossdomain feature to solve the problem that SSL embeddings may lack fine-grained information to regenerate speech signals. By integrating the SSL representation and spectrogram, the result can be significantly boosted. We further study the relationship between the noise robustness of SSL representation via clean-noisy distance (CN distance) and the layer importance for SE. Consequently, we found that SSL representations with lower noise robustness are more important. Furthermore, our experiments on the VCTK-DEMAND dataset demonstrated that fine-tuning an SSL representation with an SE model can outperform the SOTA SSL-based SE methods in PESQ, CSIG and COVL without invoking complicated network architectures. In later experiments, the CN distance in SSL embeddings was observed to increase after fine-tuning. These results verify our expectations and may help design SE-related SSL training in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Speech is an effective and efficient way of communication between individuals, playing an essential role in human-computer interactions. However, during communication, there is often undesired interference from the environment and surroundings, such as environmental noise, background noise and reverberations, so that speech quality and intelligibility often degrade. The process of reducing the background noise with optimal preservation of the original speech quality is then referred to as speech enhancement (SE).</p><p>With recent developments in deep learning (DL), deep learning-based SE models have mostly outperformed traditional SE methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. The DL-based SE framework generally concerns input features <ref type="bibr" target="#b3">[4]</ref>, advanced SE models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, objective functions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and data augmentations <ref type="bibr" target="#b9">[10]</ref>. This study aims to evaluate the relationship and impact of input features regarding DL-based SE performance.</p><p>Self-supervised learning (SSL) utilizes a large amount of unlabeled data to extract meaningful representations <ref type="bibr" target="#b10">[11]</ref>. In many applications, supervised learning generally outperforms unsupervised learning. However, collecting a large amount of labeled data is time-consuming and sometimes unrealistic. Therefore, the SSL can be leveraged in the circumstances with amounts of unlabeled data to provide expressive (latent) representations and use these latent features as inputs for downstream tasks. It has been verified in various fields that the SSL improves the performance of downstream tasks. Particularly, a few promising SSL models have been proposed for speech-related tasks. One major application is speech recognition, where the contributing SSL models include contrastive predictive coding (CPC) <ref type="bibr" target="#b11">[12]</ref>, wav2vec <ref type="bibr" target="#b12">[13]</ref>, and HuBERT <ref type="bibr" target="#b13">[14]</ref>. Recently, there have been some application scenarios where the output representation of an SSL model is used to replace the conventional (data) feature and it turns out to achieve better performance than the original input features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Currently, there are only few studies applying SSL features to SE. Huang et al. <ref type="bibr" target="#b17">[18]</ref> proposed applying SSL features to SE, where the authors observed that when training with weightedsum representations "for most SSL models, lower layers generally obtain higher weights.". This may be because ''some local signal information necessary for speech reconstruction tasks is lost in the deeper layers". In this study, to solve the aforementioned problem, we propose two simple solutions: 1) utilizing cross-domain features as model inputs to compensate for the information loss in SSL features, 2) fine-tuning an SSL model together with an SE model such that the extracted SSL features can derive useful information for SE. Additionally, we analyze the noise-robustness property of SSL features and provide some insight into the relationship with SE. In summary, without introducing complicated or advanced models, our results are comparable to those of state-of-the-art (SOTA) SE methods in the VCTK-DEMAND dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we first briefly review some commonly-used SSL models and studies using cross-domain features and fine-tuned SSLs in other specific tasks. Related research using SSL on speech enhancement is also surveyed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">SSL models</head><p>The SSL model can be categorized into generative modeling, discriminative modeling and multi-task learning. Generative modelling extracts the data input to the representation by the encoder and reconstruct it back to the input by the decoder. These include APC <ref type="bibr" target="#b18">[19]</ref>, Mockingjay <ref type="bibr" target="#b19">[20]</ref>, DecoAR 2.0 <ref type="bibr" target="#b20">[21]</ref>, and Audio2Vec <ref type="bibr" target="#b21">[22]</ref>. Discriminative modeling extracts the input into an representation and measures the corresponding similarity. Such studies include CPC, HuBERT <ref type="bibr" target="#b13">[14]</ref>, WavLM <ref type="bibr" target="#b22">[23]</ref> and SPIRAL <ref type="bibr" target="#b23">[24]</ref>. One of the most representative works for the multi-task learning approach is PASE+ <ref type="bibr" target="#b24">[25]</ref>, which picks up a meaningful speech representation capable of the multi-tasking objective. In this study, we adopted three SSL models to extract the latent representations: Wav2vec 2.0, HuBERT and WavLM, where they all achieved excellent performances in SUPERB <ref type="bibr" target="#b25">[26]</ref>, a challenge to gauge the performance of SSL models under different speech tasks. arXiv:2204.03339v2 [eess.AS] 5 Jul 2022</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cross-domain feature and fine-tuning SSL</head><p>Studies <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b26">[27]</ref> have shown that the cross-domain feature can help increase task accuracy on ASR and speech assessment metrics. Additionally, fine-tuning an SSL on the downstream task has also been shown to provide a significant improvement. In wav2vec 2.0, the SSL model was fine-tuned on label data with the CTC loss for downstream recognition tasks. There is other literature fine-tuning SSL models for non-ASR speech tasks, e.g., on speech emotion recognition <ref type="bibr" target="#b14">[15]</ref>, spoken language understanding <ref type="bibr" target="#b16">[17]</ref>, and MOS prediction <ref type="bibr" target="#b15">[16]</ref>. The research above showed that fine-tuning the SSL or combining SSL embeddings with raw features can achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">SSL for SE</head><p>Self-supervised pre-trained models have been increasingly applied to many speech-related tasks, including SE <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Several works PFPL <ref type="bibr" target="#b29">[30]</ref>, PERL <ref type="bibr" target="#b30">[31]</ref>, and K-SENet <ref type="bibr" target="#b31">[32]</ref> applied SSL pretrained models as perceptual loss. Some other works extracted latent representations as SE model input, such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref> extracted the latent variables and evaluated 11 SSL upstream methods on the SE downstream task. SSPF <ref type="bibr" target="#b33">[34]</ref> utilized phonetic characteristics into a deep complex convolutional network via a CPC model pre-trained with self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Incorporate spectrogram with SSL embedding as input</head><p>In <ref type="bibr" target="#b17">[18]</ref>, the authors utilized the SSL representation on the SE as the downstream task. When training with weighted-sum representations, they found that the lower layers generally obtained higher weightings. In additional, <ref type="bibr" target="#b34">[35]</ref> analyzed the layer-wise representation of Wav2vec 2.0, which showed that the transformer layers in Wav2vec 2.0 followed an autoencoder-style behavior. The latent representation in the lower layers correlates with raw acoustic features such as FBANK. Therefore, combining these two observations, we reasoned that for generation tasks such as SE, the raw acoustic feature should be provided to compensate for the fine-grained deficiencies. In this study, we assess the combination of the log 1p [36] 1 spectrogram feature with SSL representation. The model architecture is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The noisy waveform is first fed into the SSL model to generate a latent representation, which is subsequently concatenated with the noisy log 1p feature as the model input. The enhancement prediction is therefore obtained by multiplying the model output with noisy log 1p features. In the inference stage, a noisy phase is applied to reconstruct the enhanced waveform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fine-tune SSL model with SE</head><p>Fine-tuning the pre-trained SSL model with the downstream tasks can generally obtain better results as mentioned in Sec. 2.2. In this study, we follow <ref type="bibr" target="#b14">[15]</ref> to fine-tune the pre-trained SSL model in two ways: partial fine-tuning (PF) and entire finetuning (EF). SSL models are generally separated into CNN-based feature extractors and transformer-based encoders. For PF, only the transformer-based encoder is fine-tuned during downstream training. As for EF, both the feature extractor and the encoder are fine-tuned. To our best knowledge, this is the first study applying various fine-tuning SSL models for SE tasks. <ref type="bibr" target="#b0">1</ref> The log 1p feature is referred to as the scaled transformation (log 1p)(X tf ) := log(1 + X tf ) ? 0 for a spectrogram amplitude X = (X tf ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Noise robustness analysis</head><p>Noise robustness is usually desired when training the SSL. Therefore, we intended to investigate whether a representation equipped with noise robustness may help improve SE. Given an SSL model f , we define the notion of the distance between the latent variables of noisy speech and clean speech as:</p><formula xml:id="formula_0">d f, Z ( ) c , Z ( ) n = 1 T T t=1 g ( ) (z ( ) c (t)) ? g ( ) (z ( ) n (t)) 2 (1) where ? N is the layer depth, Z ( ) c := {z ( ) c (t)} T t=1 and Z ( ) n := {z ( ) n (t)} T</formula><p>t=1 denote the collection of clean and noisy latent representations of layer in model f and frame t, respectively. Particularly, = 0 corresponds to the output of feature extractor <ref type="figure" target="#fig_0">(Fig. 1)</ref>. A normalization function g ( ) on each layer is deployed to normalize the latent features z ( ) (t) for balancing scales and ensuring equal comparisons. Specifically, we shall utilize g ( ) (z ( ) (t)) := (z ( ) (t) ? ? )/? with ? and ? denoting the mean and variance of the latent points Z ( ) := {z ( ) (t)} T t=1 . This study computes the layer-wise distance d f, (Z ( ) c , Z ( ) n ) as the noise robustness, so that when the distance d f, is small, this layer is regarded as noise robust. In addition to noise robustness, we further consider the layer weighting as an importance index for the SSL layers on SE via the weighted-sum approach <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_1">ZWS := L?1 =0 w( ) Z ( )<label>(2)</label></formula><p>with parameters w( ) ? 0, w( ) = 1 denoting the weight of layer determined by the network and Z ( ) is required to keep same dimension for all layers. Later in Sec. 4.3.2, a bundle of CN distance curves shall be computed to analyze the averaging tendency under stochastic training process with different SSL models. Namely, a SSL model f with randomly sampled inputs (Xc, Xn) drawn from the entire training set yield ? d f, (Z ( ) c , Z ( ) n ) by Eq. (1). Thus, random sampling from data distribution P gives an averaging CN distance curve,</p><formula xml:id="formula_2">d f, = E (Xc,Xn)?P d f, Z ( ) c , Z ( ) n<label>(3)</label></formula><p>Experiments will be set up in the next section to verify our boosting method and the relationship with SSL latent representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation metrics</head><p>A commonly used premixed dataset VCTK-DEMAND <ref type="bibr" target="#b36">[37]</ref> was adopted to evaluate our method. There were 11,572 utterances with four signal-to-noise ratios (SNRs) (15, 10, 5, and 0 dB) in the training set and 824 utterances with four SNRs (17.5, 12.5, 7.5, and 2.5 dB) in the testing set. The experimental results are assessed with wideband PESQ and STOI for speech quality and intelligibility. Another three widely used metrics: CSIG, CBAK and COVL are applied to measure signal distortion, noise distortion, and overall quality evaluation, respectively. Our implementation is available at https://github.com/khhungg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Structures</head><p>A BLSTM is adopted as an SE model for light weight purpose with decent performance. The model architecture is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>, consisting of (a) 2 linear layers, (b) two-layered BLSTM of 256 hidden units and (c) a sigmoid activation to generate the prediction mask. During the training stage, to obtain fixed-length data within a batch, each utterance was randomly sampled as 20,480 samples (128 frames ? 160 hop length). The signal approximation (SA) method <ref type="bibr" target="#b37">[38]</ref> was used to estimate the target spectrum via the mask prediction. L1-loss and the Adam optimizer were deployed for the SE model, along with a random splitting of training and validation set by 95% and 5% ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Including spectrogram as extra input features</head><p>In a previous study <ref type="bibr" target="#b17">[18]</ref>, only fixed self-supervised embeddings were used as the input features of the SE model, and the performance improvement was somewhat limited. This may be because most of the self-supervised models were trained by maximizing the prediction probability of the target class. Features aimed for classification tasks may not fully retain detailed speech information and thus the suitability for direct application on SE generation tasks can be questioned. To solve the aforementioned problem, we concatenated the log 1p spectrogram with SSL embedding to preserve useful information in speech. For the spectrogram, the window size and hop length are set as 400 and 160, respectively. As SSL features have twice the stride length of the spectrogram, we duplicated the latent representation to align the lengths of the embedding and spectrogram. To show the impact of adding the spectrogram as extra model input, comparisons were made in <ref type="table" target="#tab_0">Table 1</ref>. As the baselines, we trained the SE model with the embeddings from 1) the last hidden layer (LL) and 2) the weighted sum (WS) of all the embedding layers with the learned weights.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, as a conventional method, we first show the results of applying only log 1p spectrogram as the model input. For SSL embedding, we prepared three different models: wav2vec 2.0, HuBERT, and WavLM-Base. From the table, we can first observe that using the last hidden layer of SSL models did not bring benefits compared to the spectrogram features. However, using WS of all the embedding layers can significantly improve the performance, implying that other layers in the SSL models also contain useful information for SE. For both the LL and WS, when the log 1p spectrogram is concatenated with the SSL features, the performance can improve further. The best performance results from the cross-domain feature (WS + log 1p). Thus, this verified that including acoustic features improved the SE performance. An SSL model f results in layer weighting curves <ref type="figure" target="#fig_1">(Fig. 2)</ref>,</p><formula xml:id="formula_3">? w (f ) SSL ( ) and ? w (f )</formula><p>SSL+log1p ( ), corresponding to the use of the SSL feature and the cross-domain features, respectively. As <ref type="bibr" target="#b34">[35]</ref> reported that there existed some peculiarity in the last two layers of wav2vec 2.0, we removed the last two layers in wav2vec 2.0 for comparison here. In these three models, it was observed that the layer weightings and CN distance d f, are highly correlated (? 0.85) via the Pearson correlation P (w (f ) SSL ( ), d f, ), for f ? {WavLM, HuBERT, wav2vec 2.0}. Thus, we concluded that large CN distances might provide more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Fine-tuning SSL models</head><p>From <ref type="table" target="#tab_0">Table 1</ref>, it can be observed that applying WavLM as an SSL model outperforms the other two methods. Under the same model size, WavLM achieves 3.05 and 0.952 in PESQ and STOI, respectively, higher than HuBERT (PESQ: 2.99/ STOI: 0.949) and Wav2vec 2.0 (PESQ: 2.95/ STOI: 0.949). Henceforth, we shall only adopt WavLM for the discussion of the fine-tuning experiments later. <ref type="table" target="#tab_1">Table 2</ref> presents the fine-tuned results of WavLM, for both WavLM-Base and WavLM-Large, the best performance comes from PF with log 1p. It can also be observed that EF with log 1p did not improve much compared with PF. Fine-tune models with log 1p have only incremental improvements compared with the original pre-trained models (cf. <ref type="table" target="#tab_0">Table 1</ref>). From these observations, we believe that the SSL models after fine-tune acquired better latent representations for SE, such that the additional log 1p features can only provide limited extra information for enhancement. We also observed that the performance sharply degraded if we used a model with the same architecture but trained from scratch (TFS), which indicated that pre-training certainly contributes. Some SOTA SE models using a pre-trained SSL model are listed in <ref type="table" target="#tab_1">Table 2</ref> for comparison.</p><p>In addition to evaluating the performance of SSL fine-tuned model, the corresponding CN distances were also calculated, as shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. The figure reveals that the CN distances in the first and last few layers increased after fine-tuning. This was particularly obvious in the last few layers. From <ref type="figure" target="#fig_2">Fig. 3(b)</ref>, we saw similar trends for learned weights, which verify the observation that large CN distances may provide more information as given in Sec. 4.3.2.</p><p>Another interesting finding in <ref type="figure" target="#fig_2">Fig. 3(b)</ref> is that when using the SSL representations only (orange and green dotted lines), the weights in the first few layers also increase and become larger than that of SSL+log 1p (red and purple dotted lines). We argue that after fine-tuning without log 1p input, the first few layers learn more information about raw acoustic features. Since the first few layers can keep more local information now, the performance gained by log 1p feature becomes much smaller, as shown in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this study, we propose two techniques: 1) utilizing crossdomain features as model inputs, 2) fine-tuning the SSL model with the SE task to compensate for the information loss in the  SSL features. The results met our expectations in that the SE performance with the cross-domain feature was significantly improved than using only SSL representation or the spectrogram feature. Compared to SOTA SSL-based SE methods, our proposal for fine-tuning an SSL representation with the SE task can outperform them in PESQ, CSIG, and COVL without invoking complicated network architectures or training flow. Furthermore, we studied the relationship between the noise robustness of SSL representation and the importance of SE. Our observation showed that less noise-robust SSL features possess higher corresponding importance. Although this fact appeared counterintuitive, we addressed the rationale behind the scenes. In the end, we found that the SSL representation with the weighted-sum method has proven superior to the spectrogram feature. However, training an SSL representation to entirely replace raw acoustic features is yet to be explored. Our findings serve to help train SE-related SSL representation in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flowchart of the proposed method with cross-domain feature to boost the performance of SE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Correlation between the CN distance (solid line bundle, see Eq. (3)) and the learned weights w( ) (blue/orange dotted line) of Eq. (2) using SSL only and SSL + log 1p features, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The averaging CN distance (a) and the layer weighting curves (b) before (solid line) and after (dotted line) different fine-tuning strategies. Plot (a) and (b) shares the same legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Scores for various combinations of log 1p and latent representations under different SSL models. LL denotes the last layer embedding and WS as the weighted sum of all SSL layers.</figDesc><table><row><cell>Method</cell><cell cols="5">PESQ CSIG CBAK COVL STOI</cell></row><row><cell>Noisy</cell><cell>1.97</cell><cell>3.35</cell><cell>2.44</cell><cell>2.63</cell><cell>0.915</cell></row><row><cell>log 1p</cell><cell>2.75</cell><cell>4.15</cell><cell>3.36</cell><cell>3.46</cell><cell>0.944</cell></row><row><cell></cell><cell cols="3">wav2vec 2.0-Base</cell><cell></cell><cell></cell></row><row><cell>LL</cell><cell>2.71</cell><cell>4.10</cell><cell>3.26</cell><cell>3.40</cell><cell>0.942</cell></row><row><cell cols="2">LL + log 1p 2.91</cell><cell>4.29</cell><cell>3.42</cell><cell>3.60</cell><cell>0.948</cell></row><row><cell>WS</cell><cell>2.85</cell><cell>4.22</cell><cell>3.38</cell><cell>3.54</cell><cell>0.946</cell></row><row><cell cols="2">WS + log 1p 2.94</cell><cell>4.32</cell><cell>3.45</cell><cell>3.64</cell><cell>0.949</cell></row><row><cell></cell><cell></cell><cell cols="2">HuBERT-Base</cell><cell></cell><cell></cell></row><row><cell>LL</cell><cell>2.67</cell><cell>4.04</cell><cell>3.20</cell><cell>3.35</cell><cell>0.942</cell></row><row><cell cols="2">LL + log 1p 2.94</cell><cell>4.31</cell><cell>3.46</cell><cell>3.63</cell><cell>0.948</cell></row><row><cell>WS</cell><cell>2.84</cell><cell>4.23</cell><cell>3.37</cell><cell>3.54</cell><cell>0.947</cell></row><row><cell cols="2">WS + log 1p 2.98</cell><cell>4.34</cell><cell>3.48</cell><cell>3.67</cell><cell>0.949</cell></row><row><cell></cell><cell></cell><cell cols="2">WavLM-Base</cell><cell></cell><cell></cell></row><row><cell>LL</cell><cell>2.74</cell><cell>4.05</cell><cell>3.22</cell><cell>3.39</cell><cell>0.944</cell></row><row><cell cols="2">LL + log 1p 2.94</cell><cell>4.32</cell><cell>3.44</cell><cell>3.64</cell><cell>0.950</cell></row><row><cell>WS</cell><cell>2.90</cell><cell>4.28</cell><cell>3.43</cell><cell>3.59</cell><cell>0.949</cell></row><row><cell cols="2">WS + log 1p 3.05</cell><cell>4.40</cell><cell>3.52</cell><cell>3.74</cell><cell>0.952</cell></row><row><cell cols="6">4.3.2. Noise robustness and learned weights of weighted sum</cell></row><row><cell>representation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Fig. 2 shows the CN distances (solid lines) and layer weightings</cell></row><row><cell cols="6">(dotted lines) for the various SSL models. Each solid line follows</cell></row><row><cell cols="6">from Eq. (1) with randomly sampled inputs (Xc, Xn) out of the</cell></row><row><cell cols="6">training set, as described in Sec. 3.3. The averaging CN distance</cell></row><row><cell cols="6">curve is then calculated by Eq. (3) to compare with the trend</cell></row><row><cell cols="6">of layer weighting curves. To ensure the distance d f, falling</cell></row><row><cell cols="6">into [0, 1], the min-max normalization was employed after the</cell></row><row><cell cols="2">computation of Eq. (1).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Scores for different SSL fine-tuning strategies. TFS denotes training from scratch (random initial weights in WavLM).</figDesc><table><row><cell>Method</cell><cell cols="5">PESQ CSIG CBAK COVL STOI</cell></row><row><cell cols="2">SSPF [34] 3.13</cell><cell>4.30</cell><cell>3.61</cell><cell>3.72</cell><cell>0.950</cell></row><row><cell cols="2">PERL [31] 3.04</cell><cell>4.23</cell><cell>3.42</cell><cell>3.63</cell><cell>0.947</cell></row><row><cell cols="2">PFPL [30] 3.15</cell><cell>4.18</cell><cell>3.60</cell><cell>3.67</cell><cell>0.950</cell></row><row><cell cols="2">Huang [18] 2.80</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.945</cell></row><row><cell></cell><cell></cell><cell cols="2">WavLM-Base (WS)</cell><cell></cell><cell></cell></row><row><cell>TFS</cell><cell>2.83</cell><cell>4.21</cell><cell>3.41</cell><cell>3.53</cell><cell>0.946</cell></row><row><cell>PF</cell><cell>3.09</cell><cell>4.42</cell><cell>3.54</cell><cell>3.77</cell><cell>0.955</cell></row><row><cell>EF</cell><cell>3.11</cell><cell>4.44</cell><cell>3.56</cell><cell>3.79</cell><cell>0.955</cell></row><row><cell>log 1p</cell><cell>3.05</cell><cell>4.40</cell><cell>3.52</cell><cell>3.74</cell><cell>0.952</cell></row><row><cell cols="2">log 1p + PF 3.16</cell><cell>4.50</cell><cell>3.57</cell><cell>3.86</cell><cell>0.956</cell></row><row><cell cols="2">log 1p + EF 3.12</cell><cell>4.49</cell><cell>3.56</cell><cell>3.83</cell><cell>0.956</cell></row><row><cell></cell><cell></cell><cell cols="2">WavLM-Large (WS)</cell><cell></cell><cell></cell></row><row><cell>TFS</cell><cell>2.87</cell><cell>4.24</cell><cell>3.41</cell><cell>3.57</cell><cell>0.945</cell></row><row><cell>PF</cell><cell>3.14</cell><cell>4.47</cell><cell>3.57</cell><cell>3.82</cell><cell>0.957</cell></row><row><cell>EF</cell><cell>3.17</cell><cell>4.49</cell><cell>3.58</cell><cell>3.85</cell><cell>0.956</cell></row><row><cell>log 1p</cell><cell>3.09</cell><cell>4.45</cell><cell>3.53</cell><cell>3.79</cell><cell>0.954</cell></row><row><cell cols="2">log 1p + PF 3.20</cell><cell>4.53</cell><cell>3.60</cell><cell>3.88</cell><cell>0.957</cell></row><row><cell cols="2">log 1p + EF 3.15</cell><cell>4.50</cell><cell>3.59</cell><cell>3.85</cell><cell>0.957</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech enhancement based on deep denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech enhancement from fused features based on deep neural network and gated recurrent unit network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A perceptually-motivated approach for lowcomplexity, real-time enhancement of fullband speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04259</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Phase-aware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tensor-to-vector regression for multi-channel speech enhancement based on tensor-train network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Speech enhancement in multiplenoise conditions using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Florencio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02427</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Personalized speech enhancement through self-supervised data augmentation and purification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02018</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Specmix: A mixed sample data augmentation method for training withtime-frequency domain features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Audio self-supervised learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallol-Ragolta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parada-Cabeleiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01205</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hubert: Self-supervised speech representation learning by masked prediction of hidden units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3451" to="3460" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boumadane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02735</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalization ability of mos prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP 2022</title>
		<meeting>ICASSP 2022</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Investigating self-supervised pre-training for end-to-end speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Est?ve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Investigating self-supervised learning for speech enhancement and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP 2022</title>
		<meeting>ICASSP 2022</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Decoar 2.0: Deep contextualized acoustic representations with vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06659</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pre-training audio representations with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="600" to="604" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">WavLM: Large-scale selfsupervised pre-training for full stack speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13900</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Spiral: Self-supervised perturbation-invariant representation learning for speech pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10207</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised learning for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Superb: Speech processing universal performance benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><forename type="middle">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-T</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01051</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep learning-based non-intrusive multi-objective speech assessment model with cross-domain features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Zezario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Fuh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02363</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Selfsupervised learning for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10388</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Selfsupervised denoising autoencoder with linear regression decoder for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Zezario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving perceptual quality by phone-fortified perceptual loss using wasserstein distance for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15174</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Perceptual loss based speech denoising with an ensemble of audio pattern recognition and selfsupervised models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP 2021</title>
		<meeting>ICASSP 2021</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boosting the intelligibility of waveform speech enhancement networks through self-supervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMLA 2021</title>
		<meeting>ICMLA 2021</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Superbsg: Enhanced speech processing universal performance benchmark for semantic and generative capabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><forename type="middle">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06849</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-supervised learning based phone-fortified speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Layer-wise analysis of a self-supervised speech representation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU 2021</title>
		<meeting>ASRU 2021</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Boosting objective scores of a speech enhancement model by metricgan post-processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Zezario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Investigating rnn-based speech enhancement methods for noiserobust text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SSW</title>
		<meeting>SSW</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Supervised speech enhancement with real spectrum approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
