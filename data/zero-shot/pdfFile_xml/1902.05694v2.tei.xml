<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lightweight Feature Fusion Network for Single Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuifa</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingmin</forename><surname>Liao</surname></persName>
						</author>
						<title level="a" type="main">Lightweight Feature Fusion Network for Single Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image super-resolution(SISR) has witnessed great progress as convolutional neural network(CNN) gets deeper and wider. However, enormous parameters hinder its application to real world problems. In this letter, We propose a lightweight feature fusion network (LFFN) that can fully explore multi-scale contextual information and greatly reduce network parameters while maximizing SISR results. LFFN is built on spindle blocks and a softmax feature fusion module (SFFM). Specifically, a spindle block is composed of a dimension extension unit, a feature exploration unit and a feature refinement unit. The dimension extension layer expands low dimension to high dimension and implicitly learns the feature maps which is suitable for the next unit. The feature exploration unit performs linear and nonlinear feature exploration aimed at different feature maps. The feature refinement layer is used to fuse and refine features. SFFM fuses the features from different modules in a self-adaptive learning manner with softmax function, making full use of hierarchical information with a small amount of parameter cost. Both qualitative and quantitative experiments on benchmark datasets show that LFFN achieves favorable performance against stateof-the-art methods with similar parameters. Code is avaliable at https://github.com/qibao77/LFFN-master.</p><p>Index Terms-Super-resolution, convolutional neural network, softmax feature fusion module, spindle block.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S INGLE image super-resolution (SISR) is an important low-level computer vision task which aims at recovering a high-resolution (HR) image from a low-resolution (LR) image. It is a seriously ill-posed problem since an LR image can be mapped to an infinite number of HR images. Recently, deep convolutional neural network (CNN) has greatly facilitated improvements in this field. Dong et al. <ref type="bibr" target="#b0">[1]</ref> firstly proposed a three-layer CNN to establish a mapping between LR and HR. <ref type="bibr">Kim et al.</ref> proposed the well-known VDSR <ref type="bibr" target="#b1">[2]</ref>, which introduced residual learning and adaptive gradient clipping to alleviate the difficulty of training deep network. In DRCN <ref type="bibr" target="#b2">[3]</ref>, the recursive network was used to reduce the model parameters and a multi-supervised strategy was adopted to fuse intermediate results. Benefiting from skip-connection can alleviate the vanishing-gradient problem <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, Lim et al. <ref type="bibr" target="#b5">[6]</ref> built a very deep network MDSR (more than 160 layers) with residual blocks.</p><p>Researchers usually deepen and widen the network to achieve better performance. However, even constructed with W. <ref type="bibr">Yang</ref>  small convolution kernels, such as 3 ? 3, the network will take up large memories. In order to lighten the deep network, some strategies have been adopted. DRRN <ref type="bibr" target="#b6">[7]</ref> employed parameter sharing strategy to reduce parameters, but it still requires large computation objectively. CARN-M <ref type="bibr" target="#b7">[8]</ref> adopt group convolution to attack a trade-off between computation and performance of the model. Unfortunately, applying group convolution directly to SISR will obviously impair performance. To address these problems, we propose a lightweight network LFFN to compute the HR image from the original LR image. In LFFN, we introduce a new organization of the inception-residual block <ref type="bibr" target="#b8">[9]</ref>, named spindle block, which contains a dimension extension unit, a feature exploration unit and a feature refinement unit. The dimension extension unit can learn the feature maps suitable for the next unit, and the architecture can be mitigated by fewer filters in backbone. Inspired by ResNeXt <ref type="bibr" target="#b9">[10]</ref> and Xception <ref type="bibr" target="#b10">[11]</ref>, we introduce a feature exploration unit to explore the linear and nonlinear as well as multi-scale information for 4 different channel groups. This unit can improve the representational power of the model and can further alleviate the architecture due to fewer filters in each group. We also consider using feature maps of different receptive fields to enhance the performance. Taking computation into account and motivated by feature recalibration demonstrated in SENets <ref type="bibr" target="#b11">[12]</ref>, we develop a softmax feature fusion module (SFFM) to aggregate the features of different levels in a self-adaptive channel-wise convex weighted way rather than the multi-supervised method used in DRCN <ref type="bibr" target="#b2">[3]</ref> and arXiv:1902.05694v2 [cs.CV] 13 Apr 2019</p><p>MemNet <ref type="bibr" target="#b12">[13]</ref>. The parameters of SFFM are not large, since there is only one dense layer applied to each global feature of different levels. And SFFM can learn how to combine the features that are most conducive to reconstruction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network structure</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the overall architecture consists of B ? M spindle blocks, a softmax feature fusion module (SFFM) and an up-sampling module. We denote I lr and I sr as the input and output of LFFN, respectively. First of all, we use a single 3 ? 3 convolutional layer of 48 filters to extract the feature maps from the original LR image:</p><formula xml:id="formula_0">M 0 = F sf (I lr ),<label>(1)</label></formula><p>where F sf (?) represents convolution operation and M 0 serves as the input of next part. The next part is M stacked local feature fusion modules. Inspired by MemNet <ref type="bibr" target="#b12">[13]</ref> and SR-DenseNet <ref type="bibr" target="#b13">[14]</ref>, we concatenate feature maps from B stacked spindle blocks to further make full use of local features. We also introduce residual learning for each module to make deep network training easier. This procedure can be expressed as</p><formula xml:id="formula_1">M d = F d (M d?1 ) = F df ([B d,1 , ? ? ?, B d,k , ? ? ?, B d,B ]) + M d?1 ,<label>(2)</label></formula><p>where F d denotes the d-th module function and F df is the function of the 1 ? 1 convolution in d-th module. M d and B d,k indicate the output of the d-th module and k-th spindle block respectively. More details about spindle block will be given in next section. After extracting complicated features progressively with M modules, we further conduct softmax feature fusion (SFFM).</p><formula xml:id="formula_2">R = F sf f m (M d , ? ? ?, M d , ? ? ?, M d ),<label>(3)</label></formula><p>where R is the output feature maps of SFFM, F sf f m denotes a composite function. Finally, like <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b5">[6]</ref> , we utilize ESPCN <ref type="bibr" target="#b15">[16]</ref> followed by a convolution layer to upscale the refined feature maps and get the output of LFFN. It is worth mentioning that we replace the 3 ? 3 convolution with 1 ? 1 convolution in upscale module and the last layer to further reduce parameters.</p><formula xml:id="formula_3">I sr = F last (F up (F f use (R) + M 0 )),<label>(4)</label></formula><p>where F last and F f use denote the 1 ? 1 convolution and F up is the function of upscale module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spindle Block</head><p>To reap the benefits of inception residual block <ref type="bibr" target="#b8">[9]</ref> and group convolution, we propose a well-designed residual block, named spindle block. The overall block can be formulated as:</p><formula xml:id="formula_4">B d,k = B d,k?1 + F f ru (F f eu (F deu (B d,k?1 )),<label>(5)</label></formula><p>where F f ru , F f eu and F deu represent compound function of three basic units respectively. And more details about them explicated as follows. 1) dimension extension unit: The number of filters is a critical factor to improve the efficiency of deep networks, which is fixed to 64 in many deep methods for SISR currently. We can lighten the architecture by decreasing the filters, but the performance fluctuates accordingly. Using "bottleneck layer" <ref type="bibr" target="#b16">[17]</ref> (1?1 convolution) to compress dimensions resemble pooling operation in channel dimension. We believe that reducing feature channels before non-linear layer can lead to information loss. Here, we expand the dimensions from 48 to 64 before non-linear mapping to maintain performance with fewer parameters.</p><p>2) feature exploration unit: As shown in <ref type="figure" target="#fig_2">Fig.3(b)</ref>, we first slice the feature maps into four different 16-dimensional groups. Then, we explore nonlinear information in three groups and linear information in the other group. Specifically, we adopt a sequence of 3 ? 3 convolutional layers followed by parametric rectified linear units (PReLUs) to make full use of the image multi-scale information. Different from <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b8">[9]</ref>, we assemble linear and nonlinear information to boost representational power of basic blocks and directly dispose the expanded feature maps instead of reducing dimension by additional 1 ? 1 convolutions.</p><p>3) feature refinement unit: Then the concatenate feature maps are sent to a 1 ? 1 convolutional layer which acts as refining features, compressing dimensions and overcoming the impact of the slice operation on weakening the information flow.</p><p>Basically, as shown in <ref type="figure" target="#fig_2">Fig.3</ref>, our spindle block can take advantage of linear and nonlinear and multi-scale information with fewer parameters than baseline residual block. In particular, when we use the configuration expressed in <ref type="figure" target="#fig_2">Fig.3</ref>, a spindle block has 30.21% of parameters of a residual block. This ratio can be further decreased to 12.13% by replacing 3?3 convolution in spindle block with depthwise convolution. More analysis will be described in experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Softmax Feature Fusion Module</head><p>Information in different levels of feature maps can complement each other for reconstruction. In order to gain more abundant and efficient information, we focus on hierarchical features and achieve a fusion mechanism. As shown in <ref type="figure">Fig.4</ref>, we take all intermediate feature maps M i as input and generate a fusion representation R. And M i = [m i1 , ..., m ij , ..., m iC ], i = 1, ..., M , j = 1, ..., C, where m ij ? R W ?H denotes the j-th channel of the i-th feature maps M i , and C is the total number of channels. Inspired by squeeze operation in <ref type="bibr" target="#b11">[12]</ref>, we apply global average pooling to each channel to obtain the global channel feature X i = [x i1 , ..., x ij , ..., x iC ] , X i ? R C . Then, we follow it with a dense layer to fully exploit interchannel correlation, as formulated below:</p><formula xml:id="formula_5">Y i = ? i X i ,<label>(6)</label></formula><p>where ? i represent the weight set of i-th dense layer and Y i = [y i1 , ..., y ij , ..., y iC ], Y i ? R C . We utilize concatenation and slice operation and softmax function to produce the weight of the corresponding channel of different features. This process can be expressed as:</p><formula xml:id="formula_6">W j = sof tmax(Y j ),<label>(7)</label></formula><p>where Y j = [y 1j , ..., y ij , ..., y M j ], Y j ? R M and W j = [w 1j , ..., w ij , ..., w M j ], W j ? R M . The final output of SFFM is obtained as the following formula:</p><formula xml:id="formula_7">r j = M i=1 m ij = M i=1 w ij ? m ij , s.t. M i=1 w ij = 1,<label>(8)</label></formula><p>where R = [r 1 , ..., r j , ..., r C ] , r j ? R W ?H and m ij ? R W ?H denotes the j-th channel of the i-th rescaled feature maps M i . SFFM aims to incorporate hierarchical features with as few parameters as possible and each weight vector W i , W i ? R C in SFFM depends on global features of all intermediate feature maps, which is different from channel attention in SENets <ref type="bibr" target="#b11">[12]</ref>. <ref type="figure">Fig. 4</ref>. Softmax feature fusion module (SFFM) architecture. ? denotes element-wise product. GP represents global average pooling. "dense" represents the fully connected layers applied to global features from different modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENT A. Implementation details</head><p>At first, we pre-train our model on 91 images from Yang et al. <ref type="bibr" target="#b17">[18]</ref> and 200 images from the Berkeley Segmentation Dataset <ref type="bibr" target="#b18">[19]</ref>. To further improve the performance, we use a newly-proposed high-quality image dataset DIV2K <ref type="bibr" target="#b19">[20]</ref> which consists of 800 images to fine-tune our pre-trained model. Data augmentation (rotation and flip) is also performed on the 291image dataset and DIV2K dataset. To produce LR images, we downscale the HR images on particular scaling factors with bicubic interpolation. The proposed method is compared on four widely used benchmark datasets: Set5 <ref type="bibr" target="#b20">[21]</ref>, Manga109 <ref type="bibr" target="#b21">[22]</ref>, BSD100 <ref type="bibr" target="#b22">[23]</ref>, Urban100 <ref type="bibr" target="#b23">[24]</ref>. For fair comparison, we evaluate the model with PSNR and SSIM on Y channel (i.e., luminance) of transformed YCbCr space.</p><p>In our final architecture LFFN, 15 spindle modules, each contains 4 spindle blocks, are constructed (i.e., B4M15). We initialize all convolutional filters using the method of He et al. <ref type="bibr" target="#b24">[25]</ref>. We use the L1 loss as our loss function instead of the L2. For optimization, we use the ADAM optimizer <ref type="bibr" target="#b25">[26]</ref> by setting ? 1 = 0.9, ? 2 = 0.999, and = 10 ?8 . We use 16 RGB input patches of size 32 ? 32 from the LR images for training, and the initial learning rate is set to 8 ? 10 ?4 and then decreased to half every 20 epochs. In order to accelerate the convergence, we adopt the adjustable gradient clipping <ref type="bibr" target="#b1">[2]</ref> which has been well implemented in tensorflow. Both training stages are configured the same as demonstrated above except that the initial learning rate is set to 4 ? 10 ?4 during the finetuning. Training a LFFN roughly takes four days with a GTX 1080Ti GPU on the ?2 model. <ref type="table" target="#tab_1">Table I</ref> shows the effects of spindle block and softmax feature fusion module (SFFM). LFFN-NF is LFFN without softmax feature fusion module (SFFM) and we replace spindle blocks with residual blocks <ref type="figure" target="#fig_2">(Fig.3(a)</ref>) in LFFN-NS. The three  networks have the same number of basic blocks (B4M15). Compared with LFFN, the performance of LFFN-NS degraded and the parameters increased by three times, indicating that the proposed spindle block is more effective than residual block. LFFN is obviously superior to LFFN-NF, and the parameters are not increased much, revealing that SFFM is valid for incorporating hierarchical information. Beyond that, as shown in <ref type="figure">Fig.5</ref>, the different channel information of the feature maps used for reconstruction come from all levels. And high-level features play a major role in some channels, while low-level features dominate in other channels, which indicates that aggregating hierarchical features is important for SISR and SFFM can implement it well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons With State-of-the-Art Methods</head><p>We compare LFFN (B4M15) and LFFN-S (B4M4 + depthwise convolution) with state-of-the-art methods. We also compare parameters and computation (Mult-Adds) of each method. And Mult-Adds is calculated by assuming that the spatial resolution of HR image is 1280 ? 720. As shown in <ref type="table" target="#tab_1">Table II</ref>, our LFFN performs favorably against state-of-the-art methods on all datasets. LFFN exceeds Memnet <ref type="bibr" target="#b12">[13]</ref> by a margin of 0.41 PSNR while being 30.32 times less compute than Memnet for upscaling ?4 on Set5. Our smallest network LFFN-S has Mult-Adds about 0.44% of MemNet, 0.17% of DRRN and 3.40% of DWSR on ?4 enlargement, respectively, but still achieves comparable performance. <ref type="figure" target="#fig_0">Fig.1</ref> shows the execution time of different methods. We use the original codes of state-of-the-art methods to evaluate the runtime on the same machine with 2.1 GHz Intel Xeon CPU and GTX 1080 Ti GPU (12G Memory). LFFN faster, lighter and more accurate than the latest lightweight network CARN <ref type="bibr" target="#b7">[8]</ref>. LFFN-S is about 400 times faster and 3.7 times smaller than MemNet</p><p>We also provide qualitative comparison in <ref type="figure" target="#fig_3">Fig.6</ref>. Our smallest network LFFN-S can produce almost the same result as other state-of-the-art methods (e.g., MemNet). Besides, LFFN recovers clearer, more accurate contours and less artifacts than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper,we propose a novel lightweight feature fusion network (LFFN) for single image super-resolution. In order to build a more effective and accurate architecture, we pay more attention to full usage of the feature map information. Whether softmax feature fusion module (SFFM) or the proposed spindle block which serves as the basic building unit can significantly improve the representational capacity of a network with fewer parameters. Experiments well demonstrate the effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Speed and accuracy trade-off. The average PSNR and the average inference time for upscaling ?4 on Set5. LFFN and LFFN-S contain B4M15 and B4M4, respectively. The 3*3 convolution of all spindle blocks in LFFN-S is replaced by depthwise convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of our proposed lightweight feature fusion network (LFFN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Basic block of different architecture. (a) residual block. (b) spindle block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Visual comparison for ?3 SR on "img013", "img062", "img085"from the Urban100 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, W. Wang, X. Zhang and Q. Liao are with the Department of Electronic Engineering, Graduate School at Shenzhen, Tsinghua University, China (E-mail: {yang.wenming@sz, wangwei17@mails, xc-zhang16@mails, liaoqm@}.tsinghua.edu.cn.</figDesc><table /><note>S. Sun is with the Department of Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering, China Three Gorges University, China (E-mail: watersun@ctgu.edu.cn).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDY OF SPINDLE BLOCK AND SFFM FOR SCALE FACTOR ?4</figDesc><table><row><cell></cell><cell cols="2">ON DATASET URBAN100</cell><cell></cell></row><row><cell></cell><cell>LFFN-NF</cell><cell>LFFN-NS</cell><cell>LFFN</cell></row><row><cell>Spindle Block</cell><cell></cell><cell>?</cell><cell></cell></row><row><cell>SFFM</cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>Params.(k)</cell><cell>1,497</cell><cell>4,770</cell><cell>1,531</cell></row><row><cell>PSNR/SSIM</cell><cell>25.97/0.7821</cell><cell>26.20/0.7893</cell><cell>26.24/0.7902</cell></row></table><note>Fig. 5. Information source distribution for different channels of the feature maps used for reconstruction. The figure is drawn by visualizing the weight vector W of intermediate feature maps for the "butterfly" image from Set5 dataset on ?4 enlargement.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II BENCHMARK</head><label>II</label><figDesc>SISR RESULTS. AVERAGE PSNR/SSIM FOR SCALE FACTOR ?2, ?3 AND ?4 ON DATASETS SET5, MANGA109, BSD100 AND URBAN100. RED COLOR INDICATES THE BEST PERFORMANCE.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Set5</cell><cell cols="2">Manga109</cell><cell cols="2">BSD100</cell><cell cols="2">Urban100</cell></row><row><cell>Algorithm</cell><cell>scale</cell><cell>Params(K)</cell><cell>Mult-Adds(G)</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>VDSR [2]</cell><cell>2</cell><cell>665</cell><cell>612.6</cell><cell>37.53</cell><cell>0.9587</cell><cell>37.22</cell><cell>0.9750</cell><cell>31.90</cell><cell>0.8960</cell><cell>30.76</cell><cell>0.9140</cell></row><row><cell>DWSR [27]</cell><cell>2</cell><cell>373</cell><cell>344.0</cell><cell>37.42</cell><cell>0.9568</cell><cell>37.27</cell><cell>0.9719</cell><cell>31.85</cell><cell>0.8944</cell><cell>30.46</cell><cell>0.9162</cell></row><row><cell>DRRN [7]</cell><cell>2</cell><cell>297</cell><cell>6796.9</cell><cell>37.74</cell><cell>0.9591</cell><cell>37.60</cell><cell>0.9736</cell><cell>32.05</cell><cell>0.8973</cell><cell>31.23</cell><cell>0.9188</cell></row><row><cell>MemNet [13]</cell><cell>2</cell><cell>677</cell><cell>2665.0</cell><cell>37.78</cell><cell>0.9597</cell><cell>37.72</cell><cell>0.9740</cell><cell>32.08</cell><cell>0.8978</cell><cell>31.31</cell><cell>0.9195</cell></row><row><cell>CARN [8]</cell><cell>2</cell><cell>1592</cell><cell>222.8</cell><cell>37.76</cell><cell>0.9590</cell><cell>38.28</cell><cell>0.9754</cell><cell>32.09</cell><cell>0.8978</cell><cell>31.92</cell><cell>0.9256</cell></row><row><cell>LFFN</cell><cell>2</cell><cell>1522</cell><cell>342.8</cell><cell>37.95</cell><cell>0.9597</cell><cell>38.73</cell><cell>0.9765</cell><cell>32.20</cell><cell>0.8994</cell><cell>32.39</cell><cell>0.9299</cell></row><row><cell>LFFN-S</cell><cell>2</cell><cell>173</cell><cell>37.9</cell><cell>37.66</cell><cell>0.9585</cell><cell>37.93</cell><cell>0.9746</cell><cell>31.96</cell><cell>0.8963</cell><cell>31.28</cell><cell>0.9192</cell></row><row><cell>VDSR [2]</cell><cell>3</cell><cell>665</cell><cell>612.6</cell><cell>33.66</cell><cell>0.9213</cell><cell>32.01</cell><cell>0.9340</cell><cell>28.82</cell><cell>0.7976</cell><cell>27.14</cell><cell>0.8279</cell></row><row><cell>DWSR [27]</cell><cell>3</cell><cell>373</cell><cell>344.0</cell><cell>33.75</cell><cell>0.9209</cell><cell>32.14</cell><cell>0.9323</cell><cell>28.80</cell><cell>0.7972</cell><cell>27.22</cell><cell>0.8293</cell></row><row><cell>DRRN [7]</cell><cell>3</cell><cell>297</cell><cell>6796.9</cell><cell>34.03</cell><cell>0.9244</cell><cell>32.42</cell><cell>0.9359</cell><cell>28.95</cell><cell>0.8004</cell><cell>27.53</cell><cell>0.8378</cell></row><row><cell>MemNet [13]</cell><cell>3</cell><cell>677</cell><cell>2665.0</cell><cell>34.09</cell><cell>0.9248</cell><cell>32.51</cell><cell>0.9369</cell><cell>28.96</cell><cell>0.8001</cell><cell>27.56</cell><cell>0.8376</cell></row><row><cell>CARN [8]</cell><cell>3</cell><cell>1592</cell><cell>118.8</cell><cell>34.29</cell><cell>0.9255</cell><cell>33.47</cell><cell>0.9429</cell><cell>29.06</cell><cell>0.8034</cell><cell>28.06</cell><cell>0.8493</cell></row><row><cell>LFFN</cell><cell>3</cell><cell>1534</cell><cell>153.6</cell><cell>34.43</cell><cell>0.9266</cell><cell>33.65</cell><cell>0.9445</cell><cell>29.13</cell><cell>0.8059</cell><cell>28.34</cell><cell>0.8558</cell></row><row><cell>LFFN-S</cell><cell>3</cell><cell>185</cell><cell>18.1</cell><cell>34.04</cell><cell>0.9233</cell><cell>32.80</cell><cell>0.9381</cell><cell>28.91</cell><cell>0.8005</cell><cell>27.51</cell><cell>0.8372</cell></row><row><cell>VDSR [2]</cell><cell>4</cell><cell>665</cell><cell>612.6</cell><cell>31.35</cell><cell>0.8838</cell><cell>28.83</cell><cell>0.8870</cell><cell>27.29</cell><cell>0.7251</cell><cell>25.18</cell><cell>0.7524</cell></row><row><cell>DWSR [27]</cell><cell>4</cell><cell>373</cell><cell>344.0</cell><cell>31.39</cell><cell>0.8829</cell><cell>29.01</cell><cell>0.8855</cell><cell>27.27</cell><cell>0.7246</cell><cell>25.27</cell><cell>0.7552</cell></row><row><cell>DRRN [7]</cell><cell>4</cell><cell>297</cell><cell>6796.9</cell><cell>31.68</cell><cell>0.8888</cell><cell>29.18</cell><cell>0.8914</cell><cell>27.38</cell><cell>0.7284</cell><cell>25.44</cell><cell>0.7638</cell></row><row><cell>MemNet [13]</cell><cell>4</cell><cell>677</cell><cell>2665.0</cell><cell>31.74</cell><cell>0.8893</cell><cell>29.42</cell><cell>0.8942</cell><cell>27.40</cell><cell>0.7281</cell><cell>25.50</cell><cell>0.7630</cell></row><row><cell>CARN [8]</cell><cell>4</cell><cell>1592</cell><cell>90.9</cell><cell>32.13</cell><cell>0.8937</cell><cell>30.45</cell><cell>0.9073</cell><cell>27.58</cell><cell>0.7349</cell><cell>26.07</cell><cell>0.7837</cell></row><row><cell>LFFN</cell><cell>4</cell><cell>1531</cell><cell>87.9</cell><cell>32.15</cell><cell>0.8945</cell><cell>30.66</cell><cell>0.9099</cell><cell>27.52</cell><cell>0.7377</cell><cell>26.24</cell><cell>0.7902</cell></row><row><cell>LFFN-S</cell><cell>4</cell><cell>183</cell><cell>11.7</cell><cell>31.79</cell><cell>0.8886</cell><cell>29.76</cell><cell>0.8979</cell><cell>27.42</cell><cell>0.7308</cell><cell>25.52</cell><cell>0.7673</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel multiconnected convolutional network for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="946" to="950" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition (CVPR) workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast, accurate, and, lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08664</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">357</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4809" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="21" to="811" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
	<note>in Computer Vision, 2001. ICCV 2001. Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep wavelet prediction for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
