<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point Cloud Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoran</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Macau</orgName>
								<orgName type="institution" key="instit2">Avenida da Universidade Taipa</orgName>
								<address>
									<postCode>999078</postCode>
									<settlement>Macau</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point Cloud Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LiDAR</term>
					<term>point cloud</term>
					<term>semantic segmentation</term>
					<term>spherical projection</term>
					<term>autonomous driving</term>
					<term>scene understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene understanding based on LiDAR point cloud is an essential task for autonomous cars to drive safely, which often employs spherical projection to map 3D point cloud into multi-channel 2D images for semantic segmentation. Most existing methods simply stack different point attributes/modalities (e.g. coordinates, intensity, depth, etc.) as image channels to increase information capacity, but ignore distinct characteristics of point attributes in different image channels. We design FPS-Net, a convolutional fusion network that exploits the uniqueness and discrepancy among the projected image channels for optimal point cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead of simply stacking multiple channel images as a single input, we group them into different modalities to first learn modality-specific features separately and then map the learnt features into a common high-dimensional feature space for pixel-level fusion and learning. Specifically, we design a residual dense block with multiple receptive fields as a building block in encoder which preserves detailed information in each modality and learns hierarchical modalityspecific and fused features effectively. In the FPS-Net decoder, we use a recurrent convolution block likewise to hierarchically decode fused features into output space for pixel-level classification. Extensive experiments conducted on two widely adopted point cloud datasets show that FPS-Net achieves superior semantic segmentation as compared with state-of-the-art projection-based methods. In addition, the proposed modality fusion idea is compatible with typical projection-based methods and can be incorporated into them with consistent performance improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A majority of recent works on point cloud segmentation make use of deep neural networks (DNNs) due to their powerful representation capability. As point cloud data are not grid-based with disordered structures, they cannot be directly processed by using deep convolution neural networks (CNNs) like 2D images. Several approaches have been proposed for DNN-based point cloud semantic segmentation <ref type="bibr" target="#b0">[1]</ref>. For example, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> construct graphs by treating points as vertexes and extracts features via graph learning networks. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> instead employ multi-layer perceptron (MLP) to learn from raw cloud points directly.</p><p>However, all these works require neighbor search for building up neighborhood relations of points which is a computationally intensive process for large-scale point cloud <ref type="bibr" target="#b9">[10]</ref>. In addition, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> leverage 3D convolution to structurize cloud points, but they often face a dilemma of voxelization -it loses details if outputting low-resolution 3D grids but the computational costs and memory requirement increase cubically while voxel resolution increases.</p><p>In recent years, spherical projection has been exploited to map LiDAR sequential point cloud scans to depth images and achieved superior segmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. As LiDAR point cloud is usually collected by linearly rotating scans, spherical projection provides an efficient way to sample and represent LiDAR point cloud scans in grid structure and the projected 2D images can be processed by developed CNNs in the similar way as regular 2D images. On the other hand, most existing spherical projection methods stack different point attributes (i.e. point coordinates x, y, and z, point intensity and point depth) as different channels in the projected images and process them straightly without considering their very diverse characteristics. We observe that the projected channel images actually have clear modality gaps, and simply stacking them as regular images tends to produce sub-optimal segmentation. Specifically, we note that the projected channel images have three types of modality that have very different nature and distributions: The x, y, z record spatial coordinates of cloud points in 3D Cartesian space, the depth measures the distance between cloud points and LiDAR sensor in polar coordinate system, and the intensity captures laser reflection. By simply stacking channel images of the three modalities as regular images, CNNs will process them with the same convolution kernels which tends to learn modality-agnostic common features but loses the very useful modality-specific features and information.  images, and intensity images, respectively. The segmentation is not accurate (e.g. in blue-color car that is close to the LiDAR sensor) but complementary across modalities. Stacking all channel images as one in learning does not improve segmentation clearly as in (d), large due to the negligence of modality gaps. Learning modality-specific features separately followed by fusion effectively mitigates the modality gap which improves the point cloud segmentation greatly as in (e).</p><p>We consider the modality gap and propose to learn modality-specific features from separate instead of stacked image channels. Specifically, we first learn features from modality-specific channel images separately and then map the learned modality features into a common high-dimensional feature space for fusion. We design an end-to-end trainable convolutional fusion network FPS-Net that first learns features from each image modality and then fuses the learned modality features. <ref type="figure" target="#fig_2">Fig. 1</ref> shows a sample scan that illustrates how FPS-Net addresses the modality gaps effectively. As the car does not get complete point cloud scan due to its proximity to LiDAR sensor, the predictions are very inconsistent across the three image modalities as illustrated in Figs. 1(a), (b) and (c). Stacking all channels images as a single input does not improve the segmentation much as shown in <ref type="figure" target="#fig_2">Fig. 1(d)</ref> as it learns largely modality-agnostic features but misses modality-specific ones. As a comparison, FPS-Net produces much better segmentation as in <ref type="figure" target="#fig_2">Fig. 1</ref>(e) due to its very different design and learning strategy.</p><p>The contributions of this work can be summarized in three aspects. First, we identify an important but largely neglected problem in point cloud segmentation, i.e. modality gaps exist widely in the projected channel images which often leads to sub-optimal segmentation if unaddressed. Based on this observation, we propose to learn modality-specific features from the respective channel images separately instead of stacking all channel images as one in learning. Second, we design an end-to-end trainable modality fusion network that first learns modality-specific features and then maps them to a common high-dimensional space for fusion. Third, we conduct extensive experiments over two widely used public datasets, and experiments show that our network outperforms state-of-the-art methods clearly with similar computational complexity.</p><p>Additionally, our idea can be straightly incorporated into most existing spherical projection methods with consistent performance improvements.</p><p>The rest of this paper is organized as follows. Section 2 reviews related works on point cloud semantic segmentation and modality fusion. Section 3 describes the proposed method in details. Section 4 presents experimental results and analysis and a few concluding remarks are finally drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is closely related to semantic segmentation of point cloud and multi-modality data fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Segmentation on Point Cloud</head><p>We first introduce traditional point cloud segmentation methods briefly and then move to DNN-based methods including point-based methods and projection-based methods. We also introduce spherical projection methods due to their unique characteristics in large-scale LiDAR sequential point cloud segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Traditional methods</head><p>Extensive efforts have been made to the semantic segmentation of point cloud collected via laser scanning technology. Before the prevalence of deep learning, most works tackle point cloud segmentation with the following procedures <ref type="bibr" target="#b17">[18]</ref>: 1) unit/sample generation; 2) hand-craft feature design; 3) classifier selection; 4) post-processing. The first step aims to select a portion of points and generate processing units <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Then discriminate features over units are extracted on feature design stage <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Strong classifiers are chosen in the next stage to discriminate features of each unit <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>. Finally, post-processing such as conditional random fields (CRF) and graph cuts are implemented for better segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Deep learning methods</head><p>Deep learning methods have demonstrated their superiority in automated feature extraction and optimization <ref type="bibr" target="#b29">[30]</ref>, which also advanced the development of semantic segmentation of point cloud in recent years. One key to the success of deep learning is convolutional neural networks (CNN) that are very good at processing structured data such as images. However, CNNs do not perform well on semantic segmentation of point cloud data due to its unique characteristics in rich distortion, lack of structures, etc. The existing deep learning methods can be broadly classified into two categories <ref type="bibr" target="#b0">[1]</ref>, namely, point-based methods and projection-based methods. Point-based methods process raw point clouds directly based on the fully connected neural network while projection-based methods firstly project unstructured point cloud into structured data and further process the structured projected data by using CNN models, more details to be reviewed in the ensuing two subsections.</p><p>Point-based methods: A number of point-based methods have been reported which segment point cloud by directly processing raw point clouds without introducing any explicit information loss. One pioneer work is PointNet <ref type="bibr" target="#b4">[5]</ref> that extracts point-wise features from disordered points with shared MLPs and symmetrical pooling. Several ensuing works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> aim to improve PointNet from different aspects. For example, <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> introduce attention mechanism <ref type="bibr" target="#b35">[36]</ref> for capturing more local information. In addition, a few works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> build customized 3D convolution operations over unstructured and disordered cloud points directly. For example, <ref type="bibr" target="#b7">[8]</ref> implements tangent convolution on a virtual tangent surface from surface geometry of each point. <ref type="bibr" target="#b36">[37]</ref> presents a point-wise convolution network that groups neighbouring points into cells for convolution. <ref type="bibr" target="#b37">[38]</ref> introduces Kernel Point Convolution for point cloud processing.</p><p>Though the aforementioned methods achieved quite impressive accuracy, most of them require heavy computational costs and large network models with a huge number of parameters. These requirements are unacceptable for many applications that require instant processing of tens of thousands of point in each scan as collected by high-speed LiDAR sensors (around 5-20 Hz). To mitigate these issues, <ref type="bibr" target="#b8">[9]</ref> presents a lightweight but efficient network RandLA-Net that employs random sampling for real-time segmentation of large-scale LiDAR point cloud. <ref type="bibr" target="#b38">[39]</ref> presents a voxel-based "mini-PointNet" that greatly reduces memory and computational costs. <ref type="bibr" target="#b39">[40]</ref> presents a 3D neural architecture search technique that searches for optimal network structures and achieves very impressive efficiency.</p><p>Projection-based methods: Projection-based methods project 3D point cloud to structured images for processing by well-developed CNNs. Different projection approaches have been proposed. <ref type="bibr" target="#b40">[41]</ref> presents a multiview representation that first projects 3D point cloud to 2D planes from different perspectives and then applies multi-stream convolution to process each view. To preserve the sacrificed geometry information, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">42]</ref> presents a volumetric representation that splits 3D space into structured voxels and applies 3D voxelwise convolution for semantic segmentation. However, the volumetric representation is computationally prohibitive at high resolutions but it loses details and introduces discretization artifacts while working at low resolutions. To address these issues, SPLATNet <ref type="bibr" target="#b42">[43]</ref> introduces the permutohedral lattice representation where permutohedral sparse lattices are interpolated from raw points for bilateral convolution and the output is then interpolated back to the raw point cloud. Further, <ref type="bibr" target="#b43">[44]</ref> introduces PolarNet that projects point cloud into bird-eye-view images and quantifies points into polar coordinates for processing.</p><p>Beyond the aforementioned methods, spherical projection <ref type="bibr" target="#b13">[14]</ref> has been widely adopted for efficient largescale LiDAR sequential point cloud segmentation. Specifically, spherical projection provides an efficient way to sample points and the projected images preserve geometric information of point cloud well and can be processed by standard CNNs effectively. A series of methods adopted this approach <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>For example, <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b14">15]</ref> employ SqueezeNet <ref type="bibr" target="#b48">[49]</ref> as backbone and Conditional Random Field (CRF) for post-processing. To enhance the scalability of spherical project in handling more and fine-grained classes, <ref type="bibr" target="#b15">[16]</ref> presents an improved U-Net model <ref type="bibr" target="#b49">[50]</ref> and GPU-accelerated post-processing for back-projecting 2D prediction to 3D point cloud. However, all these methods simply stack point data of different modalities (coordinate, depth, and intensity) as inputs without considering their heterogeneous distributions. We observe that the three modalities of point data have clear gaps and propose a separate learning and fusing strategy to mitigate the modality gap for optimal point cloud segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-Modality Data Fusion</head><p>Modality refers to a certain type of information or the representation format of stored information.</p><p>Multimodal learning is intuitively appealing as it could learn richer and more robust representations from multimodal data. On the other hand, effective learning from multimodal data is nontrivial as multimodal data usually have different statistical distributions and highly nonlinear relations across modalities <ref type="bibr" target="#b50">[51]</ref>.</p><p>Multimodal learning has been investigated in different applications. For example, <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> fuse hyper-spectral images and LiDAR data for land-cover classification. <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b55">56]</ref> fuse multi-spectral images with LiDAR data for land-cover and land-use classification. <ref type="bibr" target="#b58">[59]</ref> learns visual information from visible RGB images and geometric information from point cloud for indoor semantic segmentation. <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref> fuses visible and thermal images for pedestrian detection.</p><p>In addition, different fusion techniques have been proposed <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b50">51]</ref>, including early and late fusion <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>, hybrid fusion <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref>, joint training <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>, and multiplicative multi-modal method <ref type="bibr" target="#b50">[51]</ref>. Different from these works that fuse multimodal data from different sensors, we will study multimodal data in LiDAR point cloud including coordinates (x,y,z), intensity and depth which are usually well aligned as they are produced from the same LiDAR sensor laser signal.  purpose is to minimize distance D between 3D predictionL i and its ground truth L i :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Modality Gap and Proposed Solution</head><formula xml:id="formula_0">D(L i , L i ) = D(?(F 2D (?(S i ))), L i )<label>(1)</label></formula><p>Following prior work by <ref type="bibr" target="#b13">[14]</ref>, existing methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47]</ref> stack coordinates (x,y,z), depths and intensities as five channels in the projected images, i.e. I i ? R H?W ?5 . Stacking different attribute values is able to increase informative capacity of input images but also brings modality gap problem. By counting on all pixel values in the training set of SemanticKITTI <ref type="bibr" target="#b68">[69]</ref>, as <ref type="figure" target="#fig_4">Fig. 2</ref> shows, it is obvious that these three modalities follow different distributions. Simply stacking values following various distribution in the same image is likely to mislead models to focus on modality-agnostic features while ignore modalityspecific information or even downgrade representation of the model to distinguish samples lie in distribution boundary spaces, leading to a sub-optimal segmentation result.</p><p>Our solution is dividing each modality into an individual image for separate learning, in this case includes coordinate image I Di , depth image I Ci , and intensity image I Vi ,</p><formula xml:id="formula_1">? ? ? ? ? ? ? ? ? I Di = ? D (S i ) I Ci = ? C (S i ) I Vi = ? V (S i )<label>(2)</label></formula><p>and mapping them into a common high-dimensional space for fusion and further learning. The fusion approach is taken as the following equation in this shared data space:</p><formula xml:id="formula_2">T i = [T Di ,T Ci ,T Vi ], where ? ? ? ? ? ? ? ? ?T Di = F D (I Di ) T Ci = F C (I Ci ) T Vi = F V (I Vi )<label>(3)</label></formula><p>[?] represents pixel-wise fusion. Then the 2D convolution neural network F 2D learns from fused features to further predict semantic segmentation results, i.e.? i = F 2D (T i ).</p><p>Overall, the distance could be summarize as</p><formula xml:id="formula_3">D(L i , L i ) = D(?(F 2D ([F D (I Di ), F C (I Ci ), F V (I Vi )])), L i )<label>(4)</label></formula><p>Based on this idea, we design an end-to-end trainable modality fusion network FPS-Net that fully learns hierarchical information from each individual modality as well as the fused feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modality-Fused Convolution Network</head><p>The whole architecture of the FPS-Net model is illustrated as <ref type="figure" target="#fig_6">Fig. 3</ref>. A detailed introduction of each part is demonstrated in rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Mapping From 3D to 2D</head><p>The widely-used spherical projection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b16">17]</ref> transfers 3D point cloud into 2D images through following equation ?. The pixel coordinate (u, v) of a point with spatial coordinate (x, y, z) in a single scan of point cloud could be calculated as:   </p><formula xml:id="formula_4">? ? ? u = 1 2 [1 ? arctan(y, x)? ?1 ]W v = [1 ? (arcsin(z, r ?1 ) + f up )f ?1 ]H (5) where r is depth (r = x 2 + y 2 + z 2 ); f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Modality Fusion Learning</head><p>FPS-Net adopts a well designed encoder-decoder neural network to learn modality-specific features and fused feature for semantic segmentation. As <ref type="figure" target="#fig_6">Fig. 3</ref> illustrates, at the beginning of the model, three branches of feature extractors are designed to learn detailed information of each input modality separately. In order to capture rich information of individual images, we use a multiple receptive field residual dense block (MRF-RDB) as basic building block of feature extractors. The structure of MRF-RDB is demonstrated in <ref type="figure" target="#fig_6">Fig. 3</ref>. A multi-receptive fields module is introduced at beginning. Four kernels with small to large sizes are in parallel for several considerations: Firstly, different from regular images, projected images usually contain many blank pixels due to lack of remission. A larger receptive field could capture more pixels to avoid insufficient learning. Secondly, such a parallel structure is able to capture objects in different sizes for better feature representation. Thirdly, we only use large receptive fields in the beginning of the building block to keep a smaller model size. Considering that the height of projected images is small, usually set as the number of raiser beams of LiDAR sensors (e.g. H = 64 for Velodyne HDL-64E sensor), small object features are likely to be lost in deep layers, hence we adopt the dense residual layers <ref type="bibr" target="#b69">[70]</ref> in the remaining part of MRF-RDB to learn hierarchical features. Specifically, it comprises multiple convolution layers with batchnorm layers and activation layers. The output of each layer then concatenates previous output features before forwarding to next layer, leading to a dense structure with a contiguous memory mechanism. Such  structure adaptively captures and memorizes effective information from preceding to current local features.</p><p>The output of the last convolution layer add the input of RDB for another hierarchical feature aggregation.</p><p>After learning modality-specific high dimensional features, we fuse them by concatenation operation following a 1 ? 1 convolution for dimension reduction. The fused feature is then sent into an sub-network as U-Net <ref type="bibr" target="#b49">[50]</ref> structure for further learning. The encoder of this sub-network consists of four MRF-RDB blocks and downsampling layers. The bridge between the encoder and the decoder is also a MRF-RDB but its output has the same dimension as input feature map, which is used to convey deep abstract signals into the decoder. We no longer use MRF-RDB in the decoder because after passing 4 MRF-RDB of encoder the fused feature is well extracted and a simpler decoder structure is better for classification. Instead, our previous design <ref type="bibr" target="#b70">[71]</ref> of recurrent convolution block (RCB) with upsampling layer is used as the building block. As The last step is mapping 2D prediction back to 3D space and assigning labels for each point. This step is ill-posed as some points may not be selected on images in the spherical projection step, especially when image size is small. This paper follows the post-process of RangeNet++ <ref type="bibr" target="#b15">[16]</ref> and uses a GPU-supported KNN procedure to assign labels for missing points. In spherical projection step, each point p k is projected into a pixel coordinate (u k , v k ) (although only the nearest point would be selected as pixel when multiple points fall on the same pixel). In post-process, we use a S ?S window centered on (u k , v k ) to get S 2 neighbor pixels of the projected image as candidates. For those K k points whose depth distance is smaller than a threshold ?, labels are voted from them with weight from distance by an inverse Gaussian kernel, which penalizes points with larger distances. More details could be found in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Optimization</head><p>Due to the uneven distribution of ground objects in wild, LiDAR point cloud datasets <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b71">72]</ref> often face a serious data-imbalanced problem, which negatively leads the model to focus on classes with more samples.</p><p>Take SemanticKITTI dataset <ref type="bibr" target="#b68">[69]</ref> as example, classes including vegetation, road, building and so on take majority of data while classes like bicycle, motorcycle have very limited sample points. Hence, we adopt the weighted softmax cross-entropy objective to focus more on classes with less samples, in which the weight of loss for each class is inverse to its frequency f on training set:</p><formula xml:id="formula_5">L xent = ? C j=1 w j p(y j ) log(p(? j )), w j = f ?1 j (6)</formula><p>where y j and? j are ground truth label and predicted label of class j. In this case, classes with small number of samples have larger losses and imbalanced data distribution problem could be alleviated.</p><p>We also adopt Lov?sz-Softmax loss <ref type="bibr" target="#b72">[73]</ref> used in image semantic segmentation to improve learning performances, which regards IoU as a hypercube and its vertices are the combination of each class labels. In this case, the interior area of hypercube could be considered as IoU. The Lov?sz-Softmax loss is defined as</p><formula xml:id="formula_6">L lvsz = 1 C j?C? Jj (l(j))l(j) = ? ? ? 1 ? s(j), if j = y(j) s(j), else<label>(7)</label></formula><p>where? Jj is the Lov?sz extention of Jaccard index. s(j) is probability for pixel belonging to class j and y(j) is corresponding ground truth class label.</p><p>The final objective of FPS-Net is summarized as L = L xent + ?L lvsz . And the optimization process for training set could be illustrated as</p><formula xml:id="formula_7">F 3D = arg min F 3D L = arg min F 3D (L xent + ?L lvsz )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets: We evaluated our proposed modality fusion segmentation network over two LiDAR sequential point cloud datasets that were collected under the autonomous diving scenario. Both datasets have been widely used in the literature of point cloud segmentation.</p><p>? SemanticKITTI: The SemanticKITTI dataset <ref type="bibr" target="#b68">[69]</ref> is a newly published large scale semantic segmentation LiDAR point cloud dataset by labeling point-wise annotation for raw data of odometry task in KITTI benchmark <ref type="bibr" target="#b71">[72]</ref>. It consists of 11 sequences for training (sequences 00-10) and 11 sequences for testing (sequences <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. The whole dataset contains over 43,000 scans and each scan has more than 100k points in average. Labels over 19 classes for each points on the training set are provided and users should upload prediction on the testing set to official website 1 for a fair evaluation.</p><p>? KITTI: Wu et al. <ref type="bibr" target="#b13">[14]</ref> introduced a semantic segmentation dataset exported from LiDAR data in KITTI detection benchmark <ref type="bibr" target="#b71">[72]</ref>. This dataset was collected by using the same LiDAR sensor as SemanticKITTI but in different scenes. It is split into a training set with 8,057 spherical projected 2D</p><p>images and a testing set with 2,791 projected images. Semantic labels over car, pedestrian and bicycle are provided.  </p><formula xml:id="formula_8">IoU c = P c ? G c P c ? G c<label>(9)</label></formula><p>And the mIoU is the average of IoU over all classes</p><formula xml:id="formula_9">mIoU = 1 C C c=1</formula><p>IoU c (10) We use the Frame Per Scan (FPS) to evaluate the efficiency of different cloud segmentation methods.</p><p>For fair comparison, all of inference FPS results including our model and compared models are evaluated with a GeForce RTX 2080 TI Graphics Card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">SemanticKITTI</head><p>We compare our FPS-Net with state-of-the-art projection-based approaches on the test set of Se-manticKITTI benchmark, and <ref type="table" target="#tab_2">Table 1</ref> shows experimental results. As we can see, classical projection-based methods including SqueezeSeg <ref type="bibr" target="#b13">[14]</ref> and SqueezeSegV2 <ref type="bibr" target="#b14">[15]</ref> achieve very efficient computation but both of them sacrificed accuracy greatly. The recently proposed RangeNet++ <ref type="bibr" target="#b15">[16]</ref>, PolarNet <ref type="bibr" target="#b43">[44]</ref> and Squeeze-SegV3 <ref type="bibr" target="#b45">[46]</ref> achieved higher accuracy but their processing speed dropped instead. As a comparison, Our FPS-Net achieves the best segmentation accuracy in mIoU, and its efficiency is also comparable with the state-of-the-art models.</p><p>Specifically, FPS-Net outperforms PolarNet <ref type="bibr" target="#b43">[44]</ref> and SequeezeSegV3 <ref type="bibr" target="#b45">[46]</ref> in both mIoU (+2.8% and +1.2%) and computational efficiency (+5.2 FPS and +15.0 FPS). By learning modality-specific features from projected images, FPS-Net can capture detailed information from respective modality. The hierarchical leaning strategy in both MRF-RDB and RCB helps to keep features of small objects with fewer points. As a results, FPS-Net improves the segmentation accuracy by large margins for object classes bicycle, motorcycle, person and bicyclist. It also achieves the best accuracy for other classes such as traffic-sign and truck.   The segmentation is well aligned with the results in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">KITTI</head><p>We also compare FPS-Net with state-of-the-art methods over the validation set of KITTI semantic segmentation dataset, and <ref type="table" target="#tab_3">Table 2</ref> shows experimental results. It is obvious that FPS-Net outperforms other models by large margins in the metric of mIoU (+12.6%). For specific classes, our model achieves large improvement on bicycle (+14.6%) as compared with the state-of-the-art and smaller increases in pedestrian (+2.9%) and car (+1.7%). The experimental results are well aligned with that over the SemanticKITTI as our hierarchical learning learns useful features for classes with fewer points and improves more over those classes. They also show that FPS-Net is robust and can produce superior segmentation across different datasets consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We performed extensive ablation studies to evaluate individual FPS-Net components separately. In our ablation study, we uniformly sampled 1/4 data in the SemanticKITTI training set (named as 'sub-SemanticKITTI ') in training (for faster training) and the whole validation set for inference.</p><p>We first examine how different FPS-Net modules contribute to the overall segmentation performance.   <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Segmentation is more prone to errors around the class-transition regions, as highlighted in red bounding boxes of (a) and (c) with their close-up views shown at the lower-left corner (including the corresponding visual images for easy interpretation).  Firstly, we evaluated the two building blocks RCB and RDB without considering modality gaps (the same as in previous methods), where the five channel images are stacked as a single input as labelled by (stacked learning) in Rows 2-4 in <ref type="table" target="#tab_4">Table 3</ref>. We can see that the network achieves the lowest segmentation accuracy when both encoder and decoder employ RCB. As a comparison, replacing RCB by RDB in the encoder produces the best accuracy, which means that the network performs much better by concatenating instead of adding for low-and high-level features in the encoder. In addition, Rows 2 and 3 show that using RDB in the decoder degrades the segmentation performance slightly as compared with using RCB in the decoder. This means that a simpler memory mechanism makes the model more discriminative in the ensuing classification step. Further, Rows 4 and 5 show that including a multi-receptive field module in the encoder (i.e. MRF-RDB) helps to improve the segmentation mIoU greatly by 2.3%. On top of that we use the separate learning and fusion approach aiming for tackling the modality gap problem, and the result shows that the segmentation performance increases another 2.6% percentage (As the setup fused learning in <ref type="table" target="#tab_4">Table   3</ref>).</p><p>In addition, we studied the contribution of each modality in the fusion scheme.  and the "fusion model" in <ref type="figure" target="#fig_15">Fig. 6(b)</ref>. It can be seen that the three building blocks (Ec0 c, Ec0 d, Ec0 i) learn from the three individual modality, and the fused feature (Ec fuse) captures more discriminative information than block Ec0 of the stacked model. This shows that learning modality-specific feature separately helps to capture more representative information in the feature space. This can be further verified by the feature maps of the following building blocks where the fusion model always captures more representative and discriminative features than the stacked model.</p><p>We also studied the effect of different fusion positions of FPS-Net including early-fusion (fuse before U-Net structure), mid-fusion (fuse before decoder) and deep-fusion (fuse after decoder). <ref type="figure">Fig. 7</ref> shows all seven fusion strategies and <ref type="table">Table 5</ref> shows the corresponding mIoU. As <ref type="table">Table 5</ref> shows, fusing the three modalities in an early stage achieves the best segmentation while the segmentation deteriorates when the fusing position gets deeper. We conjecture that the three modalities of the same point are highly locationaligned and fusing them before the encoder allows the model to learn more representative features in the fusion space. In addition, our study shows that fusing intensity before the decoder leads to the worst segmentation performance.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>We also studies whether the proposed separate learning and fusion approach can complement stateof-the-art point cloud segmentation methods which stack all projected channel images as a single input in learning. The compared methods include SqueezeSeg <ref type="bibr" target="#b13">[14]</ref>, SqueezeSegV2 <ref type="bibr" target="#b14">[15]</ref> and RangeNet++ <ref type="bibr" target="#b15">[16]</ref> that all stacked five channel images as a single input in learning. In the experiments, we changed their networks by inputting channel images of the three modality separately (together with early-fusion) and</p><p>trained segmentation model over the dataset sub-SemanticKITTI. <ref type="table">Table 6</ref> shows experimental results on the SemanticKITTI validation set. As <ref type="table">Table 6</ref> shows, all three adapted networks (with separate learning and fusion) outperform the original networks (with stacked learning) clearly. Note that the parameter increases are relatively large for SqueezeSeg and SqueezeSegV2 which are pioneer lightweight networks. But <ref type="table">Table 6</ref>: Compatibility experiments about the proposed modality fusion idea (on the SemanticKITTI validation set). Our idea complements with state-of-the-art methods : The three state-of-the-art methods all improve clearly when our modality fusion idea is incorporated as in methods highlighted by '*'. 'FPS-Net (w/o fusion)' means using stacked images instead of modality fusion in FPS-Net. for recent models including RangeNet++ and our FPS-Net, the proposed fusion approach introduces very limited extra parameters (less than 0.3%) but significant improvement in segmentation. This further verifies the constraint of modality gaps in semantic segmentation of point cloud. More importantly, it demonstrates that our proposed separate learning and fusion idea can be incorporated into most existing techniques with consistent performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper identifies the modality gap problem existed in spherical projected images ignored by previous methods and designed a separate learning and fusing architecture for it. Based on this idea, it proposes an end-to-end convolutional neural network named FPS-Net for semantic segmentation on large scale LiDAR point cloud, which processes the projected LiDAR scans fast and efficiently. Experiments prove the effectiveness of our idea and show that our model achieves better performances in contrast to recently published models in the SemanticKITTI benchmark and a significant improvement in the KITTI benchmark with comparable computation speed. Extensive experiments show that our idea is also contributory to classical spherical projection-based methods and is able to improve their performances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>With the rapid development of LiDAR sensors and autonomous driving in recent years, LiDAR point cloud data which enable accurate measurement and representation of 3D scenes have been attracting in-creasing attention. Specifically, semantic segmentation of LiDAR point cloud data has become an essential function for fine-grained scene understanding by predicting a meaningful class label for each individual cloud point. On the other hand, semantic segmentation of large-scale LiDAR point cloud is still facing various problems. For example, LiDAR point cloud is heterogeneous by nature which captures both point geometries (with explicit point coordinates and depth) and laser intensity. In addition, it needs to store and process large-scale point cloud data collected within a limited time period, e.g., each scan in point cloud sequences may capture over 100k points. Further, the captured cloud points are often unevenly distributed in large spaces, where most areas have very sparse and even no points at all. Semantic segmentation of large-scale LiDAR point cloud remains a very open research challenge due to these problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Prediction of depth image (b) Prediction of coordinate image (c) Prediction of intensity image (d) Prediction of the stacked image with three modalities (e) Prediction of FPS-Net (f) Ground Truth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Learning modality-specific features separately addresses modality gaps effectively: For a sample point cloud scan, (a), (b), and (c) show its segmentation by learning from the projected modality-specific depth images, coordinate (in x, y, and z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>This paper focuses on semantic segmentation of LiDAR sequential point cloud. Given scans of LiDAR point cloud with corresponding C classes point-level labels in the training set and the testing set, our goal is to learn a point cloud semantic segmentation model F 3D on training set that also performs well on the testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>The distribution of 5 spherically projected channel images over the training set of SemanticKITTI, X-axis represents values for each modality and Y-axis shows corresponding number of pixels: The 5 projected channel images in modalities coordinate (x,y,z), depth and intensity have very different distribution. Simply stacking them together as one input in training ignores the modality differences, which often leads to sub-optimal model and semantic segmentation. Previous spherical projection-based methods follow three steps in the pipeline: For i th scan of LiDAR point cloud S i ? R Ni?4 (with N i points and each point has four attributes values x, y, z, intensity) in the training set : 1) A projection approach serving as a mapping function ? transfers point cloud from 3D space to a 2D projected image I i = ?(S i ); 2) A convolutional neural network F 2D extracts features from the image and predicts class distribution for all pixels, i.e.? i = F 2D (I i ). 3) A Post-processing function ? maps 2D image predictions back to 3D space and outputs final prediction for all pointsL i = ?(? i ). The</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>down and f up represent lower and upper boundaries of vertical field-of-view of LiDAR sensor (f = f up + f down ); (H, W ) are height and width of projected images.Instead of using stacked five-channel images as input as described in Section 3.1, we address the modality gaps by dividing the five-channel images into three modalities in point coordinates, point depths and point intensities.Fig. 4shows an example of point cloud and its corresponding modality images. Note each pixel in the coordinate image has three channel values (x, y, z), i.e. I Ci ? R H?W ?3 , whereas the pixel in the depth image I Di ? R H?W ?1 and intensity image I Vi ? R H?W ?1 has a single depth value and a single intensity value, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of our proposed FPS-Net : A spherical projection maps each point cloud scan into three modality images. The fusion learning network learns features of each modality separately and fuses the learned modality-specific features in a common high-dimensional space. After encoding and decoding the fused feature, FPS-Net maps 2D predictions back into 3D space for point-level semantic prediction. MRF-RDB and RCB are building blocks of encoder and decoder respectively for learning hierarchical features. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) visualization of 3D LiDAR point cloud (b) projected depth image (c) projected coordinate image (d) projected intensity image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Example of spherical projection from 3D point cloud space to 2D images. (a) a scan of point cloud; (b), (c), (d) are individual modality images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 3</head><label>3</label><figDesc>shows, RCB also comprises several convolution layers. It integrates low-level and high-level features via addition instead of concatenation, which reduces parameters without dropping performances while keeps large receptive fields and preserves more detailed spatial information. We compared different combinations of these two building blocks in ablation study (Section 4.3) and the result proved our idea. The classifier for 2D image prediction is a 3 ? 3 convolution kernel with a softmax layer to predict probabilities of label classes for each pixel. The output channel sizes for five MRF-RDBs are 32, 64, 128, 256, 512 respectively, while the output channels for RCBs in decoder are 256, 128,64, 32.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Evaluation Metrics:</head><label></label><figDesc>Performances are evaluated via per-class IoU and mean IoU (mIoU). For class c, the intersection over union (IoU) is defined as the intersection between class prediction P c and ground truth G c divided by their union part</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 5</head><label>5</label><figDesc>illustrates FPS-Net with several point cloud samples. Specifically, Figs. 5(a) and (b) show predictions of two point cloud scans in SemanticKITTI and Figs. 5(c) and (d) show the corresponding ground truth (all colored by classes as legend (m)), respectively. It can be seen that FPS-Net can correctly</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>it tends to produce segmentation errors around the class-transition regions due to a mixture in classes and lower point density there which makes point cloud segmentation a more challenging task. A typical example is highlighted in red bounding boxes of (a) and (c) with their close-up views shown at the lower-left corner (including the corresponding visual images for easy interpretation). We can observe that points around this region belong to different classes and many of them are classified as person incorrectly. In addition, Figs.5(e), (f), (g), (h) show four detailed segmentation of classes person, bicyclist, car, other-vehicle and truck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of semantic segmentation of SemanticKITTI point cloud by FPS-Net. (a) and (b) show predictions of two point cloud scans; (e), (f), (g), (h) are detailed segmentation of foreground classes; (c), (d), (i), (j), (k), (h) show the corresponding ground truth. FPS-Net can correctly classify most points which is well aligned with the results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 6 (</head><label>6</label><figDesc>c) shows visualized feature maps from different building blocks of the "stacked model" inFig. 6(a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>For 'stacked model' in (a) and 'fusion model' in (b), (c) shows the visualization of their feature maps at different building blocks (labeled by red-color text). GT represents the corresponding ground-truth labels for reference. Different modality fusion positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>5 : 7</head><label>57</label><figDesc>Ablation study for fusion position, evaluated over the SemanticKITTI validation set. Types are described asFigure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Implementation details: The resolution of the projected images is set as (H = 64, W = 2048) for SemanticKITTI and (H = 64, W = 512) for KITTI, which are the same as previous methods<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b73">74]</ref> for fair comparisons. We follow the same way to split dataset as previous methods. Further, for a faster experiments, we uniformly sample 1/4 scans in training set as sub-SemanticKITTI for training and remain whole validation set for testing in experiments of ablation studies and discussion. We train FPS-Net for 200 epochs with batch size of 16 using Adam optimizer with initial learning rate lr = 0.001. For data</figDesc><table /><note>augmentation on each scan, We randomly rotate each scan within [?1 ? , 1 ? ] and flip horizontally. ? is set to be 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of FPS-Net with state-of-the-art projection-based methods on the test set of SemanticKITTI: Both FPS and mIoU are reported for each compared method. Note FPS values were re-produced with the same configuration for fair comparison and they may not be consistent with the original papers. PolarNet[44] 15.6 54.3 90.8 74.4 61.7 21.7 90.0 93.8 22.9 40.3 30.1 28.5 84.0 65.5 67.8 43.2 40.2 5.6 67.8 51.8 57.5 SqueezeSegv3[46] 5.8 55.9 91.7 74.8 63.4 26.4 89.0 92.5 29.6 38.7 36.5 33.0 82.0 58.7 65.4 45.6 46.2 20.1 59.4 49.6 58.9 FPS-Net (ours) 20.8 57.1 91.1 74.6 61.9 26.0 87.4 91.1 37.1 48.6 37.8 30.0 80.9 61.2 65.0 60.5 57.8 7.5 57.4 49.9 59.2</figDesc><table><row><cell>Model</cell><cell>FPS</cell><cell>mIoU</cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other-ground</cell><cell>building</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicle</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>fence</cell><cell>pole</cell><cell>traffic-sign</cell></row><row><cell cols="22">SqueezeSeg[14] 72.6 29.5 85.4 54.3 26.9 4.5 57.4 68.8 3.3 16.0 4.1 3.6 60.0 24.3 53.7 12.9 13.1 0.9 29.0 17.5 24.5</cell></row><row><cell cols="22">SqueezeSegV2[15] 62.8 39.7 88.6 67.6 45.8 17.7 73.7 81.8 13.4 18.5 17.9 14.0 71.8 35.8 60.2 20.1 25.1 3.9 41.1 20.2 36.3</cell></row><row><cell cols="22">RangeNet++[16] 14.9 52.2 91.8 75.2 65.0 27.8 87.4 91.4 25.7 25.7 34.4 23.0 80.5 55.1 64.6 38.3 38.8 4.8 58.6 47.9 55.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of FPS-Net with state-of-the-art projection-based methods on the validation set of KITTI</figDesc><table><row><cell>Model</cell><cell>size</cell><cell>mIoU</cell><cell>car</cell><cell cols="2">cyclist pedestrian</cell></row><row><cell cols="2">SqueezeSeg[14] 64 ? 512</cell><cell>37.2</cell><cell>64.6</cell><cell>21.8</cell><cell>25.1</cell></row><row><cell cols="2">PointSeg [45] 64 ? 512</cell><cell>39.8</cell><cell>67.4</cell><cell>32.7</cell><cell>19.2</cell></row><row><cell cols="2">SqueezeSegv2[15] 64 ? 512</cell><cell>44.9</cell><cell>73.2</cell><cell>27.8</cell><cell>33.6</cell></row><row><cell cols="2">LuNet[74] 64 ? 512</cell><cell>55.4</cell><cell>72.7</cell><cell>46.9</cell><cell>46.5</cell></row><row><cell cols="2">RangeNet++[16] 64 ? 512</cell><cell>57.3</cell><cell>97.0</cell><cell>44.0</cell><cell>30.8</cell></row><row><cell cols="4">FPS-Net (Ours) 64 ? 512 69.9 98.7</cell><cell>61.5</cell><cell>49.4</cell></row></table><note>classify most points especially classes of road, sidewalk, building, car, and vegetation. On the other hand,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of FPS-Net over the SemanticKITTI validation set: Stacked learning means stacking multi-modal data as a single input while separate learning refers to our method in FPS-Net. RDB, MRF-RDB and RCB are building blocks as introduced in Section 3.2.2.</figDesc><table><row><cell>Input</cell><cell>Encoder</cell><cell cols="3">Decoder Fusion mIoU</cell></row><row><cell></cell><cell>RCB</cell><cell>RCB</cell><cell>None</cell><cell>46.5</cell></row><row><cell></cell><cell>RDB</cell><cell>RDB</cell><cell>None</cell><cell>49.3</cell></row><row><cell>stacked learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>RDB</cell><cell>RCB</cell><cell>None</cell><cell>50.0</cell></row><row><cell></cell><cell>MRF-RDB</cell><cell>RCB</cell><cell>None</cell><cell>52.3</cell></row><row><cell cols="2">separate learning MRF-RDB</cell><cell>RCB</cell><cell>Fused</cell><cell>54.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of different modality combinations (on SemanticKITTI validation set with image size=1024 ? 64):'-' means only one modality image is used, 'stacked' means stacking multi-modal data as one input, and 'fused' refers to our separate learning and fusing method.</figDesc><table><row><cell cols="2">xyz intensity depth type mIoU</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic-sign</cell></row><row><cell>-</cell><cell cols="20">47.3 86.9 18.0 24.8 48.4 26.9 42.4 58.4 0.0 90.8 39.9 73.3 0.3 78.3 40.5 78.9 48.6 64.2 46.5 32.0</cell></row><row><cell>-</cell><cell cols="20">39.5 78.9 21.9 11.9 30.0 25.4 28.1 43.3 0.0 86.9 27.7 67.7 0.1 71.0 33.9 75.3 34.3 62.9 24.6 27.5</cell></row><row><cell>-</cell><cell cols="20">49.3 88.1 18.3 22.8 69.1 32.4 45.7 52.4 0.0 91.0 35.1 73.9 1.0 80.5 44.5 80.0 52.8 67.0 46.6 35.5</cell></row><row><cell cols="21">stacked 46.5 87.2 11.2 21.1 58.5 28.8 39.6 51.2 0.0 90.3 36.4 72.3 0.0 78.1 38.5 77.5 50.2 64.1 45.7 32.5</cell></row><row><cell cols="21">stacked 49.3 87.3 33.2 21.5 63.3 27.1 45.3 54.7 0.0 90.8 36.5 75.6 0.1 79.5 38.1 79.9 52.2 69.1 44.4 36.8</cell></row><row><cell cols="21">stacked 50.6 87.6 31.7 26.6 66.8 28.0 46.9 60.8 0.0 90.1 34.5 74.3 0.2 81.1 45.1 80.8 52.7 67.0 48.3 39.6</cell></row><row><cell cols="21">stacked 50.2 88.9 37.6 28.6 48.8 31.5 46.7 58.9 0.0 91.4 32.4 75.6 0.8 80.6 45.2 80.3 52.0 65.3 49.5 39.4</cell></row><row><cell>fused</cell><cell cols="20">49.8 88.2 7.2 20.1 77.5 35.6 44.5 59.5 0.0 90.6 38.7 74.7 0.2 80.5 45.4 80.7 52.7 67.7 48.9 32.5</cell></row><row><cell>fused</cell><cell cols="20">51.2 88.7 38.3 26.5 55.0 34.4 48.4 58.4 0.0 91.5 31.3 76.8 0.1 82.1 47.8 81.7 52.3 68.5 50.9 39.0</cell></row><row><cell>fused</cell><cell cols="20">50.7 88.0 39.1 25.3 61.6 24.2 46.8 59.9 0.0 91.3 34.7 76.1 0.1 80.9 44.2 80.9 53.0 64.7 48.7 42.8</cell></row><row><cell>fused</cell><cell cols="20">52.0 88.6 38.3 26.4 58.1 31.6 55.4 60.9 0.1 91.9 34.8 76.5 0.1 81.6 45.1 82.5 54.2 68.8 50.7 41.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>shows experimental results, where Rows 2-4 show the segmentation while training models by using data of each of the three modalities, respectively. It can be seen that intensity performs much worse as compared with either depth or coordinate. Additionally, depth performs better than coordinates, largely because the three coordinate channels (x, y, z) are closely correlated and CNNs cannot learn such correlation well. Rows 5-8 show the segmentation while stacking different modalities in a single image as input. We can see that intensity is complementary to both coordinate and depth while stacking coordinate and depth performs clearly worse than either modality alone. The last four rows show segmentation by FPS-Net. It can be seen that FPS-Net outperforms the stacking approach consistently under all modality combinations. Additionally, fusing all three modalities achieves the best segmentation which further verifies the modality gap as well as the effectiveness of our proposed method. Note stacking depth and intensity performs similarly as fusing them, indicating smaller gaps in between the two modalities.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://semantic-kitti.org/index.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This research was conducted at Singtel Cognitive and Artificial Intelligence Lab for Enterprises (SCALE@NTU),</head><p>which is a collaboration between Singapore Telecommunications Limited (Singtel) and Nanyang Technolog-20 ical University (NTU) that is funded by the Singapore Government through the Industry Alignment Fund -Industry Collaboration Projects Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="965" to="975" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2670" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully-convolutional point networks for large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="596" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4574" to="4583" />
		</imprint>
	</monogr>
	<note>Spsequencenet: Semantic segmentation network on 4d point clouds</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel octree-based 3-d fully convolutional neural network for point cloud classification in road environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7799" to="7818" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relevance of airborne lidar and multispectral image data for urban scene classification using random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boukir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="66" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic 3d scene interpretation: A framework combining optimal neighborhood size selection with relevant features, ISPRS Annals of the Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jutzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">181</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object classification and recognition from mobile laser scanning point clouds in a road environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lehtom?ki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyypp?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaartinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kukko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Puttonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hyypp?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1226" to="1239" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Svm-based classification of segmented airborne lidar point clouds in urban areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3749" to="3775" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple-entity based classification of airborne laser scanning data in urban areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vosselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Elberink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature relevance analysis for 3d point cloud classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Winiwarter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>H?fle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Annals of Photogrammetry, Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fast semantic segmentation of 3d point clouds with strongly varying density, ISPRS annals of the photogrammetry, remote sensing and spatial information sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Super-segments based classification of 3d urban street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A structured regularization framework for spatially smoothing semantic labelings of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="102" to="118" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Classification of urban lidar data using conditional random field and random forests, in: Joint Urban Remote Sensing Event</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Soergel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="139" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classification of airborne laser scanning data using jointboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="71" to="83" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling point clouds with self-attention and gumbel subset sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3323" to="3332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pooling scores of neighboring points for improved 3d point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1475" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
	<note>Pointwise convolutional neural networks</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16100</idno>
		<title level="m">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep projective 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vv-net: Voxel vae net with group convolutions for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8500" to="8508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
	<note>Splatnet: Sparse lattice networks for point cloud processing</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06288</idno>
		<title level="m">Pointseg: Real-time semantic segmentation based on 3d lidar point cloud</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01803</idno>
		<title level="m">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Salsanext: Fast semantic segmentation of lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03653</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10893</idno>
		<title level="m">3d-mininet: Learning a 2d representation from point clouds for fast and efficient 3d lidar semantic segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and? 0.5 mb model size</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11730</idno>
		<title level="m">Learn to combine modalities in multimodal deep learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generalized graph-based fusion of hyperspectral and lidar data using morphological features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pi?urica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bellens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gautama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="552" to="556" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Land-cover classification using both hyperspectral and lidar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Phinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Image and Data Fusion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="215" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hyperspectral and lidar fusion using extinction profiles and total variation component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gloaguen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3997" to="4007" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fusion of hyperspectral and lidar data using sparse and low-rank component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6354" to="6365" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">More diverse means better: Multimodal deep learning meets remote-sensing imagery classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semantic segmentation of forest stands of pure species combining airborne lidar data and very high resolution multispectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dechesne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Le</forename><surname>Bris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gouet-Brunet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Developing a multi-filter convolutional neural network for semantic segmentation using high-resolution aerial imagery and lidar data, ISPRS journal of photogrammetry and remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Urtasun, 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fusion of multispectral data through illumination-aware deep neural networks for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="148" to="157" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Box-level segmentation supervised deep neural networks for accurate and real-time multispectral pedestrian detection, ISPRS journal of photogrammetry and remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="70" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multimodal fusion for multimedia analysis: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Saddik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="345" to="379" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Early versus late fusion in semantic video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th annual ACM international conference on Multimedia</title>
		<meeting>the 13th annual ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="399" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Affect recognition from face and body: early fusion vs. late fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3437" to="3443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multisensor image segmentation using dempstershafer fusion in markov fields context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendjebbour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Delignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fouque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Samson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pieczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1789" to="1798" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fusion of av features and external information sources for event detection in team sports video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Multimodal deep learning</title>
		<imprint>
			<publisher>ICML</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Not all areas are equal: Transfer learning for semantic segmentation via hierarchical region selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4360" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Road detection and centerline extraction via deep recurrent convolutional neural network u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="7209" to="7220" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The lov?sz-softmax loss: a tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Lu-net: An efficient network for 3d lidar point cloud semantic segmentation based on end-to-end-learned 3d features and u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Biasutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Aujol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Br?dif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bugeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
