<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Hao</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutao</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baohua</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. 2 NYU 3 CQJTU</addrLine>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-quality training data play a key role in image segmentation tasks. Usually, pixel-level annotations are expensive, laborious and time-consuming for the large volume of training data. To reduce labelling cost and improve segmentation quality, interactive segmentation methods have been proposed, which provide the result with just a few clicks. However, their performance does not meet the requirements of practical segmentation tasks in terms of speed and accuracy. In this work, we propose EdgeFlow, a novel architecture that fully utilizes interactive information of user clicks with edge-guided flow. Our method achieves state-of-the-art performance without any post-processing or iterative optimization scheme. Comprehensive experiments on benchmarks also demonstrate the superiority of our method. In addition, with the proposed method, we develop an efficient interactive segmentation tool for practical data annotation tasks. The source code and tool is avaliable at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has seen tremendous success in computer vision areas, such as image recognition <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b21">23]</ref>, object detection <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b20">22]</ref> and image segmentation <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b31">33]</ref>. In order to learn powerful abstraction, large volumes of labelled image data are usually essential for the model training process. As the amount of data increasing, the cost of manual annotation grows rapidly, especially when it comes to pixel-level segmentation tasks. Although semi-supervised or even unsupervised algorithms have been proposed to relieve label dependence, there is a great gap in accuracy between them and full supervision.</p><p>Therefore, interactive segmentation appears to be an attractive and efficient way, which allows the human annotators to quickly extract the object-of-interest <ref type="bibr" target="#b53">[55]</ref>. Unlike model-centric methods, interactive segmentation methods * PaddlePaddle Developers Experts (PPDE) <ref type="figure">Figure 1</ref>. Examples of interactive clicks. Green dots denote positive clicks, red dots denote negative clicks. Best viewed in colors. take interactive information into account. Therefore, they simplify the annotation process and improve the quality progressively. In general, interactive information could be various inputs, such as scribbles <ref type="bibr" target="#b2">[4]</ref>, clicks <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b47">49]</ref>, bounding boxes <ref type="bibr" target="#b51">[53]</ref> and so on.</p><p>The characteristics of the different interactions have been studied well by previous works, where click-based methods are the most promising, because they provide sufficient selected object information with minimal interaction time. In practice, click-based methods usually use two types of user clicks, i.e. positive clicks and negative clicks. Positive clicks aim to emphasize the target object (foreground) and negative clicks isolate non-target areas (background). Usually, such methods only need a few clicks to complete an object segmentation task, which is shown in <ref type="figure">Fig. 1</ref>.</p><p>In recent years, there are a couple of works <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b47">49</ref>] on click-based interactive segmentation, in which deep learning methods surpass traditional ones in terms of accuracy. However, most of them require extra post-processing in the evaluation process, which is time-consuming in practice. More recently, end-to-end interactive algorithms <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b48">50]</ref> were proposed to speed up the clicking interaction, but they have common problems. The clicks are the only input to the first layers, so that the specific spatial and semantic information would be diluted through early layers. The other problem is that the relation of consecutive clicks is not modelling properly, resulting in unstable annotations, e.g. the segmentation annotation dramatically changes between two consecutive clicks.</p><p>In this work, we propose a novel interactive segmentation architecture that fully utilizes clicks of users and the <ref type="figure">Figure 2</ref>. Overview of the EdgeFlow architecture. We design a coarse-to-fine network including CoarseNet and FineNet. For CoarseNet, We utilize HRNet-18+OCR as the base segmentation model and append the edge-guided flow to deal with interactive information. For FineNet, we utilize three atrous convolution blocks to refine the coarse masks.</p><p>relation of consecutive clicks. To enhance the interactive information, the feature of user clicks are embedded into both early and late layers and image features are efficiently integrated with an early-late fusion strategy. To establish the relationship between two consecutive clicks, the edge mask generated by the previous clicks is taken as an input together with the current click. It improves the stability of segmentation results significantly. In addition, we adopt a coarse-tofine network design to further obtain the fine-grained segmentation. The comprehensive evaluations show our stateof-the-art performance on well-known benchmarks.</p><p>Furthermore, upon the proposed interactive model, we develop an efficient interactive segmentation tool for practical segmentation tasks, e.g. image labelling. The tool not only generates the segmentation mask, but also allows the user to adjust the polygon vertexes of the mask to further improve accuracy. Thus, the tool provides flexible options for annotation accuracy according to different practical tasks.</p><p>Our contributions are summarized as follow:</p><p>? We propose a novel interactive architecture that fully utilizes interactive and image information with the early-late fusion. The enhancement of interactive clicks prevents feature dilution over the network and then enables it to respond to the clicks efficiently.</p><p>? We utilize the object edges produced by network to improve the segmentation stability. With the coarse-tofine network design, comprehensive experiments show our method achieves state-of-the-art performance on several benchmarks.</p><p>? We develop an efficient interactive segmentation tool that supports interactive annotation and polygon frame editing. Our tool also supports multi-scenes and various labelling formats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The interactive segmentation task aims to obtain an accurate mask of an object with minimal user interaction. Interactive information can be clicks, scratches, contours, bounding boxes, phrases and so on. According to the type of information modelling, there are two research branches for interactive image segmentation algorithms.</p><p>Optimization based methods: Most of them are traditional methods which are divided into four categories: 1) contour based methods <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b23">25]</ref>, 2) graphcut based methods <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b32">34]</ref>, 3) random walk based methods <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b3">5]</ref> and 4) region merging methods <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b44">46]</ref>. As a contour-based method, the active contours model (ACM) <ref type="bibr" target="#b42">[44]</ref> constructs and optimizes an energy equation until the force on the closed curve decreases to zero. Graphcut based methods (GC) <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b6">8]</ref> utilize min-cut/max-flow algorithm to minimize the energy function. Random walk based methods (RW) <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b12">14]</ref> construct an undirected graph by taking pixels as vertices and the relationship of neighbourhoods as edges. Region merging methods (RM) are initialized by user-interactive seeds and then gather the similar points and regions by homogeneity criterion.</p><p>The traditional methods have common drawbacks. The generalization ability of the methods is poor, where they only work well on specific scenes. In addition, they are sensitive to initial interactive information and require highquality interactions without noisy input.  Deep learning based methods: Xu et al. <ref type="bibr" target="#b50">[52]</ref> firstly introduced deep learning to solve interactive segmentation problems. They converted the interactive clicks to a distance map and then took the distance map together with the original image as an input to fine-tune FCN <ref type="bibr" target="#b33">[35]</ref>. After that, Maninis et al. <ref type="bibr" target="#b38">[40]</ref> took extreme points of an object as interactive information and utilized DeepLabv2 <ref type="bibr" target="#b10">[12]</ref> as a segmentation model. Optimization for activation has been applied on the back-propagating refinement scheme (BRS) <ref type="bibr" target="#b19">[21]</ref>, which corrected mislabeled pixels by employing the L-BFGS algorithm. However, the optimization is time-consuming. Soon after, Feature-BRS <ref type="bibr" target="#b47">[49]</ref> was proposed to improve the optimization scheme and speed up the interaction process. The mislabeled pixels would be rectified by auxiliary scales and biases which modify features from the middle of the network. Kontogianni et al. <ref type="bibr" target="#b22">[24]</ref> adjusted the mask of the target object by optimizing the model parameters at test time. Sofiiuk et al. <ref type="bibr" target="#b48">[50]</ref> applied an iterative training process by employing previous mask outputs, which improved the segmentation accuracy.</p><p>However, these methods highly rely on post-processes or additional optimization schemes in the evaluation process, which requires extra time and computations. Besides, they are sensitive to a new click, resulting in unstable segmentation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this work, we propose a novel interactive segmentation architecture that fully utilizes clicks and the relation of consecutive clicks. Compared with previous approaches that combine image and clicks together at the first layers, our proposed method 1) utilizes a separate branch to enhance the features of clicks, which enables the network to respond to interactive information better; 2) applies edge mask as prior information of the network, which stabilizes annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network architecture</head><p>As shown in <ref type="figure">Fig. 2</ref>, the proposed architecture consists of two parts, input base segmentation model (CoarseNet) and refinement model (FineNet). For network inputs, the edge mask and the positive/negative clicks are encoded by the interactive branch.</p><p>CoarseNet. We utilize HRNet-18 <ref type="bibr" target="#b49">[51]</ref> as the backbone and OCRNet <ref type="bibr" target="#b52">[54]</ref> as segmentation head. The architecture has been proved to be excellent for semantic segmentation tasks. Unlike the single image branch in the generic segmentation model, we add an interaction branch into the backbone to handle the positive clicks, negative clicks, as well as the edge mask. In the beginning, the edge mask is initialized to a zero map with the same size as the input image. After receiving user clicks, the edge mask is estimated through the segmentation result and then is adopt as a part of input. To obtain an accurate edge mask, we add an auxiliary block for the edge constraint in the segmentation head. The fusion details of the image and interaction branch will be introduced in section 3.2.</p><p>FineNet. We utilize a FineNet module to further refine the coarse segmentation mask. The module takes three parts as the input, the outputs of CoarseNet, the original image and the user clicks, where the image and the user clicks are the same as partial inputs in CoarseNet. Note that for better visualization, we do not connect them to FineNet in <ref type="figure">Fig. 2</ref>, while they are connected in our implementation. To improve computation efficiency, we use three atrous convolution blocks with lightweight operations in the FineNet.   <ref type="figure">Figure 5</ref>. The qualitative results of early-late fusion. We visualize the feature map before OCR decoder. The second and fifth column utilize model with early fusion strategy. The third and sixth column utilizes early-late fusion strategy. The architecture of edge-guided flow utilizes early-late fusion strategy and prevents the network forgetting interactive information.</p><p>The atrous convolutions also obtain large receptive fields on high-resolution features, so that they can obtain more context information and improve the quality of outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Edge-guided Flow</head><p>Clicks encoding: Interactive information contains the coordinates of positive and negative clicks. The positive clicks emphasize the target objects and the negative clicks isolate abandoned areas. To achieve the intuition and feed to the network, the clicks need to be encoded as feature maps. Usually, interactive segmentation methods utilize Gaussian algorithm or L 2 distance to generate distance maps as the feature maps. However, such distance maps change dramatically when adding a new click. Benenson et al. <ref type="bibr" target="#b4">[6]</ref> found that the disk map has more stable performance than other encoding methods. Inspired by <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b48">50]</ref>, in our implementation, we utilize the disk map with a radius of 5 pixels to encode both positive and negative clicks.</p><p>Early-late fusion: In general, a segmentation model only supports three-channel images as the inputs. Thus, the generic model should be modified to adapt to the additional interaction inputs. Some methods <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b36">38]</ref> introduce additional convolution layers in front of the backbone to accept N-channels input, which is shown in <ref type="figure" target="#fig_3">Fig. 6</ref>. Both of them fuse interaction and image features before the backbone network, which is so-called early fusion. The early fusion methods have common problems in that interaction information is not extracted properly. The interaction features are much sparser than image features and contain high-level information, e.g. position information. The early layers in the backbone network are focusing on low-level feature extraction, so that the interaction features would be diluted through the early layers and the network can not respond to user clicks timely.</p><p>To prevent feature dilution, we propose an early-late fusion strategy to integrate interaction and image features. Instead of only fusing the features at the beginning of the network, we design the multi-stage feature fusion as shown in <ref type="figure">Fig. 2</ref>. The first fusion is the same as <ref type="figure" target="#fig_3">Fig. 6(b)</ref>. The second fusion is between the first transition block and the second stage block of the backbone. The last fusion is after the fourth stage block. The multi-stage fusion promotes the propagation of interaction information over the network and also enables the network to respond to the user clicks precisely.</p><p>Edge Guidance: The key idea of interactive segmentation is improving segmentation masks progressively with user clicks. Due to the large spatial variance of the user clicks, the features of consecutive clicks would be quite different, resulting in dramatic segmentation masks. Previous methods <ref type="bibr" target="#b48">[50]</ref> introduced the segmentation mask of previous clicks as an input, which alleviates this problem to a certain extent. However, the full mask could make the model fall into local optima, e.g. a poor previous mask usually leads to poor segmentation results. To improve the stability of the segmentation mask, we propose an edge mask scheme, which takes the object edges estimated from the previous iteration as prior information, instead of direct mask estimation. Edge estimation is sparser and less fluctuating than the full mask on input, so it can improve the stability and efficiency of segmentation.</p><p>In the interactive segmentation model, the interaction image and edge mask features are heterogeneous, resulting in a large spatial bias. Thus, it is necessary to align these properly. The optical flow method is originally used to align features from two adjacent frames in a video <ref type="bibr" target="#b34">[36]</ref>. In semantic segmentation, it is effective for multi-scale features alignment while fusing different layers. Inspired by <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b26">28]</ref>, we take a flow module to align image and interaction features such that spatial information can be represented precisely. The details of the flow module are shown in <ref type="figure">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>We expect that the loss function is more focusing on the wrong pixels rather than well-classified pixels. Inspired by <ref type="bibr" target="#b48">[50]</ref>, we utilize normalize focal loss to calculate the discrepancy between prediction mask and ground truth mask. It can be denoted as Eq 1:</p><formula xml:id="formula_0">L m (i, j) = ? 1 i,j (1 ? p i,j ) ? (1 ? p i,j ) ? log p i,j ,<label>(1)</label></formula><p>where ? is the hyper-parameter for focal loss. p i,j denotes the confidence of the prediction of pixel (i, j), it is denoted as :</p><formula xml:id="formula_1">p i,j = p, y = 1 1 ? p, otherwise ,<label>(2)</label></formula><p>where p is the prediction probability on the location (i, j), y is the corresponding ground truth on the location (i, j). To minimize the difference between edge estimation from the last iteration and edges derived from ground truth masks, we employ balanced BCE loss L e to assign more attention on edge than the background. Besides, we utilize BCE as auxiliary loss L a to constrain the backbone outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Settings</head><p>Datasets: Following <ref type="bibr" target="#b48">[50]</ref>, we use the combination of COCO <ref type="bibr" target="#b29">[31]</ref> and LVIS <ref type="bibr" target="#b18">[20]</ref> as the training data. LVIS contains 164k images and 2M high-quality instance masks with over 1000 classes. Due to the long-tailed object classes from LVIS dataset, following <ref type="bibr" target="#b48">[50]</ref>, we use 10582 COCO images and corresponding 25832 instance masks as the augmented dataset.</p><p>We evaluate our method on four popular benchmarks, including GrabCut <ref type="bibr" target="#b46">[48]</ref>, Berkeley <ref type="bibr" target="#b39">[41]</ref>, Pascal VOC <ref type="bibr" target="#b14">[16]</ref>, DAVIS <ref type="bibr" target="#b43">[45]</ref>. The GrabCut dataset contains 50 images and 50 masks. The Berkeley dataset contains 96 images and 100 masks. For Pascal VOC dataset, we only use the validation set, which contains 1449 images and 3417 instances.  The DAVIS dataset is randomly sampled from the video object segmentation dataset. We adopt 345 images and corresponding 345 masks which were introduced in <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b48">50]</ref>. Evaluation metrics: Mean intersection over union (IoU) and the standard number of clicks (NoC) are used as evaluation metrics. NoC indicates the number of interactive clicks for achieving over the specified IoU threshold, which is commonly set as 85% and 90% <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b19">21]</ref>, i.e. NoC@85 and NoC@90. During the evaluation procedure, we build a simulation to generate clicks, following <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b48">50]</ref>. The first click is positive to the object of interest. The next click is selected as the centre of the largest error region. Note that in this work, the maximum number of clicks is 20, which is the same as <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b48">50]</ref>.</p><p>Implementation details: In this work, we utilize COCO+LVIS dataset to train our model. For data augmentation, we adopt random resize with a scale from 0.75 to 1.4, horizontal flip, random crop image with the resize of (320, 480). For colour distortion, we utilize random distribution on contrast, brightness and pixel value for the original image.</p><p>In our experiments, the weight of focal loss L m , balanced BCE loss L e and BCE loss L a is set as 1, 0.4, 0.4, respectively. Firstly, we train the CoarseNet for 40 epochs with the learning rate of 5 ? 10 ?4 . Then, we train the whole network for the last 30 epochs, where the learning rate of 5 ? 10 ?4 for FineNet and 5 ? 10 ?5 for CoarseNet. We utilize Adam optimizer and polynorm decay to reduce the learning rate to 1 percent of the initial learning rate value. Our model is based on PaddlePaddle <ref type="bibr" target="#b35">[37,</ref><ref type="bibr">1]</ref>. We train the model on 2 GPUs (Tesla V100), and the batch size is set to 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with SOTA methods</head><p>NoC@k.We calculate NoC@85 and NoC@90 as the metrics on GrabCut, Berkeley, DAVIS and Pascal VOC. The evaluation results are shown in table 1. Our method achieves the best performance on all datasets except for GrabCut. GrabCut has a limited number of objects with clear boundaries, so that all deep learning methods perform very well, and our method also achieves competitive results. Whereas Berkeley images have complicated object bound- aries, e.g. bicycle wheel springs and parachute ropes. In this dataset, our method achieves the best performance compared with other methods. Since our method utilizes edge masks as prior information, it provides more accurate details of segmentation objects. For DAVIS and Pascal VOC datasets, they contain more than 1.5k images with various scenes, which are more suitable for practical tasks. Our method achieves the best results compared with other methods. Analysing the result, we consider that early-late fusion strategy can prevent features of interactive clicks dilution over the network. Therefore, our method responds to the clicks efficiently and is robust in various scenes. mIoU per click. <ref type="figure" target="#fig_4">Fig. 7</ref> shows the mIoU change over the first 20 clicks on different datasets. There are three observations: 1) Our method has the most stable performance compared with other methods, where mIoU improves gradually as the clicks increases. However, other methods have the degradation problem. For example, mIoU of RITM decreases dramatically when the clicks increase from 4 to 5, and 10 to 11 on GrabCut dataset. As described above, the proposed edge estimation can improve the segmentation stability. 2) Our method requires fewer clicks to achieve higher mIoU compared with other methods. In the Berkeley dataset, NoC@95 of our method is around 4, while others are more than 10. 3) Using all 20 clicks, our method still achieves the highest mIoU on all datasets. The experiment demonstrates the superior segmentation ability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Edge Flow. We perform ablation experiments on Berkeley, DAVIS and Pascal VOC datasets to validate the effectiveness of the edge-guided flow. As shown in table 2, the baseline with edge-guided flow outperforms the baseline obviously on all three datasets. For a better understanding, we visualize the intermediate feature maps of baseline and baseline+EF, as shown in <ref type="figure">Fig. 5</ref>. The model with the earlylate fusion shows the stronger activation on coordinate information, while the early fusion has the feature dilution problem. Then, we visualize the output difference between baseline and baseline+EF, which is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. The result demonstrates that edge-guided flow improves interactive segmentation performance and stabilize the output when adding a new click. We apply edge masks as prior information of the network, which makes the segmentation results more stable.</p><p>FineNet. As shown in table 2, we find that NoC@85 and NoC@90 improve significantly after FineNet is added. As described above, FineNet adopts atrous convolution blocks which obtain large receptive fields on high-resolution features, so that they can obtain more context information. The best result is the combination of baseline+EF+F, which demonstrates the effectiveness of the early-late fusion strategy and the coarse-to-fine architecture. <ref type="table">Table 3</ref> shows the effect of prior information on different datasets. <ref type="figure">Fig 8 shows</ref> the mIoU change over the first 20 clicks using different prior information. We study two types of edge masks, 1) the edge mask estimated from the previous segmentation result, i.e. EI mask, 2) the edge mask directly calculated by the Sobel operator, i.e. SI mask. We find the model with the EI mask outperforms the other one. In the segmentation model, prior information can be dynamically adjusted with the network and user interaction clicks. Therefore, the EI mask is more related to the object of interest. However, the SI mask calculation is only based on the original image, where it contains a lot of redundant information which is irrelevant to the object of interest. Therefore, it causes network performance degradation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Interactive Segmentation Tool</head><p>In this work, we develop an interactive segmentation tool using the proposed model. Our tool aims to help users annotate segmentation datasets efficiently and accurately. The annotation pipeline includes three steps: data preparation, interactive annotation and polygon frame editing.</p><p>During data preparation, our tool supports data formats in multiple domains including natural images, medical imaging, remote sensing images and so on. After the image is loaded into the software, it can be zoomed, moved and pre-processed by adjusting brightness and contrast. The annotation process is highly flexible and configurable. Most operations in the application support keyboard shortcuts which can be changed by the user.</p><p>During interactive annotation, users add positive and negative points with left and right mouse clicks, respectively. The application runs model inference and shows the user prediction result, as shown in <ref type="figure">Fig. 9(a)</ref>. The user can adjust the target border by changing the threshold to distinguish foreground and background pixels to get more accurate segmentation results. The tool also supports filtering the largest connected region, which is shown in <ref type="figure">Fig. 9(b)</ref>. This feature is useful when there are multiple targets of the same type in an image. Suppressing small positive regions can free users from clicking negative points in each of these regions.</p><p>After finishing interactive segmentation, the tool generates a polygon frame around the target border. Users can adjust the polygon vertexes to further improve segmentation accuracy. It is flexible for practical segmentation tasks.</p><p>In some cases, adjusting the polygon frame could be faster than adding many clicks during interactive segmentation, so that it improves the overall annotation efficiency. Finally, the segmentation results can be saved in multiple formats including segmentation mask, PASCAL VOC, COCO, pseudo colour and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>A large number of labelled image data are usually essential for segmentation models. Due to the high cost of the pixel-level annotations, interactive segmentation becomes an efficient way to extract the object of interest. In this work, we propose a novel interactive architecture named EdgeFlow that fully utilizes the user interaction information without any post-processing or iterative optimization scheme. With the coarse-to-fine network design, our proposed method achieves state-of-the-art performance on common benchmarks. Furthermore, we develop an efficient interactive segmentation tool that helps the user to improve the segmentation result progressively with flexible options. In future work, we will work on lightweight models that can be deployed into various platforms. Another promising topic we are working on is multi-modality, which utilizes more input types, like audio and text. The different kinds of inputs can complement each other, so that it is vital to improve the quality further. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results of our method on Berkeley dataset. The first two columns represent original images and ground-truth masks, respectively. Other columns show segmentation results with the different numbers of clicks. Green dots denote positive clicks, red dots denote negative clicks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The qualitative results of edge-guided flow on Berkeley dataset. The fist three columns show the output of baseline on different clicks. The last three columns show the output of baseline with edge-guided flow on different clicks. Edge-guided flow produces more stable segmentation results. image and clicks model w/o early-late fusion model + early-late fusion image and clicks model w/o early-late fusion model + early-late fusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Different fusion blocks in front of backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Evaluation of different interactive segmentation methods. The plots represent the mean IoU of different methods for the first 20 clicks iteratively on GrabCut, Berkeley and DAVIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Effects of prior information. The plot represents the value of mean Intersection over Union for first 20 clicks iteratively on Berkeley, DAVIS and Pascal VOC, respectively. (a) UI for interactive clicks (b) adjustment process by polygon editing (c) application on remote sense Visualization of interactive segmentation tool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>This work was supported by the National Key Research and Development Project of China (2020AAA0103500).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results of GrabCut, Berkeley, DAVIS and Pascal VOC. Lower value is better. The best result is marked in bold.</figDesc><table><row><cell>Method</cell><cell cols="5">GrabCut NoC@85 NoC@90 NoC@90 NoC@85 NoC@90 Berkeley DAVIS</cell><cell>Pascal VOC NoC@85</cell></row><row><cell>GC [10]</cell><cell>7.98</cell><cell>10.00</cell><cell>14.22</cell><cell>15.13</cell><cell>17.41</cell><cell>-</cell></row><row><cell>GM [18]</cell><cell>13.32</cell><cell>14.57</cell><cell>15.96</cell><cell>18.59</cell><cell>19.50</cell><cell>-</cell></row><row><cell>RW [17]</cell><cell>11.36</cell><cell>13.77</cell><cell>14.02</cell><cell>16.71</cell><cell>18.31</cell><cell>-</cell></row><row><cell>ESC [19]</cell><cell>7.24</cell><cell>9.20</cell><cell>12.11</cell><cell>15.41</cell><cell>17.70</cell><cell>-</cell></row><row><cell>GSC [19]</cell><cell>7.10</cell><cell>9.12</cell><cell>12.57</cell><cell>15.35</cell><cell>17.52</cell><cell>-</cell></row><row><cell>DOS [52]</cell><cell>-</cell><cell>6.04</cell><cell>8.65</cell><cell>-</cell><cell>-</cell><cell>6.88</cell></row><row><cell>LD [29]</cell><cell>3.20</cell><cell>4.79</cell><cell>-</cell><cell>5.05</cell><cell>9.57</cell><cell>-</cell></row><row><cell>RIS-Net [30]</cell><cell>-</cell><cell>5.00</cell><cell>6.03</cell><cell>-</cell><cell>-</cell><cell>5.12</cell></row><row><cell>ITIS [38]</cell><cell>-</cell><cell>5.60</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.80</cell></row><row><cell>CAG [39]</cell><cell>-</cell><cell>3.58</cell><cell>5.60</cell><cell>-</cell><cell>-</cell><cell>3.62</cell></row><row><cell>BRS [21]</cell><cell>2.60</cell><cell>3.60</cell><cell>5.08</cell><cell>5.58</cell><cell>8.24</cell><cell>-</cell></row><row><cell>FCA [32]</cell><cell>-</cell><cell>2.08</cell><cell>3.92</cell><cell>-</cell><cell>7.57</cell><cell>2.69</cell></row><row><cell>IA+SA [24]</cell><cell>-</cell><cell>3.07</cell><cell>4.94</cell><cell>5.16</cell><cell>-</cell><cell>3.18</cell></row><row><cell>f-BRS-B [49]</cell><cell>2.50</cell><cell>2.98</cell><cell>4.34</cell><cell>5.39</cell><cell>7.81</cell><cell>-</cell></row><row><cell>RITM-H18 [50]</cell><cell>1.54</cell><cell>1.70</cell><cell>2.48</cell><cell>4.79</cell><cell>6.00</cell><cell>2.59</cell></row><row><cell>Our method</cell><cell>1.60</cell><cell>1.72</cell><cell>2.40</cell><cell>4.54</cell><cell>5.77</cell><cell>2.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Evaluation of different model architecture on Berkeley, DAVIS and Pascal VOC. 'baseline' represents coarse model without edgeguided flow branch. 'EF' represents edge-guided flow. 'F' represents FineNet. Lower value means less interaction and better performance. Evaluation of different input setting on Berkeley, DAVIS and Pascal VOC. 'SI' represents sobel edge as prior information. 'EI' represents edge mask from segmentation result. 'F' represents FineNet. Lower value means less interaction and better performance.</figDesc><table><row><cell>model</cell><cell cols="6">Berkeley NoC@85 NoC@90 NoC@85 NoC@90 NoC@85 NoC@90 DAVIS Pascal VOC</cell></row><row><cell>baseline</cell><cell>2.06</cell><cell>2.93</cell><cell>4.83</cell><cell>6.27</cell><cell>2.68</cell><cell>3.26</cell></row><row><cell>baseline+EF</cell><cell>1.83</cell><cell>2.79</cell><cell>4.82</cell><cell>6.14</cell><cell>2.66</cell><cell>3.19</cell></row><row><cell>baseline+EF+F</cell><cell>1.60</cell><cell>2.40</cell><cell>4.54</cell><cell>5.77</cell><cell>2.50</cell><cell>3.07</cell></row><row><cell>Model</cell><cell cols="6">Berkeley NoC@85 NoC@90 NoC@85 NoC@90 NoC@85 NoC@90 DAVIS Pascal VOC</cell></row><row><cell>SI + w/o F</cell><cell>1.83</cell><cell>3.20</cell><cell>5.04</cell><cell>6.32</cell><cell>2.88</cell><cell>3.41</cell></row><row><cell>EI + w/o F</cell><cell>1.83</cell><cell>2.79</cell><cell>4.82</cell><cell>6.14</cell><cell>2.66</cell><cell>3.19</cell></row><row><cell>SI + F</cell><cell>1.58</cell><cell>2.73</cell><cell>4.68</cell><cell>6.21</cell><cell>2.9</cell><cell>3.46</cell></row><row><cell>EI + F</cell><cell>1.60</cell><cell>2.40</cell><cell>4.54</cell><cell>5.77</cell><cell>2.5</cell><cell>3.07</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leanne</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="647" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image segmentation for intensity inhomogeneity in presence of high noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haider</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lavdie</forename><surname>Rada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noor</forename><surname>Badshah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3729" to="3738" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Error-tolerant scribbles based interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unifying the random walker algorithm and the SIR model for graph clustering and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><forename type="middle">George</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Maragos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large-scale interactive object segmentation with human annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Application of locally invariant robust PCA for underwater image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="29470" to="29481" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph cuts and efficient N-D image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>Funka-Lea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="131" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Interactive graph cuts for optimal boundary and region segmentation of objects in N-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Pierre</forename><surname>Jolly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Interactive graph cuts for optimal boundary and region segmentation of objects in N-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Pierre</forename><surname>Jolly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast global minimization of the active contour/snake model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selim</forename><surname>Esedoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Thiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">J</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="167" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascade centernet: Robust object detection for power line surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoci</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="60244" to="60257" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sub-markov random walk for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="516" to="527" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random walks in directed hypergraphs and application to semi-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur?lien</forename><surname>Ducournau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Bretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="91" to="102" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Geodesic star convexity for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Geodesic star convexity for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Interactive image segmentation via backpropagating refinement scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward efficient object detection in aerial images using extreme scale metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junning</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="56214" to="56227" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Localization-aware adaptive pairwise margin loss for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoseong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="8786" to="8796" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Continuous adaptation for interactive object segmentation by learning from corrections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interactive active contour with kernel descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoguo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiguang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="page" from="53" to="72" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pointflow: Flowing semantics through points for aerial image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving semantic segmentation via decoupled body and edge supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semantic flow for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Interactive image segmentation with latent diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Regional interactive image segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sim</forename><forename type="middle">Heng</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Interactive image segmentation with first click attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Ping</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Paddleseg: A highefficient development toolkit for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baohua</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Hao</surname></persName>
		</author>
		<idno>abs/2101.06175</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Joint optimization of segmentation and color clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Lobacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep optical flow feature fusion based on 3d convolutional networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihui</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudian</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Min</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Paddlepaddle: An open-source deep learning platform from industrial practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Iteratively trained interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Content-aware multilevel guidance for interactive instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumajit</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Riverbed: A novel user-steered image segmentation method based on optimum boundary tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Paulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">X</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago Vallin</forename><surname>Falc?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3042" to="3052" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust interactive image segmentation using convex active contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Nhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3734" to="3743" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust interactive image segmentation using convex active contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Nhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3734" to="3743" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Scene segmentation based on seeded region growing for foreground detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasni</forename><surname>Hongwu Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqin</forename><surname>Mohamad Zain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Homogeneous tree height derivation from tree crown delineation using seeded region growing (SRG) segmentation. Geo spatial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khairul Nizam</forename><surname>Muhamad Farid Ramli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="208" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">grabcut&quot;: interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">F-BRS: rethinking backpropagating refinement for interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konushin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Reviving iterative training with mask guidance for interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konushin</surname></persName>
		</author>
		<idno>abs/2102.06583</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deep grabcut for object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Interactive object segmentation with insideoutside guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
