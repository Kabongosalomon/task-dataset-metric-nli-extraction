<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Cluster Contrastive learning for Object Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
							<email>hantao.yao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Cluster Contrastive learning for Object Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, cluster contrastive learning has been proven effective for object ReID by computing the contrastive loss between the individual features and the cluster memory. However, existing methods that use the individual features to momentum update the cluster memory will fluctuate over the training examples, especially for the outlier samples. Unlike the individual-based updating mechanism, the centroid-based updating mechanism that applies the mean feature of each cluster to update the cluster memory can reduce the impact of individual samples. Therefore, we formulate the individual-based updating and centroid-based updating mechanisms in a unified cluster contrastive framework, named Dual Cluster Contrastive framework (DCC), which maintains two types of memory banks: individual and centroid cluster memory banks. Significantly, the individual cluster memory considers just one individual at a time to take a single step for updating. The centroid cluster memory applies the mean feature of each cluster to update the corresponding cluster memory. During optimization, besides the vallina contrastive loss of each memory, a cross-view consistency constraint is applied to exchange the benefits of two memories for generating a discriminative description for the object ReID. Note that DCC can be easily applied for unsupervised or supervised object ReID by using ground-truth labels or the generated pseudo-labels. Extensive experiments on three benchmarks, e.g., Market-1501, MSMT17, and VeRi-776, under supervised Object ReID and unsupervised Object ReID demonstrate the superiority of the proposed DCC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object Re-identification (ReID), such as person ReID and vehicle ReID, aims to search the probe image from different camera-views, attracting increasing attention because of the growing demands in practical video surveillance. Based on whether using the human-annotated labels, object ReID can be divided into supervised object ReID and unsupervised object ReID. The supervised object ReID aims to infer a discriminative description with the annotated labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b68">67]</ref>. Since collecting the massive identity annotations is time-consuming and expensive, unsupervised object ReID infers the description with the pseudo-labels <ref type="bibr">[11, 13, 16-18, 33, 52, 53, 64]</ref>, e.g., the pseudo-labels inferred by clustering methods from the unlabeled data are used for unsupervised ReID.</p><p>Recently, contrastive learning has been widely exploited for unsupervised representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref>, aiming to learn the invariance feature with a self-supervised mechanism based on sample self-augmented, e.g., Zhong et al. <ref type="bibr" target="#b66">[65]</ref> save features of all the images in the unlabeled training set in the memory bank for vallina contrastive learning. Inspired by vallina contrastive learning, cluster contrastive learning has received more attention for ReID <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b48">48]</ref>. The cluster contrastive learning in object ReID builds a cluster-level memory in which a single feature vector represents each cluster <ref type="bibr" target="#b12">[13]</ref>, shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>. For example, Dai et al. <ref type="bibr" target="#b12">[13]</ref> store the feature vectors and compute contrast loss at the cluster level. Li et al. <ref type="bibr" target="#b31">[31]</ref> propose an asymmetric contrastive learning framework to exploit both the cluster structure and the individual feature to conduct effective contrastive learning. Clusterwise Contrastive Learning (CCL) <ref type="bibr" target="#b25">[26]</ref> is proposed by iterative optimization of feature learning and cluster refinery to learn noise-tolerant representations. However, previous methods maintain the individual cluster memory by considering just one individual at a time to take a single step for updating. The cluster contrastive with individual-based updating mechanism will fluctuate over the training examples, especially for the outlier samples.</p><p>To address the above problem, we consider dual complementary updating mechanisms for the cluster contrastive learning. Besides the individual-based update mechanism of the vanilla cluster contrastive, we also introduce a novel centroid-based update mechanism to produce a centroidbased cluster memory that has a different embedding space from the individual-based cluster memory. Unlike the individual-based updating mechanism that uses the individual feature to update the cluster memory, the centroid-based update mechanism applies the mean feature of each cluster to update the corresponding cluster memory, shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. Although the provided labels might contain some incorrect labels, most of them are correct. Therefore, using the mean feature to represent each cluster can reduce the effect of the outlier samples. From the perspective of optimization, the individual-based updating mechanism can be treated as the Stochastic Gradient Descent, which considers just one individual at a time to take a single step for updating the parameters. However, considering just an individual sample will fluctuate over the training examples, especially for the outlier samples. The centroid-based updating mechanism is similar to the Batch Gradient Descent, which is proposed to reduce the impact of individual samples by considering all training data in a single step, e.g., it takes the average of the gradients of all the training examples and then uses the mean gradient to update the parameters. Furthermore, the individual-based update mechanism constructs the embedding space by considering all individual samples, and the centroid-based updating mechanism aims to generate a stable embedding space by considering the mean description of each cluster. Therefore, the cluster contrastive learning with the centroid-based updating mechanism can complement the individual-based updating mechanism. Consequently, by jointly considering those two updating mechanisms, dual cluster contrastive learning can improve the stability and discrimination of descriptions.</p><p>Based on the above motivation, we propose a novel Dual Cluster Contrastive learning (DCC) framework, in which an individual cluster memory and a centroid cluster memory are employed to implement the individual-based and centroid-based updating mechanism. As shown in the left part of <ref type="figure" target="#fig_0">Figure 1</ref>(b), the individual cluster memory uses the individual features and their labels to update the corresponding cluster memory. Meanwhile, the centroid cluster memory applies the average of features belonging to the same cluster for momentum updating. Especially, using the individual cluster memory and centroid cluster memory can embed the instance's features into the individual-level prediction and centroid-level prediction, respectively, as shown in <ref type="figure">Figure 2</ref>. To boost the discriminative of each memory, we apply two different feature extraction modules, i.e., individual backbone, and centroid backbone, to extract the independent features for updating the individual cluster memory and centroid cluster memory, and the standard contrastive loss, i.e., clusterNCE loss, is applied for optimization. Besides the standard contrastive loss for each memory bank, a cross-view contrastive loss is also used to exchange the benefit knowledge of the individual cluster memory and centroid cluster memory. For example, the contrastive loss of the individual cluster memory and centroid cluster memory is used to optimize the centroid backbone and individual backbone, respectively. In the inference stage, the combination of the features generated by the individual backbone and centroid backbone is used for retrieval. The DCC can be easily applied for unsupervised or supervised object ReID by using ground-truth labels or pseudo-labels generated with the clustering method, respectively.</p><p>In summary, to overcome the limitation of vanilla cluster contrastive learning, we propose a novel Dual Cluster Contrastive learning framework for object ReID, consisting of the individual-level and centroid-level cluster contrast. The evaluation on three benchmarks under supervised and unsupervised settings show the effectiveness of the DCC. 1) supervised object ReID: DCC with ResNet50/RestNet50-ibn obtain the mAP of 89.9%/90.6%, 65.5%/69.6%, and 82.6%/83.5% for Market-1501, MSMT17, and VeRi-776, respectively. 2) unsupervised object ReID: DCC with ResNet50/RestNet50ibn obtain the mAP of 83.4%/85.8%, 35.9%/36.6%, and 41.4%/42.1% for Market-1501, MSMT17, and VeRi-776, respectively. The experiment also shows that DCC has the advantages of fast training convergence, insensitivity to batch size, and high generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section gives a brief review of the object Reidentification (ReID) and the contrastive learning for object ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Individual-Level Prediction</head><p>Individual Backbone ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Centroid-Level Prediction</head><p>Centroid-based updating mechanism ? Individual-based updating mechanism  <ref type="figure">Figure 2</ref>. The framework of proposed Dual Cluster Contrastive learning for supervised object ReID. Given the training images, we apply the Individual Backbone and Centroid Backbone to extract the corresponding Individual-level Feature F i and Centroid-level Feature F c . Then, Individual Cluster Memory M i and Centroid Cluster Memory M c are used to represent two types of cluster centers with the Individual-based updating mechanism and Centroid-based updating mechanism, respectively. The Centroid-based updating mechanism uses the mean feature of each cluster to momentum update the Centroid Cluster Memory. The vallina contrastive loss and cross-view contrastive loss are used for optimization. Feature vectors in different shades of green are of the same identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Re-identification</head><p>Based on manually annotated labels, many methods have been proposed for supervised object ReID and achieved promising performance. For example, Zheng et al. <ref type="bibr" target="#b60">[60]</ref> and Fu et al. <ref type="bibr" target="#b15">[16]</ref> extract local features from several horizonal stripes to explore the discriminative clues. Furthermore, many novel attention modules have been proposed for supervised person ReID <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b57">57]</ref>. Recently, inspired by the success of transformer structure, He et al. <ref type="bibr" target="#b22">[23]</ref> and Lai et al. <ref type="bibr" target="#b28">[29]</ref> adopt transformer structure to explore the discriminative clues for supervised person ReID.</p><p>As manual annotations are expensive and unavailable in real-world applications, unsupervised person ReID has attracted much more attention. Some researchers use extra labeled images to assist the unsupervised training on unlabeled person ReID by transferring labeled images to the unlabeled domains with GAN-based models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b65">64]</ref> or narrowing the distribution gap in feature space <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">34]</ref>. For example, Liu et al. <ref type="bibr" target="#b33">[33]</ref> use three GAN models to reduce the discrepancy between different domains in illumination, resolution, and camera-view, respectively. To handle the lack of annotation, many methods have been proposed to acquire reliable pseudo labels <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b62">61]</ref>. For example, Lin et al. <ref type="bibr" target="#b32">[32]</ref> propose a bottom-up unsupervised clustering method that simultaneously considers both diversity and similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive Learning for Object ReID</head><p>Recently, several methods further adopt memory bank and contrastive loss for object ReID <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b66">65]</ref>. For example, Zhong et al. <ref type="bibr" target="#b66">[65]</ref> save features of all the images in the unlabeled training set in the memory bank for contrastive learning. Chen et al. <ref type="bibr" target="#b2">[3]</ref> save a proxy feature for each class in the memory bank, and Ge et al. <ref type="bibr" target="#b17">[18]</ref> propose a hybrid memory bank that saves both instance features and class proxy. Dai et al. <ref type="bibr" target="#b12">[13]</ref> present the Cluster Contrast mechanism, which stores feature vectors and compute contrast loss in cluster level memory dictionary. Li et al. <ref type="bibr" target="#b31">[31]</ref> propose an asymmetric contrastive learning framework to exploit both the cluster structure and the invariance in augmented data to conduct effective contrastive learning for person ReID. Chen et al. <ref type="bibr" target="#b3">[4]</ref> apply the Generative Adversarial Network (GAN) for data augmentation and propose a view-invariant loss to facilitate contrastive learning between original and generated views. To reduce the effect of noisy labels, Cluster-wise Contrastive Learning (CCL) <ref type="bibr" target="#b25">[26]</ref> is proposed by iterative optimization of feature learning and cluster refinery to learn noise-tolerant representations.</p><p>However, these methods update the memory bank with every single feature in training batches, making features in memory easily affected by noisy samples. Compared with previous methods, DCC additionally introduces a centroid updated memory bank that updates based on class mean features of each cluster in training batches. As mean features are robust against minority noisy samples, updating the memory bank with class centroids will enhance the robustness of the memory bank against label noise and improve the training efficiency and the ReID performance. Experiments also show that DCC has the advantages of fast training convergence, insensitivity to batch size, and high generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Given a training dataset D = {(x j , y j )} N j=1 , the object re-identification (ReID) aims to learn a robustness backbone ? for retrieval, where x i and y j ? [1, N c ] denote the j-th training image and its label, respectively. N is the number of the training images, and N c denotes the number of identities. For convenience, we use X and Y to represent the set of training images and labels, respectively. The supervised and unsupervised object ReID can be formulated with the same unified framework once providing the annotated labels Y or generating the pseudo-labels?. For example, the ground-truth Y is annotated by the human for the supervised object ReID, and the pseudo-labels? can be generated with a clustering algorithm for the unsupervised object ReID. For convenience, we denote Y as the obtained annotations for both supervised and unsupervised object ReID tasks to describe the proposed methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vallina Cluster Contrastive</head><p>With the above definition, we firstly review of the vallina cluster contrastive. It contains two components: backbone ?, and cluster memory M, in which each cluster is represented by a mean feature, and all cluster feature vectors are updated based on the individual feature, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a). Given all training images X, the backbone ? is used to extract the corresponding features F = ?(X). Then, cluster memory bank M ? R Nc?N d is initialized with the mean feature of each class, where N d and N c are the feature dimension and the number of classes, respectively. Based on the pretrained feature F, the visual center M j of the j-th class is initialized with Eq. (1),</p><formula xml:id="formula_0">M j = 1 N (F j ) f ?Fj f ,<label>(1)</label></formula><p>where F j denotes the feature set of images belonging to the j-th class, N (F j ) represents the number of features in set F j , f ? R 1?N d is an image feature, and M j denotes the j-th cluster feature in M.</p><p>Since the cluster memory M can be treated as a nonparametric classifier, it can produce the class prediction used for contrastive loss, formulated as Eq. <ref type="formula" target="#formula_1">(2)</ref>,</p><formula xml:id="formula_1">L = ? log exp(f i ? M yi )/? Nc j=1 exp(f i ? M j )/? ,<label>(2)</label></formula><p>where ? is a temperature hyper-parameter <ref type="bibr" target="#b46">[46]</ref>, and y i is the corresponding label for the image feature f i . When the feature f i has a higher similarity to its ground-truth visual centers M yi and dissimilarity to all other cluster features, the objective loss L has a lower value.</p><p>In vanilla cluster contrastive learning, the individual feature is applied to momentum update the cluster memory M during backward propagation with Eq. <ref type="formula" target="#formula_2">(3)</ref>,</p><formula xml:id="formula_2">M yi = ?M yi + (1 ? ?) ? f i ,<label>(3)</label></formula><p>where f i and y i is the feature and label for image x i , respectively. M yi is the y i -th cluster feature in cluster memory M.</p><p>Although the cluster contrastive learning can generate discriminative descriptions, it will fluctuate over the training samples because it considers just one individual at a time to take a step for updating the cluster memory with Eq. (3). Specificially, ? is usually set to a lower value for the cluster contrastive learning for object ReID, e.g., ?=0.1 in <ref type="bibr" target="#b12">[13]</ref>. From Eq. <ref type="formula" target="#formula_2">(3)</ref>, we can observe that the mean feature in the cluster memory M is severely affected by the individual feature, leading to a severe negative impact. An intuitive illustration is shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dual Cluster Contrastive</head><p>To overcome the limitation of the vallina cluster contrastive, we propose a novel Dual Cluster Contrastive (DCC) framework, as shown in <ref type="figure">Figure 2</ref>. The significant difference between the Dual Cluster Contrastive and the vallina cluster contrastive is that DCC maintains two types of memory banks to model the feature distribution from two perspectives, i.e., individual cluster memory, and centroid cluster memory. Similar to the vallina cluster contrastive, the individual cluster memory is updated based on each individual at each step. Besides, the centroid cluster memory is used to model the class distribution based on the mean feature of each class, which can reduce the impact of individual samples. In Dual Cluster Contrastive, the individual cluster memory and the centroid cluster memory are defined as M i and M c , respectively. Note that M i and M c are both initialized based on the mean pretrained feature of each cluster with Eq. (1). The critical of the Dual Cluster Contrastive is how to update the individual cluster memory M i and centroid cluster memory M c .</p><p>To boost the discriminative of each memory, we apply two different feature extraction modules, i.e., individual backbone ? i , and centroid backbone ? c , to extract the independent features for updating the individual cluster memory M i and centroid cluster memory M c . Given the training dataset, we sample the batch training images (X i , Y i ) and (X c , Y c ). For example, the images X i and labels Y i are used to optimize the individual backbone ? i and update the individual cluster memory M i . Meanwhile, the images X c Individual Feature Individual-based updating mechanism Centroid-based updating mechanism <ref type="figure">Figure 3</ref>. The update process of the Individual-based (Grey arrow line) and Centroid-based updating mechanism (Blue arrow line). Individual-based updating mechanism is easily affected by individual feature, especially outlier samples. and denote the individual feature belonging to the same class from different batches, respectively. and are the corresponding center features. and labels Y c are used for the centroid branch. Specially, we generate the individual feature F i = ? i (X i ) by feeding the images X i into the individual backbone ? i . Similarly, the centroid features are denoted as F c = ? c (X c ). Furthermore, the features F i and F c are applied for updating the corresponding memories and optimizing the backbones.</p><p>During training, once obtaining the feature f c ? F c and f i ? F i , using the cluster memories M i and M c can generate the corresponding predictions p i = f i ? (M i ) and p c = f c ? (M c ) . Meanwhile, the cross-view contrastive prediction can be represented as p i?c = f c ? (M i ) and p c?i = f i ? (M c ) . With the predictions p c , p i , p i?c , and p c?i , we can compute the contrastive loss, i.e., NCEloss. Formally, the final loss L dcc is defined as:</p><formula xml:id="formula_3">L dcc = (1 ? ?)L c (F c |(M c , M i )) + ?L i (F i |(M i , M c )),<label>(4)</label></formula><p>where L i (?) and L c (?) are the loss computed based on the features F i and F c , respectively. ? = 0.25 + e/(2 ? total epoch) is a weight to balance the effect of different losses, where e and total epoch denote the current and total epochs. For convenience, we use L i and L c to rep-</p><formula xml:id="formula_4">resent L i (F i |(M i , M c )) and L c (F c |(M c , M i ))</formula><p>, which is computed as:</p><formula xml:id="formula_5">Li = ? f ?F i (log exp(f ? M i y )/? Nc j=1 exp(f ? M i j )/? +log exp(f ? M c y )/? Nc j=1 exp(f ? M c j )/? ),<label>(5)</label></formula><p>where y is the ground-truth label for the feature f . Note that the above equation consists of two terms. The first term represents the loss between the individual features f ? F i and its own individual cluster memory M i , which is consistent with the vallina cluster contrastive. Here, "own" Sample training images X i and X c from D; <ref type="bibr">3:</ref> Extracting the corresponding features F i and F c with backbone ? i (X i ) and ? c (X c ), respectively; <ref type="bibr">4:</ref> Computing contrastive loss with Eq. (5) and Eq. (6); <ref type="bibr">5:</ref> Updating the individual cluster memory M i with Eq. (7); <ref type="bibr">6:</ref> Updating the centroid cluster memory M c with Eq. (8); 7: end while Output: The trained model ? i and ? c . emphasizes that the individual cluster memory is updated with the individual-based updating mechanism. The second term in Eq. <ref type="formula" target="#formula_5">(5)</ref>, which can be treated as a cross-view contrastive loss, apply the centroid cluster memory to embed the individual features. Since centroid cluster memory is independent to the individual features, using the cross-view contrastive loss can enhance the discriminative of the individual backbone.</p><p>Similar to L i , L c also contains two terms:</p><formula xml:id="formula_6">Lc = ? f ?F c (log exp(f ? M c y )/? Nc j=1 exp(f ? M c j )/? +log exp(f ? M i y )/? Nc j=1 exp(f ? M i j )/? ),<label>(6)</label></formula><p>where the second term is also a cross-view contrastive loss between the centroid feature F c and the individual cluster memory M i for knowledge transfering. The above description is the forward process of the Dual Cluster Contrastive(DCC). However, the other problem of DCC is how to update the individual cluster memory M i and the centroid cluster memory M c during backward, which is a critical aspect of contrastive learning. Given the feature f i ? F i along with its label y ? Y i , the individual cluster memory M i is momentum updated with Eq. <ref type="formula" target="#formula_7">(7)</ref>,</p><formula xml:id="formula_7">M i y = ?M i y + (1 ? ?) ? f i .<label>(7)</label></formula><p>For the centroid cluster memory, we apply the mean feature of each class for momentum updating. Given the centroid feature F c and labels Y c of the batch images, we first compute each class's mean feature during training, and then update the centroid cluster memory with the obtained mean feature. Therefore, the cluster memory M c is updated with Eq (8),</p><formula xml:id="formula_8">M c y = ?M c y + (1 ? ?) ? m y ,<label>(8)</label></formula><p>where m y denotes the mean feature of y -th class, where F c y denotes the subset of centroid features F c belonging to the y -th class(y ? Y c ), N (F c y ) represents the number of features in set F c y , f is an instance feature. L2normalization is used to normalize the m y . The algorithm is illustrated in Algorithm 1.</p><formula xml:id="formula_9">m y = 1 N (F c y ) f ?F c y f ,<label>(9)</label></formula><p>Inference: After training, we can obtain the inferred individual backbone ? i and centroid backbone ? c . Given the testing image x t , the final feature used for retrieval is defined as f t :</p><formula xml:id="formula_10">f t = ||? i (x t )|| 2 + ||? c (x t )|| 2 ,<label>(10)</label></formula><p>where || ? || 2 is the L2-normalization, and the final feature is f t = ||f t || 2 . Once extracting all features for gallery and query images, the Euclidean distance is used to compute the similarity for retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generalization to unsupervised object ReID</head><p>Existing unsupervised object ReID commonly uses clustering methods to generate the pseudo-labels for unlabeled images, and then perform supervised training based on the generated pseudo-labels. With the pseudo-labels, the proposed Dual Cluster Contrastive can be easily applied for unsupervised object ReID.</p><p>Specifically, given the unlabeled training images, we firstly apply the backbone to extract the corresponding features. Similar to <ref type="bibr" target="#b12">[13]</ref>, we apply DBSCAN to group all training features into several groups. Then, the cluster ID is assigned to each training image as the pseudo-label, and the unclustered outlier images are discarded from training. With the pseudo-labels, the proposed Dual Cluster Contrastive is conducted for representation learning, shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct experiments on two person ReID datasets, Market-1501 <ref type="bibr" target="#b63">[62]</ref>, MSMT17 <ref type="bibr" target="#b44">[44]</ref>, and one vehicle ReID dataset, VeRi-776, under the supervised and unsupervised settings to evaluate the effectiveness of the proposed Dual Cluster Contrastive. The details of these datasets are summarized in <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The proposed Dual Cluster Contrastive is implemented based on the existing cluster contrastive framework [13] 1 . We adopt the ResNet-50 <ref type="bibr" target="#b19">[20]</ref> pretrained on ImageNet <ref type="bibr" target="#b13">[14]</ref> as the backbone. Inspired by <ref type="bibr" target="#b35">[35]</ref>, all sub-module layers after layer4-1 are removed, and a GEM pooling followed by batch normalization layer <ref type="bibr" target="#b24">[25]</ref> and L2-normalization layer is added. Therefore, the feature dimension N d is 2,048. For the person ReID, all input images are resized 256?128 for training and evaluation. For the vehicle ReID, all input images are resized 256?256 for training and evaluation. The temperature coefficient ? is set to 0.05. For the supervised object ReID, the adam optimizer sets the weight decay as 0.0005, and the learning rate is initially set as 0.00035 and decreased to one-tenth of every 50 epochs up to 150 epochs. Inspired by <ref type="bibr" target="#b12">[13]</ref>, for the unsupervised object ReID, DB-SCAN is used to generate pseudo labels for the Dual Cluster Contrastive, and the learning rate is initially set as 0.00035 and decreased to one-tenth of every 20 epochs up to 60 epochs. We sample P person identities and a fixed number K instances for each identity during training. Therefore, the batch size is P ? K. In this work, we set K=16, and P =8 for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baselines</head><p>In this section, we give a brief definition of the baselines used for evaluation in the following.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>In this section, we conduct some ablation studies on Market-1501, MSMT17, and VeRi-776 to evaluate the effectiveness of the proposed component in Dual Cluster Contrastive.</p><p>Effect of Dual Cluster Contrastive: As mentioned above, Dual Cluster Contrastive(DCC) consists of the individual-level cluster contrastive(ICC) and centroid-level cluster contrastive(CCC), where CCC is the significant contribution module from existing methods. Therefore, we analyze the effectiveness of the centroid-level cluster contrastive and Dual Cluster Contrastive, and summarize the results in <ref type="table">Table 2</ref>. We observe that the ICC model obtains a higher performance than the CCC model, e.g., ICC obtains the best mAP of 88.4% by setting the batch size as 128. However, the centroid-level cluster contrastive is more effective for the smaller batch size, e.g., CCC obtains a higher performance than ICC for the batch size from 48 to 80. Especially for the batch size of 48, ICC only obtains the performance of 72.8%, which has a large gap with its best performance of 88.4%. Different from ICC, CCC obtains the mAP of 83.7% by setting the batch size as 48. Therefore, we can conclude that the CCC model is more robust to the smaller batch size, and ICC can obtain higher performance.</p><p>Furthermore, DCC that combines two types of cluster contrastive learning obtains the best performance on all batch sizes, proving the centroid-level cluster contrastive complements the individual-level cluster contrastive. From <ref type="table">Table 2</ref>, we also observe that DCC is insensitive to changes in batch size. For example, the ICC model obtains the mAP of 88.6% for the batch size of 128. However, once reducing the batch size to 64, its mAP quickly plummets to 83.7%. Differently, DCC obtains the mAP 89.9% and 88.6% for the batch size of 128 and 64, respectively.</p><p>Training Convergence: We further analyze the training convergence process of DCC. <ref type="figure" target="#fig_6">Figure 5a</ref> summarizes the training loss of different models, e.g., ICC, CCC, and DCC. We can observe that the CCC has faster convergence and lower loss than CCC and DCC, and the DCC is a trade-off between the ICC and CCC. Furthermore, we summarize the change in mAP of different models in <ref type="figure" target="#fig_6">Figure 5b</ref>   see that the DCC achieves a lower loss and a higher performance with a faster convergence speed. For example, DCC obtains the mAP of 89.1% for the 60-th epoch, higher than the ICC and CCC models. Therefore, the proposed DCC can obtain a higher performance with a faster convergence speed during the training process.</p><p>Effect of the Cross-view Contrastive Embedding: As mentioned above, the Individual-level Cluster Contrastive (ICC) and Centroid-level Cluster Contrastive(CCC) have different properties, such as convergence speed and sensitivity to batch size. Therefore, the DCC and ICC can generate two different identity embedding spaces for the same dataset. To min the benefit knowledge of each model, the cross-view contrastive embedding is applied for optimizing the backbone, i.e., the second term in Eq. (5) and Eq. (6). As shown in <ref type="table" target="#tab_3">Table 3</ref>, the Dual Cluster Contrastive (DCC) obtains a higher mAP than DCC v that does not consider the cross-view contrastive loss. Furthermore, the advantage of the cross-view constrastive loss is that it can exchange the benefit between the individual cluster memory and centroid cluster memory to boost the performance, e.g., DCC i and DCC c consistently outperformed the ICC and CCC during training, shown in <ref type="figure" target="#fig_6">Figure 5b</ref>. Significantly, the mAP of DCC c , which is the branch of the centroid backbone of the DCC, has been improved from 87.0% of CCC to 89%. The reason is that the cross-view contrastive loss can use the discriminative individual cluster memory to increase the discriminative of the centroid backbone. Otherwise, the benefits of the centroid cluster memory can also be used to improve the robustness of individual backbone, e.g., improving the mAP from 88.6% of ICC to 89.3% of DCC i .</p><p>Effect of momentum value ?: Similar to existing contrastive learning, momentum updating strategies are applied  to update cluster features in individual-level and centroidlevel memories. Note that the individual-level and centroidlevel cluster memory banks use the same momentum value ?. As shown in <ref type="figure">Figure 6</ref>, the smaller ? performs better than higher ?, e.g., ?=0.1 obtains the highest performance. Furthermore, we observe that the DCC is insensitive to ? during memory updating, e.g., by setting ? from 0.1 to 0.7, the mAP is slightly dropped from 89.9% to 88.8%, which is still higher than the best performance of ICC. <ref type="figure">From Figure 6</ref>, it can also be seen that the vallina cluster contrastive (ICC) obtains the best performance by setting ?=0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Batch Size:</head><p>To evaluate the impact of batch size for Dual Cluster Contrastive, we compare the batch size from 64 to 160 on Market-1501 and VeRi-776 datasets. As shown in <ref type="table">Table 7</ref>, increasing the batch size would first increase and then decrease performance, e.g., using the batch size of 128 obtains the highest mAP of 89.9% and 82.6% of Market-1501 and VeRi-776, which is higher than the batch size of 64 and 160.</p><p>Effect of K for training identities sampling: During training, we sample P person identities and a fixed number K instances for each person identity for each training batch. We thus analyze the effect of K for training identities sampling by setting the batch size as 128. As shown in <ref type="table" target="#tab_4">Table 4</ref>, setting K=16 obtains the best performance on all three datasets.  <ref type="bibr" target="#b38">[38]</ref> 87.0 94.5 97.9 -Circle Loss <ref type="bibr" target="#b40">[40]</ref> 87.4 96.1 --CLA <ref type="bibr" target="#b5">[6]</ref> 88.0 95.4 --FastReID(ResNet50) <ref type="bibr" target="#b20">[21]</ref> 88.2 95.4 --RGA-SC <ref type="bibr" target="#b56">[56]</ref> 88.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with existing methods</head><p>In this section, we conduct the comparison with existing methods following two settings: supervised Object ReID, and unsupervised Object ReID on three benchmarks, i.e., Market-1501, MSMT17, and VeRi-776.</p><p>Comparision on Supervised Object ReID: We first compare the Dual Cluster Contrastive with existing supervised ReID methods on three benchmarks and summarize the results in <ref type="table" target="#tab_7">Table 5</ref>, <ref type="table">Table 6</ref>, and <ref type="table">Table 7</ref> for Market-1501, MSMT17, and VeRi-776, respectively. We conduct the comparison from three aspects to prove the effectiveness of the proposed DCC.</p><p>Firstly, we can observe that the proposed Dual Cluster Contrastive is significantly better than existing methods with the same backbone, proving the effectiveness of the proposed DCC, e.g., with the backbone of ResNet50, DCC obtains the mAP of 89.9%, 61.2%, and 82.6% for Market-1501, MSMT17, and VeRi-776, respectively. As shown in <ref type="table">Table 6</ref>, DCC(ResNet-50) obtains the worse performance than TransReID [23] on MSMT17, e.g., 65.5% vs 67.4%. The reason is that TransReID applies the transformer-based network as the backbone for object ReID, which is stronger than the ResNet used in DCC. However, by replacing the ResNet-50 with a stronger backbone ResNet50-ibn, DCC achieves a performance of 69.6%, which is higher than 67.4% of TransReID. Otherwise, the DCC(ResNet-50) obtains a higher performance than TransReID [23] on both Market-1501 and VeRi-776.</p><p>Secondly, based on whether considering the local features or additional information, existing methods can be divided into two groups: backbone-independent methods and feature-fusion methods. The feature-fusion methods pro-  pose an additional module to min the multi-level discriminative clues for object ReID. The backbone-indpependent methods propose and add a backbone-indpependent modules on the backbone for inferring the discriminative description, e.g., Contrastive Learning <ref type="bibr" target="#b12">[13]</ref>, Circle Loss <ref type="bibr" target="#b40">[40]</ref>, Triplet Loss, and Classification Loss. Therefore, all backbone-independent methods use the same backbone for inferencing, e.g., ResNet50, or ResNet50-ibn, and making a comparison among them be a fair comparison. Therefore, Dual Cluster Contrastive can be treated as a novel backbone-independent module. Compared with existing backbone-independent methods, we can observe that DCC obtains a noticeable improvement upon the existing methods, e.g., improving the mAP from 87.4%, and 52.1% of Circle Loss <ref type="bibr" target="#b40">[40]</ref> to 89.9%, and 65.5%. The superior performance demonstrates the effectiveness of the proposed DCC. Finally, we evaluate the proposed DCC with different backbones to show its better generability. Since Instance normalization(IN) <ref type="bibr" target="#b42">[42]</ref> and Batch normalization (BN) <ref type="bibr" target="#b24">[25]</ref> can capture the appearance invariance and the content-related information, the Instance-batch normalization (IBN) <ref type="bibr" target="#b37">[37]</ref> combining the advantage of the abovementioned normalization has been proven to be effective for person ReID <ref type="bibr" target="#b20">[21]</ref>. Therefore, by replacing the BN operation in ResNet50 with IBN operation, the ResNet50-ibn is treated as a strong backbone. From <ref type="table" target="#tab_7">Table 5</ref>, <ref type="table">Table 6</ref> and <ref type="table">Table 7</ref>, we can observe that ResNet50-ibn obtain a higher performance than ResNet50, e.g., compared with the ResNet50, using ResNet50-ibn improves the mAP from 89.9%, 65.5%, and 82.6% to 90.6%, 69.6%, and 83.5% for Market-1501, MSMT17, and VeRi-776, respectively.</p><p>Comparision on Unsupervised Object ReID: In addition to the supervised object ReID, we further compare the Dual Cluster Contrastive with existing unsupervised object ReID methods and summarize the related results in <ref type="table" target="#tab_9">Table 8</ref>, <ref type="table" target="#tab_11">Table 9</ref>, and <ref type="table" target="#tab_0">Table 10</ref> for Market-1501, MSMT17, and VeRi-776, respectively. We can observe that DCC is significantly better than existing methods, proving the effectiveness of the Dual Cluster Contrastive.</p><p>Disscussion The most related work to ours is the vallina cluster contrastive. Compared with the vallina cluster contrastive, the Dual Cluster Contrastive has the following benefits based on the above analyses. Firstly, Dual Cluster Contrastive (DCC) is insensitive to the batch size and momentum factor. Secondly, DCC has a faster convergence speed during the training process. Thirdly, DCC has better generalization capabilities, e.g., DCC achieves better performance for both supervised and unsupervised object ReID on three benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>To overcome the limitation of the vallina cluster contrastive for Object ReID, we propose a novel Dual Cluster Contrative framework, which consists of the individuallevel cluster contrastive and centroid-level cluster con-   trastive. The individual-level cluster contrastive updates its memory bank based on the individual feature with the momentum update strategy, and the centroid-level cluster contrastive updates its memory bank with the mean feature of each class. The evaluations on three benchmarks under the supervised and unsupervised settings demonstrate the effectiveness of the proposed DCC. Although the proposed DCC is a practical framework, it maintains two different memory banks, with certain limitations in applying a large-scale dataset. In the future, we will explore how to use one memory bank to achieve the benefits of the two memory banks through adaptive optimization strategies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison with the Vallina Cluster Contrastive learning (a), and the Dual Cluster Contrastive Learning (b). Feature vectors in different shades of green are of the same identity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 1 :</head><label>11</label><figDesc>The procedure of DCC Require: Given the dataset D = {(xi, yi)} N i=1 Require: Initialize the individual backbone ? i and centroid backbone ? c with ResNet-50 pretrained on the ImageNet. Require: Initialize the individual cluster memory M i and centroid cluster memory M c with the mean feature of each cluster. while e? total epoch do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The framework of proposed Dual Cluster Contrastive learning for unsupervised object ReID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 .</head><label>1</label><figDesc>ICC: The vallina cluster contrastive introduced in Sec. 3.1. 2. CCC: The vallina cluster contrastive by replacing the individual update policy (Eq. (3)) with centroid cluster updating policy(Eq. (8)). 3. DCC i : The proposed Dual Cluster Contrastive employes the individual feature F i generated by the individual backbone for evaluation. 4. DCC c : The proposed Dual Cluster Contrastive employes the centroid feature F c generated by the centroid backbone for evaluation. 5. DCC v : The proposed Dual Cluster Contrastive does not consider the cross-view contrastive loss, i.e., ignoring the second term in Eq. (5) and Eq. (6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>The change of loss and mAP during training on Mar-ket1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>The effect of ? for DCC and ICC. The effect of batch size on Market1501 and MSMT17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of datasets used in our evaluation.</figDesc><table><row><cell>Datasets</cell><cell>Object</cell><cell>#ID</cell><cell>#image</cell><cell>#cam</cell></row><row><cell>Market-1501</cell><cell>Person</cell><cell>1,501</cell><cell>32,668</cell><cell>6</cell></row><row><cell>MSMT17</cell><cell>Person</cell><cell cols="2">4,101 126,441</cell><cell>15</cell></row><row><cell>VeRi-776</cell><cell>Vehicle</cell><cell>776</cell><cell>49,357</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>87.0 87.4 87.2 87.0 86.3 86.1 ICC 72.8 83.7 86.9 87.6 88.4 88.3 88.3 DCC 86.0 88.6 89.6 89.9 89.9 89.4 89.4</figDesc><table><row><cell>BatchSize</cell><cell>48</cell><cell>64</cell><cell>80</cell><cell>96</cell><cell>128</cell><cell>160</cell><cell>192</cell></row><row><cell cols="8">CCC 83.7 Table 2. Effect of batch size in DCC, CCC, and ICC on Market-</cell></row><row><cell>1501.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">6. DCC: The proposed Dual Cluster Contrastive mod-</cell></row><row><cell>ules.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Effect of cross-view contrastive loss. DCC denotes the model without considering the cross-view contrastive loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Effect of K for training identities sampling.</figDesc><table><row><cell>K</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell cols="5">Market-1501 86.6 88.6 89.9 89.3</cell></row><row><cell>MSMT17</cell><cell cols="4">59.5 63.3 65.5 63.6</cell></row><row><cell>VeRi-776</cell><cell cols="4">76.5 80.0 82.6 82.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The performance of the TransReID is the input size of 256 ? 128, which is similar to our setting.</figDesc><table><row><cell></cell><cell>4</cell><cell>96.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Pyramid-Net [60]</cell><cell>88.2</cell><cell>95.7</cell><cell>98.4</cell><cell>99.0</cell></row><row><cell>ABDNet [7]</cell><cell>88.28</cell><cell>95.6</cell><cell>-</cell><cell>-</cell></row><row><cell>SONA [47]</cell><cell>88.8</cell><cell cols="2">95.58 98.5</cell><cell>99.1</cell></row><row><cell>TransReID [23] *</cell><cell>88.9</cell><cell>95.2</cell><cell>-</cell><cell></cell></row><row><cell>DCC(ResNet50)</cell><cell>89.9</cell><cell>95.7</cell><cell>98.5</cell><cell>99.0</cell></row><row><cell>CLA(ResNet50-ibn) [6]</cell><cell>88.9</cell><cell>95.7</cell><cell>-</cell><cell>-</cell></row><row><cell>FastReID(ResNet50-ibn) [21]</cell><cell>89.3</cell><cell>95.7</cell><cell>-</cell><cell>-</cell></row><row><cell>DCC(ResNet50-ibn)</cell><cell>90.6</cell><cell>96.1</cell><cell>98.4</cell><cell>99.1</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Comparison of Supervised Person ReID on Market-1501.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table 6. Comparison of Supervised Person ReID on MSMT17.Table 7. Comparison of Supervised Vehicle ReID on VeRi-776.</figDesc><table><row><cell>Methods</cell><cell>mAP</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell></row><row><cell>MGN [43]+CircleLoss</cell><cell>52.1</cell><cell>76.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Circle Loss [40]</cell><cell>52.1</cell><cell>76.9</cell><cell>-</cell><cell>-</cell></row><row><cell>DG-Net [63]</cell><cell>52.3</cell><cell cols="3">77.2 87.4 90.5</cell></row><row><cell>CAR [66]</cell><cell>52.9</cell><cell>78.7</cell><cell>-</cell><cell>-</cell></row><row><cell>CAL [38]</cell><cell>56.2</cell><cell cols="2">79.5 89.0</cell><cell>-</cell></row><row><cell>RGA-SC [56]</cell><cell>57.5</cell><cell>80.3</cell><cell>-</cell><cell>-</cell></row><row><cell>FastReID(ResNet50) [21]</cell><cell>59.9</cell><cell>83.3</cell><cell>-</cell><cell>-</cell></row><row><cell>ABDNet [7]</cell><cell>60.8</cell><cell>82.3</cell><cell>-</cell><cell>-</cell></row><row><cell>TransReID [23] *</cell><cell>67.4</cell><cell>85.3</cell><cell>-</cell><cell></cell></row><row><cell>DCC(ResNet50)</cell><cell>65.5</cell><cell cols="3">85.1 92.1 94.0</cell></row><row><cell>FastReID(ResNet50-ibn) [21]</cell><cell>61.2</cell><cell>84.0</cell><cell>-</cell><cell>-</cell></row><row><cell>FastReID-MGN(ResNet50-ibn) [43]</cell><cell>65.4</cell><cell>85.1</cell><cell>-</cell><cell>-</cell></row><row><cell>DCC(ResNet50-ibn)</cell><cell>69.6</cell><cell cols="3">87.2 93.3 95.0</cell></row><row><cell>Methods</cell><cell>mAP</cell><cell>R1</cell><cell>R5</cell><cell></cell></row><row><cell>PRReID [19]</cell><cell>74.3</cell><cell>94.3</cell><cell>98.7</cell><cell></cell></row><row><cell>UMTS [27]</cell><cell>75.9</cell><cell>95.8</cell><cell></cell><cell></cell></row><row><cell>PGAN [54]</cell><cell>79.3</cell><cell>96.5</cell><cell>98.3</cell><cell></cell></row><row><cell>PVEN [36]</cell><cell>79.5</cell><cell>95.6</cell><cell>98.4</cell><cell></cell></row><row><cell>SAVER [28]</cell><cell>79.6</cell><cell>96.4</cell><cell>98.6</cell><cell></cell></row><row><cell>HPGN [39]</cell><cell cols="2">80.18 96.72</cell><cell>-</cell><cell></cell></row><row><cell>GLAMOR [41]</cell><cell>80.3</cell><cell>96.5</cell><cell>98.6</cell><cell></cell></row><row><cell>GFDIA [30]</cell><cell>81.0</cell><cell>96.7</cell><cell>98.6</cell><cell></cell></row><row><cell>TransReID [23] *</cell><cell>81.4</cell><cell>96.8</cell><cell></cell><cell></cell></row><row><cell>DCC(ResNet50)</cell><cell>82.6</cell><cell>96.3</cell><cell>98.5</cell><cell></cell></row><row><cell>FastReID(ResNet-ibn) [21]</cell><cell>81.9</cell><cell>97.0</cell><cell>99.0</cell><cell></cell></row><row><cell>HRCN(ResNet-ibn) [58]</cell><cell>83.1</cell><cell>97.3</cell><cell>98.9</cell><cell></cell></row><row><cell>DCC(ResNet50-ibn)</cell><cell>83.5</cell><cell>97.6</cell><cell>99.0</cell><cell></cell></row><row><cell>Methods</cell><cell>mAP</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell></row><row><cell>SSG [16]</cell><cell>58.3</cell><cell cols="3">80.0 90.0 92.4</cell></row><row><cell>UGA [45]</cell><cell>70.3</cell><cell>87.2</cell><cell>-</cell><cell>-</cell></row><row><cell>NRMT [51]</cell><cell>71.7</cell><cell cols="3">87.8 94.6 96.5</cell></row><row><cell>JVTC+ [4]</cell><cell>75.4</cell><cell cols="3">90.5 96.2 97.1</cell></row><row><cell>MMT [17]</cell><cell>75.6</cell><cell cols="3">89.3 95.8 97.5</cell></row><row><cell>SPCL [18]</cell><cell>77.5</cell><cell cols="3">89.7 96.1 97.6</cell></row><row><cell>ICE [3]</cell><cell>79.2</cell><cell cols="3">92.0 97.0 98.1</cell></row><row><cell>GLT [61]</cell><cell>79.5</cell><cell cols="3">92.2 96.5 97.8</cell></row><row><cell>FastReID [21]</cell><cell>80.5</cell><cell>92.7</cell><cell>-</cell><cell>-</cell></row><row><cell>CACAL [31]</cell><cell>80.9</cell><cell cols="3">92.7 97.4 98.5</cell></row><row><cell>ClusterContrastive(DBSCAN) [13]</cell><cell>82.1</cell><cell cols="3">92.3 96.7 97.9</cell></row><row><cell>ClusterContrastive(Infomap) [13]</cell><cell>83.0</cell><cell cols="3">92.9 97.2 98.0</cell></row><row><cell>CCL [26]</cell><cell>83.4</cell><cell>94.2</cell><cell>-</cell><cell>-</cell></row><row><cell>DCC(ResNet50)</cell><cell>83.4</cell><cell cols="3">93.5 97.1 98.3</cell></row><row><cell>DCC(ResNet50-ibn)</cell><cell>85.8</cell><cell cols="3">94.3 97.6 98.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Comparison of Unsupervised Person ReID on Market-1501.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Comparison of Unsupervised Person ReID on MSMT17.</figDesc><table><row><cell>Methods</cell><cell>mAP</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell></row><row><cell>MMT [17]</cell><cell>35.3</cell><cell cols="3">74.6 82.6 87.0</cell></row><row><cell>FastReID [21]</cell><cell>27.7</cell><cell>59.5</cell><cell>-</cell><cell>-</cell></row><row><cell>SPCL [18]</cell><cell>38.9</cell><cell cols="3">80.4 86.8 89.6</cell></row><row><cell>RLCC [53]</cell><cell>39.6</cell><cell cols="3">83.4 88.8 90.9</cell></row><row><cell>VACP-DA [59]</cell><cell>40.3</cell><cell cols="2">77.4 84.6</cell><cell>-</cell></row><row><cell>ClusterContrastive(DBSCAN) [13]</cell><cell>40.3</cell><cell cols="3">84.6 89.2 91.6</cell></row><row><cell>ClusterContrastive(Infomap) [13]</cell><cell>40.8</cell><cell cols="3">86.2 90.5 92.8</cell></row><row><cell>DCC(ResNet50)</cell><cell>41.4</cell><cell cols="3">86.4 89.7 91.7</cell></row><row><cell>DCC(ResNet50-ibn)</cell><cell>42.1</cell><cell cols="3">87.9 90.8 92.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Comparison of Unsupervised Vehicle ReID on VeRi-776.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/alibaba/cluster-contrast-reid</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixed highorder attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ice: Inter-instance contrastive encoding for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Lagadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint generative and contrastive learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antitza</forename><surname>Lagadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Occlude them all: Occlusion-aware attention network for occluded person re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Qi&amp;apos;an Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Clusterlevel feature alignment for person re-identification. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: A deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Instanceguided context rendering for cross-domain person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Idm: An intermediate domain module for domain adaptive person re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxing</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11864" to="11874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cluster contrast for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11568</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dispersion based clustering for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenmin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01526</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Partregularized near-duplicate vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fastreid: A pytorch toolbox for general instance re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02631</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-domain learning and identity mining for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="582" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transreid: Transformer-based object reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain adaptive attention learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangru</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="11069" to="11076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR Workshop and Conference Proceedings</title>
		<editor>Francis R. Bach and David M. Blei</editor>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards discriminative representation learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multi-shot knowledge distillation for image-based object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11165" to="11172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The devil is in the details: Self-supervised attention for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirazh</forename><surname>Khorramshahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neehar</forename><surname>Peri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="369" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transformer meets part model: Adaptive part division for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<idno>Oc- tober 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<biblScope unit="page" from="4150" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Selfsupervised geometric features discovery via interpretable attention for vehicle re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="194" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Cluster-guided asymmetric contrastive learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07846</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive transfer network for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Domain adaptive person reidentification via coupling optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="547" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1487" to="1495" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parsing-based view-aware embedding network for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dechao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno>ECCV. 9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Counterfactual attention learning for fine-grained visual categorization and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/2108.08728</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Exploring spatial significance via hybrid pyramidal graph network for vehicle re-identification. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchang</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Looking glamorous: Vehicle re-id in heterogeneous cameras networks with global and local attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Suprem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calton</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02256</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4105" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised graph association for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Second-order non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Bryan Ning Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3760" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hierarchical clustering with hard-batch triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13657" to="13665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multiple expert brainstorming for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Refining pseudo labels with clustering consensus over generations for unsupervised object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Part-guided attention learning for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06023</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Relation-aware global attention for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Relation-aware global attention for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Person reidentification using heterogeneous local graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Heterogeneous relational complement for vehicle reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Viewpoint-aware progressive clustering for unsupervised vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aihua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/2011.09099, 2020. 10</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Group-aware label transfer for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Yihong Gong, and Nanning Zheng. Large margin learning in set-toset similarity comparison for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="593" to="604" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
