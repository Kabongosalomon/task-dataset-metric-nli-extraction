<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PowerNorm: Rethinking Batch Normalization in Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020">2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
						</author>
						<title level="a" type="main">PowerNorm: Rethinking Batch Normalization in Transformers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 37 th International Conference on Machine Learning</title>
						<meeting>the 37 th International Conference on Machine Learning <address><addrLine>Vienna, Austria</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2020">2020</date>
						</imprint>
					</monogr>
					<note>* Equal contribution 1 UC Berkeley. Correspondence to: Amir Gholami &lt;amirgh@berkeley.edu&gt;.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at https://github.com/ sIncerass/powernorm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Normalization has become one of the critical components in Neural Network (NN) architectures for various machine learning tasks, in particular in Computer Vision (CV) and Natural Language Processing (NLP). However, currently there are very different forms of normalization used in CV and NLP. For example, Batch Normalization (BN) <ref type="bibr" target="#b14">(Ioffe &amp; Szegedy, 2015)</ref> is widely adopted in CV, but it leads to significant performance degradation when naively used in NLP. Instead, Layer Normalization (LN) <ref type="bibr" target="#b0">(Ba et al., 2016)</ref> is the standard normalization scheme used in NLP. All recent NLP architectures, including Transformers <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>, have incorporated LN instead of BN as their default normalization scheme. In spite of this, the reasons why BN fails for NLP have not been clarified, and a better alternative to LN has not been presented.</p><p>In this work, we perform a systematic study of the challenges associated with BN for NLP, and based on this we propose Power Normalization (PN), a novel normalization method that significantly outperforms LN. In particular, our contributions are as follows:</p><p>? We find that there are clear differences in the batch statistics of NLP data versus CV data. In particular, we observe that batch statistics for NLP data have a very large variance throughout training. This variance exists in the corresponding gradients as well. In contrast, CV data exhibits orders of magnitude smaller variance. See <ref type="figure">Figure 2</ref> and 3 for a comparison of BN in CV and NLP.</p><p>? To reduce the variation of batch statistics, we modify typical BN by relaxing zero-mean normalization, and we replace the variance with the quadratic mean. We denote this scheme as PN-V. We show theoretically that PN-V preserves the first-order smoothness property as in BN; see Lemma 2.</p><p>? We show that using running statistics for the quadratic mean results in significantly better performance, up to 1.5/2.0 BLEU on IWSLT14/WMT14 and 7.7/3.4 PPL on PTB/WikiText-103, as compared to BN; see <ref type="table">Table 1</ref> and 2. We denote this scheme as PN. Using running statistics requires correcting the typical backpropagation scheme in BN. As an alternative, we propose an arXiv:2003.07845v2 [cs.CL] 28 Jun 2020 approximate backpropagation to capture the running statistics. We show theoretically that this approximate backpropagation leads to bounded gradients, which is a necessary condition for convergence; see Theorem 4.</p><p>? We perform extensive tests showing that PN also improves performance on machine translation and language modeling tasks, as compared to LN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14, and by 5.6/3.0 PPL on PTB/WikiText-103. We emphasize that the improvement of PN over LN is without any change of hyperparameters.</p><p>? We analyze the behaviour of PN and LN by computing the Singular Value Decomposition of the resulting embedding layers, and we show that PN leads to a more well-conditioned embedding layer; see <ref type="figure">Figure 6</ref>. Furthermore, we show that PN is robust to small-batch statistics, and it still achieves higher performance, as opposed to LN; see <ref type="figure" target="#fig_4">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Normalization is widely used in modern deep NNs such as ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref>, MobileNet-V2 <ref type="bibr" target="#b31">(Sandler et al., 2018)</ref>, and DenseNet <ref type="bibr" target="#b11">(Huang et al., 2017)</ref> in CV, as well as LSTMs <ref type="bibr" target="#b9">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b0">Ba et al., 2016)</ref>, transformers <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>, and transformerbased models <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2019)</ref> in NLP. There are two main categories of normalization: weight normalization <ref type="bibr" target="#b30">(Salimans &amp; Kingma, 2016;</ref><ref type="bibr" target="#b23">Miyato et al., 2018;</ref><ref type="bibr" target="#b28">Qiao et al., 2019)</ref> and activation normalization <ref type="bibr" target="#b14">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b15">Jarrett et al., 2009;</ref><ref type="bibr" target="#b16">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b0">Ba et al., 2016;</ref><ref type="bibr" target="#b35">Ulyanov et al., 2016;</ref><ref type="bibr" target="#b40">Wu &amp; He, 2018;</ref><ref type="bibr" target="#b17">Li et al., 2019)</ref>. Here, we solely focus on the latter, and we briefly review related work in CV and NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalization in Computer Vision</head><p>Batch Normalization (BN) <ref type="bibr" target="#b14">(Ioffe &amp; Szegedy, 2015)</ref> has become the de-facto normalization for NNs used in CV. BN normalizes the activations (feature maps) by computing channel-wise mean and variance across the batch dimension, as schematically shown in <ref type="figure" target="#fig_0">Figure 1</ref>. It has been found that BN leads to robustness with respect to sub-optimal hyperparameters (e.g., learning rate) and initialization, and it generally results in more stable training for CV tasks <ref type="bibr" target="#b14">(Ioffe &amp; Szegedy, 2015)</ref>. Following the seminal work of <ref type="bibr" target="#b14">(Ioffe &amp; Szegedy, 2015)</ref>, there have been two principal lines of research: (i) extensions/modifications of BN to improve its performance, and (ii) theoretical/empirical studies to understand why BN helps training.</p><p>With regard to (i), it was found that BN does not perform well for problems that need to be trained with small batches, e.g., image segmentation (often due to memory limits) <ref type="bibr" target="#b45">(Zagoruyko &amp; Komodakis, 2016;</ref><ref type="bibr" target="#b18">Lin et al., 2017;</ref><ref type="bibr" target="#b7">Goldberger et al., 2005)</ref>. The work of <ref type="bibr" target="#b13">(Ioffe, 2017)</ref> proposed batch renormalization to remove/reduce the dependence of batch statistics to batch size. It was shown that this approach leads to improved performance for small batch training as well as cases with non-i.i.d. data. Along this direction, the work of <ref type="bibr" target="#b34">(Singh &amp; Shrivastava, 2019)</ref> proposed "EvalNorm," which uses corrected normalization statistics. Furthermore, the recent work of <ref type="bibr" target="#b42">(Yan et al., 2020)</ref> proposed "Moving Average Batch Normalization (MABN)" for small batch BN by replacing batch statistics with moving averages.</p><p>There has also been work on alternative normalization techniques, and in particular Layer Normalization (LN), proposed by <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>. LN normalizes across the channel/feature dimension as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. This could be extended to Group Norm (GN) <ref type="bibr" target="#b40">(Wu &amp; He, 2018)</ref>, where the normalization is performed across a partition of the features/channels with different pre-defined groups. Instance Normalization (IN) <ref type="bibr" target="#b35">(Ulyanov et al., 2016)</ref> is another technique, where per-channel statistics are computed for each sample.</p><p>With regard to (ii), there have been several studies to understand why BN helps training in CV. The original motivation was that BN reduces the so-called "Internal Covariance Shift" (ICS) <ref type="bibr" target="#b14">(Ioffe &amp; Szegedy, 2015)</ref>. However, this explanation was viewed as incorrect/incomplete <ref type="bibr" target="#b29">(Rahimi, 2017)</ref>.</p><p>In particular, the recent study of <ref type="bibr" target="#b32">(Santurkar et al., 2018)</ref> argued that the underlying reason that BN helps training is that it results in a smoother loss landscape. This was later confirmed for deep NN models by measuring the Hessian spectrum of the network with/without BN <ref type="bibr" target="#b44">(Yao et al., 2019)</ref>.</p><p>Normalization in Natural Language Processing Despite the great success of BN in CV, the large computation and storage overhead of BN at each time-step in recurrent neural networks (RNNs) made it impossible/expensive to deploy for NLP tasks <ref type="bibr" target="#b2">(Cooijmans et al., 2017)</ref>. To address this, the work of <ref type="bibr" target="#b2">(Cooijmans et al., 2017;</ref><ref type="bibr" target="#b10">Hou et al., 2019)</ref> used shared BN statistics across different time steps of RNNs. However, it was found that the performance of BN is significantly lower than LN for NLP. For this reason, LN became the default normalization technique, even for the recent transformer models introduced by <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>.</p><p>Only limited recent attempts were made to compare LN with other alternatives or investigate the reasons behind the success of LN in transformer models. For instance, <ref type="bibr" target="#b46">(Zhang &amp; Sennrich, 2019)</ref> proposes RMSNorm, which removes the re-centering invariance in LN and performs re-scaling invariance with the root mean square summed of the inputs. They showed that this approach achieves similar performance to LN, but with smaller (89% to 64%) overhead. Furthermore, <ref type="bibr" target="#b24">(Nguyen &amp; Salazar, 2019</ref>) studies different variants of weight normalization for transformers in low-resource machine translation. The recent work of <ref type="bibr" target="#b41">(Xu et al., 2019)</ref> studies why LN helps training, and in particular it finds that the derivatives of LN help recenter and rescale backward gradients. From a different angle, <ref type="bibr" target="#b48">(Zhang et al., 2019b;</ref><ref type="bibr">a)</ref> try to ascribe the benefits of LN to solving the exploding and vanishing gradient problem at the beginning of training. They also propose two properly designed initialization schemes which also enjoy that property and are able to stabilize training for transformers.</p><p>However, most of these approaches achieve similar or marginal improvement over LN. More importantly, there is still not a proper understanding of why BN performs poorly for transformers applied to NLP data. Here, we address this by systematically studying the BN behavior throughout training; and, based on our results, we propose Power Normalization (PN), a new normalization method that significantly outperforms LN for a wide range of tasks in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Batch Normalization</head><p>Notation. We denote the input of a normalization layer as X P R B?d , where d is the embedding/feature size and B is the batch size 1 . We denote L as the final loss of the NN. The i-th row (column) of a matrix, e.g., X, is denoted by X i,: (X :,i ). We also write the i-th row of the matrix as its lower-case version, i.e., x i " X i,: . For a vector y, y i denotes the i-th element in y.</p><p>Without other specification: (i) for two vector x P R d and y P R d , we denote xy as the element-wise product, x`y as the element-wise sum, and xx, yy as the inner product;</p><p>Algorithm 1 Batch Normalization (Every Iteration) begin Forward Propagation: (ii) for a vector y P R d and a matrix X P R B?d , we denote ydX as ry 1 X :,1 , ..., y d A :,d s and y`X as ry`X 1,: ; ...; yX B,: s; and (iii) for a vector y P R d , y ? C means that each entry of y is larger than the constant C, i.e., y i ? C for all i.</p><formula xml:id="formula_0">Input: X P R B?d Output: Y P R B?d ? B " 1 B ? B i"1 x i // Get mini-batch mean ? 2 B " 1 B ? B i"1 px i??B q 2 // Get mini-batch variance | X " X?? B ? B // Normalize Y " ? d | X`? // Scale</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation of Batch Normalization</head><p>We briefly review the formulation of BN <ref type="bibr" target="#b14">(Ioffe &amp; Szegedy, 2015)</ref>. Let us denote the mean (variance) of X along the batch dimension as ? B P R d (? 2 B P R d ). The batch dimension is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The BN layer first enforces zero mean and unit variance, and it then performs an affine transformation by scaling the result by ?, ? P R d , as shown in Algorithm 1.</p><p>The Forward Pass (FP) of BN is performed as follows. Let us denote the intermediate result of BN with zero mean and unit variance as | X, i.e.,</p><formula xml:id="formula_1">| X " X??B ?B .<label>(1)</label></formula><p>The final output of BN, Y , is then an affine transformation applied to | X:</p><formula xml:id="formula_2">Y " ? d | X`?.<label>(2)</label></formula><p>The corresponding Backward Pass (BP) can then be derived as follows. Assume that the derivative of L with respect to Y is given, i.e., BL BY is known. Then, the derivative with respect to input can be computed as:</p><formula xml:id="formula_3">BL Bxi " ? ?B BL Byi?? ?BB ? jPB p BL Byj lo omo on from ? B : g?`B L Byjx jxi loooomoooon from ? 2 B : g ? 2 q. (3)</formula><p>See Lemma 6 in Appendix C for details. We denote the terms contributed by ? B and ? 2 B as g ? and g ? 2 , respectively. The average Euclidean distance between the batch statistics (?B, ? 2 B ) and the running statistics (?, ? 2 ) stored in first BN during forward pass for ResNet20 on Cifar-10 and Transformer on IWLST14. We can clearly see that the ResNet20 statistics have orders of magnitude smaller variation than the running statistics throughout training. However, the corresponding statistics in TransformerBN exhibit very high variance with extreme outliers. This is true both for the mean (shown in the left) as well as variance (shown in right). This is one of the contributing factors to the low performance of BN in transformers.</p><p>In summary, there are four batch statistics in BN, two in FP and two in BP. The stability of training is highly dependent on these four parameters. In fact, naively implementing the BN as above for transformers leads to poor performance. For example, using transformer with BN (denoted as Transformer BN ) results in 1.1 and 1.4 lower BLEU score, as compared to the transformer with LN (Transformer LN ), on IWSLT14 and WMT14, respectively; see <ref type="table">Table 1</ref>. This is significant performance degradation, and it stems from instabilities associated with the above four batch statistics. To analyze this, we studied the batch statistics using the standard setting of ResNet20 on Cifar-10 and Transformer BN on IWSLT14 (using a standard batch size of 128 and tokens of 4K, respectively). In the first experiment, we probed the fluctuations between batch statistics, ? B /? B , and the corresponding BN running statistics, ?/?, throughout training. This is shown for the first BN layer of ResNet20 on Cifar-10 and Transformer BN on IWSLT14 in <ref type="figure">Figure 2</ref>. Here, the y-axis shows the average Euclidean distance between batch statistics (? B , ? B ) and the running statistics (?, ?), and the x-axis is different epochs of training, where we define the average Euclidean distance as distp? B , ?q " 1 d }? B?? }. The first observation is that Transformer BN shows significantly larger distances between the batch statistics and the running statistics than ResNet20 on Cifar-10, which exhibits close to zero fluctuations. Importantly, this distance between ? B and ? significantly increases throughout training, but with extreme outliers. During inference, we have to use the running statistics. However, such large fluctuations would lead to a large inconsistency between statistics of the testing data and the BN's running statistics.</p><p>The second observation comes from probing the norm of g ? and g ? 2 defined in Eq. 3, which contribute to the gradient backpropagation of input. These results are shown in <ref type="figure">Figure   3</ref>, where we report the norm of these two parameters for ResNet20 and Transformer BN . For Transformer BN , we can see very large outliers that actually persist throughout training. This is in contrast to ResNet20, for which the outliers vanish as training proceeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Power Normalization</head><p>Based on our empirical observations, we propose Power Normalization (PN), which effectively resolves the performance degradation of BN. This is achieved by incorporating the following two changes to BN. First, instead of enforcing unit variance, we enforce unit quadratic mean for the activations. The reason for this is that we find that enforcing zero-mean and unit variance in BN is detrimental due to the large variations in the mean, as discussed in the previous section. However, we observe that unlike mean/variance, the unit quadratic mean is significantly more stable for transformers. Second, we incorporate running statistics for the quadratic mean of the signal, and we incorporate an approximate backpropagation method to compute the corresponding gradient. We find that the combination of these two changes leads to a significantly more effective normalization, with results that exceed LN, even when the same training hyperparameters are used. Below we discuss each of these two components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Relaxing Zero-Mean and Enforcing Quadratic Mean</head><p>Here, we describe the first modification in PN. As shown in <ref type="figure">Figure 2</ref> and 3, ? B and g ? exhibit significant number of large outliers, which leads to inconsistencies between training and inference statistics. We first address this by relaxing the zero-mean normalization, and we use the quadratic mean of the signal, instead of its variance. The quadratic mean exhibits orders of magnitude smaller fluctuations, as shown CIFAR10 IWSLT14 <ref type="figure">Figure 3</ref>. The average gradient norm of the input of the first BN layer contributed by ?B and ?B for ResNet20 on Cifar10 and TransformerBN on IWSLT14 during the BP (note that d " 16 for Cifar-10 and d " 512 for IWSLT experiment). It can be clearly seen that the norm of g? and g ? 2 for ResNet20 has orders of magnitude smaller variation throughout training, as compared to that for TransformerBN. Also, the outliers for ResNet20 vanish at the end of training, which is in contrast to TransformerBN, for which the outliers persist. This is true both for g? (shown in left) as well as g ? 2 (shown in right).  much smaller variations of running statistics, as compared to ?B, as shown in left, It can also be clearly seen that during BP, the norm of g ? 2 exhibits many fewer outliers, as compared to g ? 2 , throughout the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20%</head><p>in <ref type="figure" target="#fig_3">Figure 4</ref>. We refer to this normalization (i.e., no zero mean and unit quadratic mean enforcement) as PN-V, defined as follows.</p><p>Definition 1 (PN-V). Let us denote the quadratic mean of the batch as ?</p><formula xml:id="formula_4">B 2 " 1 B ? B i"1 x 2 i . Furthermore, denote x X as the signal scaled by ? B , i.e., x X " X ?B .<label>(4)</label></formula><p>Then, the output of PN-V is defined as</p><formula xml:id="formula_5">Y " ? d x X`?,<label>(5)</label></formula><p>where ? P R d and ? P R d are two parameters (vectors) in PN-V (which is the same as in the affine transformation used in BN).</p><p>Note that here we use the same notation Y as the output in Eq. 2 without confusion.</p><p>The corresponding BP of PN-V is as follows:</p><formula xml:id="formula_6">BL Bxi " ? ?B BL Byi?? B?B ? jPB BL Byjx jxi loooomoooon from ? B 2 : g ? 2 .<label>(6)</label></formula><p>See Lemma 8 in Appendix C for the full details. Here, g ? 2 is the gradient attributed by ? B 2 . Note that, compared to BN, there exist only two batch statistics in FP and BP: ? B 2 and g ? 2 . This modification removes the two unstable factors corresponding to ? B and ? B in BN (g ? , and g ? 2 in Eq. 3). This modification also results in significant performance improvement, as reported in <ref type="table">Table 1</ref> for IWSLT14 and WMT14. By directly replacing BN with PN-V (denoted as Transformer PN-V ), the BLEU score increases from 34.4 to 35.4 on IWSLT14, and 28.1 to 28.5 on WMT14. These improvements are significant for these two tasks. For example, <ref type="bibr" target="#b48">(Zhang et al., 2019b;</ref><ref type="bibr">a)</ref> only improves the BLEU score by 0.1 on IWSLT14.</p><p>As mentioned before, ? B exhibits orders of magnitude smaller variations, as compared to ? B . This is shown in <ref type="figure">Fig-</ref>ure 4, where we report the distance between the running statistics for ?, distp? 2 B , ? 2 q, and ?, distp? B 2 , ? 2 q. Similarly during BP, we compute the norm of g ? 2 and g ? 2 , and we report it in <ref type="figure" target="#fig_3">Figure 4</ref> throughout training. It can be clearly seen that during BP, the norm of g ? 2 exhibits many fewer outliers as compared to g ? 2 .</p><p>In <ref type="bibr" target="#b32">(Santurkar et al., 2018)</ref>, the authors provided theoretical results suggesting that employing BN in DNNs can lead to a smaller Lipschitz constant of the loss.</p><p>It can be shown that PN-V also exhibits similar behaviour, under mild assumptions. In more detail, let us denote the loss of the NN without normalization as p L. With mild assumptions, <ref type="bibr" target="#b32">(Santurkar et al., 2018)</ref> shows that the norm of BL BX (with BN) is smaller than the norm of B p L BX . Here, we show that, under the same assumptions, PN-V can achieve the same results that BN does. See Appendix C for details, including the statement of Assumption 9. Lemma 2 (The effect of PN-V on the Lipschitz constant of the loss). Under Assumption 9, we have</p><formula xml:id="formula_7">} BL BX:,i } 2 " ? 2 i p?Bq 2 i?} B p L BX:,i } 2?x B p L BX:,i , x X:,i ? B y 2?. (7) See the proof in Appendix C. Note that x B p L BX:,i , x X:,i ?</formula><p>B y 2 is nonnegative, and hence the Lipschitz constant of L is smaller than that of p L if ? i ? p? B q i . This is what we observe in practice, as shown in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Running Statistics in Training</head><p>Here, we discuss the second modification in PN. First note that even though Transformer PN-V outperforms Transformer BN , it still can not match the performance of LN. This could be related to the larger number of outliers present in ? B , as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. A straightforward solution to address this is to use running statistics for the quadratic mean (denoted as ? 2 ), instead of using per batch statistics, since the latter changes in each iteration. However, using running statistics requires modification of the backpropagation, which we described below. Definition 3 (PN). Denote the inputs/statistics at the t-th iteration by?p tq , e.g., X ptq is the input data at t-th iteration. In the forward propagation, the following equations are used for the calculation:</p><formula xml:id="formula_8">x X ptq " X ptq ? pt?1q ,<label>(8)</label></formula><formula xml:id="formula_9">Y ptq " ? d x X ptq`? ,<label>(9)</label></formula><p>p? ptq q 2 " p? pt?1q q 2`p 1??qp?B 2?p ? pt?1q q 2 q.</p><p>Here, 0 ? ? ? 1 is the moving average coefficient in the forward propagation, and ? B is the statistic for the current batch. Since the forward pass evolves running statistics, Algorithm 2 Power Normalization (Every Iteration) begin Forward Propagation: </p><formula xml:id="formula_11">Input: X P R B?d Output: Y P R B?d ? B 2 " 1 B ? B i"1 x 2 i // Get mini-batch statistics x X " X ? // Normalize Y " ? d x X`? // Scale</formula><formula xml:id="formula_12">P R B?d BL B x X " ? d BL BY // Intermediate Gradient ? X 1 " BL B x X?? x X // Intermediate Estimated Gradient BL BX " ? X 1 ? // Gradient of X ? " ?p1?p1??q?q`p1??q? // See Definition 3 for ? and ? Inference: Y " ? d X ?`?</formula><p>the backward propagation cannot be accurately computednamely, the accurate gradient calculation needs to track back to the first iteration. Here, we propose to use the following approximated gradient in backward propagation:</p><formula xml:id="formula_13">p ? X ptq q 1 " BL B x X ptq?? pt?1q d x X ptq ,<label>(11)</label></formula><formula xml:id="formula_14">BL BX ptq q " p ? X ptq q 1 ? pt?1q ,<label>(12)</label></formula><formula xml:id="formula_15">? ptq " ? pt?1q p1?p1??q? ptq q`p1??q? ptq ,<label>(13)</label></formula><p>where ? ptq "</p><formula xml:id="formula_16">1 B ? B i"1x ptq ix ptq i and ? ptq " 1 B ? B i"1 BL Bx ptq ix ptq i .</formula><p>This backpropagation essentially uses running statistics by computing the gradient of the loss w.r.t. the quadratic mean of the current batch, rather than using the computationally infeasible method of computing directly the gradient w.r.t. running statistics of the quadratic mean. Importantly, this formulation leads to bounded gradients which is necessary for convergence, as shown below.</p><p>Theorem 4 (Gradient of L w.r.t. X is bounded in PN). For any datum point of ? X (i.e. ? X i,: ), the gradients computed from Eq. 11 are bounded by a constant.</p><p>Furthermore, the gradient of X i,: is also bounded, as given Eq. 12.</p><p>See the proof in Appendix C. The pseudo-code for PN algorithm is presented in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Setup</head><p>We compare our PN method with LN and BN for a variety of sequence modeling tasks: Neural Machine Translation (MT); and Language Modeling (LM). We implement our code for MT using fairseq-py , and <ref type="bibr" target="#b20">(Ma et al., 2019)</ref> for LM tasks. For a fair comparison, we directly replace the LN in transformers (Transformer LN ) with BN (Transformer BN ) or PN (Transformer PN ) without varying the position of each normalization layer or changing the training hyperparameters.</p><p>For all the experiments, we use the pre-normalization setting in , where the normalization layer is located right before the multi-head attention module and point-wise feed-forward network module. Following , we generally increase the learning rate by a factor of 2.0, relative to the common post-normalization transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>. Below we discuss tasks specific settings. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Machine Translation</head><p>We evaluate our methods on two widely used public datasets: IWSLT14 Germanto-English (De-En) and WMT14 English-to-German (En-De) dataset. We follow the settings reported in <ref type="bibr" target="#b25">(Ott et al., 2018)</ref>. We use transformer big architecture for WMT14 (4.5M sentence pairs) and small architecture for IWSLT14 (0.16M sentence pairs). For inference, we average the last 10 checkpoints, and we set the length penalty to 0.6/1.0, and the beam size to 4/5 for WMT/IWSLT, following . All the other hyperparamters (learning rate, dropout, weight decay, warmup steps, etc.) are set identically to the ones reported in the literature for LN (i.e., we use the same hyperparameters for BN/PN).</p><p>Language Modeling We experiment on both PTB <ref type="bibr" target="#b22">(Mikolov et al., 2011)</ref> and <ref type="bibr">Wikitext-103 (Merity et al., 2017)</ref>, which contain 0.93M and 100M tokens, respectively. We use three layers tensorized transformer core-1 for PTB and six layers tensorized transformer core-1 for Wikitext-103, following <ref type="bibr" target="#b20">(Ma et al., 2019)</ref>. Furthermore, we apply the multi-linear attention mechanism with masking, and we report the final testing set perplexity (PPL). 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiment Results</head><p>Neural Machine Translation We use BLEU <ref type="bibr" target="#b27">(Papineni et al., 2002)</ref> as the evaluation metric for MT. Following standard practice, we measure tokenized case-sensitive 2 More detailed experimental settings and comparisons between normalization methods are provided in Appendix A, B.3. <ref type="bibr">3</ref> We also report the validation perplexity in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IWSLT14 WMT14 small big</head><p>Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref> 34.4 28.4 DS-Init <ref type="bibr" target="#b47">(Zhang et al., 2019a)</ref> 34.4 29.1 Fixup-Init <ref type="bibr" target="#b48">(Zhang et al., 2019b)</ref> 34.5 29.3 Scaling NMT <ref type="bibr" target="#b25">(Ott et al., 2018)</ref> / 29.3 Dynamic Conv  35.2 29.7 Transformer + LayerDrop <ref type="bibr" target="#b5">(Fan et al., 2020</ref> The results are reported in <ref type="table">Table 1</ref>. In the first section of rows, we report state-of-the-art results for these two tasks with comparable model sizes. In the second section of rows, we report the results with different types of normalization. Notice the significant drop in BLEU score when BN is used (34.4/28.1), as opposed to <ref type="bibr">LN (35.5/29.5)</ref>. Using PN-V instead of BN helps reduce this gap, but LN still outperforms. However, the results corresponding to PN exceeds LN results by more than 0.4/0.6 points, This is significant for these tasks. Comparing with other concurrent works like DS-Init and Fixup-Init <ref type="bibr" target="#b47">(Zhang et al., 2019a;</ref>, the improvements in Transformer PN are still significant.</p><p>Model PTB WikiText-103 Test PPL Test PPL Tied-LSTM <ref type="bibr" target="#b12">(Inan et al., 2017)</ref> 48.7 48.7 AWD-LSTM-MoS <ref type="bibr" target="#b43">(Yang et al., 2018)</ref> 56.0 29.2 Adaptive Input  57.0 20.5 Transformer-XLbase <ref type="bibr" target="#b3">(Dai et al., 2019)</ref> 54.5 24.0 Transformer-XLlarge <ref type="bibr" target="#b3">(Dai et al., 2019)</ref> -18.3 Tensor-Transformer1core <ref type="bibr" target="#b20">(Ma et al., 2019)</ref> 57.9 20.9 Tensor-Transformer2core <ref type="bibr" target="#b20">(Ma et al., 2019)</ref> 49 Language Modeling We report the LM results in <ref type="table">Table 2</ref>, using the tensorized transformer proposed in <ref type="bibr" target="#b20">(Ma et al., 2019)</ref>. Here we observe a similar trend. Using BN results in a significant degradation, increasing testing PPL by more than 7.5/6.3 for PTB/WikiText-103 datasets (achieving 60.7/27.2 as opposed to 53.2/20.9). However, when we incorporate the PN normalization, we achieve state-of-theart results for these two tasks (for these model sizes and without any pre-training on other datasets). In particular, PN results in 5.6/3 points lower testing PPL, as compared to LN. Importantly, note that using PN we achieve better results than <ref type="bibr" target="#b20">(Ma et al., 2019)</ref>, with the same number of parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Effect of Batch Size for Different Normalization</head><p>To understand better the effects of our proposed methods PN and PN-V, we change the batch size used to collect statistics in BN, LN, and PN. To this end, we keep the total batch size constant at 4K tokens, and we vary the mini-batch size used to collect statistics from 512 to 4K. Importantly, note that we keep the total batch size constant at 4K, and we use gradient accumulation for smaller mini-batches. For example, for the case with mini-batch of 512, we use eight gradient accumulations. The results are reported in <ref type="figure" target="#fig_4">Figure  5</ref>. We can observe that BN behaves poorly and abnormally across different mini-batches. Noticeably, after relaxing the zero-mean normalization in BN and replacing the variance estimation with quadratic mean, PN-V matches the performance of LN for 4K mini-batch and consistently outperforms BN. However, it underperforms LN. In contrast, we can see that PN consistently achieves higher results under different mini-batch settings.</p><p>Representation Power of learned Embedding To investigate further the performance gain of PN, we compute the Singular Value Decomposition of the embedding layers, as proposed by , which argued that the singular value distribution could be used as a proxy for measuring representational power of the embedding layer. It has been argued that having fast decaying singular values leads to limiting the representational power of the embeddings to a small sub-space. If this is the case, then it may be preferable to have a more uniform singular value distribution <ref type="bibr" target="#b37">(Wang et al., 2020)</ref>. We compute the singular values for word embedding matrix of LN and PN, and we report the results in <ref type="figure">Figure 6</ref>. It can be clearly observed that the singular values corresponding to PN decay more slowly than those of LN. Intuitively, one explanation for this might be that PN helps by normalizing all the tokens across the batch dimension, which can result in a more equally distributed embeddings. This may illustrate one of the reasons why PN outperforms LN.  <ref type="figure">Figure 6</ref>. Singular values of embedding matrix trained with LN/PN on WMT14. We normalize the singular values of each matrix so that they are comparable with the largest one as 1. Note that the singular values corresponding to PN decay more slowly than those of LN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we systematically analyze the ineffectiveness of vanilla batch normalization (BN) in transformers. Comparing NLP and CV, we show evidence that the batch statistics in transformers on NLP tasks have larger variations. This further leads to the poor performance of BN in transformers. By decoupling the variations into FP and BP computation, we propose PN-V and PN to alleviate the variance issue of BN in NLP. We also show the advantages of PN-V and PN, both theoretically and empirically. We use a vocabulary of 10K tokens based on a joint source and target byte pair encoding (BPE) <ref type="bibr" target="#b33">(Sennrich et al., 2016)</ref>. For the WMT14 dataset, we follow the setup of <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>, which contains 4.5M training parallel sentence pairs. Newstest2014 is used as the test set, and Newstest2013 is used as the validation set. The 37K vocabulary for WMT14 is based on a joint source and target BPE factorization.</p><p>Hyperparameter Given the unstable gradient issues of decoders in NMT <ref type="bibr" target="#b47">(Zhang et al., 2019a)</ref>, we only change all the normalization layers in the 6 encoder layers from LN to BN/PN, and we keep all the 6 decoder layers to use LN. For Transformer PN-V big and Transformer BN big (not Transformer PN big), we use the synchronized version, where each FP and BP will synchronize the mean/variance/quadratic mean of different batches at different nodes. For PN, we set the ? in the forward and backward steps differently, and we tune the best setting over 0.9/0.95/0.99 on the validation set. To control the scale of the activation, we also involve a layer-scale layer <ref type="bibr" target="#b46">(Zhang &amp; Sennrich, 2019)</ref> in each model setting before the normalization layer. The warmup scheme for accumulating ? is also employed, as suggested in <ref type="bibr" target="#b42">(Yan et al., 2020)</ref> . Specifically, we do not tune the warmup steps, but we set it identical to the warmup steps for the learning rate schedule in the optimizer <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>. We set dropout as 0.3/0.0 for Transformer big/small model, respectively. We use the Adam optimizer and follow the optimizer setting and learning rate schedule in . We set the maximum number of updates following <ref type="bibr" target="#b25">(Ott et al., 2018)</ref> to be 300k for WMT and 100k for IWSLT. We used early stopping to stop the experiments by showing no improvement over the last 10/5 epochs. For the big model, we enlarge the batch size and learning rate, as suggested in , to accelerate training. We employ label smoothing of value ls " 0.1 in all experiments. We implement our code for MT using fairseq-py .</p><p>Evaluation We use BLEU 4 <ref type="bibr" target="#b27">(Papineni et al., 2002)</ref> as the evaluation metric for MT. Following standard practice, we measure tokenized case-sensitive BLEU and case-insensitive BLEU for WMT14 En-De and IWSLT14 De-En, respectively. For a fair comparison, we do not include other external datasets. For inference, we average the last 10 checkpoints, and we set the length penalty to 0.6/1.0 and beam size to 4/5 for WMT/IWSLT, following .</p><p>A.2. Language Modeling.</p><p>Dataset PTB <ref type="bibr" target="#b22">(Mikolov et al., 2011)</ref> has 0.93M training tokens, 0.073M validation words, and 0.082M test word. Wikitext-103 <ref type="bibr" target="#b21">(Merity et al., 2017)</ref> contains 0.27M unique tokens, and 100M training tokens from 28K articles, with an average length of 3.6K tokens per article. We use the same evaluation scheme that was provided in <ref type="bibr" target="#b3">(Dai et al., 2019)</ref>.</p><p>Hyperparameter We use three layers tensorized transformer core-1 for PTB and six layers tensorized transformer core-1 for Wikitext-103, following <ref type="bibr" target="#b20">(Ma et al., 2019)</ref>. This means there exists only one linear projection in multi-linear attention. We replace every LN layer with a PN layer. For PN, we set the ? in forward and backward differently, and we tune the best setting over 0.9/0.95/0.99 on the validation set. The warmup scheme and layer-scale are also the same as the hyperparameter setting introduced for machine translation. We set the dropout as 0.3 in all the datasets. The model is trained using 30 epochs for both PTB and WikiText-103. We use the Adam optimizer, and we follow the learning rate setting in <ref type="bibr" target="#b20">(Ma et al., 2019)</ref>. We set the warmup steps to be 4000 and label smoothing to be ls " 0.1 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extra Results</head><p>B.1. Empirical Results for Lemma 2.</p><p>Under Assumption 9, mentioned in Section 4.1 and discussed in Appendix C.1, we show</p><formula xml:id="formula_17">} BL BX :,i } 2 " ? 2 i p? B q 2 i`} B p L BX :,i } 2?x B p L BX :,i , x X :,i ? B y 2?.</formula><p>Given that x B p L BX:,i , x X:,i ? B y 2 is non-negative, the Lipschitz constant of L is smaller than that of p L if ? i ? p? B q i . Here, we report the empirical results to show that ? i ? p? B q i holds for each i P t1, 2, ..., du on IWSLT14; see <ref type="figure" target="#fig_6">Figure 7</ref>. Observe that the Lipschitz constant of L is smaller than that of p L empirically in our setting.</p><p>Layer <ref type="formula" target="#formula_1">1</ref>   As shown in B.3, we present more comparison results for different normalization method including Moving Average Batch Normalizatio (MABN) <ref type="bibr" target="#b42">(Yan et al., 2020)</ref>, Batch Renormalization (BRN) <ref type="bibr" target="#b13">(Ioffe, 2017)</ref>, and Group Normalization (GN) <ref type="bibr" target="#b40">(Wu &amp; He, 2018)</ref>. We can observe that BRN/MABN/PN-V is better than BN but worse than LN, which suggests the small batch-size setting (main focus of <ref type="bibr" target="#b42">(Yan et al., 2020;</ref><ref type="bibr" target="#b13">Ioffe, 2017;</ref><ref type="bibr" target="#b40">Wu &amp; He, 2018)</ref>) may have similar characteristic of the setting in NLP, where there exists large variance across batches. Obviously, GN performs the best among the previous proposed methods given LN can be viewed as the special case of GN (group number as 1). 5 Throughout the comparisons, PN still performs the best in the two tasks, which may validate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Theoretical Results</head><p>In this section, we discuss the theoretical results on BN and PN. We assume ? and ? to be constants for our analysis on BN, PN-V and PN.</p><p>Since the derivative of loss L w.r.t. Y is known as BL BY , trivially, we will have ? d BL B | X " BL BY . Also, it is not hard to get the following fact. </p><p>Replacing BL B x X by ? d BL BY , we can get Eq. 6. In order to show the effect of PN-V on the Lipschitz constant of the loss, we make the following standard assumption, as in <ref type="bibr" target="#b32">(Santurkar et al., 2018)</ref>.</p><p>Assumption 9. Denote the loss of the non-normalized neural network, which has the same architecture as the PN-V normalized neural network, as p L. We assume that</p><formula xml:id="formula_19">BL By i " B p L Bx i ,<label>(20)</label></formula><p>where y i is the i-th row of Y .</p><p>Based on these results, we have the following proof of Lemma 2, which was stated in Section 4.</p><p>Proof of Lemma 2. Since all the computational operator of the derivative is element-wise, here we consider d " 1 for notational simplicity 6 . When d " 1, Lemma 8 can be written as</p><formula xml:id="formula_20">BL Bx i " 1 ? B BL Bx i?1 B? B x BL B x X , x Xyx i .<label>(21)</label></formula><p>Therefore, we have</p><formula xml:id="formula_21">BL BX " 1 ? B BL B x X?1 B? B x BL B x X , x Xy x X.<label>(22)</label></formula><p>Since } x X} 2 "</p><formula xml:id="formula_22">? iPBx i 1 B ? iPBx i " B,<label>(23)</label></formula><p>the following equation can be obtained</p><formula xml:id="formula_23">} BL BX } 2 " 1 ? B 2 } BL B x X?x BL B x X , x X ? B y x X ? B } 2 " 1 ? B 2`} BL B x X } 2?2 x BL B x X , x BL B x X , x X ? B y x X ? B y`}x BL B x X , x X ? B y x X ? B y} 2" 1 ? B 2`} BL B x X } 2?x BL B x X , x X ? B y 2" ? 2 ? B 2`} BL BY } 2?x BL BY , x X ? B y 2" ? 2 ? B 2`} B p L BX } 2?x B p L BX , x X ? B y 2?.</formula><p>(24)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Proof of PN</head><p>In order to prove that after the replacement of BL BpX ptq q with Eq. 12, the gradient of the input is bounded, we need the following assumptions.</p><p>Assumption 10. We assume that</p><formula xml:id="formula_24">}x i } ? C 1 and } BL Bx i } ? C 2 ,<label>(25)</label></formula><p>for all input datum point and all iterations. We also assume that the exponentially decaying average of each element ofx i is bounded away from zero, p1??q t ? j"0 ? t?jx ixi ? C 3 ? 0, @t,</p><p>where we denote ? as the decay factor for the backward pass. In addition, we assume that ? satisfies pC 1 q 2 ? 1 1??</p><p>.</p><p>W.l.o.g., we further assume that every entry of ? ptq is bounded below, i.e.</p><p>C 0 ? ? ptq , @t.</p><p>If we can prove or ? ptq is bounded by some constant C 4 (the official proof is in Lemma 11), then it is obvious to prove the each datum point of ? X 1 is bounded.</p><p>Based on these results, we have the following proof of Theorem 4, which was stated in Section 4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FFigure 1 .</head><label>1</label><figDesc>The illustration of layer normalization (left) and batch/power normalization (right). The entries colored in blue show the components used for calculating the statistics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2. The average Euclidean distance between the batch statistics (?B, ? 2 B ) and the running statistics (?, ? 2 ) stored in first BN during forward pass for ResNet20 on Cifar-10 and Transformer on IWLST14. We can clearly see that the ResNet20 statistics have orders of magnitude smaller variation than the running statistics throughout training. However, the corresponding statistics in TransformerBN exhibit very high variance with extreme outliers. This is true both for the mean (shown in the left) as well as variance (shown in right). This is one of the contributing factors to the low performance of BN in transformers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Results for Transformer on IWSLT14. (Left) The average Euclidean distance between batch statistics (?B, ?B) and the running statistics (?, ?) stored in first BN/PN-V during forward propagation (FP). (Right) The average norm of gradient of the input of the first BN/PN-V contributed by ?B/?B. During FP, ?B has</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Ablation study of the performance of PN, PN-V, LN and BN on IWSLT14 trained using different batch sizes. Note that the performance of PN consistently outperforms LN. In the meanwhile, PN-V can only match the result of LN when minibatch gets to 4K. Among all the settings, BN behaves poorly and abnormally across different mini-batches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>The empirical results of the distribution of ? p? B q P R d in different layers of TransformerPN-V on IWSLT14. Given that ?i ? p?Bqi holds for each i P t1, 2, ..., du, Lemma 2 holds as well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>WikiText-103) by a large margin. We also conduct further analysis of the effect of PN-V/PN/BN/LN under different batch size settings to show the significance of statistical estimations, and we investigate the representation power of learned embeddings matrix by LN/PN to illustrate the effectiveness of PN.A. Training DetailsA.1. Machine Translation.Dataset The training/validation/test sets for the IWSLT14 dataset contain about 153K/7K/7K sentence pairs, respectively.</figDesc><table><row><cell>IWSLT14/WMT14) and language modeling (5.6/3.0 PPL</cell></row><row><cell>on PTB/</cell></row><row><cell>Theoreti-</cell></row><row><cell>cally, PN-V preserves the first-order smoothness property</cell></row><row><cell>as in BN. The approximate backpropagation of PN leads to</cell></row><row><cell>bounded gradients. Empirically, we show that PN outper-</cell></row><row><cell>forms LN in neural machine translation (0.4/0.6 BLEU on</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>B.2. Validation Results on Language Modeling.Table 3. Additional Validation and Test results with state-of-the-art results on PTB and WikiText-103. '-' indicates no reported results in that setting, '?' indicates that the results are from our own implementation. PN achieves 5.6/3.0 points lower testing PPL on PTB and WikiTest-103, respectively, as compared to LN.</figDesc><table><row><cell>Model</cell><cell cols="4">PTB Val PPL Test PPL Val PPL Test PPL WikiText-103</cell></row><row><cell>Tied-LSTM (Inan et al., 2017)</cell><cell>75.7</cell><cell>48.7</cell><cell>-</cell><cell>48.7</cell></row><row><cell>AWD-LSTM-MoS (Yang et al., 2018)</cell><cell>58.1</cell><cell>56.0</cell><cell>29.0</cell><cell>29.2</cell></row><row><cell>Adaptive Input (Baevski &amp; Auli, 2019)</cell><cell>59.1</cell><cell>57.0</cell><cell>19.8</cell><cell>20.5</cell></row><row><cell>Transformer-XLbase (Dai et al., 2019)</cell><cell>56.7</cell><cell>54.5</cell><cell>23.1</cell><cell>24.0</cell></row><row><cell>Transformer-XLlarge (Dai et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>18.3</cell></row><row><cell>Tensor-Transformer1core (Ma et al., 2019)</cell><cell>55.4</cell><cell>57.9</cell><cell>23.6</cell><cell>20.9</cell></row><row><cell>Tensor-Transformer2core (Ma et al., 2019)</cell><cell>54.3</cell><cell>49.8</cell><cell>19.7</cell><cell>18.9</cell></row><row><cell>Tensor-Transformer1core + LN</cell><cell>58.0*</cell><cell>53.2*</cell><cell>22.7*</cell><cell>20.9*</cell></row><row><cell>Tensor-Transformer1core + BN</cell><cell>71.7</cell><cell>60.7</cell><cell>28.4</cell><cell>27.2</cell></row><row><cell>Tensor-Transformer1core + PN-V</cell><cell>59.7</cell><cell>55.3</cell><cell>23.6</cell><cell>21.3</cell></row><row><cell>Tensor-Transformer1core + PN</cell><cell>51.6</cell><cell>47.6</cell><cell>18.3</cell><cell>17.9</cell></row></table><note>B.3. More Comparisons.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 4. (Left) NMT performance (BLEU) on IWSLT14 De-En. (Right) LM performance (Test PPL) on PTB.</figDesc><table><row><cell>Model</cell><cell cols="2">IWSLT14 PTB</cell></row><row><cell>Transformer BN</cell><cell>34.4</cell><cell>60.7</cell></row><row><cell>Transformer BRN</cell><cell>34.7</cell><cell>58.3</cell></row><row><cell>Transformer MABN</cell><cell>34.9</cell><cell>57.2</cell></row><row><cell>Transformer LN</cell><cell>35.5</cell><cell>53.2</cell></row><row><cell>Transformer GN</cell><cell>35.7</cell><cell>51.7</cell></row><row><cell>Transformer PN-V</cell><cell>35.5</cell><cell>55.3</cell></row><row><cell>Transformer PN</cell><cell>35.9</cell><cell>47.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Fact 5. The derivatives of ? B and ? 2 B w.r.t. x i are The derivatives of ? B w.r.t. x i are, With the help of Fact 7, we can prove the following lemma Lemma 8 (Derivative of L w.r.t. x i in PN-V). Based on the Fact 7, it holds that that</figDesc><table><row><cell cols="7">Fact 7. B? B Bx i 2</cell><cell>"</cell><cell cols="2">2 B</cell><cell>x i .</cell><cell>(17)</cell></row><row><cell cols="2">BL Bx i</cell><cell cols="2">"</cell><cell cols="2">1 ? B</cell><cell cols="4">BL Bx i?1 B? B</cell><cell>? jPB</cell><cell>BL Bx jx</cell><cell>jxi .</cell><cell>(18)</cell></row><row><cell>Proof. Based on chain rule, we will have</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BL Bx i</cell><cell cols="2">"</cell><cell cols="2">BL Bx i</cell><cell cols="3">Bx i Bx i`? jPB</cell><cell cols="2">BL Bx j</cell><cell>Bx j B? B</cell><cell>2</cell><cell>2 Bx i B? B</cell></row><row><cell></cell><cell cols="2">"</cell><cell cols="2">BL Bx i</cell><cell cols="3">Bx i Bx i`? jPB</cell><cell cols="2">BL Bx j</cell><cell>p?1 2</cell><cell>x j ? B 3 q</cell><cell>2x i B</cell></row><row><cell></cell><cell cols="2">"</cell><cell cols="2">1 ? B</cell><cell cols="4">BL Bx i?1 B? B</cell><cell>? jPB</cell><cell>BL Bx jx</cell><cell>jxi .</cell></row><row><cell cols="2">B? B Bx i</cell><cell></cell><cell>"</cell><cell>1 B</cell><cell></cell><cell>and</cell><cell cols="2">B? 2 Bx i</cell><cell>"</cell><cell>2 B</cell><cell>px i??B q.</cell><cell>(14)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For NLP tasks, we flatten sentences/word in one dimension, i.e., the batch size actually corresponds to all non-padded words in a training batch.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We empirically found that setting group number the same as head number leads to the best performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For d ? 2, we just need to separate the entry and prove them individually.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by funds from Intel and Samsung. We are grateful to support from Google Cloud, Google TFTC team, as well as support from the Amazon AWS. We would like to acknowledge ARO, DARPA, NSF, and ONR for providing partial support of this work. We are also grateful to Zhuohan Li, Zhen Dong, Yang Liu, the members of Berkeley NLP, and the members of the Berkeley RISE Lab for their valuable feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are now ready to show the derivative of L w.r.t. x i under BN.</p><p>Lemma 6 (Derivative of L w.r.t. x i in BN). Based on the Fact 5, it holds that</p><p>Proof. Based on chain rule, we will have</p><p>Replacing BL B | X by ? d BL BY , we can get Eq. 3. In the following, we will first discuss the theoretical properties of PN-V in Appendix C.1; and then we discuss how to use running statistics in the forward propagation and how to modify the corresponding backward propagation in Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Proof of PN-V</head><p>Before showing the gradient of L w.r.t. x i under PN-V, we note the following fact, which is not hard to establish.</p><p>Proof of Theorem 4. It is easy to see that</p><p>All these inequalities come from Cauchy-Schwarz inequity and the fact that</p><p>In the final step of Theorem 4, we directly use that ? ptq is uniformly bounded (each element of ? ptq is bounded) by C 4 . The exact proof is shown in below. Lemma 11. Under Assumption 10, ? ptq is uniformly bounded.</p><p>Proof. For simplicity, denote BL Bxi asx 1 i . It is not hard to see,</p><p>Similarly, we will have }? ptq } ? C 1 C 2 as well as p1??q</p><p>p1?p1??q? pt?k`1q q?? pt?jq y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PowerNorm: Rethinking Batch Normalization in Transformers</head><p>Notice that with the definition,</p><p>we will have that all entries of ? pmq are positive, for m P t0, 1, ..., tu. It is clear that when all entries of ? pmq , for m P t0, 1, ..., tu, have the same sign (positive or negative), the above equation achieves its upper bound. W.l.o.g., we assume they are all positive.</p><p>Since 0 ? ? ? 1, it is easy to see that, when K " rplogp p1??qC3 2C1C1 q{ logp?qqs, then the following inequality holds,</p><p>Since }? pkq } ? C 1 , the value of any entry of ? pkq is also bounded by C 1 . Therefore, based on this and Eq. 30, when t ? K, we will have</p><p>where 1 is the unit vector. Then, for t ? K, we can bound from below the arithmetic average of the K corresponding items of ?,</p><p>This inequality shows that after the first K items, for any K consecutive ? pkq , the average of them will exceeds a constant number, C 5 . Therefore, for any t ? T ? K, we will have</p><p>Let us split ? t j"0`? j?1 k"0 p1?p1??q? pt?k`1q q?? pt?jq into two parts: (i) ? t j"K`? j?1 k"0 p1?p1??q? pt?k`1q q?? pt?jq , and (ii) ? K?1 j"0`? j?1 k"0 p1?p1??q? pt?k`1q q?? pt?jq . From so on, we will discuss how we deal with these two parts respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PowerNorm: Rethinking Batch Normalization in Transformers</head><p>Case 1: ? t j"K`? j?1 k"0 p1?p1??q? pt?k`1q q?? pt?jq Notice that for 0 ? a j ? 1, the following inequality can be proven with simply induction,</p><p>Replacing a j with p1??q? pt?j`1q , we will have</p><p>Here the second inequality comes from Eq. 33, and the third inequality comes form the the fact each entry of ? pmq is smaller than C 1 C 2 , given }? pmq } ? C 1 C 2 . The final inequality comes from Eq. 31, where 0 ? C 5 ? pC 1 q 2 ? 1{p1??q, then we can have 0 ?`1?p1??qC 5 {2?? 1.</p><p>Case 2: ? K?1 j"0`? j?1 k"0 p1?p1??q? pt?k`1q q?? pt?jq It is easy to see </p><p>Combining Case 1 and 2, we have 1 p1??q 2 }? ptq } 2 " x t ? j"0`j?1 ? k"0 p1?p1??q? pt?k`1q q?? pt?jq , t ? j"0`j?1 ? k"0 p1?p1??q? pt?k`1q q?? pt?jq y ? xC 6 1`KC 1 C 2 1, C 6 1`KC 1 C 2 1y ? C 7 ,</p><p>which indicates }? ptq } is bounded and C 4 " p1??q ? C 7 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation degeneration problem in training natural language generation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Normalization helps training of quantized lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Positional normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A tensorized transformer for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Empirical evaluation and combination of advanced language modeling techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An? Cernock?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05895</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL: Demonstrations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10520</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">A. Weight standardization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Nuerips 2017 test-of-time award presentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How does batch normalization help optimization? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimating batch normalization statistics for evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evalnorm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving neural language generation with spectrum control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Understanding and improving layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards stabilizing batch statistics in backward propagation of batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank rnn language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Py-Hessian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07145</idno>
		<title level="m">Neural networks through the lens of the Hessian</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving deep transformer with depth-scaled initialization and merged attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Residual learning without normalization via better initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
