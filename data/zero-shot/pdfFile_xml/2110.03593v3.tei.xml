<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRANSALNET: TOWARDS PERCEPTUALLY RELEVANT VISUAL SALIENCY PREDICTION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">Cardiff University United Kingdom</orgName>
								<address>
									<postCode>CF24 4AX</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhe</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Subsea Centre Robert Gordon University United Kingdom</orgName>
								<address>
									<postCode>AB10 7AQ</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marshall</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Informatics Cardiff University United Kingdom</orgName>
								<address>
									<postCode>CF24 4AX</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Konstanz Germany</orgName>
								<address>
									<postCode>78464</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Liu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science and Informatics Cardiff University United Kingdom</orgName>
								<address>
									<postCode>CF24 4AX</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TRANSALNET: TOWARDS PERCEPTUALLY RELEVANT VISUAL SALIENCY PREDICTION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Saliency prediction ? Deep Learning ? Transformer ? Convolutional neural network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual saliency prediction using transformers 1 -Convolutional neural networks (CNNs) have significantly advanced computational modelling for saliency prediction. However, accurately simulating the mechanisms of visual attention in the human cortex remains an academic challenge. It is critical to integrate properties of human vision into the design of CNN architectures, leading to perceptually more relevant saliency prediction. Due to the inherent inductive biases of CNN architectures, there is a lack of sufficient long-range contextual encoding capacity. This hinders CNN-based saliency models from capturing properties that emulate viewing behaviour of humans. Transformers have shown great potential in encoding long-range information by leveraging the self-attention mechanism. In this paper, we propose a novel saliency model that integrates transformer components to CNNs to capture the long-range contextual visual information. Experimental results show that the transformers provide added value to saliency prediction, enhancing its perceptual relevance in the performance. Our proposed saliency model using transformers has achieved superior results on public benchmarks and competitions for saliency prediction models. The source code of our proposed saliency model TranSalNet is available at: https://github.com/ LJOVO/TranSalNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual attention represents an important mechanism of the human visual system (HVS), which allows humans to select and interpret the most relevant information in the visual scene <ref type="bibr" target="#b0">[1]</ref>. Simulating visual attention in the form of an <ref type="figure">Figure 1</ref>: Examples of visual saliency prediction. The first row shows the images that stimulate the human eye to view freely. The so-called "Ground Truth" in the second line refers to the fixation density maps, also called saliency maps, generated from the human fixation location. The third and fourth rows show the prediction results of the traditional (GBVS) and deep learning-based (SAM-ResNet) saliency models, respectively. Image (a) and (b) are from MIT1003 dataset; (c) and (d) are from SALICON dataset. It can be seen that both traditional and deep learning-based models are capable of capturing human viewing behaviour, but the deep learning-based model provides better results in demanding scenes, such as (b) and (d) to a considerable extent. algorithm is regarded as visual saliency prediction. Being able to automatically predict saliency is beneficial to many research fields including computer vision, robotics, healthcare, and multimedia <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Existing saliency prediction models can be categorized into two types, traditional and deep learning-based models. Traditional models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> apply low-level visual features such as colour, luminance, texture, and contrast, to simulate the visually salient areas in the scene. These models remain rather limited as higher-level features such as objects are often omitted; but these features exhibit significant determinants of visual saliency <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Although some traditional models <ref type="bibr" target="#b14">[15]</ref> have been extended with specific higher-level features, e.g., faces and texts, there are still obstacles in combining low-level and higher-level visual features. Rather than designing handcrafted features, deep learning-based saliency models automatically discover representations from images <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. These methods typically use convolutional neural networks (CNNs) to construct feature encoders and decoders to generate visual saliency maps. Deep learning-based visual saliency models have achieved remarkable success, mainly due to the availability of well-established deep CNNs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> and large-scale datasets relevant to human visual attention <ref type="bibr" target="#b28">[29]</ref>. <ref type="figure">Figure 1</ref> illustrates examples of visual saliency prediction using both traditional and deep learning-based models, and the correspondences between the ground truth (i.e., where humans look in an image) and prediction (i.e., output of a computational saliency model).</p><p>Accurately predicting saliency as perceived by humans remains an academic challenge. One way to improve the reliability of saliency prediction is to incorporate the properties of the HVS in the construction of computational models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref>. Despite the significant progress made by the deep learning-based models, each convolution kernel in CNNs only receives information from a local subset of pixels in an image, which makes fully CNN-based models deficient in obtaining long-range contextual information. When humans view an image, foveal vision provides the highest resolution visual information, but in the meantime peripheral vision still provides the HVS with non-detailed but long-range visual information <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. In other words, the HVS uses the long-range information of an image to modulate the local maxima of saliency in the visual field <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>. Therefore, the ground truth saliency map represents the perceptual spatial interactions of local and non-local (i.e., long-range) information. This HVS property could be beneficial for predicting visual saliency in a perceptually more relevant manner so that the machine generated saliency map can faithfully reflect human perception. Previous studies mainly attempted to solve this problem through two approaches. One approach is capturing multi-scale information through the CNNs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17]</ref>, which introduces image representations with different granularities to some extent. This approach may not provide the optimal solution as it still lacks the ability to model the way visual information is perceived by the HVS, e.g., some studies have used multi-scale images or multi-scale representations to improve saliency prediction <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17]</ref>, but challenges remains for models in optimally fusing multi-scale information to mimic the functionality of the HVS. Another approach is adding long-range modelling capabilities to network structures to increase spatial representations. By using a Long-Short Term Memory (LSTM)-based architecture <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37]</ref>, this approach has proven effective in handling local and long-range visual information thus refining the accuracy of saliency prediction. Although these studies have demonstrated promising outcomes, much work is needed to close the gap between saliency prediction and human perception. The transformer <ref type="bibr" target="#b37">[38]</ref>, which consists of a self-attention mechanism, provides an elegant solution to process long-range information. By effectively modelling long-range dependency, the transformer has proven efficacy in the field of natural language processing <ref type="bibr" target="#b38">[39]</ref> and more recently achieved promising results in computer vision tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. However, the use of transformers in visual saliency prediction has not been fully explored until now.</p><p>To address the above-mentioned challenges and to build a human-like saliency model, we propose a novel saliency prediction model called TranSalNet, which integrates transformers into a CNN-based architecture. Transformer encoders can learn spatially long-range dependencies by using a self-attentive mechanism, resulting in a perceptually more relevant saliency representation. To the best of our knowledge, this is the first study to explore the combination of CNNs and transformers to enhance saliency prediction. Also, we demonstrate the benefits of transformer components in saliency prediction. Our model achieves superior performance not only on the MIT300 benchmark (the most widely recognised dataset for saliency benchmark) but also on the SALICON Saliency Prediction Challenge (the largest dataset available for saliency prediction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>We contribute towards a perceptually more relevant saliency prediction method using deep learning models with transformers. This section provides a comprehensive review on deep learning-based saliency prediction models , methods for evaluating saliency models (especially evaluating the perceptual relevance of saliency prediction), transformer applications in vision tasks, and multi-scale and long-range information in visual saliency prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep learning-based visual saliency prediction</head><p>A number of deep learning-based visual saliency prediction models have been proposed in recent years. The Ensembles of Deep Networks (eDN) <ref type="bibr" target="#b41">[42]</ref> represents one of the first models that adopts shallow CNNs to detect the visual saliency of natural images. The saliency features are extracted by CNNs and combined by a linear classifier to create saliency maps. Since then, with the application of deep neural networks and large-scale saliency datasets, deep learning-based saliency prediction has achieved further remarkable successes. DeepGaze and DeepGaze II <ref type="bibr" target="#b22">[23]</ref>, which are based on AlexNet <ref type="bibr" target="#b24">[25]</ref> and VGGNet <ref type="bibr" target="#b25">[26]</ref>, respectively, successfully build pre-trained networks as feature extractors to train deeper networks for saliency prediction. By comparing VGGNet, AlexNet, and GoogleNet <ref type="bibr" target="#b42">[43]</ref>, Huang et al. <ref type="bibr" target="#b34">[35]</ref> found that VGGNet detects saliency more effectively than the other two models. Many visual saliency prediction models based on VGGNet have since been proposed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>. EML-NET <ref type="bibr" target="#b19">[20]</ref> focuses on exploring the use of more sophisticated feature extractors (i.e., a parallel two-stream CNN-based encoder) to enhance the performance of saliency prediction. By comparing ResNet-50 with DenseNet <ref type="bibr" target="#b27">[28]</ref> and NASNet <ref type="bibr" target="#b43">[44]</ref>, it is argued that in the field of saliency prediction, the widely used ResNet-50 could still be "shallow" for the large-scale saliency datasets, such as SALICON. Similarly, DeepGaze II-E discusses the contribution of different backbones to saliency prediction. It is found that appropriately concatenating multiple backbones pre-trained on ImageNet <ref type="bibr" target="#b44">[45]</ref> is effective in improving the performance of saliency models.</p><p>In addition to the efforts mentioned above, there are several studies that adopt multi-scale or long-range information to improve visual saliency prediction. We discuss this issue below in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation methods for saliency models</head><p>A number of metrics have been proposed to measure the agreement between the predicted saliency map and the ground truth produced by human eye movements. By investigating commonly used metrics, Bylinskii et. al. <ref type="bibr" target="#b45">[46]</ref> found that under general assumptions the Linear Correlation Coefficient (CC) and Normalized Scanpath Saliency (NSS) metrics could be used as representative metrics for benchmarking saliency models. More importantly, they also suggested different evaluation metrics should be used for different applications, for example, metrics that are more appropriate for evaluating the capability of salient object detection may not be necessarily useful for the evaluation of saliency prediction of other vision applications <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. Li et. al. <ref type="bibr" target="#b47">[48]</ref> found that only a limited number of evaluation metrics, i.e., NSS, CC, and Similarity (SIM) are in close agreement with human judgements through a large-scale subjective experiment. Similarly, Yang et. al. <ref type="bibr" target="#b46">[47]</ref> found that CC and SIM are the most in line with human evaluation of saliency maps. Kummerer et. al. <ref type="bibr" target="#b48">[49]</ref> also demonstrated that it is difficult for a saliency model to perform equally well on all popular saliency evaluation metrics. They proposed a novel approach that allows a saliency model to generate different "saliency maps" according to the characteristics and behaviours of different metrics; and the model that adopts this evaluation method is referred to as a "probabilistic model." As a distinction, without targeting any specific evaluation metric, a saliency model that generates a single saliency map for a given image is referred to as a "classical model". Since our aim is to generate a single saliency map for each image that can faithfully reflect human perception, we evaluate models in the "classical model" framework. In evaluating models, we apply all commonly used evaluation metrics to quantify model performance, but make a clear distinction between "perception-based metrics (i.e., NSS, CC, and SIM)" and "non-perception-based metrics", as defined in <ref type="bibr" target="#b45">[46]</ref>. By doing so, the perceptual relevance of the predicted saliency maps can be appropriately measured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformer in visual tasks</head><p>The transformer was first introduced to the tasks of natural language processing (NLP) <ref type="bibr" target="#b37">[38]</ref>. Because of its powerful long-range dependency modelling capabilities, the transformer has achieved remarkable success in the field of NLP. Consequently, a number of studies in the field of computer vision are also exploring the effectiveness of the use of transformer.</p><p>The vision transformer is one of the first pure transformer architectures for image processing, which uses a vanilla version of the transformer to form a network that achieves performance comparable to that of state-of-the-art CNN-based models. After this work, several models, such as DeepViT <ref type="bibr" target="#b49">[50]</ref> and Swin Transformer <ref type="bibr" target="#b50">[51]</ref>, have achieved further success in visual tasks by using the transformer.</p><p>Currently, the transformer has also demonstrated excellent performance in the field of salient object detection <ref type="bibr" target="#b51">[52]</ref>, which is related to the current work, even though it is a substantially different task <ref type="bibr" target="#b52">[53]</ref>. Salient object detection aims to segment salient objects from an image and generate a binary map <ref type="bibr" target="#b53">[54]</ref>. However, in visual saliency prediction, the aim is to predict the density map of human fixations (i.e., the spatial deployment of visual attention).</p><p>In summary, the previous studies have shown the powerful representation capabilities of the transformer, particularly for capturing long-range information, which could have potential contributions to predicting gaze. However, the use of transformers in visual saliency prediction has not been fully explored until now. In this paper, we will investigate the benefits as well as application of transformer components in saliency prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-scale and long-range information in visual saliency prediction</head><p>By using multi-scale image representations to simulate different perceptual scales, successful results have been achieved in vision tasks such as image segmentation <ref type="bibr" target="#b54">[55]</ref>, human pose estimation <ref type="bibr" target="#b55">[56]</ref>, and salient object detection <ref type="bibr" target="#b56">[57]</ref>. In the filed of visual saliency prediction, Huang et al. <ref type="bibr" target="#b34">[35]</ref> and Fan et al. <ref type="bibr" target="#b21">[22]</ref> proposed CNN-based models that extract multi-scale features from images of different resolutions separately and concatenate the results to obtain salient semantic objects with different granularities hence to optimise saliency prediction. In order to obtain multi-scale contextual information, Deep Visual Attention (DVA) <ref type="bibr" target="#b16">[17]</ref> constructs three decoders of different granularities to generate multi-scale saliency estimates for saliency prediction. EML-NET <ref type="bibr" target="#b19">[20]</ref> also uses multi-scale feature maps from encoder networks to obtain holistic scene features for saliency prediction. MSI-Net <ref type="bibr" target="#b18">[19]</ref> adopts convolutional layers with different dilation rates to augment multi-scale information for saliency prediction. GazeGAN <ref type="bibr" target="#b35">[36]</ref> is a generative adversarial network for saliency prediction, which uses a modified U-Net with multi-scale information by using skip-connections to construct its generator. UNISAL <ref type="bibr" target="#b20">[21]</ref> adopts skip-connections to provide the decoder network with multi-scale features. These studies have demonstrated that multi-scale information is beneficial to visual saliency prediction.</p><p>Similarly to other vision tasks <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b57">58]</ref>, visual saliency prediction has also benefitted from neural networks with longrange modelling capabilities to simulate the spatial attentional mechanisms. DSCLSTM <ref type="bibr" target="#b36">[37]</ref> extracts local feature maps by using CNNs first, and then incorporates non-local scene contexts into the local feature maps by using LSTM-based components to predict human eye fixation points in natural scenes. Cornia et al. <ref type="bibr" target="#b17">[18]</ref> developed visual saliency models that integrate an LSTM module into the CNN-based network to simulate explicit properties of the human attention mechanism. Similarly, Fang et al. <ref type="bibr" target="#b58">[59]</ref> used LSTM to obtain pseudo sequential information to simulate the human visual attention shift. These studies suggest that modelling the relevant dependence between spatial information can refine the saliency prediction models.  , and w 32 ? h 32 , respectively. Then the contextual information of these feature maps is enhanced by transformer encoders. The predicted saliency map is generated by the CNN decoder, which uses skip-connection (orange arrows) and element-wise production to fuse multi-scale context-enhance feature maps. The illustration of the transformer encoder is shown below the architecture diagram, which consists of standard Multi-head Self-Attention (MSA) and Multi-layer Perceptron (MLP) blocks.</p><p>In this paper, we combine these two strategies. More specifically, we integrate transformer encoders into a CNN-based architecture to provide multi-scale image representations with enhanced long-range contextual information, resulting in perceptually more relevant visual saliency prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The proposed model</head><p>The schematic overview of our proposed TranSalNet model is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Firstly, a given image is fed into a CNN encoder. In order to obtain multi-scale image representations, three sets of feature maps with different spatial sizes are extracted from the CNN encoder. Due to the inherent inductive biases of CNN encoder architectures, the extracted image representations lack long-range contextual information, which potentially makes a saliency model less humanlike (note the human visual system is proficient in capturing both local and long-range visual information). Therefore, to obtain perceptually more relevant visual saliency prediction, these feature maps are passed through three transformer encoders, yielding long-range context-enhanced feature maps. Then the CNN decoder fuses these feature maps for saliency prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The CNN encoder</head><p>Previous research has shown that the use of CNN-based networks to extract features for saliency prediction is effective. Likewise, we used a CNN encoder as the feature extractor in this study.</p><p>The CNN models used in this study were initially constructed for image classification. In order to provide image feature maps to the downstream networks, the fully connected layer at the end of these CNNs is removed to form a viable CNN encoder. We extract feature maps with three sets of different spatial sizes from the CNN encoder. Given an input image with size w ? h ? 3, the spatial dimensions of the extracted feature maps are w 8 ? h 8 , w 16 ? h 16 , and w 32 ? h 32 , respectively. In this study, two feature extraction networks are adopted to construct two versions of TranSalNet models. One version uses ResNet-50 <ref type="bibr" target="#b26">[27]</ref> as an encoder, which is a feature extraction network widely used in saliency prediction. This version of the model is referred to as TranSalNet_Res. The CNN body of ResNet-50 is composed of five convolutional blocks that are denoted as conv1 and conv2_x to conv5_x. We extract feature maps from the deeper conv3_x, conv4_x, and conv5_x blocks. However, <ref type="bibr" target="#b19">[20]</ref> suggests that ResNet-50 itself as an encoder is probably relatively "shallow." Therefore, we use DenseNet-161 <ref type="bibr" target="#b27">[28]</ref>, which has higher performance on the ImageNet benchmark, as the CNN encoder to build another version referred to as TranSalNet_Dense. For DenseNet-161, it mainly consists of four "Dense Blocks" denoted as DenseBlock 1 to 4. We extract feature maps from the deeper DenseBlock 2, DenseBlock 3, and DenseBlock 4.</p><p>Although previous work <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref> showed that adopting multi-scale feature maps is beneficial to saliency prediction, our experiments found that using feature maps from shallower network blocks, i.e. the conv1 and conv2_x, may cause undesired artefacts to appear in the saliency maps. Therefore, we exclude feature maps from the shallower network blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The transformer encoder</head><p>The three sets of multi-scale feature maps are respectively fed into three transformer encoders to enhance the long-range and contextual information. The details of transformer are depicted at the bottom of <ref type="figure" target="#fig_0">Figure 2</ref>. Let x 1 , x 2 , and x 3 be the feature maps that have spatial dimensions of w 32 ? h 32 , w 16 ? h 16 , and w 8 ? h 8 , respectively, first, a 1 ? 1 convolution layer (Conv 1?1 ) is used to reduce the computational cost and align with the acceptable input size of the transformer encoder. More specifically, both x 1 and x 2 are reduced to 768 dimensions, and x 3 changed to 512 dimensions. Following this, as there is no relative or absolute position information in the feature maps, it is necessary to utilise position embedding (POS) to enable position-awareness before feeding the input into the transformer encoders. Therefore, the absolute POS <ref type="bibr" target="#b39">[40]</ref> is implemented before feeding input into the transformer encoders, which performs an element-wise addition to the input and a learnable matrix with the same shape as the input. Each transformer encoder contains two same layers of standard Multi-head Self-Attention (MSA) and Multi-layer Perceptron (MLP) blocks <ref type="bibr" target="#b39">[40]</ref>. In our model, we apply 12-heads attention in transformer encoder 1 and 2, and 8-heads in encoder 3. The MLP block contains two layers with a GELU activation function. Besides, Layer Normalization (LN) and residual connection are applied before and after each block respectively. The processing in each transformer encoder can be represented as:</p><formula xml:id="formula_0">z 0 = Conv 1?1 (x i ) ? POS i , i = 1, 2, 3<label>(1)</label></formula><formula xml:id="formula_1">z l = MSA(LN(z l?1 )) ? z l?1 , l = 1, 2<label>(2)</label></formula><formula xml:id="formula_2">z l = MLP(LN(z l )) ? z l , l = 1, 2<label>(3)</label></formula><p>where z l is the output feature maps of the l-th layer in transformer encoder, and x i is the input feature maps from the CNN encoder. The feature maps that are passed through transformer encoder 1, 2, and 3 are context-enhanced and denoted as x c 1 , x c 2 , and x c 3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The CNN decoder</head><p>A CNN decoder is used to fuse the long-range context-enhanced feature maps from the transformer encoders and restore the original image resolution. The CNN decoder is a fully CNN network containing block_1 to block_7, which is used to implement pixel-level classification to predict saliency maps. Batch normalization (BN) and the activation function (ReLU for block_1 to block_6; Sigmoid for block_7) are applied after each 3 ? 3 convolution operation (Conv 3?3 ), where the former is used to promote the convergence and the latter is used to increase the nonlinear factor of the model. Since the input image is 32-scale downsampled by the encoder network, a 2-scale upsampling that adopts nearest-neighbor interpolation is performed to the feature map in block_1 to block_5 to obtain a saliency map of the same size as the input. In order to enhance the long-range and multi-scale context of the feature map during the decoding process, the upsampled feature map and the transformer's output from the corresponding skip-connection are fused by an element-wise product operation. The processes from block_1 to block_6 can be expressed as:</p><formula xml:id="formula_3">x f i = ? ? ? ? ? x c i , i = 1 (4) ReLU(Upsample(x f i?1 ) x c i ), i = 2, 3<label>(5)</label></formula><p>Upsample(x f i?1 ), i = 4, 5, 6 (6)</p><formula xml:id="formula_4">x f i = ReLU(BN(Conv 3?3 (x f i ))), i = 1, 2, . . . , 6<label>(7)</label></formula><p>where x f i andx f i are the input and output features of the i-th block. The output block, i.e., block_7, is used to reduce the dimensionality of the feature maps to a 2D map for pixel-level classification. Therefore, the sigmoid activation function is applied to the feature map:</p><formula xml:id="formula_5">? = sigmoid(Conv 3?3 (x f 6 )),<label>(8)</label></formula><p>where? is the predicted saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss function</head><p>Recent saliency prediction studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref> have shown that taking advantage of the saliency evaluation metrics to define the loss function can significantly improve the performance of saliency prediction models.</p><p>Following a similar idea, we adopt a linear combination of four metrics as the loss function to train our model, including the Normalized Scanpath Saliency (NSS), Kullback-Leibler divergence (KLD), Linear Correlation Coefficient (CC), and Similarity (SIM). Let y s , y f , and? be the ground truth saliency map, fixation map, and predicted saliency map, and i indicates the ith pixel of y s and?, our loss function is defined as:</p><formula xml:id="formula_6">L(y s , y f ,?) =? 1 L NSS (y f ,?) + ? 2 L KLD (y s ,?) + ? 3 L CC (y s ,?) + ? 4 L SIM (y s ,?),<label>(9)</label></formula><p>where ? 1 , ? 2 , ? 3 , and ? 4 are the weights of each metric, and</p><formula xml:id="formula_7">L NSS (y f ,?) = 1 i y f i i? i ? ?(?) ?(?) y f i ,<label>(10)</label></formula><p>where ?(?) and ?(?) stand for standard deviation and mean respectively;</p><formula xml:id="formula_8">L KLD (y s ,?) = i y s i log( + y s i +? i ),<label>(11)</label></formula><p>where is a regularization constant and set to 2.2204 ? 10 ?16 ;</p><formula xml:id="formula_9">L CC (y s ,?) = cov(y s ,?) ?(y s )?(?) ,<label>(12)</label></formula><p>where cov(?) is the covariance and ?(?) is standard deviation;</p><formula xml:id="formula_10">L SIM (y s ,?) = i min(y s i ,? i ).<label>(13)</label></formula><p>In L KLD , L CC and L SIM , y s , and? are normalized so that i y s i = i?i = 1. Since the higher NSS, SIM, and CC values and the lower KLD value represent the better agreement between predicted saliency maps and ground truth, we set ? 1 , ? 3 , and ? 4 to negative and ? 2 to positive. In order to balance the impact of different sub-loss functions on the module result, we determine the weights of individual sub-loss functions based on TranSalNet's performance on the SALICON validation set. In our experiments, the weights are adjusted to ensure these sub-loss functions (note the ranges of output values are different for these functions) contribute relatively equally to the model outcome. This is achieved by training and validating TranSalNet on the SALICON training and validation sets each time by a single sub-loss function. According to the recorded minimal loss values on the validation set, weights are initially assigned to the sub-loss functions so that their contributions to the combined loss are relatively equal. In a second step, these weights in a combined loss are further adjusted to achieve balanced results on all evaluation metrics. As per our empirical studies, the default weights ? 1 , ? 2 , ? 3 , and ? 4 of the combined loss function are set to ?1, 10, ?2, and ?1, respectively.  Model variants</p><formula xml:id="formula_11">E 1 E 2 E 3 SC Loss function Backbone BaseNet - - - - L BCE ResNet-50 BaseNet+ - - - L BCE ResNet-50 SkipNet - - - L BCE ResNet-50 TranSalNet_Res_BCE L BCE ResNet-50 BaseNet(L CB ) - - - - L CB ResNet-50 BaseNet+(L CB ) - - - L CB ResNet-50 SkipNet(L CB ) - - - L CB ResNet-50 TranSalNet_Res L CB ResNet-50 TranSalNet_Dense L CB DenseNet-161</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>Various metrics have been proposed to evaluate the agreement between the predicted saliency map and the ground truth. In general, these metrics can be described as location-based and distribution-based metrics depending on how the ground truth is represented <ref type="bibr" target="#b45">[46]</ref>; the former adopts the fixation map (i.e., in the form of a binary image) and the latter uses the saliency map (i.e., in the form of a gray-scale image) as the ground truth for visual saliency evaluation. Six popular metrics are widely used to quantify the general performance of saliency models, including CC, SIM, KLD, NSS, AUC (Area under ROC Curve), and sAUC (Shuffled AUC). Details of these metrics can be found in <ref type="bibr" target="#b45">[46]</ref>. The first three are distribution-based metrics, and the remaining three are location-based metrics. For KLD, the closer the value is to zero, the better the agreement between prediction and ground truth. For the other five metrics, higher values represent higher consistency. Now, in this paper, we aims to evaluate the general performance of our proposed model, but in the meantime the perceptual relevance of the saliency model is the focus of our study. To this end, on the basis of the study of <ref type="bibr" target="#b45">[46]</ref>, we classify the six metrics into two categories based on their capability of being in close agreement with human judgements of saliency maps: "perception-based metrics", which include NSS, CC, and SIM; and "non-perception-based metrics", which include sAUC, AUC, and KLD <ref type="bibr" target="#b45">[46]</ref>. Note, "non-perception-based metrics" do not necessarily mean they are not measuring the gaze behaviour, they may focus on specific properties of viewing behaviour, such as detecting salient objects in the visual field. It is stated in <ref type="bibr" target="#b45">[46]</ref> that "AUC, KL are appropriate for detection applications, as they penalize target detection failures. However, where it is important to evaluate the relative importance of different image regions, such as for image-retargeting, compression, and progressive transmission, metrics like NSS or SIM are a better fit." This provides sufficient grounds for building perceptually more relevant saliency prediction models, which is the primary goal of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Setup</head><p>By following a similar procedure in the state-of-the-art <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref>, a model should be first initialised by the weights pre-trained on ImageNet <ref type="bibr" target="#b44">[45]</ref>, then trained on the 10,000 images of the SALICON training set to reduce the risk of overfitting. Consequently, the best model on its validation set should be selected for further testing on the SALICON test set and training on MIT1003 and CAT2000.</p><p>To obtain fair results in each dataset, k-fold cross-validation (k = 10) is applied for each model. More specifically, each dataset is divided into 10 non-overlapping subsets. For MIT1003, each subset contains around 100 images; For CAT2000, each subset contains 200 images (10 from each category). Each time, one subset is kept as a test set, one as a validation set, and the remaining eight subsets altogether are used as the training set. To eliminate randomness, each test set corresponds to a fixed validation set and training set. We report the overall performance of 10 times test results.</p><p>To reduce the computational cost while aligning with the aspect ratio (4:3) of the images in SALICON, all input images are resized and padded to a same size of 384?288 pixels. A consistent standard is followed in all training phases. The Adam optimizer <ref type="bibr" target="#b63">[64]</ref> is used to minimize the loss function. The learning rate is set to 1 ? 10 ?5 , which is then multiplied by 0.1 for every 3 epochs. Models are trained with a batch size of 4 for 30 epochs with a stop patience of 5 epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>Ablation experiments are conducted to investigate the contribution of three key components in our modelling: (1) Transformer encoders (E 1 , E 2 , and E 3 denote Transformer encoder 1, 2, and 3 in <ref type="figure" target="#fig_0">Figure 2</ref>, respectively), (2) Skipconnections (SC), (3) Combined loss function (L CB ). To this end, nine model variants are constructed to demonstrate the added value of one or more of the above components, as shown in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Among them, BaseNet is constructed as a baseline that adopts the widely used ResNet-50 as the CNN encoder, removes all transformer encoders and skip-connections except for the Conv 1?1 layer before transformer encoder 1, and is trained by the BCE loss function. BaseNet+ adds the transformer encoder 1 based on the BaseNet. SkipNet is equipped with skip-connections based on the BaseNet. TranSalNet_Res_BCE adds the transformer encoder 1, 2, and 3 based on the SkipNet, which utilises ResNet-50 as the CNN encoder and is identical in architecture to the proposed TranSalNet (demonstrated in <ref type="figure" target="#fig_0">Figure 2</ref>) but is trained by the BCE loss. The model variants trained by the combined loss that are consistent with the architecture of the above four model variants are denoted as BaseNet(L CB ), BaseNet+(L CB ), SkipNet(L CB ), and TranSalNet_Res, respectively. TranSalNet_Dense replaces ResNet-50 with DenseNet-161 as the CNN encoder. The overall performance of these model variants on the MIT1003 and CAT2000 datasets is shown in <ref type="table" target="#tab_3">Table 2</ref>. The illustration of saliency maps of four images from these two datasets can also be found in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>By comparing BaseNet/BaseNet(L CB ) and BaseNet+/BaseNet+(L CB ), it can be found that adding a transformer encoder improves the overall performance, i.e., BaseNet+/BaseNet+(L CB ) outperforms BaseNet/BaseNet(L CB ) in the majority of instances. Especially, on the perception-based metrics, i.e., CC, SIM, and NSS, BaseNet+/BaseNet+(L CB ) give consistently better performance than BaseNet/BaseNet(L CB ), suggesting that the transformer encoder contributes to the perceptual relevance of saliency prediction. Besides, the benefits of enhancing saliency prediction by providing multi-scale image representations through skip-connections have been demonstrated in previous studies. Similarly, by adding skip-connections to BaseNet/BaseNet(L CB ), the performance of model variants SkipNet/SkipNet(L CB ) improves on most instances in the ablation study.</p><p>By uniting transformer encoders and skip-connections, the decoder network can obtain multi-scale feature maps with long-range context enhanced by transformer encoders. As a result, the performance of TranSal-Net_Res_BCE/TranSalNet_Res is further boosted on all instances of perception-based metrics as well as most instances of non-perception-based metrics. This provides additional evidence that the transformer is of added value for visual saliency prediction. Also, this demonstrates the effectiveness of the TranSalNet architecture, which integrates transformer encoders into CNN-based models via skip-connections to obtain multi-scale representations with enhanced long-range visual information.  <ref type="table">)</ref>, BaseNet+(L CB ), SkipNet(L CB ), and TranSalNet_Res) achieve higher performance on the majority of saliency metrics. In particular, the TranSalNet_Res outperforms the TranSalNet_Res_BCE on all instances in the ablation study. In summary, the effectiveness of the transformer encoder, the TranSalNet architecture, and the combined loss function has now been demonstrated in this ablation study.</p><p>In addition, previous research <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref> has shown that using backbones with greater representational capability could improve saliency prediction. Similarly, by simply replacing the backbone from the widely used but comparatively It can be seen that by adopting transformer encoder, skip-connection to provide multi-scale information, and combined loss function, the generated saliency maps are significantly refined relative to the ground truth. "shallow" ResNet-50 (used by TranSalNet_Res) with DenseNet-161 <ref type="bibr" target="#b27">[28]</ref>, TranSalNet_Dense has been further improved as shown in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5</head><p>Comparison with state-of-the-art methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">On MIT1003 and CAT2000 datasets</head><p>Seven state-of-the-art deep learning-based saliency models that adopt multi-scale representations or attention mechanisms, including FastSal <ref type="bibr" target="#b64">[65]</ref>, UNISAL <ref type="bibr" target="#b20">[21]</ref>, MSI-Net <ref type="bibr" target="#b18">[19]</ref>, SAM-VGG <ref type="bibr" target="#b17">[18]</ref>, SAM-ResNet <ref type="bibr" target="#b17">[18]</ref>, ML-Net <ref type="bibr" target="#b15">[16]</ref>, and Deep Visual Attention (DVA) <ref type="bibr" target="#b16">[17]</ref> are selected for the general performance comparison on the MIT1003 and CAT2000 datasets. In order to ensure a fair comparison, the same k-fold Cross-Validation (k = 10 for MIT1003 and CAT2000) strategy and the dataset splitting method used in TranSalNet are employed for fine-tuning and testing of these models. The corresponding pre-trained weights on the SALICON dataset is loaded for each fine-tuning instance. For MIT1003 and CAT2000 datasets, the overall performance of 10 times test results is reported in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>It can be seen that our models (both TranSalNet_Res and TranSalNet_Dense) achieve the best performance on all perception-based metrics in both MIT1003 and CAT2000, while producing competitive results on non-perceptionbased metrics (i.e., being best or 2nd best in most instances in the comparative study). It should be noted that our TranSalNet_Res and the five state-of-the-art models all use ResNet-50 or VGGNet (representing similar network capacity) as the feature extraction network. TranSalNet_Res achieves the best performance on most instances (except for sAUC and KLD in CAT2000), implying the contribution of enhanced long-range information to saliency prediction using transformers. Moreover, the performance our TranSalNet_Res could be further enhanced by replacing ResNet-50 by a network with higher capacity, namely DenseNet-161. <ref type="figure" target="#fig_2">Figure 4</ref> shows saliency maps generated by our models and other models for images including common contexts such as objects, portraits, natural, indoor, social, and cartoon scenes. By visually assessing these saliency maps, our models are in closer agreement with the ground truth than other models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">On MIT300 competition</head><p>For the MIT300 competition, we use the MIT1003 to train an optimal model, in which 703 images are randomly selected as a training set and the rest as a validation set. The optimal model is submitted to and tested by the MIT/Tuebingen Saliency Benchmark <ref type="bibr" target="#b62">[63]</ref>. It should be noted that the benchmark evaluates models by different standards, i.e., models must be explicitly claimed as either probabilistic or non-probabilistic models, so they can be fairly evaluated within the category they belong to <ref type="bibr" target="#b45">[46]</ref>. In this paper, same as the original MIT Saliency Benchmark <ref type="bibr" target="#b45">[46]</ref>, we "do not assume that our model is probabilistic". Note that for evaluating probabilistic models, metric-specific adaptations are applied using regularization and scaling of saliency values, hence, a probabilistic model generates optimal saliency maps for individual metrics <ref type="bibr" target="#b48">[49]</ref>. But a non-probabilistic model only outputs a single saliency map for all metrics. So it is nontrivial to compare a non-probabilistic (i.e., classical) model to a probabilistic model <ref type="bibr" target="#b45">[46]</ref>. To avoid unfair model comparison under different assumptions, <ref type="table" target="#tab_6">Table 4</ref> shows only non-probabilistic classical saliency models on the leader-board of <ref type="bibr" target="#b62">[63]</ref>. It can be seen that our models (both TranSalNet_Res and TranSalNet_Dense) consistently rank in the top 1st or 2nd positions on the perception-based metrics (note the only exception is for TranSalNet_Res on NSS, but its performance score is fairly comparable to the 1st or 2nd scores as shown in <ref type="table" target="#tab_6">Table 4</ref>). On the non-perception-based metrics, our models exhibit competitive performance on sAUC and AUC, with the performance scores comparable to the results in the 1st and 2nd positions. In addition, even though we include top probabilistic models such as DeepGaze II-E <ref type="bibr" target="#b23">[24]</ref>, MSI-Net <ref type="bibr" target="#b18">[19]</ref>, UNISAL <ref type="bibr" target="#b20">[21]</ref>, SalFBNet <ref type="bibr" target="#b66">[67]</ref>, and DeepGaze II <ref type="bibr" target="#b22">[23]</ref> for performance comparison, our model can still remain competitive in perception-based metrics (results available on website of <ref type="bibr" target="#b62">[63]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">On LSUN'17 competition</head><p>Although our aim is to predict the spatial distribution of human fixations, the human attention measured by mouse tracking can still reflect eye movement behaviour to a certain extent <ref type="bibr" target="#b28">[29]</ref>. SALICON provides so far the largest-scale saliency dataset (via mouse tracking), which allows the opportunity to examine the saliency models from the perspective of being "data rich". Moreover, for LSUN'17 competition (on SALICON test set), a unified evaluation process is  adopted, i.e., the saliency models are not treated differently because of their type of being probabilistic or classical. In the competition each model submitted should generate one single saliency map for each image. Therefore, in order to provide a complementary comparison of state-of-the-art saliency models, <ref type="table" target="#tab_7">Table 5</ref> reports the results of models submitted to the competition based on the 2017 version (i.e., the latest version). It can be seen that our TranSalNet_Res and TranSalNet_Dense achieve superior performance on the perception-based metrics and promising results on other non-perception-based metrics. This shows that our model are competitive on the LSUN 2017 leaderboard, in particular for prediction saliency in a perceptually relevant manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Discussion</head><p>It is crucial to note that metric selection for saliency model evaluation should be based on specific modelling assumptions and specific target applications <ref type="bibr" target="#b45">[46]</ref>. The study in <ref type="bibr" target="#b45">[46]</ref> concludes that "under the assumptions of non-probabilistic modelling, NSS and CC provide the fairest comparison"; "if evaluating probabilistic models, KLD is recommended"; and "specific tasks and applications also call for a difference choice of metrics". In <ref type="bibr" target="#b47">[48]</ref>, researchers have verified that "NSS, CC and SIM best correspond to human perception". In a recent study <ref type="bibr" target="#b46">[47]</ref>, it is found that CC and SIM are the most appropriate saliency evaluation metrics for image quality assessment applications. Therefore, as the results demonstrated in <ref type="table" target="#tab_5">Table 3</ref>, <ref type="table" target="#tab_6">Table 4</ref>, and <ref type="table" target="#tab_7">Table 5</ref>, the proposed saliency models (TranSalNet_Res and TranSalNet_Dense) could be the best "human-like" models (i.e., based on perception-based metrics CC and SIM) to evaluate the relative importance of different image regions for the applications such as image re-targeting, image compression and transmission, and visual quality assessment.</p><p>Using skip-connections to provide multi-scale features from encoder to decoder has been shown in previous studies to be an effective method for computer vision tasks. For example, the widely used U-Net <ref type="bibr" target="#b54">[55]</ref> style networks usually connect feature maps of each spatial size to the decoders from shallow to deep encoder blocks. However, as can be seen in <ref type="figure">Figure 5</ref>, using skip-connections to connect shallow encoder blocks (i.e., the blocks provide feature maps with spatial sizes of w 4 ? h 4 and w 2 ? h 2 ) with decoder blocks (i.e., block_4 and block_5 in the decoder) may lead to some shapes of objects and texts appearing in the predicted saliency maps, which are not consistent with the ground truth. This implies that adding low-level features from the encoder directly to the decoder may interfere with the saliency prediction of TranSalNet. <ref type="figure">Figure 5</ref>: The column on the righthand side illustrates the salinency maps with undesired artefacts caused by adding skip-connections to TranSalNet_Res to connect shallow encoder blocks with decoder block_4 and block_5. From left to right, the remaining three columns are: stimuli, ground truth saliency maps, and saliency maps generated from TranSalNet_Res, respectively. Multi-head Self Attention (MSA) is part of the transformer encoder. Previous research has shown that the number of heads of MSA could affect the model's performance <ref type="bibr" target="#b67">[68]</ref>. According to the suggestions from <ref type="bibr" target="#b45">[46]</ref>, we use CC and SIM as the performance metrics to illustrate the impact of the head number of MSA on our proposed TranSalNet in <ref type="figure" target="#fig_3">Figure 6</ref>. For each head number combination, the model is trained on the SALICON training set, validated with 2000 images of its validation set, and tested on the rest of the validation set three times. The demonstrated results are the mean results. As can be seen in <ref type="figure" target="#fig_3">Figure 6</ref>, the scores of CC and SIM tend to increase with the increase in the head number of MSA. However, when the transformer encoders 1 and 2 (E 1 and E 2 ) adopt 12 heads each, and transformer encoder 3 (E 3 ) adopts 8 heads of MSA, the performance of the model tends to be saturated in the CC-SIM performance space. Therefore, considering the trade-off between computational resource consumption and model performance, we chose 12 heads for the transformer encoder 1 and 2, and 8 heads for the transformer encoder 3 in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a novel saliency model for predicting saliency maps that are perceptually in close agreement with the ground truth. By integrating transformers into CNNs, saliency models can significantly benefit from capturing long-range spatial information at multiple perceptual levels. An ablation study has demonstrated the contributions of the transformer encoders to a CNN model, especially the added value of transformers in enhancing the perceptual relevance of saliency prediction. Experimental results show that the proposed models have achieved superior performance on the public benchmarks and competitions for saliency models, particularly having yielded notable results on perception-based saliency evaluation metrics. The perceptually more relevant saliency models have the potential to advance many image processing applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>This work is funded in part by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -Project-ID 251654672 -TRR 161 and the China Scholarship Council -ID 202008220129.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Schematic overview of TranSalNet. Assume that the spatial size of inputs is w ? h. After the input image is processed by the CNN encoder, which provides three sets of multi-scale feature maps have spatial size of w8 ? h 8 , w 16 ? h 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of the saliency prediction performance of nine model variants in our ablation study. The images of top two rows are from the MIT1003 dataset and the bottom two rows are from the CAT2000 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of saliency maps generated by our models (TranSalNet_Res and TranSalNet_Dense) and other state-of-the-art saliency models. The images from (a) to (d) are from the MIT1003 dataset, and the images from (e) to (h) are from the CAT2000 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Illustrations of the impact of the head number of MSA on the performance of TranSalNet. Values in brackets (i, j, k) indicate the head numbers of transformer encoder 1, 2, and 3 are set to i, j, and k respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Four commonly used benchmark saliency datasets are used to train and evaluate our proposed saliency model and variants.? SALICON<ref type="bibr" target="#b28">[29]</ref> contains 10,000 training, 5,000 validation, and 5000 testing images. The ground truth annotations of its test set are unpublished and used for a challenge named LSUN 2017 2 to test the performance of the saliency models. Different from other benchmark datasets, it employs mouse clicks instead of an eye tracker to record human visual attention. ? CAT2000 [60] contains 2,000 publicly available images of 20 categories such as action, art, cartoon etc, where each category includes 100 images. Each image is associated with its eye-tracking data of 24 observers. ? MIT1003 [61] consists of 1,003 natural indoor and outdoor images with eye-tracking data of 15 observers.</figDesc><table><row><cell>4 Experimental results and discussion</cell></row><row><cell>4.1 Datasets</cell></row></table><note>? MIT300 [62] includes 300 natural indoor and outdoor images. The eye-tracking data is unpublished, it is used as the test set of the MIT/T?bingen benchmark [63].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Model variants purposely constructed for ablation study to explore the contribution of skip-connections (SC), transformer encoders (E 1 to E 3 ), and combined loss function (L CB ).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study: performance of nine model variants purposely constructed for ablation study to explore the contribution of skip-connections, transformer encoders, and the combined loss function based on MIT1003 and CAT2000 datasets. Red and orange font indicate the best and 2nd best performance, respectively.MIT1003CAT2000 perception-based metrics non-perception-based metrics perception-based metrics non-perception-based metricsVariation CC ? SIM ? NSS ? sAUC ? AUC ? KLD ? CC ? SIM ? NSS ? sAUC ? AUC ? KLD ?</figDesc><table><row><cell>BaseNet</cell><cell>0.7361 0.5858 2.7129 0.7460 0.9018</cell><cell>0.7886</cell><cell>0.8536 0.7277 2.3043 0.6062 0.8775</cell><cell>0.5190</cell></row><row><cell>BaseNet+</cell><cell>0.7446 0.6003 2.7437 0.7472 0.9046</cell><cell>0.8369</cell><cell>0.8622 0.7343 2.3286 0.6054 0.8784</cell><cell>0.5056</cell></row><row><cell>SkipNet</cell><cell>0.7393 0.5381 2.7571 0.7505 0.9011</cell><cell>0.7327</cell><cell>0.8582 0.7187 2.3298 0.6111 0.8782</cell><cell>0.3207</cell></row><row><cell cols="2">TranSalNet_Res_BCE 0.7593 0.6122 2.8326 0.7507 0.9086</cell><cell>0.7785</cell><cell>0.8765 0.7458 2.3887 0.6035 0.8803</cell><cell>0.5040</cell></row><row><cell>BaseNet(L CB )</cell><cell>0.7386 0.5966 2.7156 0.7508 0.9046</cell><cell>0.7839</cell><cell>0.8589 0.7332 2.3349 0.6063 0.8787</cell><cell>0.5146</cell></row><row><cell>BaseNet+(L CB )</cell><cell>0.7449 0.6046 2.7588 0.7505 0.9066</cell><cell>0.8321</cell><cell>0.8666 0.7381 2.3578 0.6076 0.8794</cell><cell>0.5053</cell></row><row><cell>SkipNet(L CB )</cell><cell>0.7471 0.5998 2.7692 0.7534 0.9067</cell><cell>0.7558</cell><cell>0.8612 0.7347 2.3409 0.6081 0.8797</cell><cell>0.4730</cell></row><row><cell>TranSalNet_Res</cell><cell>0.7595 0.6145 2.8501 0.7546 0.9093</cell><cell>0.7779</cell><cell>0.8786 0.7492 2.4154 0.6054 0.8811</cell><cell>0.5036</cell></row><row><cell>TranSalNet_Dense</cell><cell>0.7743 0.6279 2.9214 0.7547 0.9116</cell><cell>0.7862</cell><cell>0.8823 0.7512 2.4290 0.6099 0.8820</cell><cell>0.4715</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>also demonstrates the practical plausibility of training the proposed model with the linear combination of sub-loss functions. Compired with the model variations trained by L BCE (i.e., BaseNet, BaseNet+, SkipNet, and TranSalNet_Res_BCE), the model variations trained by L CB (i.e., BaseNet(L CB</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of state-of-the-art saliency models on MIT1003 and CAT2000. Red and orange font indicate the best and 2nd best performance, respectively.</figDesc><table><row><cell></cell><cell>MIT1003</cell><cell>CAT2000</cell></row><row><cell></cell><cell cols="2">perception-based metrics non-perception-based metrics perception-based metrics non-perception-based metrics</cell></row><row><cell>Model name</cell><cell>CC ?</cell><cell></cell></row><row><cell>FastSal [65]</cell><cell>0.5901 0.4783 2.0078 0.7060 0.8745 1.0359</cell><cell>0.7213 0.6031 1.8590 0.6180 0.8599 0.5523</cell></row><row><cell>UNISAL [21]</cell><cell>0.7340 0.5973 2.7593 0.7326 0.9026 1.0138</cell><cell>0.8417 0.7207 2.2575 0.5982 0.8758 0.5302</cell></row><row><cell>MSI-Net [19]</cell><cell>0.7473 0.6081 2.8007 0.7454 0.9068 0.8155</cell><cell>0.8655 0.7398 2.3547 0.6071 0.8809 0.4280</cell></row><row><cell>SAM-VGG [18]</cell><cell>0.7260 0.5976 2.7520 0.7256 0.9003 1.2195</cell><cell>0.8680 0.7391 2.4138 0.5966 0.8784 0.6383</cell></row><row><cell cols="2">SAM-ResNet [18] 0.7466 0.6068 2.8001 0.7365 0.9024 1.2470</cell><cell>0.8706 0.7395 2.4108 0.5932 0.8778 0.6702</cell></row><row><cell>ML-Net [16]</cell><cell>0.5979 0.4960 2.3329 0.7218 0.8623 1.3496</cell><cell>0.5221 0.5407 1.4485 0.6212 0.8104 1.1101</cell></row><row><cell>DVA [17]</cell><cell>0.6990 0.5663 2.5740 0.7258 0.8970 0.7528</cell><cell>0.8616 0.7335 2.3447 0.6014 0.8783 0.4492</cell></row><row><cell>TranSalNet_Res</cell><cell>0.7595 0.6145 2.8501 0.7546 0.9093 0.7779</cell><cell>0.8786 0.7492 2.4154 0.6054 0.8811 0.5036</cell></row><row><cell cols="2">TranSalNet_Dense 0.7743 0.6279 2.9214 0.7547 0.9116 0.7862</cell><cell>0.8823 0.7512 2.4290 0.6099 0.8820 0.4715</cell></row></table><note>SIM ? NSS ? sAUC ? AUC ? KLD ? CC ? SIM ? NSS ? sAUC ? AUC ? KLD ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>MIT300 competition for saliency models. The results are administered and reported by the Benchmark<ref type="bibr" target="#b62">[63]</ref>. Red and orange font indicate the best and 2nd best performance, respectively. perception-based metrics non-perception-based metrics Model name CC ? SIM ? NSS ? sAUC ? AUC ? KLD ?</figDesc><table><row><cell>EML-NET [20]</cell><cell>0.7893 0.6756 2.4876</cell><cell>0.7469 0.8762</cell><cell>0.8439</cell></row><row><cell>CASNet II [22]</cell><cell>0.7054 0.5806 1.9859</cell><cell>0.7398 0.8552</cell><cell>0.5857</cell></row><row><cell>GazeGAN [36]</cell><cell>0.7579 0.6491 2.2118</cell><cell>0.7316 0.8607</cell><cell>1.3390</cell></row><row><cell>SAM-VGG [18]</cell><cell>0.6630 0.5986 1.9552</cell><cell>0.7305 0.8473</cell><cell>1.2746</cell></row><row><cell cols="2">SAM-ResNet [18] 0.6897 0.6122 2.0628</cell><cell>0.7396 0.8526</cell><cell>1.1710</cell></row><row><cell>DVA [17]</cell><cell>0.6631 0.5848 1.9305</cell><cell>0.7257 0.8430</cell><cell>0.6293</cell></row><row><cell>ML-Net [16]</cell><cell>0.6633 0.5819 1.9748</cell><cell>0.7399 0.8386</cell><cell>0.8006</cell></row><row><cell>eDN [42]</cell><cell>0.4518 0.4112 1.1399</cell><cell>0.6180 0.8171</cell><cell>1.1369</cell></row><row><cell>SalGAN [66]</cell><cell>0.6740 0.5932 1.8620</cell><cell>0.7354 0.8498</cell><cell>0.7574</cell></row><row><cell>HATES</cell><cell>0.7897 0.5313 2.3762</cell><cell>0.7549 0.8744</cell><cell>0.7146</cell></row><row><cell>TranSalNet_Res</cell><cell>0.7991 0.6852 2.3758</cell><cell>0.7471 0.8730</cell><cell>0.9019</cell></row><row><cell cols="2">TranSalNet_Dense 0.8070 0.6895 2.4134</cell><cell>0.7467 0.8734</cell><cell>1.0141</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>LSUN'17 competition (on SALICON test set) for saliency models. Results are provided by the authors. Red and orange font indicate the best and 2nd best performance, respectively.</figDesc><table><row><cell></cell><cell cols="5">perception-based metrics non-perception-based metrics</cell></row><row><cell>Model name</cell><cell>CC ? SIM ?</cell><cell>NSS ?</cell><cell cols="2">sAUC ? AUC ?</cell><cell>KLD ?</cell></row><row><cell cols="2">DeepGaze II-E [24] 0.872 0.733</cell><cell>1.996</cell><cell>0.767</cell><cell>0.869</cell><cell>0.285</cell></row><row><cell>SalFBNet [67]</cell><cell>0.892 0.772</cell><cell>1.952</cell><cell>0.740</cell><cell>0.868</cell><cell>0.236</cell></row><row><cell>MSI-Net [19]</cell><cell>0.889 0.784</cell><cell>1.931</cell><cell>0.736</cell><cell>0.865</cell><cell>0.307</cell></row><row><cell>UNISAL [21]</cell><cell>0.879 0.775</cell><cell>1.952</cell><cell>0.739</cell><cell>0.864</cell><cell>-</cell></row><row><cell>EML-NET [20]</cell><cell>0.886 0.780</cell><cell>2.050</cell><cell>0.746</cell><cell>0.866</cell><cell>0.520</cell></row><row><cell>GazeGAN [36]</cell><cell>0.879 0.773</cell><cell>1.899</cell><cell>0.736</cell><cell>0.864</cell><cell>0.376</cell></row><row><cell>SAM-ResNet [18]</cell><cell>0.899 0.793</cell><cell>1.990</cell><cell>0.741</cell><cell>0.865</cell><cell>0.610</cell></row><row><cell>TranSalNet_Res</cell><cell>0.901 0.796</cell><cell>1.998</cell><cell>0.742</cell><cell>0.866</cell><cell>0.414</cell></row><row><cell>TranSalNet_Dense</cell><cell>0.907 0.803</cell><cell>2.014</cell><cell>0.747</cell><cell>0.868</cell><cell>0.373</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://competitions.codalab.org/competitions/17136</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Integrating visual information from successive fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jonides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">4529</biblScope>
			<biblScope unit="page" from="192" to="194" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Saliency prediction in the deep learning era: Successes and limitations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="679" to="700" />
			<date type="published" when="2021-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Boosting image sentiment analysis with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="218" to="228" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human scanpath estimation based on semantic segmentation guided by common eye fixation behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page" from="705" to="717" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic contrast enhancement technology with saliency preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1480" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video saliency prediction via spatio-temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">462</biblScope>
			<biblScope unit="page" from="59" to="68" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast saliency-aware multi-modality image fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Zeeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="70" to="80" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-scale network (mssg-cnn) for joint image and saliency map learning-based compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kedia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">460</biblScope>
			<biblScope unit="page" from="95" to="105" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling attention to salient proto-objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1395" to="1407" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Neural Information Processing Systems (NIPS), ser. NIPS&apos;06</title>
		<meeting>the 19th International Conference on Neural Information Processing Systems (NIPS), ser. NIPS&apos;06<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual saliency estimation by nonlinearly integrating features using region covariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="11" to="11" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning discriminative subspaces on random contrasts for image saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1095" to="1108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overt attention in natural scenes: Objects dominate features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nuthmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Einh?user</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="36" to="48" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Objects predict fixations better than early saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Einh?user</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="18" to="18" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faces and text attract gaze independent of the task: Experimental data and computer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cerf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Frady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="10" to="10" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A deep multi-level network for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3488" to="3493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual attention prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting human eye fixations via an LSTM-based saliency attentive model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5142" to="5154" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contextual encoder-decoder network for visual saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kroner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Senden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Driessens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">EML-NET: An expandable multi-layer network for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">103887</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unified image and video saliency modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Droste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part V, ser. Lecture Notes in Computer Science</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J. Frahm</editor>
		<meeting>Part V, ser. Lecture Notes in Computer Science<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12350</biblScope>
			<biblScope unit="page" from="419" to="435" />
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2020 -16th European Conference</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emotional attention: A study of image sentiment and visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding low-and high-level contributions to fixation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kummerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S A</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DeepGaze IIE: Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Linardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?mmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="12" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of The ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Learning Representations(ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SALICON: Saliency in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1072" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Computational versus psychophysical bottom-up image saliency: A comparative evaluation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2131" to="2146" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A review of interactions between peripheral and foveal vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E M</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valsecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sch?tz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Capabilities and limitations of peripheral vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Vision Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="437" to="457" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long-range interactions in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="428" to="434" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A saliency-based search mechanism for overt and covert shifts of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1489" to="1506" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SALICON: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How is gaze influenced by image transformations? Dataset and model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2287" to="2300" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A deep spatial contextual long-term recurrent convolutional network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3264" to="3274" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ser. Proc. of Machine Learning Research</title>
		<editor>H. D. III and A. Singh</editor>
		<meeting>the 37th International Conference on Machine Learning, ser. Proc. of Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2798" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">What do different evaluation metrics tell us about saliency models?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="740" to="757" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A measurement for distortion induced saliency variation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A data-driven metric for comprehensive evaluation of saliency models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Saliency benchmarking made easy: Separating models, maps and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?mmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S A</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018, ser. Lecture Notes in Computer Science</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="798" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deepvit: Towards deeper vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visual saliency transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="4722" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Part-object relational visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">RGB-T Salient Object Detection via Fusing Multi-Level CNN Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3321" to="3335" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<editor>N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">HigherHRNet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep salient object detection with contextual information guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="360" to="374" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6877" to="6886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Visual attention prediction for autism spectrum disorder with hierarchical semantic fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">116186</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">CAT2000: A large scale fixation dataset for boosting saliency research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03581</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2015 workshop on &quot;Future of Datasets</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A benchmark of computational models of saliency to predict human fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>MIT-CSAIL-TR-2012-001</idno>
	</analytic>
	<monogr>
		<title level="m">MIT Computer Science and Artificial Intelligence Lab (CSAIL)</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?mmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="https://saliency.tuebingen.ai/" />
		<title level="m">MIT/T?bingen Saliency Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">FastSal: a Computationally Efficient Network for Visual Saliency Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9054" to="9061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">SalGAN: Visual Saliency Prediction with Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G</forename><surname>Nieto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">SalFBNet: Learning Pseudo-Saliency Distribution via Feedback Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imamouglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nakamura</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
