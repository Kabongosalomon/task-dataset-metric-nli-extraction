<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-25">25 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Kumar Tomar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Debesh</forename><surname>Jha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">H?vard</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Dag</forename><surname>Johansen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jens</forename><surname>Rittscher</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">P?l</forename><surname>Halvorsen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Sharib</forename><surname>Ali</surname></persName>
						</author>
						<title level="a" type="main">FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2022-03-25">25 Mar 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Medical image segmentation</term>
					<term>deep learning</term>
					<term>feedback attention</term>
					<term>colon polyps</term>
					<term>skin lesion</term>
					<term>retinal vessels</term>
					<term>cell nuclei</term>
					<term>lung segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increase of available large clinical and experimental datasets has contributed to a substantial amount of important contributions in the area of biomedical image analysis. Image segmentation, which is crucial for any quantitative analysis, has especially attracted attention. Recent hardware advancement has led to the success of deep learning approaches. However, although deep learning models are being trained on large datasets, existing methods do not use the information from different learning epochs effectively. In this work, we leverage the information of each training epoch to prune the prediction maps of the subsequent epochs. We propose a novel architecture called feedback attention network (FANet) that unifies the previous epoch mask with the feature map of the current training epoch. The previous epoch mask is then used to provide a hard attention to the learned feature maps at different convolutional layers. The network also allows to rectify the predictions in an iterative fashion during the test time. We show that our proposed feedback attention model provides a substantial improvement on most segmentation metrics tested on seven publicly available biomedical imaging datasets demonstrating the effectiveness of FANet. The source code is available at https://github.com/nikhilroxtomar/FANet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I MAGE segmentation is one of the most studied problems in computer vision, where the main goal is to classify each pixel of an image to a specific class instance. This can either be pixels of any arbitrary objects such as cars or humans in natural scene data <ref type="bibr" target="#b0">[1]</ref>, satellite data in remote sensing <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, or pixels of cancerous area or cells in biomedical imaging data <ref type="bibr" target="#b3">[4]</ref>. Substantial progress has been made in biomedical imaging due to which various modalities such as X-ray, Computerized Tomography (CT), Magnetic Resonance Imaging (MRI), endoscopy imaging, fundus imaging, Electron Microscopy (EM), and histology imaging exists. While Machine Learning (ML) methods usually provide improved performance over traditional computer vision methods, most of them require ground truth labels from domain experts, which are often scarce and may not represent enough variability in biomedical imaging data. This can affect ML models resulting in only sub-optimal predictions. Furthermore, existing methods for semantic segmentation are based on a singlestep prediction process that does not allow them to rectify their own predicted segmentation masks. Thus, these networks are constrained to only one set of learned weights that may not be enough to capture inter-and intra-class differences present in biomedical imaging data. In this work, we introduce an iterative approach that can refine the segmentation masks from previous mask predictions in a few iterative steps. This iteration process enables the network to steer towards the improved feature representation by taking advantage of subsequent attention mechanisms from previous mask, unlike classically used one-step segmentation methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Thus, aggregating these results over a few iterations provides improved segmentation masks (see illustration in <ref type="figure">Figure 1</ref>). Current developments of Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and attention modules have improved automated methods in biomedical image analysis. Widely used supervised end-to-end CNNs methods require a large and diverse training dataset to avoid overfitting. RNNs can be used to preserve the model compactness and can be effectively used for segmentation tasks in resource-constrained settings via an iterative update on internal states of network layers <ref type="bibr" target="#b5">[6]</ref>. However, they are known for their memory inefficient memory-bandwidth-bound computation and model complexity <ref type="bibr" target="#b6">[7]</ref>. Additionally, spatial visual attention mechanisms used for image captioning for natural scene images <ref type="bibr" target="#b7">[8]</ref> and for medical image segmentation <ref type="bibr" target="#b8">[9]</ref> showed improvements in terms of both model convergence and performance metric. An attention mechanism allows networks to focus on a concrete class instance, thereby penalizing nonspecific regions. Our model is thus inspired by the success of both visual attention mechanism and recurrent learning paradigm.</p><p>A mask-guided contrastive attention model was used by Song et al. <ref type="bibr" target="#b9">[10]</ref> to deal with the background clutter. Unlike classical training mechanisms and motivated by the work of Song et al. <ref type="bibr" target="#b9">[10]</ref>, we propose to propagate the samplespecific mask output from the previous epoch to the successive epoch in a recursive fashion. Such a feedback mechanism can provide prior information that can help to learn sample variability, thereby enabling to train effectively on diverse <ref type="figure">Fig. 1</ref>: Semantic segmentation using our FANet architecture. Otsu thresholding is used for generating the initial mask used during 0 th iteration. Then the predictions are iteratively updated with the predicted mask. It can be observed that already at the 2 nd iteration, the results converge. The corresponding feature maps before and after feedback attention at the last decoder layer of our FANet are shown as color images on the right. datasets. Here, iterative prediction can be used to prune the predicted masks during the inference (see <ref type="figure">Figure 1</ref>). This allows the network to learn both local and global features that can rectify the mask output from the learned weights. Unlike Test-Time Augmentation (TTA) <ref type="bibr" target="#b10">[11]</ref>, where different transforms are utilized to mimic sample representations and data diversity, we embed mask rectification during the training process. To our knowledge, Feedback Attention Network (FANet) is the first deep learning model that incorporates the ability to self-rectify its predictions without requiring heavy transformations, ensemble strategies, and prior sample-specific knowledge. FANet uses a single end-to-end trainable network that allows information propagation during both train and test time.</p><p>A feedback mechanism during the training is central to our novel FANet approach for semantic segmentation. The predicted map of each sample from the previous epoch unified with the current state feature map is used to provide attention. FANet uses an attention mechanism to different feature scales in the network, allowing it to capture variability in image samples. Additionally, our residual block with Squeeze and Excitation (SE) layer allows us to improve channel interdependencies, which can be critical to tackle image quality issues. The main contributions of this work can be summarized as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Feedback attention learning -A novel mechanism</head><p>to utilize the variability present in each training sample. The mask outputs are propagated from one to subsequent epochs to supress the unwanted feature clutter. 2) Iterative refining of prediction masks -Using feedback information helps in refining the predicted masks in training as well as inference. During testing, we iterate over the input image and keep updating the input mask with the predicted mask for up to 10 iterations (empirically set).</p><p>3) Embedded run-length encoding strategy -Binary mask outputs of each samples are efficiently compressed before being propagated to the next epoch. This provides a memory efficient mechanism for passing sample specific masks. 4) Systematic evaluation -Experiments on seven vastly different biomedical datasets suggest that FANet outperforms other state-of-the-art (SOTA) algorithms. 5) Efficient training -FANet achieves near SOTA performance with far fewer training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we summarize relevant advances in medical image segmentation and feedback attention networks. We also highlight recent contributions to iterative refinement methods for image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Biomedical image segmentation</head><p>The basis of most modern CNN-based semantic segmentation architectures are either Fully Convolutional Network (FCN) <ref type="bibr" target="#b11">[12]</ref> or an encoder-decoder architecture such as U-Net <ref type="bibr" target="#b4">[5]</ref> originally designed for cell segmentation. Various modifications of these networks have been proposed both for semantic segmentation of natural images <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and biomedical image segmentation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b19">[20]</ref>. In general, in the encoder, the image content is encoded using multiple convolutions to capture from low-level to high-level features, whereas in the decoder part of the network the prediction masks are obtained by multiple upsampling mechanism or deconvolution operations. Methods like PSPNet <ref type="bibr" target="#b12">[13]</ref> and DeepLab <ref type="bibr" target="#b0">[1]</ref> incorporate convolutional feature maps of varying resolutions to segment both small and large-sized objects effectively. While PSPNet used a pyramid pooling module, DeepLab used Atrous Spatial Pyramidal Pooling (ASPP) for encoding the multi-scale contextual information. Both PSPNet and DeepLab based architectures have been used widely in the medical imaging community for biomedical image segmentation <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feedback attention networks</head><p>Visual attention has been widely used in computer vision for pose estimation <ref type="bibr" target="#b22">[23]</ref>, object detection <ref type="bibr" target="#b23">[24]</ref>, and image segmentation <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Chu et al. <ref type="bibr" target="#b22">[23]</ref> incorporated the multi-context attention method into their end-to-end eight stack hourglass CNN network where each sub-network of the hourglass generated a multi-resolution attention map. Attention mechanisms <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> have also been utilized for posing explicit focus on the target region in medical imaging. Schlemper et al. <ref type="bibr" target="#b27">[28]</ref> proposed a novel attention gate model that automatically learned to focus on the target structure of the varying shape and sizes by suppressing the irrelevant features and highlighting the silent feature for the specified medical image segmentation task. Attention U-Net <ref type="bibr" target="#b8">[9]</ref> used a gated operation in the U-Net architecture to focus on the target abdominal regions of CT datasets. Feedback mechanism for attention using two U-Net architectures with shared weights was used for cell segmentation <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. The latter used a standard U-Net architecture with the second U-Net incorporating ConvLSTM <ref type="bibr" target="#b30">[31]</ref> to store the feature map (inputto-state) from the first U-Net network. However, feedback is only applied to the same epoch with state-to-state transitions. On the contrary, our approach utilizes a feedback mechanism that propagates information flow from the previous epoch to the current epoch in an attention mechanism. We employ the predicted masks from the previous epoch as hard attention to prune the segmentation output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Iterative refinement for segmentation</head><p>An iterative refinement of the segmentation mask by feeding the input image and the predicted segmentation mask to a modified U-Net architecture was done by Mosinska et al. <ref type="bibr" target="#b31">[32]</ref>. In this work, the authors used an iterative refinement pipeline to enhance the quality of the predicted segmentation mask. Similarly, iterative update of latent space and minimization of the Structure Similarity Index Measure (SSIM) loss was used to refine the predicted segmentation maps during test time in <ref type="bibr" target="#b32">[33]</ref>. Recently, iterative refinement strategies have also been used for pose estimation <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> that used consecutive modules for refinement of the predictions with a loss function for the evaluation of output in each module. These iterative refinement processes show improved predictions and are able to handle domain shifts or object shape variability without requiring very deep networks <ref type="bibr" target="#b32">[33]</ref>. However, a major bottleneck in these methods is the requirement of a large number of iterations for model convergence. Unlike these methods, our proposed FANet provides attention to the specific regionof-interest and can prune the predicted segmentation masks in less than ten iterations without requiring any optimization scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we describe the components of the proposed FANet architecture. The overall design along with the proposed feedback attention learning mechanism is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SE-Residual block</head><p>Deeper networks improve the performance of the model significantly, but an increase in depth can cause either vanishing or exploding gradients problem <ref type="bibr" target="#b42">[43]</ref>. To deal with this, we take advantage of shortcut connections between layers in the residual learning paradigm. Our SE-Residual block uses two 3?3 convolutions and an identity mapping, where each convolution layer is followed by a Batch Normalization (BN) layer and a Rectified Linear Unit (ReLU) non-linear activation function. The identity mapping is used to connect the input and the output of the convolution layer <ref type="figure" target="#fig_0">(Figure 2 a)</ref>.</p><p>Similar to the work by Hu et al. <ref type="bibr" target="#b43">[44]</ref>, we add a SE layer in the residual network. The SE layer acts as a content-aware mechanism that re-weights each channel accordingly to create robust representations. Hence, it allows the network to become more sensitive to significant features while suppressing irrelevant features. This goal is accomplished in two steps. First, the feature maps are squeezed by using the global average pooling to get a global understanding of each channel. The squeeze operation results in a feature vector of size n, where n refers to the number of channels. In the second step: excitation, this feature vector is feed through a two-layered feed-forward neural network, where the number of features is first reduced and then expanded to the original size n. Now, this n sized vector represents the weight of the original feature maps, which is used to scale each channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MixPool block</head><p>The proposed MixPool block shown in <ref type="figure" target="#fig_0">Figure 2</ref> (b) is used in multiple layers of our FANet architecture. This block facilitates the flow of sample-wise feedback information between consecutive epochs providing a hard attention to the learned features from SE-Residual block. The layer provides focus to the relevant features in both contraction path and expansion path layers. The 'hard' attention map consists of the values 0 and 1, i.e., attention to a specific region only unlike soft attention where the probability map is estimated. The advantage of hard attention it that it allows to keep only the important features and ignore irrelevant features. During the element-wise multiplication, the values from the input feature map, if multiplied by 0, becomes 0, leaving the essential features for further operations. Another advantage of such methods is their computational speed, scalability, and ease of interpretation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b44">[45]</ref>. The input mask used during training is compressed using the run-length encoding technique to save the memory footprint.</p><p>As in <ref type="figure" target="#fig_0">Figure 2</ref> (b-c), fist feature maps from the SE-Residual blocks F l in each layer is passed through a 3 ? 3 convolution followed by a BN and a ReLU activation function. Then, we apply a 1 ? 1 convolution and a sigmoid activation function ?(?) with a threshold of 0.5 to obtain the binary mask M ? l to contribute to the spatial attention map generation given by:</p><formula xml:id="formula_0">M ? l = ? (conv (F l )) = 1, if ?(?) ? 0.5 0, otherwise.<label>(1)</label></formula><p>Secondly, we apply appropriate max-pooling on the input mask (from the previous epoch) and resize it to the size of the spatial attention map M ? l . A union operation is then applied between the resized mask and the spatial attention map. This confirms that we obtain the feature from both the feedback and the spatial attention maps to further create a new unified spatial attention map. Next, an element-wise multiplication operation is applied between the unified mask and the original feature map that suppresses the irrelevant features and enhances the important ones. The enhanced and the original feature maps are then followed by a 3 ? 3 convolution, BN, and a ReLU. These operations are used to improve the network's ability to learn non-linearity in the model prediction.</p><p>Finally, we concatenate the output of both activation functions, which constitutes the output of our MixPool block given by:</p><formula xml:id="formula_1">Output MixPool = F ? ? l F l ? (M l ? M ? l ) ? ,<label>(2)</label></formula><p>where ? denotes the concatenation operator, ? is elementwise multiplication, and ? represents the union operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proposed FANet architecture</head><p>The block diagram of FANet is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> (c). It uses an encoder-decoder design common to many semantic segmentation architectures. We combine the strength of a residual network enhanced with SE as SE-Residual block, and MixPool block that facilitates the attention and propagation of information flow from the current learning paradigm and that of the previous epoch. We implement a recurrent learning mechanism in both encoder and decoder layers that allows to achieve efficient segmentation. The MixPool block uses the previous segmentation map (as an input mask through RLE encoding), which contains the information from prior training and uses it to improve the semantic representation of the feature maps.</p><p>We first use the Otsu thresholding <ref type="bibr" target="#b45">[46]</ref> to generate an initial input mask for training the proposed architectural model. The variability in the input mask is refined over the training epochs and the model learns over time to prune input or previous epoch masks with learned semantically meaningful features together. To achieve this, we use the novel MixPool block that uses the input mask and applies hard attention over the subsequent input feature maps. The hard attention enables the network to highlight semantically meaningful features for the target region-of-interest in the entire network. The network thus not only learns to predict features maps but also strengthens a joint pruning mechanism that is dependent on the input mask. As a result, the devised network is able to rectify the predicted segmentation maps in an iterative fashion unlike conventional methods which do not have such pruning ability. This provides a strong rational behind our work that is applicable beyond single step inference prediction with capability of refining prediction maps.</p><p>The proposed network architecture is a Fully Convolutional Neural Network (FCNN) consisting of four encoder and four decoder blocks. The encoder takes the input image, downsamples it gradually, and encodes it in a compact representation.</p><p>Then, the decoder takes this compact representation and tries to reconstruct the semantic representation by gradually upsampling it and combining the features from the encoder. Finally, we receive a pixel-wise categorization of the input image. Both the encoder and the decoder are built using the SE-Residual block, and an additional concatenation of the original resolution feature representation in the encoder is added at each resolution scale. This mechanism minimizes the loss of feature representations during downscaling and upscaling processes.</p><p>Each encoder network starts with two SE-Residual blocks, which consist of two 3 ? 3 convolutions and a shortcut connection known as identity mapping, connecting the input and output of the two convolution layers. Each convolution is followed by a BN and a ReLU activation function. The output of the second SE-Residual block acts as skip connection for the corresponding decoder block. After that, it is followed by the MixPool block, which has the previous epoch segmentation mask and provides a hard-attention over the incoming feature maps. This process is repeated for each of the downscaled layers.</p><p>Each decoder network starts with a 4 ? 4 transpose convolution that doubles the spatial dimensions of the incoming feature maps. These feature maps are concatenated with feature maps from the corresponding encoder block through skip connections. The skip connections help to propagate the information from the upper layers, which are sometimes lost due to the depth of the network. The skip connections are followed by two SE-Residual blocks, which help to eliminate the problem of vanishing gradient. The MixPool block that utilizes the segmentation mask from the previous epoch is then applied creating a hard-attention over the learned feature maps. Next, we concatenate the feature maps from the last decoder block and the segmentation mask from the previous epoch. Finally, we apply a 1 ? 1 convolution with the sigmoid activation function. The output of this is used to both minimize the training loss, using a combined binary cross-entropy and dice loss, and to generate segmentation masks that are stored as a run-length encoded compression for each sample and propagated during the next epoch. The RLE is updated after each epoch. Similarly, the network learns to adapt the weights in iterative training, this mechanism is also utilized during the test time. As shown in <ref type="figure">Figure 1</ref>, test results are pruned in a few iterations during the test time. Unlike many methods in literature <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, we utilize the same network without any complementary loss function optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Setup 1) Dataset and Evaluation Metrics:</head><p>To evaluate the proposed architecture, we have selected seven datasets that capture different segmentation tasks in biomedical imaging. The details of each dataset can be found in <ref type="table" target="#tab_0">Table I</ref>. The dataset images contain the images of organs and lesions acquired under different imaging protocols. For the retina vessel segmentation task, we use DRIVE and CHASE-DB1 datasets. These two datasets are aimed at various diseases related to diseases of retina vessels, such as retinopathy, retinal vein occlusion, and retinal artery occlusion. The ISIC 2018 dataset, which is a dermoscopy dataset that is useful in the diagnosis of skin cancer, is the third dataset focused on medical imaging data. This dataset contains a wide variety of skin cancer images of different sizes and shapes, which helps in a better understanding of the disease. We have further included Kvasir-SEG and CVC-ClinicDB colonoscopy datasets. These datasets contain the image frames extracted from different colonoscopy interventions and are focused on colorectal polyps that are one of the cancer precursors in the colon and rectum. It highly increases the chance of avoiding lethal cancer by early detection. In addition, we have included two datasets acquired from biological imaging aimed at understanding of the cellular processes. These include the 2018 Data Science Bowl and the EM datasets. The 2018 Data Science Bowl dataset contains images with a large number of variable shaped nuclei acquired from different cell types, magnification, and imaging modalities. This dataset is designed for automated nuclei segmentation. Similarly, the EM dataset contains the transmission EM images of the neural structures of the Drosophila nerve cord. This dataset is aimed at the automated segmentation of the neural structures. All experiments on these datasets are conducted on the same train, validation, and test splits as provided by the previously published works reported in this paper.</p><p>To evaluate SOTA deep learning methods and our proposed FANet, we have used standard evaluation metrics that includes Dice Coefficient (DSC) (a.k.a. F1), mean Intersection over Union (mIoU), precision, and recall. We have additionally calculated specificity for those datasets where this metric was previously used for benchmarking.</p><p>2) Implementation details: All the training is performed on a Volta 100 GPU and an NVIDIA DGX-2 system using the PyTorch 1.6. framework. For test inference, we have used an NVIDIA GTX 1050 Ti GPU for our method and all SOTA methods used in the paper as this hardware is widely available. Our model is trained for 100 epochs (empirically set) using an Adam optimizer with a learning rate of 1e ?4 for all the experiments except for the Digital Retinal Images for Vessel Extraction (DRIVE) and the CHASE-DB1 dataset where the learning rate was adjusted to 1e ?3 due to the small size of the training dataset. Datasets were chosen such that the efficiency of our model could be compared to the SOTA methods. A combination of binary cross-entropy and dice loss has been used as the loss function. ReduceLROnPlateau callback was used to monitor the learning rate and adjust it to obtain optimal training performance. All the images used in the  3) Ablation study: In order to evaluate the strength of our proposed FANet architecture, we perform a thorough ablation study. For this, we have used all seven datasets and evaluated on several metrics for baseline (FANet without MixPool), baseline with MixPool, and the combination of baseline, MixPool, and feedback (proposed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>Below we present quantitative results on seven different biomedical imaging datasets and compare with corresponding SOTA methods.</p><p>1) Results on Kvasir-SEG: Kvasir-SEG <ref type="bibr" target="#b35">[36]</ref> is a publicly available polyp segmentation dataset acquired from clinical colonoscopy procedures. This dataset has been widely used for algorithm benchmarking. We have trained our model and compared it with recent SOTA methods on Kvasir-SEG. A comparison with widely accepted segmentation methods with different backbones (see <ref type="table" target="#tab_0">Table II</ref>) shows that our approach is improved performance compared to the SOTA methods (on the same train-test split). Our FANet outperforms all the SOTA methods on almost all metrics. While outperforming most U-Net and its variants, it can be observed that FANet achieved an F1 score of 0.8803, which is 1.6% and 3.57% better than the most accurate DeepLabv3+ with ResNet101 backbone and the recent HRNet.   <ref type="bibr" target="#b51">[52]</ref> 0.6790 0.5810 0.7920 0.9280 -Attention R2U-Net <ref type="bibr" target="#b51">[52]</ref> 0.6910 0.5920 0.7260 0.9710 -BCDU-Net (d=1) <ref type="bibr" target="#b52">[53]</ref> 0.8470 -0.7830 0.9800 -BCDU-Net (d=3) <ref type="bibr" target="#b52">[53]</ref> 0.8510 -0.7850 0.9820 -U-Net++ <ref type="bibr" target="#b48">[49]</ref> 0 2) Results on CVC-ClinicDB dataset: CVC-ClinicDB is another commonly used dataset for colonoscopy image analysis. FANet architecture outperforms all the SOTA methods on this dataset by a large margin with F1 of 0.9355, mIoU of 0.8937, recall of 0.9339, and precision of 0.9401 (see <ref type="table" target="#tab_0">Table III</ref>). FANet achieves the best trade-off between recall and precision compared to the ResUNet-based architectures <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b46">[47]</ref>. The strength of the FANet can be observed by the large improvement of 23.17% in the recall and 5.24% in the precision over the SOTA ResUNet++ <ref type="bibr" target="#b17">[18]</ref>. The recall suggests that our method is more clinically preferable than the SOTA. A higher recall is desired in the systems used for clinical diagnosis <ref type="bibr" target="#b50">[51]</ref>.</p><p>3) Results on 2018 Data Science Bowl: Cell nuclei segmentation in microscopy imaging is a common task in the biological image analysis <ref type="bibr" target="#b37">[38]</ref>. We used the publicly available 2018 Data Science Bowl (DSB) challenge dataset and compared our results with the SOTA methods. <ref type="table" target="#tab_0">Table IV</ref> shows that FANet produces an F1 of 0.9176, mIoU of 0.8569, and recall of 0.9222 with an improvement of 2.02% in F1 with respect to SOTA UNet++ <ref type="bibr" target="#b15">[16]</ref> and 28.15% improvement in recall compared to the best performing DoubleU-Net <ref type="bibr" target="#b18">[19]</ref>. In general, FANet achieves the best trade-off between precision and recall compared to the SOTA methods resulting in the highest F1 score (0.9176). The qualitative results with 2018 DSB also show that the predicted FANet produces high-quality segmentation masks for cell nuclei with respect to the ground truth (see <ref type="figure">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Results on ISIC 2018 dataset:</head><p>Skin cancer is one of the most commonly diagnosed cancers in the US. Early detection of melanoma can improve the five-year survival rate and help prevent it in 99% of the cases <ref type="bibr" target="#b53">[54]</ref>. <ref type="table" target="#tab_4">Table V</ref> shows the results on the publicly available International Skin Imaging Collaboration (ISIC) 2018 dataset. FANet outperformed all the methods on almost all evaluation metrics (F1, mIoU, and recall). FANet achieved 0.8731 on F1 and recall of 0.8650 with an improvement of 2.21% and 8.00%, respectively, over the most accurate SOTA BCDU-Net (d=3) method. A competitive  specificity and precision were also recorded. From the qualitative results in <ref type="figure">Figure 3</ref>, we can see that the input mask produced by Otsu thresholding shows under segmentation, which is improved significantly using FANet. The masks produced by FANet have smooth boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Results on DRIVE dataset:</head><p>The automated segmentation of vessels in fundus images can assist in the diagnosis and treatment of diabetic retinopathy. The quantitative result on the publicly available DRIVE dataset is presented in <ref type="table" target="#tab_0">Table VI</ref>. We can observe that the proposed FANet achieves an F1 score of 0.8183, mIoU of 0.6927, recall of 0.8215, and precision of 0.8189. The proposed method achieves an improvement of 4.24% in the recall over SOTA IterNet <ref type="bibr" target="#b56">[57]</ref>. Although the F1 of the IterNet is 0.35% higher than FANet, the recall is relatively lower, and other metrics such as mIoU and precision are not presented. For our proposed FANet, the precision of 0.8189 is well balanced with the obtained recall. The higher recall produced by FANet shows that our method is more clinically relevant. The quality of the segmentation masks in Figure3 demonstrates the efficiency of FANet. 6) Results on CHASE-DB1 dataset: CHASE-DB1 is the second retinal image segmentation dataset used to evaluate our method. For this dataset, there is no official training and test split. We have used 20 images to train our model and 8 images to test as reported in the work of Li et al. <ref type="bibr" target="#b56">[57]</ref>. From Table VII, we can observe that our method achieved the highest F1 of 0.8108, mIoU of 0.6820, and the highest recall of 0.8544. FANet achieved an improvement of 3.67% in the recall compared to the SOTA DenseBlock-UNet. 7) Results on EM dataset: The EM dataset aims to develop an automatic ML algorithm for the segmentation of the neural structures so that difficulties due to manual labeling can be resolved. <ref type="table" target="#tab_0">Table VIII</ref> shows the quantitative results on the EM dataset. The proposed FANet also obtains F1 of 0.9547, mIoU of 0.9134, and a recall of 0.9568. The presented results <ref type="figure">Fig. 3</ref>: Qualitative results of FANet on seven biomedical image segmentation datasets. The initial "input mask" is generated using Otsu thresholding. The "output mask" is the predicted segmentation mask from the FANet model. demonstrate that FANet produces SOTA results, surpassing other recent methods in terms of mIoU metric that was used by other methods for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative results</head><p>The qualitative results on all seven datasets are presented in <ref type="figure">Figure 3</ref>. It can be observed that for colonoscopy datasets (Kvasir-SEG and CVC-ClinicDB), even though the initial input mask covers the entirety of the image, our model is able to prune and provide accurate masks. The same can be observed for the two retina vessel segmentation datasets, DRIVE and CHASE-DB1. It can be observed that our model is able to segment the challenging retinal vessels, including small retinal vessel bifurcations, and it well resembles the ground truth mask. For the 2018 DSB, ISIC-2018, and EM cell data, again, the input masks are finely rectified, achieving close to ground truth results by the proposed FANet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation study</head><p>In this section, we ablate our model architecture and present extensive experimental results related to the effectiveness of the proposed FANet. To evaluate the contribution of the MixPool block and the feedback, we created the following configurations:</p><p>1) Baseline (B1): It refers to the FANet without the MixPool block, which means "no feedback mechanism" or "iterative pruning". We require the MixPool block to provide feedback as it unifies the attention from the network feature map and input mask (refer to <ref type="figure" target="#fig_0">Figure 2 (b)</ref>). 2) Baseline + MixPool (B2): We integrate the MixPool block in all the encoder blocks and decoder blocks. During the inference, we directly apply the trained model weights with the Ostu thresholding (initial input mask) only once, i.e., no iterative pruning is used. 3) Baseline + MixPool(E1, D4) + Feedback (B3): Here, we integrate the MixPool block in the first encoder block and the last decoder block. Feedback (iterative pruning) is used during the inference. 4) Baseline + MixPool + Feedback (B4): This is the final FANet architecture, with MixPool block in all encoder and decoder blocks and the feedback (iterative pruning) mechanism is used during the inference. <ref type="table" target="#tab_0">Table IX</ref> presents the ablation results on these four configurations performed on all seven datasets. Below we provide detailed analyses of the use of different model architectural settings and validate them with the above described four network configurations (B1-B4):</p><p>1) Effectiveness of MixPool block: The MixPool block is an essential part of the proposed FANet architecture. It uses the previously predicted mask as the attention to improve the semantically meaningful features and allows higher-level abstractions. The effectiveness of the MixPool block can be evaluated by comparing the network configurations B1 and B4.</p><p>From the experiments in <ref type="table" target="#tab_0">Table IX</ref>, we can conclude that the B4 outperforms the B1 on all the datasets. On the F1 metric, B4 shows an improvement of 2.87% on the Kvasir-SEG dataset, 1.89% improvement on the CVC-ClinicDB, 0.55% improvement on the 2018 Data Science Bowl dataset, 0.84% improvement on the ISIC 2018 dataset, 0.11% improvement on the DRIVE dataset, 2.92% improvement on the CHASE-DB1 dataset, and a 0.03% improvement on the EM dataset. These performance gains are significant and thus demonstrate the effectiveness of the use of MixPool block in the proposed FANet.</p><p>TABLE IX: Detailed ablation study of the FANet architecture. Flop is calculated in terms of GMac. "Rec" stands for Recall, "Prec" stands for precision, "Spec" stands for Specificity, "Acc" stands for Accuracy, and "Param" stands for total number parameters. B1 -B4 denote different network configurations. 2) Optimum position of MixPool block in FANet architecture: The positioning of the MixPool is an important factor determining the performance of the model. In the FANet (B4), we integrate the MixPool block in all the encoder blocks and the decoder blocks. In B3, we integrate MixPool block in the first encoder block and the last decoder block only. To evaluate the effectiveness of the integrating MixPool block, we compare B3 with B4 in <ref type="table" target="#tab_0">Table IX</ref>. It can be observed that out of the seven datasets, on three datasets, i.e., Kvasir-SEG, CVC-ClinicDB, and 2018 Data Science Bowl, a significant improvement in B4 is observed as compared to the B3. On the F1 metric, we can observe that B4 achieves an improvement of 3.43 % on Kvasir-SEG, 1.93 % on CVC-ClinicDB, 0.11 % on the 2018 Data Science Bowl, and 0.5 % on the EM dataset.</p><p>3) Significance of feedback during evaluation: The proposed architecture uses the feedback information (input mask) while training. This feedback mechanism is also used during the evaluation for iterative pruning the predicted the mask. To evaluate the effectiveness of the feedback mechanism, we compare the B2 (FANet without feedback) with the B4 (FANet with feedback) in <ref type="table" target="#tab_0">Table IX</ref>. On all the datasets, we used feedback during inference and compared its performance with the model without feedback. We can observe that the majority of performance gains in mIoU and F1. For Kvasir-SEG, B4 shows a 17.75 % improvement in the mIoU, 15.01 % improvement in the F1, and a 20.76 % improvement in the Recall. Likewise, on the CVC-ClinicDB, we can see that B4 has 3.96 % improvement in mIoU and 2.27 % improvement in the F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Algorithm efficiency</head><p>We have analyzed the algorithm efficiency in terms of the number of parameters, flops, and inference time for SOTA methods and FANet (see <ref type="table" target="#tab_10">Table X</ref>). During the architectural design, we limit the number of trainable parameters in order to minimize the computational cost of our model. The proposed FANet has only 7.72 million parameters and 94.75 GMac flops, i.e. FANet has the least number of parameters and flops as compared to other deeper architectures. However, our inference time is higher than the other baseline networks which is due to the introduction of novel MixPool block in the FANet that incorporates additional operations such as element-wise multiplication from the readout of the RLE encoded mask that resulted in larger computational time. However, in terms of FPS per iteration this is still above 60 (see <ref type="table" target="#tab_0">Table IX</ref>). In the FANet, the MixPool block facilitates attention and propagation of information flow from the current learning paradigm and that of the previous epoch, which helps to achieve a performance boost (refer <ref type="table" target="#tab_0">Table IX</ref>). To verify the efficiency of the MixPool block, we have compared our network with and without the MixPool block in <ref type="table" target="#tab_0">Table IX</ref>. It is also evident that removing the MixPool block reduces the overall performance in all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Extended ablation study</head><p>We have performed an extended ablation study to demonstrate the architectural effectiveness of the proposed FANet. Here, we begin with the experimental verification of the MixPool block by removing its certain components. From the <ref type="table" target="#tab_0">Table XI</ref>, we can observe a performance drop, F1 drops by 22.76% and mIoU drops by 26.3% when feature map F l is not used during feature concatenation. In order to justify the use of TABLE XI: Extended ablation study demonstrating effectiveness of our proposed FANet architecture on the Kvasir-SEG dataset. Here, we ablate our network using different configuration that includes: (i) removing F l of MixPool block in FANet <ref type="figure" target="#fig_0">(Figure 2 (b)</ref>), (ii) removing and adding SE-Residual networks in FANet <ref type="figure" target="#fig_0">(Figure 2 (c)</ref>), and (iii) series concatenation of FANet in contrast to iterative mechanism. Further, we modified the FANet architecture by adding three SE-Residual blocks and we observed again a decrease in the performance. For this case, F1 drops by 2.33% and mIoU by 2.45%. Next, we added one more SE-Residual block and a severe performance drop can be observed. The F1 dropped by 6.45% and the mIoU drops by 7.36%. In our proposed architecture we used an iterative pruning. However, we experimented an alternative strategy by concatenating the four FANet together in a series. From this experiment we observed a drop of F1 by 1.21% and 1.49% drop in mIoU with nearly four times increase in the number of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>While deep learning semantic segmentation has been widely implemented, to the best of our knowledge, only direct inference strategies have been published till date. In this work, we utilize a segmentation map pruning mechanism that demonstrates a clear advantage over the current SOTA models due to its ability to self-rectify the predicted mask during the evaluation process (see <ref type="table" target="#tab_0">Table II</ref>- <ref type="table" target="#tab_0">Table VIII</ref>). The process of self-rectification or iterative pruning helps to improve the performance of the proposed FANet architecture. This improvement is due to the feedback provided by the input mask in the MixPool block which is further validated from our two ablation studies <ref type="table" target="#tab_0">(Table IX and Table XI</ref>). Furthermore, a joint configuration together with mask and the feature embeddings allow learning to achieve better feature representation of target regions and learning to adjust weights dependent on the input mask. This establishes an effective pruning mechanism of the network enabling input mask to be steered in the direction of the relevant learned features of the network. Additionally, it can capture the variability in datasets (e.g., shape distributions, surface morphology etc.), allowing the network to rectify the predicted/input masks. <ref type="table" target="#tab_0">Table IX</ref> shows the complete ablation study of the Mix-Pool block in the FANet architecture. In this ablation, we For each dataset, we have included three diverse images. The provided heatmaps demonstrate the impact of the weights for different networks. Here, red and yellow regions in the heat map refer to the most important features, and the blue region refers to the region of less importance. From the heat map, it can be observed that FANet has a better feature representation than other baseline networks for most of the datasets. F l represents the input feature map in the MixPool block (refer <ref type="figure" target="#fig_0">Figure 2)</ref>. provide experimental results with (proposed network, B4) and without the MixPool block (B1). Here, B1 refers to the "no feedback mechanism" as no MixPool block is applied. In the proposed FANet, we require the MixPool block to provide feedback through a unified attention mechanism taking into account the network feature map and the input mask from the previous epoch (refer to <ref type="figure" target="#fig_0">Figure 2 (b)</ref>). However, for the MixPool block without feedback (i.e., B2), we provide the attention from the generated feature map and the input mask but we do not perform the iterative pruning during the evaluation. Thus, even though B2 and B4 networks have the same number of parameters (7.72 Million parameters), the removal of the feedback mechanism affects the algorithm performance (see <ref type="table" target="#tab_0">Table IX</ref>). SE-Residual blocks that serve as a self-attention mechanisms on feature channels by performing global average pooling followed by multi layer perceptron that allows to explicitly model the interdependencies between feature channels. Further, in our network we introduce spatial attention mechanisms. Multiple SE-residual blocks allow to learn complex non-linear feature interdependencies (also see <ref type="table" target="#tab_0">Table XI</ref> for different combinations of SE-Residual blocks). Further, other ablation experiments such as series concatenation of FANet and removal of F l layer in MixPool block in <ref type="table" target="#tab_0">Table XI</ref> showed that the proposed FANet achieves the highest performance. This justifies the importance of different components integrated in the proposed FANet architecture. Further, qualitative results in <ref type="figure" target="#fig_1">Figure 4</ref> demonstrate the effectiveness of our network over different configurations, for example, removing of F l layer in mixpool block and using only one SE-Residual block. Also, it can be observed that FANet has more apparent segmentation maps, that is easily distinguishable regions from the background, than the SOTA methods.</p><p>With the introduction of the iterative pruning in our FANet architecture, we introduce a new hyperparameter, i.e., the number of iterations during the evaluation. The optimal number of iterations is 10, which was empirically established across datasets. The computed number of iterations is the same for all datasets. For this, we have plotted a graph ( <ref type="figure" target="#fig_2">Figure 5</ref>) showing the iterative pruning on different dataset images. From the graph, it is observed that there is a significant improvement from iteration 1 to 5. However, from 5 to 10 iterations, there is a minor to negligible improvement. Thus, we considered the highest of 10 iterations during the evaluation. The iterative pruning over the input image increases the inference time. However, this process allows us to refine the predicted segmentation masks, unlike most current methods. For obtaining a better trade-off between efficiency and accuracy, we advise using a lesser number of iterations. We plot the F1 score for different dataset images for five iterations during evaluation. <ref type="figure" target="#fig_2">Figure 5</ref> shows that our proposed FANet benefits with just two iterations. Additionally, we have used the NVIDIA GTX 1050 Ti (released in 2016) for inference, and thus using a more recent GPU with higher performance can provide better inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>With the FANet architecture, we proposed a novel approach for biomedical image segmentation that can self-rectify the predicted masks. By introducing a feedback mechanism, we achieved an improvement on seven publicly available biomedical datasets when compared with existing SOTA methods. Our approach requires far fewer epochs for training and is wellsuited to diverse biomedical imaging datasets. The feedback mechanism integrated in the FANet design effectively acts as hard attention that is used with the existing feature maps to boost the strength of feature representations. The experimental results demonstrate that the proposed architecture achieves accurate and consistent segmentation results across several biomedical imaging datasets despite its simple and straightforward network architecture. The ablation study also reveals that FANet requires less training time to achieve near SOTA performance. In the future, we will use a contrastive learning approach to improve the performance of FANet further and test it on additional multimodal biomedical images. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>FANet with squeeze and excite residual (SE-Residual) block and MixPool block. (a) SE-Residual block integrated with squeeze and excite layer uses 1 ? 1 convolution to concatenate the high-resolution feature representation with the the encoded feature vector. (b) MixPool block represents attention mechanism in our network. The input mask is downscaled to the corresponding layer feature map size M l which is fused with the masked feature map representation M ? l for hard attention of input feature in that layer F l . Finally, the attenuated feature map F M ? l and the feature maps F ? l are both concatenated. c) Proposed FANet showing the complete network architecture. Encoder-decoder architecture with skip-connections (in dotted arrows) from SE-Residual blocks to preserve high-and intermediate resolution feature representations and MixPool block connections (with solid arrows) that allow to feedback the previous mask predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Comparison of the intermediate feature map of the different networks on the Kvasir-SEG, 2018 Data Science Bowl and DRIVE datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Iterative pruning on different dataset images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>ACKNOWLEDGMENTD.</head><label></label><figDesc>Jha is funded by the Research Council of Norway project number 263248 (PRIVATON). S. Ali is supported by the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre (BRC). The computations in this paper are performed on equipment provided by the Experimental Infrastructure for Exploration of Exascale Computing (eX3), which is financially supported by the Research Council of Norway under contract 270053. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR, or the Department of Health.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Details of the biomedical datasets used in our experiments. "Train", "Train after aug." and "Test" denote the number of training samples, number of training samples after image augmentation, and number of test samples, respectively.</figDesc><table><row><cell>Dataset</cell><cell>Images</cell><cell>Size</cell><cell>Train</cell><cell>Train after aug.</cell><cell>Test</cell><cell>Application</cell></row><row><cell>Kvasir-SEG [36]</cell><cell>1000</cell><cell>Variable</cell><cell>880</cell><cell>16720</cell><cell>120</cell><cell>Colonoscopy</cell></row><row><cell>CVC-ClinicDB [37]</cell><cell>612</cell><cell>384 ? 288</cell><cell>490</cell><cell>14210</cell><cell>61</cell><cell>Colonoscopy</cell></row><row><cell>2018 Data Science Bowl [38]</cell><cell>670</cell><cell>256 ? 256</cell><cell>335</cell><cell>10720</cell><cell>134</cell><cell>Nuclie</cell></row><row><cell>ISIC 2018 (Lesion Boundary Segmentation) [39], [40]</cell><cell>2596</cell><cell>Variable</cell><cell>1815</cell><cell>39930</cell><cell>259</cell><cell>Dermoscopy</cell></row><row><cell>EM dataset [4]</cell><cell>30</cell><cell>512 ? 512</cell><cell>24</cell><cell>384</cell><cell>3</cell><cell>Cell</cell></row><row><cell>DRIVE Database [41]</cell><cell>40</cell><cell>584 ? 565</cell><cell>20</cell><cell>640</cell><cell>20</cell><cell>Retina Vessel</cell></row><row><cell>CHASE-DB1 [42]</cell><cell>28</cell><cell>584 ? 565</cell><cell>20</cell><cell>640</cell><cell>8</cell><cell>Retina Vessel</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Results on the Kvasir-SEG<ref type="bibr" target="#b35">[36]</ref>.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Prec.</cell></row><row><cell>U-Net [5]</cell><cell>-</cell><cell>0.5969</cell><cell>0.4713</cell><cell>0.6171</cell><cell>0.6722</cell></row><row><cell>ResUNet [47]</cell><cell>-</cell><cell>0.6902</cell><cell>0.5721</cell><cell>0.7248</cell><cell>0.7454</cell></row><row><cell>ResUNet++ [18]</cell><cell>-</cell><cell>0.7143</cell><cell>0.6126</cell><cell>0.7419</cell><cell>0.7836</cell></row><row><cell>FCN8 [12]</cell><cell>VGG 16</cell><cell>0.8310</cell><cell>0.7365</cell><cell>0.8346</cell><cell>0.8817</cell></row><row><cell>HRNet [14]</cell><cell>-</cell><cell>0.8446</cell><cell>0.7592</cell><cell>0.8588</cell><cell>0.8778</cell></row><row><cell>DoubleU-Net [19]</cell><cell>VGG 19</cell><cell>0.8129</cell><cell>0.7332</cell><cell>0.8402</cell><cell>0.8611</cell></row><row><cell>PSPNet [13]</cell><cell>ResNet50</cell><cell>0.8406</cell><cell>0.7444</cell><cell>0.8357</cell><cell>0.8901</cell></row><row><cell>DeepLabv3+ [48]</cell><cell>MobileNet</cell><cell>0.8425</cell><cell>0.7575</cell><cell>0.8377</cell><cell>0.9014</cell></row><row><cell>DeepLabv3+ [48]</cell><cell>ResNet50</cell><cell>0.8572</cell><cell>0.7759</cell><cell>0.8616</cell><cell>0.8907</cell></row><row><cell>DeepLabv3+ [48]</cell><cell>ResNet101</cell><cell>0.8643</cell><cell>0.7862</cell><cell>0.8592</cell><cell>0.9064</cell></row><row><cell>U-Net [5]</cell><cell>VGG19</cell><cell>0.7535</cell><cell>0.6571</cell><cell>0.7364</cell><cell>0.8565</cell></row><row><cell>U-Net++ [49]</cell><cell>-</cell><cell>0.8002</cell><cell>0.7000</cell><cell>0.8716</cell><cell>0.7992</cell></row><row><cell>Attention U-Net [9]</cell><cell>-</cell><cell>0.7944</cell><cell>0.6959</cell><cell>0.8383</cell><cell>0.8287</cell></row><row><cell>FANet</cell><cell>-</cell><cell>0.8803</cell><cell>0.8153</cell><cell>0.9058</cell><cell>0.9005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Results on the CVC-ClinicDB<ref type="bibr" target="#b36">[37]</ref>.</figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>U-Net (MICCAI'15) [5]</cell><cell>0.8230</cell><cell>0.7550</cell><cell>-</cell><cell>-</cell></row><row><cell>ResUNet-mod [47]</cell><cell>0.7788</cell><cell>0.4545</cell><cell>0.6683</cell><cell>0.8877</cell></row><row><cell>ResUNet++ [18]</cell><cell>0.7955</cell><cell>0.7962</cell><cell>0.7022</cell><cell>0.8785</cell></row><row><cell>SFA (MICCAI'19) [50]</cell><cell>0.7000</cell><cell>0.6070</cell><cell>-</cell><cell>-</cell></row><row><cell>PraNet [17]</cell><cell>0.8990</cell><cell>0.8490</cell><cell>-</cell><cell>-</cell></row><row><cell>U-Net++ [49]</cell><cell>0.9377</cell><cell>0.8890</cell><cell>0.9405</cell><cell>0.9432</cell></row><row><cell>Attention U-Net [9]</cell><cell>0.9325</cell><cell>0.8856</cell><cell>0.9276</cell><cell>0.9546</cell></row><row><cell>FANet</cell><cell>0.9355</cell><cell>0.8937</cell><cell>0.9339</cell><cell>0.9401</cell></row></table><note>study were resized to 512 ? 512 except for the 2018 Data Science Bowl and the CVC-ClinicDB dataset, where images were resized to 256?256. Data augmentation, such as random crop, flipping, rotation, elastic transformation, grid distortion, optical distortion, grayscale conversion, random brightness, contrast, channel, and course dropout were used.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Results on the 2018 Data Science Bowl<ref type="bibr" target="#b37">[38]</ref>.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Prec.</cell></row><row><cell>U-Net [5]</cell><cell>ResNet101</cell><cell>0.7573</cell><cell>0.9103</cell><cell>-</cell><cell>-</cell></row><row><cell>DoubleU-Net [19]</cell><cell>VGG19</cell><cell>0.7683</cell><cell>0.8407</cell><cell>0.6407</cell><cell>0.9596</cell></row><row><cell>U-Net++ [49]</cell><cell>-</cell><cell>0.9117</cell><cell>0.8477</cell><cell>0.9203</cell><cell>0.9107</cell></row><row><cell>Attention U-Net [9]</cell><cell>-</cell><cell>0.9179</cell><cell>0.8570</cell><cell>0.9183</cell><cell>0.9235</cell></row><row><cell>FANet</cell><cell>None</cell><cell>0.9176</cell><cell>0.8569</cell><cell>0.9222</cell><cell>0.9194</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell cols="6">: Results on the ISIC 2018 (Skin Caner Segmenta-</cell></row><row><cell>tion) [39], [40].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Spec.</cell><cell>Prec.</cell></row><row><cell>U-Net [5]</cell><cell>0.6740</cell><cell>0.5490</cell><cell>0.7080</cell><cell>0.9640</cell><cell>-</cell></row><row><cell>R2U-Net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Results on the DRIVE dataset<ref type="bibr" target="#b40">[41]</ref>.</figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Spec.</cell><cell>Prec.</cell></row><row><cell>U-Net [5]</cell><cell>0.8174</cell><cell>-</cell><cell>0.7822</cell><cell>0.9808</cell><cell>-</cell></row><row><cell>Residual U-Net [52]</cell><cell>0.8149</cell><cell>-</cell><cell>0.7726</cell><cell>0.9820</cell><cell>-</cell></row><row><cell>Recurrent U-Net [52]</cell><cell>0.8155</cell><cell>-</cell><cell>0.7751</cell><cell>0.9816</cell><cell>-</cell></row><row><cell>R2U-Net [52]</cell><cell>0.8171</cell><cell>-</cell><cell>0.7792</cell><cell>0.9813</cell><cell>-</cell></row><row><cell>DenseBlock-UNet [55]</cell><cell>0.8146</cell><cell>-</cell><cell>0.7928</cell><cell>0.9776</cell><cell>-</cell></row><row><cell>DUNet [56]</cell><cell>0.8190</cell><cell>-</cell><cell>0.7863</cell><cell>0.9805</cell><cell>-</cell></row><row><cell>IterNet [57]</cell><cell>0.8218</cell><cell>-</cell><cell>0.7791</cell><cell>0.9831</cell><cell>-</cell></row><row><cell>IterNet(Patched) [57]</cell><cell>0.8205</cell><cell>-</cell><cell>0.7235</cell><cell>0.9838</cell><cell>-</cell></row><row><cell>U-Net++ [49]</cell><cell>0.7960</cell><cell>0.6615</cell><cell>0.7903</cell><cell>0.9818</cell><cell>0.8070</cell></row><row><cell>Attention U-Net [9]</cell><cell>0.7984</cell><cell>0.6648</cell><cell>0.7877</cell><cell>0.9827</cell><cell>0.8146</cell></row><row><cell>FANet</cell><cell>0.8183</cell><cell>0.6927</cell><cell>0.8215</cell><cell>0.9826</cell><cell>0.8189</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Results on the CHASE-DB1 dataset<ref type="bibr" target="#b41">[42]</ref>.</figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Spec.</cell><cell>Prec.</cell></row><row><cell>U-Net [5]</cell><cell>0.7993</cell><cell>-</cell><cell>0.7840</cell><cell>0.9880</cell><cell>-</cell></row><row><cell>DenseBlock-UNet [55]</cell><cell>0.8005</cell><cell>-</cell><cell>0.8177</cell><cell>0.9848</cell><cell>-</cell></row><row><cell>DUNet [56]</cell><cell>0.8000</cell><cell>-</cell><cell>0.7858</cell><cell>0.9880</cell><cell>-</cell></row><row><cell>IterNet [57]</cell><cell>0.8072</cell><cell>-</cell><cell>0.7969</cell><cell>0.9881</cell><cell>-</cell></row><row><cell>U-Net++ [49]</cell><cell>0.7954</cell><cell>0.6606</cell><cell>0.8114</cell><cell>0.9847</cell><cell>0.7818</cell></row><row><cell>Attention U-Net [9]</cell><cell>0.7941</cell><cell>0.6589</cell><cell>0.8049</cell><cell>0.9852</cell><cell>0.7852</cell></row><row><cell>FANet</cell><cell>0.8108</cell><cell>0.6820</cell><cell>0.8544</cell><cell>0.9830</cell><cell>0.7722</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII :</head><label>VIII</label><figDesc>Results on the EM dataset<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Specificity</cell><cell>Prec.</cell></row><row><cell>U-Net [5]</cell><cell>-</cell><cell>0.8830</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wide U-Net [15]</cell><cell>-</cell><cell>0.8837</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>U-Net++ [49]</cell><cell>0.9495</cell><cell>0.9038</cell><cell>0.9520</cell><cell>0.7875</cell><cell>0.9474</cell></row><row><cell>Attention U-Net [9]</cell><cell>0.9492</cell><cell>0.9033</cell><cell>0.9502</cell><cell>0.7912</cell><cell>0.9484</cell></row><row><cell>FANet</cell><cell>0.9547</cell><cell>0.9134</cell><cell>0.9568</cell><cell>0.8096</cell><cell>0.9529</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE X :</head><label>X</label><figDesc>Algorithm complexity of our proposed FANet and other SOTA methods.</figDesc><table><row><cell></cell><cell>Params.</cell><cell>Flops</cell><cell>Inf. Time</cell><cell>Image size</cell></row><row><cell>Method</cell><cell>(million)</cell><cell>(GMac)</cell><cell>(in ms.)</cell><cell>(pixels)</cell></row><row><cell>U-Net [5]</cell><cell>31.04</cell><cell>219.01</cell><cell>3.14</cell><cell>512 ? 512</cell></row><row><cell>ResU-Net [47]</cell><cell>8.22</cell><cell>181.68</cell><cell>2.93</cell><cell>512 ? 512</cell></row><row><cell>U-Net++ [49]</cell><cell>9.16</cell><cell>138.6</cell><cell>4.07</cell><cell>512 ? 512</cell></row><row><cell>Attention U-Net [9]</cell><cell>34.88</cell><cell>266.54</cell><cell>4.47</cell><cell>512 ? 512</cell></row><row><cell>FANet</cell><cell>7.72</cell><cell>94.75</cell><cell>8.25</cell><cell>512 ? 512</cell></row><row><cell cols="4">Params.: parameters; Inf.: inference ; ms: milliseconds</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Research progress on few-shot learning for remote sensing image interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2387" to="2402" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dabnet: Deformable contextual and boundary-weighted network for cloud detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An integrated micro-and macroarchitectural analysis of the Drosophila brain by computer-assisted serial section electron microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1000502</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent unet for resource-constrained segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2142" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1803.01271</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Attention U-Net: Learning where to look for the pancreas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UNet++: A nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DLMIAMLCDS</title>
		<meeting>DLMIAMLCDS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">UNet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PraNet: parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI, 2020</title>
		<meeting>MICCAI, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ResUnet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISM</title>
		<meeting>ISM</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DoubleU-Net: a deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CBMS</title>
		<meeting>CBMS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Boundary-aware context neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Evaluation of Deep Segmentation Models for the Extraction of Retinal Lesions from Multimodal Retinal Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Usman</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Colorectal polyp segmentation by u-net with dilation convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMLA</title>
		<meeting>ICMLA</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="851" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5669" to="5678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SCA-CNN: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An overview of deep learning in medical imaging focusing on MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Lundervold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lundervold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift f?r Medizinische Physik</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="102" to="127" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention gated networks: Learning to leverage salient regions in medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feedback attention for cell image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCVW</title>
		<meeting>ECCVW</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feedback U-Net for cell image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Worksh</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Worksh</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="974" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional LSTM Network: A machine learning approach for precipitation Nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond the pixel-wise loss for topology-aware delineation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kozi?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images Through Generative Latent Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pandey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="413" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MMM, 2020</title>
		<meeting>MMM, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Med Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nucleus segmentation across imaging experiments: the 2018 data science bowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1247" to="1253" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (ISBI)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISBI</title>
		<meeting>ISBI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180161</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abr?moff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Measuring retinal vessel tortuosity in 10-year-old children: validation of the computer-assisted image analysis of the retina (CAIAR) program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ARVO</publisher>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning visual question answering by bootstrapping hard attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on systems, man, and cybernetics</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual U-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote. Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation,&quot; in Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Selective feature aggregation network with area-boundary constraints for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The missing pieces of artificial intelligence in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gilvary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Madhukar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elkhader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Elemento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Pharmacol. Sci</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="555" to="564" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06955</idno>
		<title level="m">Recurrent residual convolutional neural network based on U-Net (r2u-net) for medical image segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bidirectional ConvLSTM U-Net with densely connected convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asadi-Aghbolaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCVW</title>
		<meeting>ICCVW</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Cancer facts &amp; figures 2018</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Society</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Eye3dvas: three-dimensional reconstruction of retinal vascular structures by integrating fundus image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Optics</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">DUNet: a deformable network for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="149" to="162" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">IterNet: retinal image segmentation utilizing structural redundancy in vessel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kawasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WCACV, 2020</title>
		<meeting>WCACV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="3656" to="3665" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
