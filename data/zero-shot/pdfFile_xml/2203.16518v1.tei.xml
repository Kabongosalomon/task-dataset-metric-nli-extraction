<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collaborative Transformers for Grounded Situation Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyeong</forename><surname>Cho</surname></persName>
							<email>junhyeong99@postech.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of AI</orgName>
								<address>
									<country>POSTECH</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngseok</forename><surname>Yoon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of AI</orgName>
								<address>
									<country>POSTECH</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
							<email>suha.kwak@postech.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of AI</orgName>
								<address>
									<country>POSTECH</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<address>
									<postBox>POSTECH</postBox>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Collaborative Transformers for Grounded Situation Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grounded situation recognition is the task of predicting the main activity, entities playing certain roles within the activity, and bounding-box groundings of the entities in the given image. To effectively deal with this challenging task, we introduce a novel approach where the two processes for activity classification and entity estimation are interactive and complementary. To implement this idea, we propose Collaborative Glance-Gaze TransFormer (CoFormer) that consists of two modules: Glance transformer for activity classification and Gaze transformer for entity estimation. Glance transformer predicts the main activity with the help of Gaze transformer that analyzes entities and their relations, while Gaze transformer estimates the grounded entities by focusing only on the entities relevant to the activity predicted by Glance transformer. Our CoFormer achieves the state of the art in all evaluation metrics on the SWiG dataset. Training code and model weights are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans make decisions via dual systems of thinking as stated in the cognitive theory by Kahneman <ref type="bibr" target="#b13">[14]</ref>. Those two systems are known to work in tandem and complement each other <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">28]</ref>. Consider a comprehensive scene understanding task as a specific example of such decision making. As illustrated in <ref type="figure">Figure 1</ref>, humans cast a quick glance to figure out what is happening, and slowly gaze at details to analyze which objects are involved and how they are related. These two processes are mutually supportive, e.g., understanding involved objects and their relations leads to more accurate recognition of the event depicted in the scene.</p><p>Inspired by this, we propose a collaborative framework which leverages the two processes for Grounded Situation Recognition (GSR) <ref type="bibr" target="#b29">[29]</ref>. GSR is a comprehensive scene understanding task that is recently introduced as an extension of Situation Recognition (SR) <ref type="bibr" target="#b41">[41]</ref>. The objective of SR is to produce a structured image summary that describes the main activity and entities playing certain roles within the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glance Gaze</head><p>Dog Ribbon Outdoors ... <ref type="figure">Figure 1</ref>. Two processes in comprehensive scene understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tugging</head><p>Glance figures out what is happening, and Gaze analyzes entities engaged in the main activity and their relations. In our CoFormer, these two processes are interactive and complementary. activity, where the roles are predefined for each activity by a lexical database called FrameNet <ref type="bibr" target="#b6">[7]</ref>. In GSR, those involved entities are grounded with bounding boxes; <ref type="figure">Figure 2</ref> presents example results of GSR. Following conventions, we call an activity verb and an entity noun in this paper. The common pipeline of SR and GSR in the literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref> resembles the two processes: predicting a verb (Glance), then estimating a noun for each role associated with the predicted verb (Gaze). Regarding this pipeline, correctness of the predicted verb is extremely important since noun estimation entirely depends on the predicted verb. If the result of verb prediction is incorrect, then estimated nouns cannot be correct either because the predicted verb determines the set of roles, i.e., the basis of noun estimation. Moreover, verb prediction is challenging since a verb is highly abstract and situations for the same verb could significantly vary as shown in <ref type="figure">Figure 2</ref>. In spite of its importance and difficulty, verb prediction has been made in na?ve ways, e.g., using a single classifier on top of a convolutional neural network (CNN), which is analogous to Glance only. Existing methods allow Glance to assist Gaze by informing the predicted verb but not vice versa; this could limit the performance of verb prediction, and consequently, that of the entire pipeline.</p><p>We resolve the above issue by a collaborative framework that enables Glance and Gaze to interact and complement <ref type="bibr">Figure 2</ref>. Two examples of Grounded Situation Recognition <ref type="bibr" target="#b29">[29]</ref>. These show various situations for the same verb. each other. To fully utilize this framework, we propose Collaborative Glance-Gaze TransFormer (CoFormer) that consists of Glance transformer and Gaze transformer as illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. Glance transformer predicts a verb by aggregating image features through self-attentions, and Gaze transformer estimates nouns and their groundings by allowing each role to focus on its relevant image region through self-attentions and cross-attentions. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, there are two steps for Gaze in our CoFormer. Gaze-Step1 transformer estimates nouns for all role candidates and assists Glance transformer for more accurate verb prediction. Meanwhile, Gaze-Step2 transformer estimates a noun and its grounding for each role associated with the predicted verb by exploiting the aggregated image features obtained by Glance transformer.</p><p>The collaborative relationship between Glance and Gaze transformers lead to more accurate verb and grounded noun predictions for GSR. In CoFormer, Gaze-Step1 supports Glance by analyzing involved nouns and their relations, which enables noun-aware verb prediction. Glance assists Gaze-Step2 by informing the predicted verb, which reduces the role candidates considered in grounded noun prediction. Contributions. (i) We propose a collaborative framework where the two processes for verb prediction and noun estimation are interactive and complementary, which is novel in GSR. (ii) Our method achieves state-of-the-art accuracy in every evaluation metric on the SWiG dataset. (iii) We demonstrate the effectiveness of CoFormer by conducting extensive experiments and provide in-depth analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual reasoning such as image captioning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b42">42]</ref>, scene graph generation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39]</ref>, and human-object-interaction detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b43">43]</ref> has been widely studied for comprehensive understanding of images. Given an image, image captioning aims at describing activities and entities using natural language, and scene graph generation or human-object-interaction detection aims at capturing a set of triplets subject, predicate, object or human, object, interaction . However, it is not straightforward to evaluate the quality of natural language captions, and the triplets have limited expressive power. To overcome such limitations, Yatskar et al. <ref type="bibr" target="#b41">[41]</ref> introduce SR along with the imSitu dataset. SR has more expressive power based on linguistic sources from FrameNet <ref type="bibr" target="#b6">[7]</ref>, and its quality evaluation is straightforward. GSR builds upon SR by additionally estimating bounding-box groundings. Situation Recognition. Yatskar et al. <ref type="bibr" target="#b41">[41]</ref> propose a conditional random field <ref type="bibr" target="#b16">[17]</ref> model, and also present a tensor composition method with semantic augmentation <ref type="bibr" target="#b40">[40]</ref>. Mallya and Lazebnik <ref type="bibr" target="#b27">[27]</ref> employ a recurrent neural network to capture role relations in the predefined sequential order. Li et al. <ref type="bibr" target="#b18">[19]</ref> propose a gated graph neural network (GGNN) <ref type="bibr" target="#b19">[20]</ref> to capture the relations in more flexible ways. To learn context-aware role relations depending on an input image, Suhail and Sigal <ref type="bibr" target="#b32">[32]</ref> apply a mixture kernel method to GGNN. Cooray et al. <ref type="bibr" target="#b3">[4]</ref> employ inter-dependent queries to capture role relations, and present a verb model which considers nouns from the two predefined roles; they construct a query based on two nouns for verb prediction. Compared with this, CoFormer considers nouns from all role candidates for accurate verb prediction. Grounded Situation Recognition. Pratt et al. <ref type="bibr" target="#b29">[29]</ref> propose GSR along with the SWiG dataset, and present two models: Independent Situation Localizer (ISL) and Joint Situation Localizer (JSL). They first predict a verb using a single classifier on top of a CNN backbone, then estimate nouns and their groundings. In both models, LSTM <ref type="bibr" target="#b11">[12]</ref> produces output features to predict nouns in the predefined sequential order, while RetinaNet <ref type="bibr" target="#b22">[23]</ref> estimates their groundings. ISL separately predicts nouns and their groundings, and JSL jointly predicts them. Cho et al. <ref type="bibr" target="#b2">[3]</ref> propose a transformer encoder-decoder architecture, where the encoder effectively captures high-level semantic features for verb prediction and the decoder flexibly learns the role relations. Compared with these models, CoFormer leverages involved nouns and their relations for accurate verb prediction via transformers. Transformer Architecture. Transformers <ref type="bibr" target="#b34">[34]</ref> have driven remarkable success in vision tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. Dosovitskiy et al. <ref type="bibr" target="#b5">[6]</ref> propose a transformer encoder architecture for image classification by aggregating image features using a learnable token in the encoder. Carion et al. <ref type="bibr" target="#b0">[1]</ref> present a transformer encoder-decoder architecture for object detection by predicting a set of bounding boxes using a fixed number of learnable queries in the decoder. Such learnable queries have been widely used to extract features in other transformer architectures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>. Compared with those transformers, CoFormer employs two learnable tokens which aggregate different kinds of features through self-attentions. In addition, CoFormer constructs a different number of learnable queries by explicitly leveraging the prediction result obtained by two encoders and a classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role Tokens RL Token IL Token Selected Role Tokens Verb Token</head><p>Glance Transformer  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaze?Step2 Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction Verb Prediction Grounded Noun Prediction Prediction Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Task Definition. GSR assumes discrete sets of verbs V, nouns N , and roles R. Each verb v ? V is paired with a frame derived from FrameNet <ref type="bibr" target="#b6">[7]</ref>, where the frame defines the set of roles R v ? R associated with the verb. For example, a verb Mowing is paired with a frame which defines the set of roles R Mowing = {Agent, Item, Tool, Place} as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Each role r ? R v is fulfilled by a noun n ? N grounded by a bounding box b ? R 4 , called grounded noun. Formally speaking, the set of fulfilled roles is</p><formula xml:id="formula_0">F v = {(r i , n i , b i ) | r i ? R v , n i ? N ? {? n }, b i ? R 4 ? {? b } for i = 1, .</formula><p>.., |R v |}; ? n and ? b denote unknown and not grounded, respectively. The output of GSR is a grounded situation denoted by S = (v, F v ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>CoFormer predicts a verb, then estimates grounded nouns as illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. As shown in <ref type="figure" target="#fig_2">Figure 5</ref>, our transformers consist of common building blocks, encoder and decoder, whose architectures are illustrated in <ref type="figure">Figure 6</ref>. For simplicity, we abbreviate Step1 as S1, and Step2 as S2 in the remaining of this paper. Overview. Given an image, CoFormer extracts flattened image features via a CNN backbone and flatten operation, which are fed as input to Glance transformer and Gaze-S1 transformer. From these transformers, the output features corresponding to Image-Looking (IL) and Role-Looking (RL) tokens are used for verb prediction. Considering the predicted verb, Gaze-S2 transformer estimates grounded nouns for the roles associated with the predicted verb by exploiting image features obtained by Glance transformer. <ref type="figure" target="#fig_1">Figure 4</ref> shows the collaborative relationship between the modules; transformers for verb prediction and noun estimation are interactive and complementary in CoFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glance</head><p>Gaze?S2</p><formula xml:id="formula_1">(a) (c) (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noun Estimation Verb Prediction</head><p>Gaze?S1 Gaze-S1 Transformer. This transformer is composed of a decoder and an encoder. The decoder takes the flattened image features and learnable role tokens as input, where the role tokens correspond to all role candidates. This module extracts role features from the image features via the role tokens. Then, the encoder takes the role features and learnable RL token as input. RL token captures involved nouns and their relations for verb prediction, while the encoder aggregates the role features through self-attentions.</p><p>Gaze-S2 Transformer. This transformer consists of a single decoder, which takes learnable tokens and the aggregated image features obtained from Glance transformer as input. The input tokens correspond to the predicted verb and its associated roles. Note that a verb token is added to role tokens as shown in <ref type="figure" target="#fig_2">Figure 5</ref>; conditioning on the predicted verb significantly reduces the search space of the roles, e.g., the search space of Mowing Tool is much smaller than that of Tool. Gaze-S2 transformer extracts role features from the aggregated image features, and the extracted role features are used for grounded noun prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extraction</head><p>Given an input image, a single CNN backbone extracts image features of size h ? w ? c, where h ? w is the resolution, and c is the number of channels. Then, a 1 ? 1 convolution followed by a flatten operation produces flattened image features X F ? R hw?d , where d is the number of channels. The flattened image features X F are fed as input to Glance transformer ( <ref type="figure" target="#fig_2">Fig. 5(a)</ref>) and Gaze-S1 transformer ( <ref type="figure" target="#fig_2">Fig. 5(b)</ref>). For the flattened image features X F , positional encodings are introduced to retain spatial information. As shown in <ref type="figure">Figure 6</ref>, positional encodings are added to the queries and keys at the self-attention layers in an encoder, and to the keys at the cross-attention layers in a decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Verb Prediction</head><p>The input of the encoder in Glance transformer is obtained by the concatenation of the image features X F and learnable IL token. IL token captures the essential features for verb prediction, while the encoder aggregates the image features through self-attentions. As its output, the encoder produces aggregated image features X A ? R hw?d and IL token feature. For the aggregated image features X A , positional encodings are applied.</p><p>Gaze-S1 transformer supports Glance transformer for more accurate verb prediction, while predicting nouns for all role candidates. To be specific, the decoder of Gaze-S1 transformer takes the flattened image features X F and learnable role tokens corresponding to all predefined roles; each role token embedding is denoted by w r ? R d , where r ? R. This decoder extracts role features through selfattentions on the role tokens and cross-attentions between the tokens and the image features X F . The input of the encoder in Gaze-S1 transformer is obtained by the concatenation of the extracted role features and learnable RL token. RL token captures involved nouns and their relations from all role candidates, while the encoder aggregates the V K Q</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feed Forward Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LayerNorm LayerNorm</head><p>Multi-Head Self-Attention</p><formula xml:id="formula_2">3? (b) Decoder (a) Encoder V K Q 3? V K Q LayerNorm LayerNorm</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feed Forward Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LayerNorm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Cross-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positional Encodings</head><p>Positional Encodings X X X? <ref type="figure">Figure 6</ref>. Detailed architectures of encoder and decoder. We use Pre-Layer Normalization <ref type="bibr" target="#b37">[37]</ref> for these two modules. An encoder performs feature aggregation through self-attentions on X, and a decoder performs feature extraction through self-attentions on X and cross-attentions between X and X .</p><p>role features through self-attentions. For this encoder, positional encodings are not added to the queries and keys at the self-attention layers since roles are permutation-invariant in GSR. Regarding to Gaze-S1 transformer, the extracted and aggregated role features are fed as input to noun classifiers; these classifiers are auxiliary modules and their results are ignored at inference time. Note that Gaze-S1 transformer assists Glance transformer via RL token feature which is aware of involved nouns and their relations.</p><p>IL token feature and RL token feature are concatenated, then fed as input to the feed forward network (FFN) for verb classification, which consists of learnable linear layers with activation functions. The verb classifier FFN Verb followed by a softmax function produces a verb probability distribution p v , which is used to estimate the most probable verb v = arg max v p v . The predicted verbv supports Gaze-S2 transformer so that the transformer concentrates only on the roles associated with the predicted verb and estimates their grounded nouns more accurately in consequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Grounded Noun Prediction</head><p>The aggregated image features X A from Glance transformer are fed as input to Gaze-S2 transformer ( <ref type="figure" target="#fig_2">Fig. 5(c)</ref>). The decoder in this transformer takes the image features X A and frame-role queries as input. Specifically, for each role r in the frame of the predicted verbv, its frame-role query q r ? R d is constructed by an addition of the learnable role token embedding w r ? R d and the learnable verb token embedding wv ? R d , i.e., q r = w r +wv for r ? Rv. The decoder extracts role features through self-attentions on the frame-role queries and cross-attentions between the queries and the image features X A to capture the involved nouns and their relations from roles relevant to the verbv. Those extracted role features are used for grounded noun prediction. Note that this task requires to predict a noun, a bounding box, and a box existence for each role. Accordingly, we employ three feed forward networks FFN Noun , FFN Box , and FFN BoxExist that take the role features as input for noun classification, bounding box estimation, and box existence prediction, respectively. Each of these FFNs consists of learnable linear layers with activation functions.</p><p>For each role r ? Rv, FFN Noun followed by a softmax function produces a noun probability distribution p nr . FFN Box followed by a sigmoid function produces a bounding boxb r ? [0, 1] 4 which indicates the center coordinates, height and width relative to the input image size. The predicted boxb r can be transformed into the top-left and bottom-right coordinatesb r ? R 4 . FFN BoxExist followed by a sigmoid function produces a box existence probability p br ? [0, 1]. If p br &lt; 0.5, the predicted boxb r is ignored. Note that the predicted verbv assists Gaze-S2 transformer via the construction of frame-role queries, while the loss gradients propagated from Gaze-S2 transformer through the aggregated image features X A enable Glance transformer to implicitly consider involved nouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training CoFormer</head><p>The predicted verb, nouns and bounding boxes are used for computing losses to train CoFormer. At training time, we construct frame-role queries based on the ground-truth verb for stable training of Gaze-S2 transformer. Please refer to the supplementary material for more training details.</p><p>Verb Classification Loss. The verb classification loss is the cross-entropy between the verb probability distribution p v and the ground-truth verb distribution. Noun Classification Losses. As illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>, CoFormer has three noun classifiers; two of them are placed on top of Gaze-S1 transformer and the other is incorporated with Gaze-S2 transformer. For each noun classifier, we compute the cross-entropy between the estimated noun probability distribution and the ground-truth noun distribution for each role r ? R?, where? is the ground-truth verb. The computed cross-entropy loss is averaged over roles R?. Note that we only train role tokens for the roles in the frame of the ground-truth verb?, since noun annotations are given for the roles associated with the verb? in the dataset. Box Existence Prediction Loss. To deal with roles which have no ground-truth boxes (i.e., ? b ), e.g., by occlusion, CoFormer estimates a box existence probability p br for each role r ? R?. The box existence prediction loss is the crossentropy between the probability p br and the ground-truth box existence, which is averaged over roles R?. Box Regression Losses. We employ L1 loss and GIoU loss <ref type="bibr" target="#b30">[30]</ref> for box regression. Let b r denote the ground-truth box in the form of the center coordinates, height and width relative to the given image size. In the computation of box regression losses, we ignore the roles which have no ground-truth boxes (i.e., ? b ). The L1 box regression loss L L1 is computed by</p><formula xml:id="formula_3">L L1 = 1 |R| r?R b r ?b r 1 ,<label>(1)</label></formula><formula xml:id="formula_4">whereR = {r | b r = ? b for r ? R?}.</formula><p>To compute the GIoU loss, GIoU(?) is first computed by</p><formula xml:id="formula_5">GIoU(b r ,b r ) = |b r ?b r | |b r ?b r | ? |C(b r ,b r ) \ (b r ?b r )| |C(b r ,b r )| ,<label>(2)</label></formula><p>where b r indicates the top-left and bottom-right coordinates transformed from b r , and C(b r ,b r ) denotes the smallest box which encloses b r andb r . The GIoU box regression loss L GIoU is then computed by</p><formula xml:id="formula_6">L GIoU = 1 |R| r?R 1 ? GIoU(b r ,b r ) .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>CoFormer is evaluated on the SWiG dataset <ref type="bibr" target="#b29">[29]</ref>, which is constructed by adding box annotations to the imSitu dataset <ref type="bibr" target="#b41">[41]</ref>. The imSitu dataset contains 75K, 25K and 25K images for train, development and test set, respectively. This dataset contains 504 verbs, 11K nouns and 190 roles. The number of roles in the frame of a verb ranges from 1 to 6. Each image is paired with the annotation of a verb, and three nouns from three different annotators for each role. In addition to this annotation, the SWiG dataset provides a box annotation for each role (except role Place).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Metric</head><p>Metric Details. The prediction accuracy of verb is measured by verb, that of noun is evaluated by value and value-all, and that of grounded noun is assessed by grounded-value and grounded-value-all. Regarding to the noun metrics, value measures whether a noun is correct for each role, and value-all measures whether all nouns are correct for entire roles in a frame simultaneously. The noun prediction is considered correct if the predicted noun matches any of the three noun annotations given by three annotators. For the grounded noun metrics, grounded-value measures whether a noun and its grounding are correct for each role, and grounded-value-all measures whether all nouns and their groundings are correct for entire roles in a frame simultaneously. The grounding prediction is considered correct if the predicted box existence is correct and the predicted bounding box has Intersection-over-Union (IoU) value at least 0.5 with the box annotation. Note that the above metrics are calculated per verb and then averaged over all verbs, since the number of roles in a frame depends on a verb and each verb might be associated with a different number of samples in the dataset.</p><p>Evaluation Settings. Three evaluation settings are proposed for comprehensive evaluation: Top-1 Predicted Verb, Top-5 Predicted Verbs, and Ground-Truth Verb. In Top-1 Predicted Verb setting, the predicted nouns and their groundings are considered incorrect if the top-1 verb prediction is incorrect. In Top-5 Predicted Verbs setting, the predicted nouns and their groundings are considered incorrect if the ground-truth verb is not contained in the top-5 predicted verbs. In Ground-Truth Verb setting, the predicted nouns and their groundings are obtained by conditioning on the ground-truth verb.  <ref type="figure">Figure 7</ref>. Attention scores from IL token to image features, from RL token to role features, and on frame-role queries. We visualize the attention scores computed from the last self-attention layer of the encoder in Glance transformer, the encoder in Gaze-S1 transformer, and the decoder in Gaze-S2 transformer, respectively. Higher attention scores are highlighted in red color on images.  <ref type="figure">Figure 8</ref>. Attentions scores from frame-role queries to image features. We visualize the attention scores computed from the last crossattention layer of the decoder in Gaze-S2 transformer. Higher attention scores are highlighted in red color on images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use ResNet-50 <ref type="bibr" target="#b10">[11]</ref> pretrained on ImageNet <ref type="bibr" target="#b4">[5]</ref> as a CNN backbone following existing models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">29]</ref> in GSR. Given an image, the CNN backbone extracts image features of size h ? w ? c, where h = w = 22 and c = 2048. The embedding dimension of each token is d = 512. We employ AdamW Optimizer <ref type="bibr" target="#b24">[25]</ref> with 10 ?4 weight decay, ? 1 = 0.9, and ? 2 = 0.999. We train CoFormer with 10 ?4 learning rate (10 ?5 for the CNN backbone) which decreases by a factor of 10 at epoch 30. Training CoFormer with batch size of 16 for 40 epochs takes about 30 hours on four RTX 3090 GPUs. Complete details including loss coefficients are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Evaluations</head><p>CoFormer achieves the state of the art in all evaluations as shown in <ref type="table" target="#tab_1">Table 1</ref>. Existing SR models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32]</ref> use at least two VGG-16 <ref type="bibr" target="#b31">[31]</ref> backbones, and GSR models <ref type="bibr" target="#b29">[29]</ref> employ two ResNet-50 <ref type="bibr" target="#b10">[11]</ref> backbones for verb and noun prediction, while CoFormer only employs a single ResNet-50 backbone. Compared with GSRTR <ref type="bibr" target="#b2">[3]</ref>, the improvements in the verb prediction accuracies range from 3.35%p to 4.03%p. Regarding to the noun prediction accuracies, the improvements range from 1.84%p to 3.89%p, and those in the grounded noun prediction accuracies range from 2.11%p to 4.09%p. These results demonstrate that the proposed collaborative framework is effective for GSR.  Ablation Study. We analyze the effects of different components in CoFormer as shown in <ref type="table">Table 2</ref>. When we train our model without using Gaze-S1 transformer or Gaze-S2 transformer, the accuracies in verb prediction or grounded noun prediction largely decrease, which demonstrates the effectiveness of the collaborative framework. Training our CoFormer without using the two noun classifiers placed on top of Gaze-S1 transformer leads to significant drops in the verb prediction accuracies. In this case, it is difficult for role features to learn involved nouns and their relations, while the encoder in Gaze-S1 transformer aggregates the role features through self-attentions. To figure out whether Gaze-S2 transformer assists Glance transformer by forcing it to implicitly consider involved nouns, we train CoFormer by restricting the flow of loss gradients through the aggregated image features from Gaze-S2 transformer to Glance transformer. As shown in the fourth row of <ref type="table">Table 2</ref>, the verb prediction accuracies drop, which demonstrates that Gaze-S2 transformer supports Glance transformer via loss gradients through the aggregated image features. In CoFormer, each frame-role query is constructed by an addition of a role token embedding and a verb token embedding. We study how effective it is by training CoFormer without using a verb token embedding for the construction of frame-role queries. The fifth row of <ref type="table">Table 2</ref> shows that the grounded noun prediction accuracies drop, which demonstrates that the verb token embedding is helpful for grounded noun prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Evaluations</head><p>We visualize the attention scores computed in the attention layers of CoFormer. <ref type="figure">Figure 7(a)</ref> shows that IL token captures the essential features to estimate a verb for two Boating images. <ref type="figure">Figure 7(b)</ref> shows how much RL token focuses on the roles in the frame of the ground-truth verb, and the classification results from the noun classifier placed on top of the encoder in Gaze-S1 transformer; attention scores among 190 roles sum to 1. This demonstrates that RL token effectively captures involved nouns and their relations through self-attentions in the encoder of Gaze-S1 transformer. <ref type="figure">Figure 7</ref>(c) shows how role relations are captured through self-attentions on frame-role queries, which demonstrates that CoFormer similarly captures the relations if the situations in images are similar; attention scores sum to 1 in each column. <ref type="figure">Figure 8</ref> shows the local regions where frame-role queries focus on, and the predicted grounded nouns corresponding to the queries. <ref type="figure" target="#fig_4">Figure 9</ref> shows prediction results of CoFormer on the SWiG test set. The first row shows the correct predictions, and the second row shows several incorrect predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a collaborative framework for GSR, where the two processes for verb prediction and noun estimation interact and complement each other. Using this framework, we present CoFormer which outperforms existing methods in all evaluation metrics on the SWiG dataset. We also provide in-depth analyses of how CoFormer draws attentions on images and captures role relations with the ablation study on the effects of different components used in our model. A limitation of CoFormer is that the model sometimes suffers from predicting the boxes which have extreme aspect ratios or small scales. This issue will be explored in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collaborative Transformers for Grounded Situation Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Supplementary Material -</head><p>This supplementary material provides method details (Section A), implementation details (Section B), qualitative evaluations (Section C), an application of this task (Section D), computational evaluations (Section E) and a limitation (Section F), which could not be included in the main paper due to the limited space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Method Details</head><p>Transformer architectures in our CoFormer consist of common building blocks, encoder and decoder. The main components of these building blocks are attention layers. Section A.1 provides more details of the attention layers.</p><p>In Section 3.5 of the main paper, the losses to train our model are described: verb classification loss, noun classification losses, box existence prediction loss, and box regression losses. Section A.2 provides more details of the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Attention Layer</head><p>Multi-Head Attention. The input of the multi-head attention layer is the sequence of query, key and value. The query sequence is denoted by Q ? R L Q ?d , where L Q is the sequence length and d is the size of the hidden dimension. The key sequence is denoted by K ? R L KV ?d and value sequence is denoted by V ? R L KV ?d , where L KV is the sequence length. In the multi-head attention layer, we employ H attention heads; the hidden dimension of each attention head is d h = d/H. For each attention head i, Q, K and V are linearly projected via parameter matrices</p><formula xml:id="formula_7">W Q i , W K i , W V i ? R d?d h .</formula><p>In details,</p><formula xml:id="formula_8">Q i = QW Q i ? R L Q ?d h , (A.1) K i = KW K i ? R L KV ?d h , (A.2) V i = VW V i ? R L KV ?d h . (A.3)</formula><p>The output of each attention head i is obtained by a weighted summation of the value V i , where the weights are computed by the scaled dot-product between the query Q i and the key K i followed by a softmax function. In details,</p><formula xml:id="formula_9">Attention(Q i , K i , V i ) = Softmax( Q i K T i ? d h )V i . (A.4)</formula><p>The output of each attention head i is concatenated along hidden dimension, then linearly projected via a parameter matrix W O ? R d?d . In details,</p><formula xml:id="formula_10">MultiHead(Q, K, V) = [Head 1 ; ...; Head H ]W O , (A.5)</formula><p>where [; ] is a concatenation along hidden dimension and</p><formula xml:id="formula_11">Head i = Attention(Q i , K i , V i ) for i = 1, ..., H.</formula><p>Multi-Head Cross-Attention. This is the multi-head attention layer where the key sequence K is same with the value sequence V, but the query sequence Q is different.</p><p>Multi-Head Self-Attention. This is the multi-head attention layer where the query sequence Q, key sequence K, and value sequence V are same, i.e., Q = K = V. <ref type="figure">Figure A1</ref> shows the losses to train CoFormer. The verb classification loss is denoted by L Verb . The noun classification loss from the classifier involved in the decoder of Gaze-S1 transformer is denoted by L 1 Noun , the loss from the classifier involved in the encoder of Gaze-S1 transformer is denoted by L 2 Noun , and the loss from the classifier involved in the decoder of Gaze-S2 transformer is denoted by L 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Loss</head><p>Noun . The box existence prediction loss is denoted by L BoxExist . The L1 box regression loss is denoted by L L1 . The GIoU box regression loss is denoted by L GIoU .</p><p>The total training loss is the linear combination of L Verb , L 1 Noun , L 2 Noun , L 3 Noun , L BoxExist , L L1 , and L GIoU . In this total loss, the loss coefficients are as follows: ? Verb , ? 1 Noun , ? 2 Noun , ? 3 Noun , ? BoxExist , ? L1 , ? GIoU &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>In Section 4.2 of the main paper, some implementation details are described. For completeness, we describe more architecture details (Section B.1), loss details (Section B.2), augmentation details (Section B.3), and training details (Section B.4) of our CoFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Architecture Details</head><p>Following previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">29]</ref>, we use ResNet-50 <ref type="bibr" target="#b10">[11]</ref> pretrained on ImageNet <ref type="bibr" target="#b4">[5]</ref> as a CNN backbone. Given an image, the CNN backbone produces image features of size h ? w ? c, where h = w = 22 and c = 2048. A 1 ? 1 convolution followed by a flatten operation produces flattened image features X F ? R hw?d , where d = 512. To retain spatial information, we employ positional encodings. We use learnable 2D embeddings for the positional encodings. We initialize encoders and decoders using Xavier Initialization <ref type="bibr" target="#b8">[9]</ref>, and these modules are trained with the dropout rate of 0.15. The number of heads in the attention layers of these modules is 8. Each of feed forward networks in these modules is 2-fully connected layers with a ReLU activation function, whose hidden dimensions are 4d and dropout rate is 0.15. These modules take learnable tokens, and each embedding dimension of the tokens is d.</p><p>The verb classifier FFN Verb is 2-fully connected layers with a ReLU activation function, whose hidden dimensions are 2d and dropout rate is 0.3. Each of the two noun classifiers placed on top of Gaze-S1 transformer is a linear layer. The noun classifier FFN Noun is 2-fully connected layers with a ReLU activation function, whose hidden dimensions are 2d and dropout rate is 0.3. The bounding box estimator FFN Box is 3-fully connected layers with two ReLU activation functions, whose hidden dimensions are 2d and dropout rate is 0.2. The box existence predictor FFN BoxExist is 2-fully connected layers with a ReLU activation function, whose hidden dimensions are 2d and dropout rate is 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Loss Details</head><p>Complete Details of Noun Losses. In the SWiG dataset, each image is associated with three noun annotations given by three different annotators for each role. For the noun classification losses L 1 Noun , L 2 Noun , L 3 Noun , each noun loss is obtained by the summation of three classification losses corresponding to three different annotators. Regularization. We employ label smoothing regularization <ref type="bibr" target="#b33">[33]</ref> in the loss computation for verb classification loss L Verb and noun classification losses L 1 Noun , L 2 Noun , L 3 Noun .</p><p>In details, the label smoothing factor in the computation of verb classification loss is 0.3, and the factor in the computation of noun classification losses is 0.2. Loss Coefficients. Total loss to train CoFormer is a linear combination of losses. In our implementation, the loss coefficients are ? Verb = ? 3 Noun = 1, ? 1 Noun = ? 2 Noun = 2, and ? BoxExist = ? L1 = ? GIoU = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Augmentation Details</head><p>For data augmentation, we employ random scaling, random horizontal flipping, random color jittering, and random gray scaling. The input images are randomly scaled with the scaling factors of 0.5, 0.75, and 1.0. Also, the input images are horizontally flipped with the probability of 0.5. The brightness, saturation and hue of the input images are randomly changed with the factor of 0.1 for each change. The input images are randomly converted to grayscale with the probability of 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Training Details</head><p>We employ AdamW Optimizer <ref type="bibr" target="#b24">[25]</ref> with the weight decay of 10 ?4 , ? 1 = 0.9, and ? 2 = 0.999. For stable training, we apply gradient clipping with the maximal gradient norm of 0.1. The transformers, classifiers and learnable embeddings are trained with the learning rate of 10 ?4 . The CNN backbone is fine-tuned with the learning rate of 10 ?5 . Note that we have a learning rate scheduler and the learning rates are divided by 10 at epoch 30. For batch training, we set the batch size to 16. We train CoFormer for 40 epochs, which takes about 30 hours on four RTX 3090 GPUs.  <ref type="figure" target="#fig_0">Figure C3</ref>. Attention scores from RL token to role features. We visualize the attention scores computed from the last self-attention layer of the encoder in Gaze-S1 transformer. Note that we show the roles where RL token has top-10 attentions scores. In <ref type="figure">Figure 7</ref>(b) of the main paper, we show the results corresponding to the roles in the frame of the ground-truth verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Evaluations</head><p>We visualize the attention scores computed in the attention layers of the transformers in our CoFormer. <ref type="figure" target="#fig_6">Figure C2</ref> shows that IL token captures the essential features to estimate the main activities for two Cramming images and two Ballooning images. <ref type="figure" target="#fig_0">Figure C3</ref> shows the roles where RL token has top-10 attention scores, and the classification results from the noun classifier placed on top of the encoder in Gaze-S1 transformer; attention scores among 190 roles sum to 1. Note that several roles where RL token has high attention scores are not relevant to the main activity, but the noun classification results corresponding to those roles are highly relevant to the activity. Since RL token leverages the role features which are fed as input to the noun classifier, it is reasonable to aggregate those role features for accurate verb prediction; the role features are aware of involved nouns and their relations. <ref type="figure" target="#fig_0">Figure C3</ref> demonstrates that RL token can effectively capture involved nouns and their relations for noun-aware verb prediction through self-attentions on the role features in the encoder of Gaze-S1 transformer. <ref type="figure" target="#fig_1">Figure C4</ref> shows how role relations are captured through self-attentions on frame-role queries, which demonstrates that CoFormer similarly captures the role relations if the situations in images are similar; attention scores sum to 1 <ref type="figure" target="#fig_1">Figure C4</ref>. Attention scores on frame-role queries. We visualize the attention scores computed from the last self-attention layer of the decoder in Gaze-S2 transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent Item Tool Place</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Role #1 Role #4 Prediction </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Item Grass</head><p>Role #2 Role #3 <ref type="figure" target="#fig_2">Figure C5</ref>. Attentions scores from frame-role queries to image features. We visualize the attention scores computed from the last crossattention layer of the decoder in Gaze-S2 transformer. Higher attention scores are highlighted in red color on images.</p><p>in each column. <ref type="figure" target="#fig_2">Figure C5</ref> shows the local regions where frame-role queries focus on, and the predicted grounded nouns corresponding to the queries. This demonstrates that each query effectively captures its relevant local regions through cross-attentions between the queries and image features in the decoder of Gaze-S2 transformer. Note that those queries are constructed by leveraging the predicted verb, which significantly reduces the number of role candidates handled in noun estimation; Gaze-S1 transformer considers all role candidates, but Gaze-S2 transformer handles a few roles associated with the predicted verb. <ref type="figure">Figure C6</ref> shows the prediction results of CoFormer on the SWiG test set. The first and second row show correct prediction results. The third and fourth row show incorrect prediction results. As shown in <ref type="figure">Figure C6</ref>, three noun annotations are given for each role in the SWiG dataset. Note that the noun prediction is considered correct if the predicted noun matches any of the three noun annotations. The grounded noun prediction is considered correct if a noun, a bounding box, and box existence are correctly predicted for a role.  <ref type="figure">Figure C6</ref>. Prediction results of our CoFormer on the SWiG test set. Dashed boxes denote incorrect grounding predictions. Incorrect noun predictions are highlighted in gray color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Image</head><p>Retrieval #1 Retrieval #2 Retrieval #3 Retrieval #4 Retrieval #5 <ref type="figure">Figure D7</ref>. Grounded semantic aware image retrieval on the SWiG dev set. For each query image, we show the retrieval results which have top-5 similarity scores computed by GrSitSim(?) <ref type="bibr" target="#b29">[29]</ref>. This retrieval computes the similarity between two images considering the predicted verbs, nouns, and bounding-box groundings of the nouns.</p><p>GrSitSim(I, J) = max</p><formula xml:id="formula_12">? ? ? ? ? ? ? 1 [v I i =v J j ] i ? j ? |RvI i | |Rv I i | k=1 1 [n I i,k =n J j,k ] ? 1 + IoU(b I i,k ,b J j,k ) 1 ? i, j ? 5 ? ? ? ? ? ? ? .</formula><p>(D.6)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Application</head><p>As shown in <ref type="figure">Figure D7</ref>, we can apply GSR models to grounded semantic aware image retrieval. This image retrieval computes the similarity between two images considering their grounded situations. In details, a similarity score between an image I and an image J is computed by GrSitSim(I, J) (Eq. D.6). Given an image I, a GSR model predicts the top-5 most probable verbsv I 1 , ...,v I 5 . For each predicted verbv I i , the model predicts noun? n I i,1 , ...,n I i,|Rv I i | and bounding boxesb I i,1 , ...,b I i,|Rv I i | . These prediction results are used in the computation of GrSitSim(I, J). By this score function, the similarity score is maximized if the top-1 predicted verb and the predicted grounded nouns are same for the two images I and J. Using this retrieval, we can retrieve images which have similar grounded situations with the situation of a query image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Computational Evaluations</head><p>The number of parameters and inference time of our CoFormer are shown in <ref type="table" target="#tab_1">Table E1</ref>. We also evaluate JSL <ref type="bibr" target="#b29">[29]</ref> and GSRTR <ref type="bibr" target="#b2">[3]</ref> on the SWiG test set using a single 2080Ti GPU with a batch size of 1. JSL uses two ResNet-50 <ref type="bibr" target="#b10">[11]</ref> and a feature pyramid network (FPN) <ref type="bibr" target="#b21">[22]</ref> in the CNN backbone, while GSRTR and our CoFormer only employ a single ResNet-50 in the backbone; these two models demand much shorter inference time than JSL, which is crucial for real-world applications. GSRTR and CoFormer are trained in an end-to-end manner, but JSL is trained separately in terms of verb model and grounded noun model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone #Params Inference Time JSL <ref type="bibr" target="#b29">[29]</ref> R50   <ref type="table">Table F2</ref>. Quantitative analysis of our CoFormer on the SWiG dev set in Ground-Truth Verb evaluation setting. The effects of box scales and aspect ratios are evaluated. Each range denotes the ratio of ground-truth boxes when sorted by the value of area or aspect ratio in ascending order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Limitation</head><p>As shown in <ref type="table">Table F2</ref>, CoFormer suffers from estimating the noun labels and boxes for objects which have small scales (Area 0-10% and 10-20%) or extreme aspect ratios (Aspect Ratio 0-5% and 95-100%). To overcome such limitation, one may leverage multi-scale image features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Overall architecture of Collaborative Glance-Gaze TransFormer (CoFormer). Glance transformer predicts a verb with the help of Gaze-Step1 transformer that analyzes nouns and their relations by leveraging role features, while Gaze-Step2 transformer estimates the grounded nouns for the roles associated with the predicted verb. Prediction results are obtained by feed forward networks (FFNs). The results from the two noun classifiers placed on top of Gaze-Step1 transformer are ignored at inference time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Interactive and complementary processes in CoFormer. (a) RL token feature, (b) predicted verb, (c) loss gradients. Glance Transformer. This transformer consists of a single encoder which takes the flattened image features and learnable IL token as input. IL token captures the essential features for verb prediction, while Glance transformer aggregates the image features through self-attentions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Transformer architectures in CoFormer are composed of common building blocks, encoder and decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Prediction results. Dashed boxes denote incorrect grounding predictions. Incorrect noun predictions are highlighted in gray color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5 ?</head><label>5</label><figDesc>Figure A1. Transformer architectures in CoFormer including the losses to train our model. The losses for training our CoFormer are as follows:L Verb , L 1 Noun , L 2 Noun , L 3 Noun , LBoxExist, LL1,LGIoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure C2 .</head><label>C2</label><figDesc>Attention scores from IL token to image features. We visualize the attention scores computed from the last self-attention layer of the encoder in Glance transformer. Higher attention scores are highlighted in red color on images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>FFNVerb Flattened Image Features CNN MowingTransformer FFNNoun FFNBox FFNBoxExist Positional Encodings Noun Classifiers ? ? Noun Classifiers</head><label></label><figDesc></figDesc><table><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Gaze?Step1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>?</cell><cell>Agent</cell><cell>Item</cell><cell>Tool</cell><cell>Place</cell></row><row><cell></cell><cell></cell><cell>Male Child</cell><cell>Grass</cell><cell>Lawn Mower</cell><cell>Backyard</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>-all value value-all verb value value-all value value-all value value-all value value-all Methods for Situation Recognition Gradient Flow from Gaze-S2 Transformer to Glance Transformer 42.96 33.82 25.77 70.97 54.59 41.11 73.91</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Top-1 Predicted Verb</cell><cell></cell><cell cols="3">Top-5 Predicted Verbs</cell><cell></cell><cell></cell><cell cols="2">Ground-Truth Verb</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell></row><row><cell cols="4">Set Method CRF [41] CRF w/ DataAug [40] 34.20 26.56 32.25 24.56 RNN w/ Fusion [27] 36.11 27.74 GraphNet [19] 36.93 27.52 CAQ w/ RE-VGG [4] 37.96 30.15 verb value valuedev 14.28 15.61 16.60 19.15 18.58 Kernel GraphNet [32] 43.21 35.18 19.46</cell><cell>------</cell><cell>------</cell><cell>58.64 42.68 62.21 46.72 63.11 47.09 61.80 45.23 64.99 50.30 68.55 56.32</cell><cell>22.75 25.66 26.48 29.98 29.17 30.56</cell><cell>------</cell><cell>------</cell><cell>65.90 70.80 70.48 68.89 73.62 73.14</cell><cell>29.50 34.82 35.56 41.07 38.71 41.68</cell><cell>------</cell><cell>------</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Methods for Grounded Situation Recognition</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ISL [29]</cell><cell>38.83 30.47</cell><cell>18.23</cell><cell>22.47</cell><cell>7.64</cell><cell>65.74 50.29</cell><cell>28.59</cell><cell>36.90</cell><cell>11.66</cell><cell>72.77</cell><cell>37.49</cell><cell>52.92</cell><cell>15.00</cell></row><row><cell></cell><cell>JSL [29]</cell><cell>39.60 31.18</cell><cell>18.85</cell><cell>25.03</cell><cell>10.16</cell><cell>67.71 52.06</cell><cell>29.73</cell><cell>41.25</cell><cell>15.07</cell><cell>73.53</cell><cell>38.32</cell><cell>57.50</cell><cell>19.29</cell></row><row><cell></cell><cell>GSRTR [3]</cell><cell>41.06 32.52</cell><cell>19.63</cell><cell>26.04</cell><cell>10.44</cell><cell>69.46 53.69</cell><cell>30.66</cell><cell>42.61</cell><cell>15.98</cell><cell>74.27</cell><cell>39.24</cell><cell>58.33</cell><cell>20.19</cell></row><row><cell></cell><cell>CoFormer (Ours)</cell><cell>44.41 35.87</cell><cell>22.47</cell><cell>29.37</cell><cell>12.94</cell><cell>72.98 57.58</cell><cell>34.09</cell><cell>46.70</cell><cell>19.06</cell><cell>76.17</cell><cell>42.11</cell><cell>61.15</cell><cell>23.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Methods for Situation Recognition</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CRF [41]</cell><cell>32.34 24.64</cell><cell>14.19</cell><cell>-</cell><cell>-</cell><cell>58.88 42.76</cell><cell>22.55</cell><cell>-</cell><cell>-</cell><cell>65.66</cell><cell>28.96</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">CRF w/ DataAug [40] 34.12 26.45</cell><cell>15.51</cell><cell>-</cell><cell>-</cell><cell>62.59 46.88</cell><cell>25.46</cell><cell>-</cell><cell>-</cell><cell>70.44</cell><cell>34.38</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>RNN w/ Fusion [27]</cell><cell>35.90 27.45</cell><cell>16.36</cell><cell>-</cell><cell>-</cell><cell>63.08 46.88</cell><cell>26.06</cell><cell>-</cell><cell>-</cell><cell>70.27</cell><cell>35.25</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>GraphNet [19]</cell><cell>36.72 27.52</cell><cell>19.25</cell><cell>-</cell><cell>-</cell><cell>61.90 45.39</cell><cell>29.96</cell><cell>-</cell><cell>-</cell><cell>69.16</cell><cell>41.36</cell><cell>-</cell><cell>-</cell></row><row><cell>test</cell><cell cols="2">CAQ w/ RE-VGG [4] 38.19 30.23 Kernel GraphNet [32] 43.27 35.41</cell><cell>18.47 19.38</cell><cell>--</cell><cell>--</cell><cell>65.05 50.21 68.72 55.62</cell><cell>28.93 30.29</cell><cell>--</cell><cell>--</cell><cell>73.41 72.92</cell><cell>38.52 42.35</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Methods for Grounded Situation Recognition</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ISL [29]</cell><cell>39.36 30.09</cell><cell>18.62</cell><cell>22.73</cell><cell>7.72</cell><cell>65.51 50.16</cell><cell>28.47</cell><cell>36.60</cell><cell>11.56</cell><cell>72.42</cell><cell>37.10</cell><cell>52.19</cell><cell>14.58</cell></row><row><cell></cell><cell>JSL [29]</cell><cell>39.94 31.44</cell><cell>18.87</cell><cell>24.86</cell><cell>9.66</cell><cell>67.60 51.88</cell><cell>29.39</cell><cell>40.60</cell><cell>14.72</cell><cell>73.21</cell><cell>37.82</cell><cell>56.57</cell><cell>18.45</cell></row><row><cell></cell><cell>GSRTR [3]</cell><cell>40.63 32.15</cell><cell>19.28</cell><cell>25.49</cell><cell>10.10</cell><cell>69.81 54.13</cell><cell>31.01</cell><cell>42.50</cell><cell>15.88</cell><cell>74.11</cell><cell>39.00</cell><cell>57.45</cell><cell>19.67</cell></row><row><cell></cell><cell>CoFormer (Ours)</cell><cell>44.66 35.98</cell><cell>22.22</cell><cell>29.05</cell><cell>12.21</cell><cell>73.31 57.76</cell><cell>33.98</cell><cell>46.25</cell><cell>18.37</cell><cell>75.95</cell><cell>41.87</cell><cell>60.11</cell><cell>22.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Top-1 Predicted Verb</cell><cell cols="3">Top-5 Predicted Verbs</cell><cell></cell><cell cols="2">Ground-Truth Verb</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell></cell><cell></cell><cell>grnd</cell><cell>grnd</cell></row><row><cell cols="2">Method</cell><cell></cell><cell></cell><cell></cell><cell cols="2">verb value value</cell><cell cols="3">verb value value</cell><cell cols="4">value value-all value value-all</cell></row><row><cell cols="2">w/o Gaze-S1 Transformer</cell><cell></cell><cell></cell><cell></cell><cell cols="6">42.46 34.21 28.23 70.89 55.47 45.34 76.02</cell><cell>41.96</cell><cell>61.21</cell><cell>23.15</cell></row><row><cell cols="2">w/o Gaze-S2 Transformer</cell><cell></cell><cell></cell><cell></cell><cell cols="6">43.02 31.24 23.27 71.17 51.70 36.59 69.68</cell><cell>32.94</cell><cell>48.44</cell><cell>13.05</cell></row><row><cell cols="3">w/o Noun Classifiers on Gaze-S1 Transformer</cell><cell></cell><cell></cell><cell cols="6">41.30 33.33 27.50 69.76 55.05 44.96 75.97</cell><cell>41.94</cell><cell>61.32</cell><cell>23.39</cell></row><row><cell cols="12">w/o 38.59</cell><cell>55.10</cell><cell>17.10</cell></row><row><cell cols="3">w/o Verb Token in Gaze-S2 Transformer</cell><cell></cell><cell></cell><cell cols="6">44.36 35.57 29.16 72.84 56.79 46.19 74.53</cell><cell>39.83</cell><cell>60.07</cell><cell>21.83</cell></row><row><cell cols="2">CoFormer (Ours)</cell><cell></cell><cell></cell><cell></cell><cell cols="6">44.41 35.87 29.37 72.98 57.58 46.70 76.17</cell><cell>42.11</cell><cell>61.15</cell><cell>23.09</cell></row></table><note>. Quantitative evaluations of methods in SR and GSR. SR models are evaluated on the imSitu dataset, and GSR models are evaluated on the SWiG dataset. The only difference between the two datasets is the existence of bounding box annotation.Table 2. Ablation study of CoFormer on the SWiG dev set. The contributions of different components used in our model are evaluated.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Prediction Agent Horse Item Tool Cart Place Street Carting Agent Item Tool Place Agent Person Reference City Tool Pen Place Agent Reference Tool Place Drawing Man ?</head><label></label><figDesc></figDesc><table><row><cell>Role #1</cell><cell>Role #2</cell><cell>Role #3</cell><cell>Role #4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Agent Target Tool Place Mowing Agent Man Tool Lawn Mower Place Backyard Agent Person Target Paper Tool Pen Place ? Writing</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table E1 .</head><label>E1</label><figDesc>Number of parameters and inference time. Inference time was measured on the SWiG test set using one 2080Ti GPU.</figDesc><table><row><cell></cell><cell cols="3">Area (width ? height)</cell><cell cols="2">Aspect Ratio (width/height)</cell></row><row><cell>Metric</cell><cell cols="4">0-10% 10-20% 20-100% 0-5% 5-95%</cell><cell>95-100%</cell></row><row><cell>value</cell><cell>66.82</cell><cell>69.68</cell><cell>78.64</cell><cell>72.75 76.24</cell><cell>71.88</cell></row><row><cell>grnd value</cell><cell>7.42</cell><cell>25.38</cell><cell>65.49</cell><cell>36.88 62.62</cell><cell>31.01</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Endto-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human-Like Controllable Image Captioning With Verb-Specific Semantic Roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16846" to="16856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grounded Situation Recognition with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyeong</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngseok</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-Based Context Aware Reasoning for Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilini</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4736" to="4745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><forename type="middle">R L</forename><surname>Petruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Background to Framenet. International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Oxford research encyclopedia of psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><forename type="middle">L</forename><surname>Rebar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Habit Formation and Behavior Change</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Normalized and Geometry-Aware Self-Attention Network for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longteng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention on Attention for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4634" to="4643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Maps of Bounded Rationality: Psychology for Behavioral Economics. The American Economic Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="1449" to="1475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation-Grounded Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhesh</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15879" to="15889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HOTR: End-to-End Human-Object Interaction Detection With Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eun-Sol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CTRL-C: Camera Calibration TRansformer With Line-Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsung</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16228" to="16237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Situation Recognition with Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4173" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3585" to="3594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Paint Transformer: Feed Forward Neural Painting With Stroke Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6598" to="6607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context-Aware Scene Graph Generation With Seq2Seq Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15931" to="15941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent Models for Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="455" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Numeracy and Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>V?stfj?ll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Slovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ketti</forename><surname>Mazzocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Dickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="407" to="413" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grounded Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mixture-Kernel Graph Attention Network for Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="10363" to="10372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Human-Object Interaction Detection Using Interaction Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On Layer Normalization in the Transformer Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR, 2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph R-CNN for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Commonly Uncommon: Semantic Sparsity in Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7196" to="7205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Situation Recognition: Visual Semantic Role Labeling for Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image Captioning With Semantic Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatially Conditioned Graphs for Detecting Human-Object Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13319" to="13327" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
