<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-based Multimodal Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Pouthier</surname></persName>
							<email>baptiste.pouthier@nxp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NXP Semiconductors</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Universit? C?te d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LJAD</orgName>
								<address>
									<postCode>I3S</postCode>
									<region>Inria, Maasai</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Pilati</surname></persName>
							<email>laurent.pilati@nxp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NXP Semiconductors</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leela</forename><forename type="middle">K</forename><surname>Gudupudi</surname></persName>
							<email>leela.k.gudupudi@nxp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NXP Semiconductors</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Bouveyron</surname></persName>
							<email>charles.bouveyron@univ-cotedazur.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Universit? C?te d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LJAD</orgName>
								<address>
									<postCode>I3S</postCode>
									<region>Inria, Maasai</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Precioso</surname></persName>
							<email>frederic.precioso@univ-cotedazur.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Universit? C?te d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LJAD</orgName>
								<address>
									<postCode>I3S</postCode>
									<region>Inria, Maasai</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-based Multimodal Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: active speaker detection</term>
					<term>audiovisual</term>
					<term>multimodal fusion</term>
					<term>multi-objective</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is now well established from a variety of studies that there is a significant benefit from combining video and audio data in detecting active speakers. However, either of the modalities can potentially mislead audiovisual fusion by inducing unreliable or deceptive information. This paper outlines active speaker detection as a multi-objective learning problem to leverage best of each modalities using a novel self-attention, uncertaintybased multimodal fusion scheme. Results obtained show that the proposed multi-objective learning architecture outperforms traditional approaches in improving both mAP and AUC scores. We further demonstrate that our fusion strategy surpasses, in active speaker detection, other modality fusion methods reported in various disciplines. We finally show that the proposed method significantly improves the state-of-the-art on the AVA-ActiveSpeaker dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Active Speaker Detection (ASD) contemplates on identifying active speakers in a video by analyzing both visual and audio features. Hence, ASD is inherently multimodal in nature, where video and audio data are essential attributes. In recent years there has been considerable interest in the ASD methods based upon audio-visual cues <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Despite this interest, the lack of large-scale in-the-wild datasets impeded scientific progress in the field. Unfortunately, most of the prior works are challenged by skewed results owing to the poor quality of the considered datasets.</p><p>The recent AVA-ActiveSpeaker dataset <ref type="bibr" target="#b5">[6]</ref> can potentially overcome these limitations and reshape the ASD field of study. Together with the large in-the-wild dataset, the authors presented a baseline model based on a two-stream network that merges video and audio modalities in an end-to-end fashion. Within the annual AVA-ActiveSpeaker challenge <ref type="bibr" target="#b6">[7]</ref>, Chung <ref type="bibr" target="#b7">[8]</ref> and Zhang et al. <ref type="bibr" target="#b8">[9]</ref> improved this baseline using hybrid 3D-2D CNNs pre-trained on large-scale multimodal datasets <ref type="bibr" target="#b9">[10]</ref>. Unfortunately, this approach encounters two major challenges in practice, as illustrated in <ref type="figure" target="#fig_0">Fig.1:</ref> (1) multi-speaker scenario where multiple persons in a video frame are speaking, and (2) low-resolution and/or indiscernible faces in video frames. In <ref type="bibr" target="#b10">[11]</ref>, Alc?zar et al. addressed the multi-speakers scenario by learning long-term relationships between speakers, and Huang et al. <ref type="bibr" target="#b11">[12]</ref> handled the uncertainty in video modality by adding optical flow to raw pixel representation to strengthen face characterisation. Nevertheless, these studies focused on ad-hoc objectives with limited scope and little attention has been paid to the aggregated approaches which exploit correlation between both the challenges. In this paper, we propose to learn this correlation using a cross-modal fusion that involves simultaneous learning of the uncertainty in both the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detected face</head><p>Zoomed face <ref type="figure" target="#fig_0">Figure 1</ref>: This is an illustration of how ambiguous both video and audio modalities are within the ASD problem. (a) represents the scene's audio track that contains speech. But the uncertainty here is who is speaking? It is difficult to say it from video also, because some face resolutions are insufficient (b,c) or characters lips may be partially (c) or entirely (d) concealed.</p><p>modalities using a multi-objective learning scheme as described in Section 2.1. Traditionally, in multi-task learning, uncertainty is handled by learning adaptive weighting between each task's loss functions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>In the field of Automatic Video Description, researchers have investigated attention-based <ref type="bibr" target="#b14">[15]</ref> fusion mechanisms that capture the importance of each of the modalities. Hore et al. <ref type="bibr" target="#b15">[16]</ref> proposed a model that selectively uses features from different modalities. This approach was later improved using hierarchical attention fusion in <ref type="bibr" target="#b16">[17]</ref>. Despite the growing interest of these fusion mechanisms in other disciplines, to the best of our knowledge, no studies have been conducted on ASD problem. The present paper aims to investigate different fusion schemes to solve the ASD problem. We also propose a novel audiovisual fusion mechanism as a first attempt to enrich the self-attention model with uncertainty information to disentangle the practical challenges.</p><p>The proposed method consists of learning a self-attention <ref type="bibr" target="#b14">[15]</ref> and uncertainty-based fusion mechanism that weights video and audio embeddings in an end-to-end fashion. We use the typical two-stream DNN architecture from the literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>, with a stream per each modality, to encode both embeddings. By characterizing uncertainty of each modality, we aim at disentangling the problem presented in <ref type="figure" target="#fig_0">Fig.1</ref>. Indeed, the fusion scheme we propose determines the viability of each modality at any given time to learn a comprehensive understanding of every situation towards ASD disambiguation. Our solution significantly outperforms state-of-the-art in the AVA-ActiveSpeaker dataset by 4.8% and 1.7% on validation and test sets, respectively.  <ref type="figure">Figure 2</ref>: End-to-end multi-objective audiovisual network. Video and Audio Embedding Networks are detailed in <ref type="figure" target="#fig_2">Fig.3</ref>. "Aux" block represents an auxiliary classifier composed with two fully connected layers of 128-dim and 2-dim. Conflicting video and audio information lead to a compromised audio-visual learning. Most recent ASD studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> defined a trade-off optimizing both video and audio streams towards video label, sometimes using auxiliary classifiers to increase the discriminative power of the individual streams towards the unique objective <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>. However, this approach undermines the learning of the audio stream. In this section, we detail the multi-objective optimization of the video and audio streams towards their respective ground-truth labels to learn unbiased and accurate representations of video and audio modalities. Then, we introduce a novel audiovisual fusion mechanism that uses self-attention and uncertainty indicators to better disentangle the ambiguous scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Active Speaker Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-Objective Learning</head><p>Let (Xt = {x n t } n?N ) t?N be a series of consecutive frames, where x n t denotes the n th face detected in the t th frame, and (At) t?N is the corresponding audio segment. Each audiovisual sequence is noted M n t = (x n t , At) t?N . During training, each multimodal sample M n t is associated with a video-based ground-truth label y nV t where y nV t = 1 if the face x n t is speaking and y nV t = 0 otherwise. To cater multiobjective learning, we also define audio-based ground-truth by aggregating video-based labels of each frame using Eq.1.</p><formula xml:id="formula_0">y A t = 0 iif n y nV t = 0, 1 otherwise (1)</formula><p>We train a standard two-stream architecture, presented in <ref type="figure">Fig.2</ref>, that leverages auxiliary classifiers (Aux) to encourage video and audio networks to minimize their own loss function, thus optimizing both modalities intermediate feature representations.</p><p>The outputs of the Video, Audio and Multimodal networks are denoted as? V ,? A , and? M respectively; we formulate these quantities as? nV</p><formula xml:id="formula_1">t = f W V (x n t ),? A t = f W A (At), and y nM t = f W M (M n t )</formula><p>where WV , WA, and WM are the weights of the respective networks. To train these weights, we define the loss function L as a cross-entropy loss function L(y,?)</p><formula xml:id="formula_2">= ? i yilog(?i) + (1 ? yi)log(1 ??i). The final multi-objective loss-function L f is formulated as L f = LM + LV + LA, where LM = L(y V ,? M ), LV = L(y V ,? V ), and LA = L(y A ,? A ) respectively.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cross-Modality Fusion</head><p>This section describes a multimodal fusion that focuses on disentangling ambiguous scenarios in ASD problem with  self-attention and uncertainty estimation. The key strategy here is to consider dynamically varying weights for each modality at any given time by computing an estimate of their relative uncertainty. <ref type="figure" target="#fig_3">Fig.4</ref> illustrates the concept of our fusion mechanism, inspired by the method presented in <ref type="bibr" target="#b16">[17]</ref>, that fuses the information from both modalities in a hierarchical fashion. First, we feed each video (u V t ) T t=1 and audio (u A t ) T t=1 embedded sequences to separated 128-dim BiGRU layers with hidden state h {V,A} t . Then, we compute the multimodal embedding u M t using weighted concatenation as given by Eq.2:</p><formula xml:id="formula_3">u M t = ? V t h V t ? ? A t h A t<label>(2)</label></formula><p>where the weights ? V t and ? A t characterise, at each time t, the uncertainty of the video and audio modalities. Finally, we use post-fusion 100-dim BiGRU layers with hidden state h M t , a 2-dimensional Fully Connected (FC) layer, and a softmax.</p><p>In <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, the ? weights are learnt using perceptrons that fully-connect, at any given time, the hidden representation of each modality. This paper presents a novel approach to compute the weights of the modalities using: (1) an estimation of the uncertainty of both video and audio embedded representations, and (2) self-attention to measure the importance of video and audio modalities in their local temporal context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-Level Attributes:</head><p>To characterize the uncertainty, we first rely on a high-level assessment of the quality of the video and audio data. We associate each multimodal training sample M n t with two attributes ?t and ? n t that characterize the number of detected faces in frame Xt and the resolution of the targeted face x n t , respectively. Indeed, higher the number of potential speakers lower the chance for the audio to be discriminative. Similarly, low-resolution face coincides with prediction uncertainty as it is more difficult to interpret facial cues and lips movements on a smaller face thumbnail. We denote ?t = card(Xt) and ? n t the number of pixels in the face thumbnail x n t before any resizing. To scale the two quantities ? and ?, we first use data binning then target encoding <ref type="bibr" target="#b17">[18]</ref> to replace each value with a blend of posterior and prior Auxiliary Uncertainty: We leverage auxiliary classifiers predictions to estimate the uncertainty in video and audio embeddings. Intuitively, the data leading to a highly polarized auxiliary decision can be considered as reliable. The output of a softmax layer cannot be used reliably as true probabilities <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Therefore, we use a simple yet effective workaround called temperature scaling <ref type="bibr" target="#b19">[20]</ref> to generate relevant auxiliary output scores using a calibrated softmax. We denote ? V t and ? A t the uncertainty values linked to the video and audio auxiliary predictions, respectively. Given? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention:</head><p>We aim at using dot-product global selfattention <ref type="bibr" target="#b14">[15]</ref> mechanism to evaluate how important video and audio embedded representations are within their local temporal context. Given H ? R T ?128 as defined in Eq.3, we compute the self-attention scores S {V,A} ? R T ?T using Eq.4.</p><formula xml:id="formula_4">H {V,A} = [h {V,A} 1 , ..., h {V,A} T ]<label>(3)</label></formula><formula xml:id="formula_5">S {V,A} = sof tmax(H {V,A} H {V,A} T + B)<label>(4)</label></formula><p>B ? R T ?T is the bias matrix that limits the temporal context: the future timestamps are masked to preserve self-attention causality, and the distant past timestamps are masked too, to keep only the recent past events. The motivation is to transform the score matrix to a one-dimensional array by assigning a scalar value to each intermediate feature. We extract the diagonal values of S {V,A} such as a {V,A} t = diag(S {V,A} )t is the attention scalar that characterizes h {V,A} t within its local temporal context [t-3, .., t]. Here, the normalization effect of the softmax is critical, as each diagonal element will be scaled according to its relative importance within its context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attributes Combination:</head><p>Let v be the combination of all the uncertainty indicators such that vt = {?t, ?t, ? V t , ? A t , a V t , a A t }. ? V t and ? A t in Eq.2 are dynamically computed using Eq.5:</p><formula xml:id="formula_6">? {V,A} t = W {V,A} vt + b {V,A}<label>(5)</label></formula><p>where W {V,A} and b {V,A} are trainable weights and biases, respectively, that are updated during the end-to-end training of the whole architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mAP AUC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation subset</head><p>Our method 91.9 96.3 Active Speakers Context <ref type="bibr" target="#b10">[11]</ref> 87.1 -Huang et al. <ref type="bibr" target="#b11">[12]</ref> -93.2 Google Baseline <ref type="bibr" target="#b5">[6]</ref> 86.3 92.0 Naver Corp. (Temporal Convolutions) <ref type="bibr" target="#b7">[8]</ref> 85.5 -Zhang et al. <ref type="bibr" target="#b8">[9]</ref> 84.0 -ActivityNet Challenge Leaderboard Our method 89.5 -Naver Corp. <ref type="bibr" target="#b7">[8]</ref> 87.8 -Active Speakers Context <ref type="bibr" target="#b10">[11]</ref> 86.7 -Zhang et al. <ref type="bibr" target="#b8">[9]</ref> 83.5 -Google Baseline <ref type="bibr" target="#b5">[6]</ref> 82.1 -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setup</head><p>AVA-ActiveSpeaker Dataset <ref type="bibr" target="#b5">[6]</ref>: It is the most comprehensive, largest, and challenging publicly available dataset for audiovisual ASD problem. It consists of 262 movies divided into training (120), validation (33) and test (109) sets. In total, 5,498K faces are labeled with normalized bounding boxes. For training and validation sets, ground-truths on whether someone is speaking are also provided. The ground-truths for the test set are withheld for the annual ActivityNet Challenge <ref type="bibr" target="#b6">[7]</ref>.</p><p>Training Strategy: The ADAGRAD optimizer <ref type="bibr" target="#b21">[22]</ref> is used with a learning rate of 0.015 to train the network in end-to-end fashion for 20 epochs with mini-batches of 16 sequences and without any pre-training. Roth et al. <ref type="bibr" target="#b5">[6]</ref> demonstrated that stacking few consecutive frames as input of the first 2D convolutional layer is beneficial to learn short temporal motion. Therefore, the input to the visual network <ref type="figure" target="#fig_2">(Fig.3b</ref>) is a stack of 3 consecutive grayscale face thumbnails. The faces are extracted using the provided bounding boxes and resized to 224x224. We feed the audio embedding network <ref type="figure" target="#fig_2">(Fig.3c</ref>) with 13 MFCC features extracted from the preceding 0.5s of audio with a 25ms window and a 10ms step. Our BiGRUs are trained with 1.12s long segments (28 frames) to capture most of the different speech patterns within the AVA-Active-Speaker dataset, the average continuous speech duration being 1.11s <ref type="bibr" target="#b5">[6]</ref>.</p><p>Metrics: For ease of comparison with previous studies, we evaluate the proposed method using the official ActivityNet Challenge evaluation script <ref type="bibr" target="#b5">[6]</ref> that computes the mean Average Precision (mAP) score. When available, Area Under Receiver Operating Characteristic Curve (AUC) score is also provided in <ref type="table" target="#tab_1">Table 1</ref>. As the AVA-ActiveSpeaker test set ground-truths are kept private for the official challenge, most of our performance analysis is conducted on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison with State Of The Art</head><p>Experimental results in <ref type="table" target="#tab_1">Table 1</ref> show that the proposed method outperforms all existing approaches on both test and validation sets, in terms of mAP and AUC scores. It is worth highlighting that the proposed method outperforms one of the best approaches in <ref type="bibr" target="#b10">[11]</ref> by a significant margin of 4.8% and 2.8% in mAP score on the validation and test sets respectively. Furthermore, our approach surpasses Naver's method <ref type="bibr" target="#b7">[8]</ref>   . For the number of faces, we split the validation set into three subsets by gathering one, two, and three faces frames. Altogether, these three cases cover more than 90% of the AVA-ActiveSpeaker dataset. For the face sizes, we sort the validation dataset by ascending face-size order and split it in three equal parts denoted Small (S), Medium (M) and Large (L) respectively. <ref type="table" target="#tab_3">Table 2</ref> compares the number of parameters of the topranked models on the ActivityNet Challenge Leaderboard. Our results are very favorable since our approach has significantly fewer parameters compared to the state of the art and does not necessitates any pre-training and/or ensemble-models.  <ref type="figure" target="#fig_2">Fig.3</ref> when used alone. The presented results are with 128-dim BiGRU added on top of each embedding network. We compare the performance of the audio embedding network optimized towards either video or audio ground-truth labels as discussed in Section 2.1. As expected, the performance of the audio embedding network trained with video ground-truths strongly degrades on the number of speakers. It suffers in the ambiguous scenario presented in <ref type="figure" target="#fig_0">Fig.1</ref> where multiple persons share the same audio track. On the contrary, the performance of the audio network trained with audio labels is almost constant while increasing the number of speakers. We also observed a major mAP score increase of 26.9% compared with using video labels. Thus, the multi-objective approach using an independent audiobased labels allows the reliability on the audio network in difficult/ambiguous scenarios.</p><p>Figures 5b and 5d compare the performance of our fusion method with the Na?ve and Hierarchical <ref type="bibr" target="#b16">[17]</ref> Fusion schemes in different scenarios. Our multi-objective model is first evaluated using a na?ve concatenation fusion (NF) to combine video and audio modalities. Here it is crucial to note that the method improves Active Speakers Context <ref type="bibr" target="#b10">[11]</ref> mAP by 3.1% on validation subset <ref type="table" target="#tab_1">(Table 1</ref>). This result highlights the effectiveness of our end-to-end multi-objective learning. Hierarchical Fusion (HF) refers the fusion scheme presented in <ref type="bibr" target="#b16">[17]</ref>. Note that our adaptation implies a slight modification of the initial method to match our "many-to-many" architecture. As shown in <ref type="figure" target="#fig_5">Fig.5b</ref>, the proposed fusion scheme clearly has an advantage over NF and HF. <ref type="figure" target="#fig_5">Fig.5d</ref> presents additional comparative analysis results by varying the number of faces detected (left) and the size of the face thumbnails (right). The proposed method clearly outperforms both NF and HF schemes, especially in challenging scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we proposed a self-attention and uncertaintybased fusion mechanism that learns a comprehensive understanding of every situation towards ASD disambiguation. Our approach catered multi-objective optimization to encourage the learning of unbiased multimodal features. Experimental results on the challenging AVA-ActiveSpeaker dataset demonstrate that the proposed method achieves superior performance than existing methods. Besides, the proposed method outperformed the state-of-the-art on both validation and test datasets and ranked first in the ActivityNet Challenge, despite having fewer parameters and without any pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head><p>This work has been supported by the French government, through the 3IA C?te d'Azur Investment in the Future project managed by the National Research Agency (ANR) with the reference numbers ANR-19-P3IA-0002.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>demonstrates video and audio modalities may conflict, with non-talking faces affiliated with speech-labeled audio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>x2 MaxPool, k:3x3, s:2x2 Block, k:3x3, s:1x1 MaxPool, k:3x3, s:2x2 Block, k:3x3, s:1x1 Block, k:3x3, s:1x1 MaxPool, k:3x3, s:2x2 Block, k:6x6, s::3x3, s:1x1 MaxPool, k:3x3, s:2x2 Block, k:3x3, s:1x1 Block, k:3x3, s:1x1 MaxPool, k:3x3, s:2x2 Block, k:6x6, s:Block (b) Video Network (c) Audio Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) presents the building block of both video (b) and audio (c) embedding networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Multimodal fusion scheme where video and audio embedded representations are merged. FC represents a 2-dim Fully Connected layer with a softmax function. probabilities of target over the entire training data. Five bins are used to pre-process the number of detected faces (four bins for ? ? 4 plus a bin handling ? ? 5). Face-size values are discretized within eight bins with first and last bins being left-open and right-open intervals, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>probabilities of the video or audio auxiliary output, we define ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Performance comparison of the embedding (a,c) and the multimodal (b,d) models on the validation set. (c) and (d) present a performance analysis according to the number of detected faces (left) and the face size (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of the proposed method with the state of the art. Results are reported on both validation and ActivityNet Challenge hidden test sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the gross number of parameters.</figDesc><table><row><cell>Method</cell><cell>#params pre-training</cell></row><row><cell>Our method Naver Corp. [8]</cell><cell>2M 13M</cell></row><row><cell>Active Speakers Context [11]</cell><cell>22M</cell></row><row><cell>Zhang et al. [9]</cell><cell>22M</cell></row><row><cell>3.3. Performance Breakdown</cell><cell></cell></row><row><cell cols="2">Multi-objective learning, as formulated in Section 2.1, aims at (1) learning accurate representations of each modality, and (2) allowing unbiased estimation of the uncertainty of video and audio intermediate features. We therefore evaluate the discriminative power of video and audio embedded representations. Figures 5a and 5c detail the performance of the embedding networks detailed in</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Look who&apos;s talking: speaker detection using video and audio correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Cat. No.00TH8532</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Who&apos;s speaking? audio-supervised classification of active speakers in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="87" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-modal supervision for learning active speaker detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Active speaker detection with audio-visual co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Hamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 18th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lip-reading, ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ava-activespeaker: An audio-visual dataset for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Naver at activitynet challenge 2019 -task b active speaker detection (ava)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-task learning for audio-visual active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perfect match: Improved cross-modal embeddings for audio-visual synchronisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active speakers in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Alcazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved active speaker detection based on optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Apathy classification by exploiting task relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Happy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG2020 -15th IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion for video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harsham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical Multimodal Attention for Deep Video Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Precioso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Menguy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Micci-Barreca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="27" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks,&quot; ser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
