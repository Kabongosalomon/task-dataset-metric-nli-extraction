<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate Sentence Similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07">July, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Nayantara Jeyaraj</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sri Lanka Institute of Information Technology</orgName>
								<address>
									<settlement>Malabe</settlement>
									<country key="LK">Sri Lanka</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharshana</forename><surname>Kasthurirathna</surname></persName>
							<email>2dharshana.k@sliit.lk</email>
							<affiliation key="aff0">
								<orgName type="department">Sri Lanka Institute of Information Technology</orgName>
								<address>
									<settlement>Malabe</settlement>
									<country key="LK">Sri Lanka</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate Sentence Similarity</title>
					</analytic>
					<monogr>
						<title level="j" type="main">International Journal of Engineering Trends and Technology</title>
						<imprint>
							<biblScope unit="volume">69</biblScope>
							<biblScope unit="page" from="181" to="189"/>
							<date type="published" when="2021-07">July, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.14445/22315381/IJETT-V69I7P225</idno>
					<note>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multi-layer network</term>
					<term>network science</term>
					<term>semantic similarity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Similarity is a comparative -subjective measure that varies with the domain within which it is considered. In several NLP applications such as document classification, pattern recognition, chatbot questionanswering, sentiment analysis, etc., identifying an accurate similarity score for sentence pairs has become a crucial area of research. In the existing models that assess similarity, the limitation of effectively computing this similarity based on contextual comparisons, the localization due to the centering theory, and the lack of non-semantic textual comparisons have proven to be drawbacks. Hence, this paper presents a multi-layered semantic similarity network model built upon multiple similarity measures that render an overall sentence similarity score based on the principles of Network Science, neighboring weighted relational edges, and a proposed extended node similarity computation formula. The proposed multi-layered network model was evaluated and tested against established state-of-the-art models and is shown to have demonstrated better performance scores in assessing sentence similarity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The advent of a mass generation of digital content has brought about an inevitable overflow of user-generated content and articles across the web. However, the verifiability of all these contents is not guaranteed and keeps growing exponentially.</p><p>As such, multiple articles that exist may refer to the same subject, which can be a particular topic, an idea, an entity, or an object. Nevertheless, the information that these articles bear with regard to the subject may vary across different sources. Pieced together, the various information about a particular subject helps in building an overall perspective of that subject.</p><p>Considering the conventional ETL (Extract, Transform and Load) functions pertaining to the database context, it can be observed that data is being collected from a multitude of sources about similar subjects and is transformed into a general format where a particular subject's features are keyed in as its data values before being loaded onto the target database. Such data processing enables a system to gather more vivid and lucid information about each and every subject and store it in an easily accessible manner, paving the way for a data seeker to retrieve expected information efficiently in the future.</p><p>Hence, intense research has been focused on the area of identifying similarities between documents, sentences, and, moreover, purely facts. "Similarity" is, however, a comparative measure that varies with what is being compared and the subjective area within which it functions. As such, this research focuses on the identification of similarity within the subjective area of computational linguistics. Thereby, this research presents a novel multilayered semantic similarity network that efficiently outperforms existing similarity inspection algorithms that evaluate sentence similarity based on a single metric by applying the principles of network science and the proposal of an extended node similarity formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Within the domain of Natural Language Processing, Semantic Similarity has been considered a crucial aspect and paramount for many applications that lie within the related fields. Semantic Textual Similarity (STS) is a measure applied to a group of sentences or documents in order to determine their semantic similarity. This is assessed based on their overt and indirect associations or relationships with other documents or sentences in the corpus. As such, the existence of such semantic relationships is harnessed to quantify and understand the similarity between them. Semantic similarity assessments were vividly researched in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref>.</p><p>Over the years, several solutions have been proposed to assess semantic similarity, and the following elucidates the current state-of-the-art models established in this particular area of NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SMART</head><p>The most general approach to semantic similarity assessment has been pre-training on massive datasets before fine-tuning on subsequent use-cases ([3]- <ref type="bibr" target="#b4">[5]</ref>). However, such vigorous fine-tuning frequently overfit the training data close to the model when it comes to downstream tasks and thereafter struggles to generalize to new datasets. This occurs due to the insufficient amount of data available for downstream tasks and the highly complex structure of the pre-trained models.</p><p>As such, <ref type="bibr" target="#b5">[6]</ref> proposed a model that counters the above issues and is built upon a learning framework that stabilizes the fine-tuning process of pre-trained models such that they generalize effectively to new datasets. This model is primarily built on two concepts. ? Smoothness-inducing adversarial regularization ? Bregman proximal point approximation</p><p>Smoothness-inducing adversarial regularization controls the complexity of the model during the fine-tuning process by enforcing a precise regularization technique. Having a model (. ; ?) with number of data points for the intended task represented as ( , ) =1 .Here, is the vectorized input sentence while bears the corresponding labels.</p><formula xml:id="formula_0">( ) = ( ) + ( )<label>(1)</label></formula><p>In equation <ref type="formula" target="#formula_0">(1)</ref>, (?) is the loss function, ? is the tuning parameter and is the smoothness inducing adversarial regularization?</p><p>As aggressively updating the model can lead to overfitting, a Bregman proximal point approximation is applied to equation (1). This enforces a large penalty during each iteration to reduce the aggressive updates. This model has the ability to adapt well to different domains and is currently the top benchmark model for semantic textual similarity tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. StructBERT</head><p>Extended from BERT (Bidirectional Encoder Representations from Transformers), which conditions on the surrounding contexts from all the layers, StructBERT takes the structure of the language into consideration in pre-training <ref type="bibr" target="#b6">[7]</ref>. Initially, as an input text or pair of sentences are passed into the model, it converts the sentences into a single token and identifies a contextualized vector representation for that token. The vector is formed based on the words present in the text, the position in which each word occurs, and the context or the part of the text where it occurs.</p><p>The embeddings are subsequently passed on to a stack of multi-layer bidirectional transformers <ref type="bibr" target="#b7">[8]</ref> that observe the entire sequence of the sentence or text and implement self-attention to render the textual representation. StructBERT derives concepts of pre-training objectives from BERT and extends them upon the "word structure" for single sentences and the "sentence structure" for paired sentences. Exploiting these objectives, StructBERT understands language structure in varying granularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. T5-11B</head><p>This model is based on transfer learning, where it is initially trained on large, data-intensive tasks, after which it is fine-tuned to specific tasks <ref type="bibr" target="#b8">[9]</ref>. T5-11B formats any text-based task into a text-to-text problem. This follows the encoder-decoder transformer presented by <ref type="bibr" target="#b7">[8]</ref>.</p><p>First, an input token sequence is vectorized and passed onto the encoder. The encoder contains a self-attention layer and a feed-forward network that normalize the input. The input of each element is then added to the model's output using a residual skip connection with dropout.</p><p>The decoder resembles the encoder with the exception of an additional standard attention model on top of the self-attention mechanism and the use of relative position embeddings <ref type="bibr" target="#b9">[10]</ref> instead of fixed position embeddings. This model performs better with larger datasets. However, in most cases, these tasks have limited availability or access to such resources and can prove to be expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. T5-3B</head><p>By exchanging parameters through encoder and decoder, the number of parameters is similar to BERT's sole encoder while trying to avoid any drop in the output <ref type="bibr" target="#b8">[9]</ref>. It also uses a masking objective to remove noise. The pretraining process is conducted on a large data corpus, and the parameters are updated during the fine-tuning process.</p><p>However, in T5-3B, the model's capability to effectively accumulate knowledge while adding masking and then fine-tuning for a specific task is restricted by what the model understands by simply predicting sequences of corrupted text. One obstacle faced by this text-to-text technique is the risk of the model failing to produce one of the terms required during the testing process.</p><p>Working on 3 billion parameters, T5-3B is outperformed by the T5-11B model that harnesses 11 billion parameters. The T5-Large and T5-small models also follow the same architecture of the T5 family of textto-text transformers, with a comparatively lower number of parameters as opposed to the T5-3B and T5-11B models.</p><p>The major issue with the T5 models is their size. Compared to established models such as BERT, these models are 30 times more in size and are expensive to be utilized on commodity GPU hardware systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Real Former</head><p>RealFormer is a Residual Attention Layer Transformer <ref type="bibr" target="#b10">[11]</ref>. Deriving from the conventional transformer models, RealFormer adds a residual score to each and every attention head's raw attention score. The two attention scores are considered to render a single attention weight value using a SoftMax function.</p><p>The RealFormer outperforms the post-LN architecture as well as the Pre-LN architecture that generates vector representations of tokens with its addition of attention scores and aggregated attention weight.</p><p>Another method that had been employed to assess semantic similarity was done through Semantic similarity graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Semantic Similarity Graph</head><p>This is an unsupervised learning approach that uses pretrained vectors to build a vector from a sentence <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. If a sentence S is made up of M number of words, then these words will be transformed into their respective word vector.</p><formula xml:id="formula_1">= { 1 , 2 , ? , } { 1 , 2 , ? , } ? 1 ???? , 2 ?????? , ? , 3 ????? }<label>(2)</label></formula><p>Aggregating the word vectors, a sentence vector s will be obtained using the following equation.</p><formula xml:id="formula_2">= 1 ? ????? =1<label>(3)</label></formula><p>There are three methods to construct a similarity graph from the text: Preceding adjacent vertex (PAV), Single Similarity Vertex (SSV), and Multiple Similarity Vertex (MSV). These vary based on the method used to decide a sentence vertex's corresponding counterpart vertex <ref type="bibr" target="#b13">[14]</ref>. a) Preceding Adjacent Vertex: This graph construction method takes the general concept behind how humans read and comprehend. In trying to comprehend particular text, humans look backward for any content that they have already read <ref type="bibr" target="#b14">[15]</ref>.</p><p>Here, the similarity measure ( , ) for a pair of sentences and will be given by;</p><p>( , ) = ( , ) + (1 ? ) (?? , ?? ) (4) Where ? is the balancing factor [0,1] and uot indicates the unique overlapping terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) Single Similarity Vertex: SSV is based on the equivalence of Semantic dependency and Semantic</head><p>Similarity. Since dependency can happen in both directions, edges are allowed from following and preceding vertices. In PAV, "Precedence" and "Adjacency" are crucial constraints. But, SSV only takes the semantic similarity of sentences into consideration with directed and weighted edges where only a single outgoing edge from each vertex is allowed. An edge between two vertices will be established and assigned a weight using equation <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_3">( _{ , }) = ( , ) | { ? } (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c) Multiple Similarity Vertex:</head><p>In contrast to PAV and SSV, which only allow a single outgoing edge from each vertex, MSV allows multiple. It chooses multiple similar sentences above a certain threshold ( ) of cosine similarity <ref type="bibr" target="#b15">[16]</ref>. Here, edges are established and assigned weights using equation <ref type="bibr" target="#b4">(5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Computing the Similarity</head><p>From the above-constructed graphs, the text or sentence similarity (Sim) will be calculated by averaging the average weight of edges departing from one vertex.</p><formula xml:id="formula_4">= 1 ? 1 ? ? ( ) =1 =1<label>(6)</label></formula><p>In the above equation <ref type="formula" target="#formula_4">(6)</ref>, is the Number of sentences in the text, is the number of outgoing edges from a particular vertex. ? ( ) is the corresponding weight assigned to the edge that connects nodes and .</p><p>As such, the similarity between sentences in the graphs constructed using the above three methods (PAV, SSV, and MSV) can be calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MOTIVATION</head><p>In the former section, most of the established models are extremely large in size, expensive, and used de-noising auto encoding objectives during training. On the other hand, the semantic similarity graph methods used to measure sentence and text similarity were based on the centering theory that strictly observes the principle of the number of attention shifts in the text being inversely proportional to the coherence or the semantic similarity of that text. Consequently, these models are localized to their near-neighborhood of sentences (preceding and following sentences) and fail to efficiently portray the similarity between distant or detached sentences.</p><p>Furthermore, these approaches constrict the models from simultaneously applying different similarity metrics in evaluating the sentences that are semantically similar to the sentencing entity in the network.</p><p>Therefore, the motivation of this research is to build a model that efficiently computes the similarity between any sentence pairs, neighboring sentences as well as detached sentences allowing the semantic similarity to be simultaneously assessed based on multiple similarity measures. Hence, instead of viewing this problem on a flat plane, this research proposes a multi-layered semantic similarity network, MNet-Sim. This paper also introduces a node similarity computation formula to gauge the overall similarity between sentence pairs based on the constructed multi-layer network. Further details with regard to the proposed model are elucidated in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>In addressing the issue of effectively computing the similarity between sentences, this research introduces a multi-layered semantic network built upon different similarity measurement metrics. The advantage in using a multi-layer network with different dimensions allows the same sentences to be modeled on different planes under different variables that build relationships among the sentences. Therefore, the principle of layering the variables which need to be considered as influences in similarity computation allows for the extension or addition of more parameters <ref type="bibr" target="#b16">[17]</ref>. As such, at present, this model takes the Cosine Similarity, Phrasal Overlap, Euclidean distance, Jaccard similarity, and Word mover's distance as dimensional parameters on a multi-layered semantic similarity graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig 1: Multi-layer graph with three layers or dimensions</head><p>The conceptual layering of the model can be visualized, as shown in <ref type="figure">Fig. 1</ref>. With regard to this research, the topmost layer will correspond to the Cosine Similarity Network. The middle layer will represent the phrasal overlap network. And the bottom-most layer or any extending layers could be related to additional variables that can be introduced as an influencing factor in computing the overall similarity measure.</p><p>Each dimension in this multi-layered network will be composed of a single network of sentence vector nodes (conceptually represented by the vertices labeled 1-7). Once the individually weighted and layered network dimensions are aligned, they correspond to the same sentences, vertices, or sentence vector nodes. Subsequently, by using the proposed mathematical computation, the neighboring weighted relational edges can be aggregated, and the overall similarity between paired sentences be computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generating the Nodes for each Layer</head><p>Each layer is composed of nodes that represent sentence vectors. Since this research intends on identifying the similarity between sentences in passages or abstracts, using the ALBERT Sentence embedding <ref type="bibr" target="#b17">[18]</ref> has been reported. The choice of ALBERT as the vectorization algorithm was based on its performance of having a higher data throughput, applying factorized embedding parameterization, and allowing cross-layer parameter sharing.</p><p>To generate these sentence embeddings, the data processing is done by initially tokenizing the sentences fed into the model in the form of a raw text dataset. Proceeding tokenization, the tokenized text is encoded as sentence embeddings or sentence vectors. a) Cosine Similarity Layer: the first layer of the multilayered network has been implemented based on the cosine similarity values between sentence vector pairs. As such, the cosine similarities between all the permutations of each sentence vector are taken, and the Euclidean dot product formula is used to calculate the Cosine similarity of two non-zero vectors <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_5">. = ?{ } ?{ }<label>(7)</label></formula><p>Given two vectors and , the cosine similarity cos (?) is represented using a dot product and a magnitude as:</p><formula xml:id="formula_6">= = . ? ?? ? . ? ?? ? = ? =1 ?? 2 =1 ?? 2 =1<label>(8)</label></formula><p>The computation of cosine similarities will render a vector matrix with the cosine similarities in the form of soft truth values ranging from [0,1]. Based on these cosine similarities and a decided threshold (?), edges weighted with cosine similarities that are greater than ? will be established as relational edges between the sentence vector nodes, thus generating the cosine similarity layer in the multi-layered network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) Phrasal Overlap Layer:</head><p>The second layer of the proposed multi-layer network is built upon the Phrasal overlap scores between sentences. This measure is based on the Zipfian relationship <ref type="bibr" target="#b19">[20]</ref>. Since conventional word overlap methods consider the sentence as a mere bag of words, the Phrasal Overlap measure has been used in order to distinguish phrases from single words in the corpus. The phrasal overlap is calculated as a nonlinear function. Here indicates the number of -word phrases, and indicates the word overlaps in the sentence pairs.</p><formula xml:id="formula_7">? ( 1 , 2 ) = ? ? 2 =1 (9)</formula><p>To minimize the effect of outliers, equation <ref type="formula">(9)</ref> is normalized using the sum of the sentences' lengths and a tanh function as shown in equation <ref type="formula" target="#formula_0">(10)</ref>.</p><formula xml:id="formula_8">( 1 , 2 ) = ? ( ? ( 1 , 2 ) | 1 | + | 2 | )<label>(10)</label></formula><p>The phrasal overlap score, ( 1 , 2 ) For the sentence, pairs will be assigned as the edge weight between the nodes in the phrasal overlap layer. As such, the second layer, i.e., the phrasal overlap layer of the multi-layer network, will be constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c) Euclidean Distance Layer:</head><p>The length of a straight line connecting two points in a Euclidean space renders the Euclidean distance between those two points. Once the points are mapped onto a Cartesian plane, this distance can be computed using the Pythagorean theorem.</p><p>Having 2 vectors represented by and , 1 is the frequency of vector in the first document or sentence and 2 is the frequency of vector in the second document or sentence. Likewise, 1 is the frequency of vector in the first document or sentence and 2 is the frequency of vector in the second document or sentence.</p><p>As such, the Euclidean distance between the two vectors will be calculated as shown in equation <ref type="bibr" target="#b10">(11)</ref>.</p><formula xml:id="formula_9">( , ) = ?( 1 ? 1 ) 2 + ( 2 ? 2 ) 2<label>(11)</label></formula><p>The same equation can be expanded using the norm of vectors to accommodate n-dimensional vectors, as stated in equation <ref type="bibr" target="#b11">(12)</ref>.</p><formula xml:id="formula_10">( , ) = ??( ? ) 2 =1 ( 1 , 2 ) = ( , )<label>(12)</label></formula><p>Since the Euclidean distance computes the deviation or level of difference in similarity between the vectors, the inverse of the Euclidean distance is taken as the edge weight between sentence vectors. Sentence vector pairs bearing an inverse Euclidean distance weight that is greater than the selected threshold are established as edges in the Euclidean distance dimension of the multi-layer network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d) Jaccard Similarity Layer:</head><p>The Jaccard similarity, which is yet another way to compute similarity, aims to identify the shared number of tokens between sentences and divide it by the number of unique tokens found in the sentences being compared. Thereby, the common equation representing the Jaccard similarity index is shown in equation <ref type="bibr" target="#b12">(13)</ref>.</p><formula xml:id="formula_11">( 1 , 2 ) = 1 ? 2 1 ? 2 ( 1 , 2 ) = 1 ? 2 | 1 | + | 2 | ? | 1 ? 2 |<label>(13)</label></formula><p>( 1 , 2 ) , represented as ( 1 , 2 ) Is taken as the weight between the two sentences being compared and is used to filter out sentence pairs that bear a weight greater than the selected threshold, thus, established as edges in the Jaccard similarity dimension of the multi-layer network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e) Word Mover's Distance Layer:</head><p>Word mover's distance is dependent on studying semantically meaningful representations for word tokens from local co-occurrences in sentences using embedded vectors.</p><p>Word mover's distance takes advantage of the effects of sophisticated embedding techniques such as word2vec, which produces word vectors of unparalleled consistency and generalizes well to bigger datasets. These embedding strategies show that semantic associations are often maintained when performing vector operations on word vectors.</p><p>This property of vector embeddings is used, where sentences are interpreted as a weighted accumulation of vectorized words.</p><p>Therefore, word mover's distance is the minimum total distance that words in sentence "A" need to move to precisely match the point cloud of words in sentence "B". This idea of the word mover's distance is visually represented in <ref type="figure">Fig. 2</ref> <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig 2: Representing word mover's distance</head><p>As such, word2vec-google-news-300 pre-trained Google news corpus word vector model with 3 billion running words was used for this research to generate the word mover distance layer.</p><formula xml:id="formula_12">?0 ? ( , ) , =1 : ? =1 = ? {1, ? , } ? =1 = ? ? {1, ? , }<label>(14)</label></formula><p>The neighbouring nodes and edges between nodes were established based on the word mover's distance scores, computed using equation <ref type="bibr" target="#b13">(14)</ref>, between sentence pairs and represented as ( 1 , 2 ). The above sections elucidate the similarity measures that have been considered in this research. Likewise, any number of layers modelled on different similarity measures can be easily added to the network in order to compute an overall similarity between sentence pairs by considering all or a selected number of measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Similarity Computation in the Multi-Layered Network</head><p>To calculate the overall similarity between two sentences 1 and 2 , i.e.,</p><p>( 1 , 2 ) Which is based on various similarity measures built as individual layers; initially, the local similarity between the same sentences for each layer is computed.</p><p>The following explains the steps involved in computing the Overall Network Similarity between 2 sentences, A and B, from a corpus of several sentences by considering the dimensional measure in the multi-layered network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a) Step 1: Computing the local layer similarity between sentence pairs</head><p>Consider <ref type="figure">Fig. 3</ref> as part of the cosine similarity layer representing 5 sentence vector nodes; A, B, C, D, and E, from which the similarity between sentences A and B needs to be computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig 3: Part of the Cosine Similarity Layer</head><p>The edges between the vector nodes are established if their cosine similarity is greater than the threshold ? &gt; 0.5. And the established edges are weighted with the cosine similarity between the corresponding vector nodes.</p><p>First, the local layer similarity between sentences A and B is computed using the empirical formula proposed here below as equation <ref type="bibr" target="#b14">(15)</ref>.</p><formula xml:id="formula_14">( , ) = ( , ) + ? ( , ) 1 + ? ( , ) _<label>(15)</label></formula><p>In equation <ref type="formula" target="#formula_0">(15)</ref>, ( , ) Refers to the weight of the edge between A &amp; B, that is, the cosine similarity of A, B, in this case. The second term ? ( , ) is the weighted sum of the edges connecting the adjacent nodes of A and B. And ? ( , ) _ is the weight of the similarity of the edges that connects the adjacent nodes of A and B.</p><p>According to <ref type="figure">Fig. 3</ref>, the adjacent node of A will be node C, and the adjacent node of B will be nodes D and E. As such, the edges that connect the adjacent nodes of A and B will be (C, D) and (C, E). Having identified the adjacent edges, the weight of the similarity of these adjacent edges, i.e., ? ( , ) _ Is calculated using equation <ref type="bibr" target="#b15">(16)</ref>.</p><formula xml:id="formula_15">? ( , ) _ = ? ( , ) + ? ( , ) ? ; ? ( , ) = ( , ) ? ( , ), ? ( , ) = ( , ) ? ( , )<label>(16)</label></formula><p>Once the weights of the similarity of these adjacent edges are calculated as shown in equation <ref type="bibr" target="#b15">(16)</ref>, the weighted sum of the edges connecting the adjacent nodes of A and B, i.e., ? ( , ) is computed using the following equation <ref type="bibr" target="#b16">(17)</ref>.</p><formula xml:id="formula_16">?( , ) = ? ( , ) + ? ( , ) ? ; ? ( , ) = ( , ) ? ? ( , ), ? ( , ) = ( , ) ? ? ( , )<label>(17)</label></formula><p>The first step in computing the local layer similarity between the 2 corresponding nodes A and B, in a single layer (i.e., ( , ) in the cosine similarity layer as shown in equation <ref type="bibr" target="#b14">(15)</ref> in the above example) will be done by substituting the values obtained using equations <ref type="bibr" target="#b15">(16)</ref> and <ref type="bibr" target="#b16">(17)</ref>.</p><p>Similarly, moving on to the next layer in the multilayered network, the edges and edge weights between the nodes are established based on the phrasal overlap measure, i.e., ( , ) using equation <ref type="bibr" target="#b9">(10)</ref>. Once the nodes and their edge weights are established for this network layer, the local layer similarity between sentences A and B is calculated using the same idea applied in equation <ref type="bibr" target="#b14">(15)</ref>. or <ref type="bibr">( , )</ref>. Applying equation <ref type="formula" target="#formula_1">(20)</ref> for a network of two layers, i.e., cosine similarity layer and phrasal overlap layer, the overall similarity between sentence pairs will be calculated as shown here below. </p><p>If additional layers of varying similarity measures are built into the multi-layer network, their local layer similarity can be aggregated into the above equation and ( , ) Will render the overall similarity between any sentence pairs in the corpus using equation <ref type="bibr" target="#b19">(20)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPLEMENTATION SPECIFICATIONS</head><p>The proposed model was implemented according to the methodology specified in the preceding section. Thus, the specifications that dictated the implementation of this model are detailed here below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>For this scenario, the SemEval text corpus 1 has been used. This dataset is composed of sentences from news headlines and image captions. This dataset is built to assess the Semantic Textual Similarity (STS) based on the semantics of sentence pairs. The sentences are scored according to the golden standard between 0-5, with 0 indicating a low similarity and 5 indicating the highest similarity. Therefore, this golden-scale scoring is normalized and use as the ground truths in evaluating predicted similarity scores of the model to render its performance index metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Models</head><p>The baselines against which are considered in comparing and contrasting the proposed model are the semantic textual similarity benchmarks; SMART, StructBERT, RealFormer, T5 family of models, namely, T5-11B, T5-3B, T5-Large, and T5-Small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Building a Multi-layer Network</head><p>With regard to the implementation of the proposed model, the NetworkX 2 python package was utilized. This permits the creation of nodes that can bear any type of data and edges that can be weighted with arbitrary values. In this research's context, the nodes are sentence vectors, the first layer's edges are cosine similarity weighted connections, and the second layer's edges are phrasal overlap score-based connections, and so on.</p><p>Having 5 layers or similarity measures implemented on top of the sentence nodes, different networks with varying combinations of layers were modelled in order to;</p><p>? identify the number of optimal layers for the multi-layered network architecture and the proposed sentence similarity computation method.</p><p>? Identify the influence of the number of layers on the predicted similarity scores. ? Identify the metric combinations that render a higher positive correlation. ? Observe the influence of similarity metrics and distance-based metrics upon the computations via the proposed overall similarity calculation method. ? Observe the inclusion of syntactic measures on the overall similarity computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Index Metrics</head><p>The similarity scores computed for the baselines and the proposed model in this research are rendered as soft truth values in the range of [0,1].</p><p>As such, the following regression analysis metrics were used in analyzing the performance of the model: Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Median Absolute Error (MedAE), Coefficient of Determination (R2), Explained Variance (EV), Maximum Error (ME). Besides these metrics, the Pearson Correlation and Spearman correlation of this model was observed against the established baseline models.</p><p>VI. DISCUSSION As stated in the preceding section, the proposed model was implemented as a multi-layered network with each layer corresponding to a particular similarity measure.</p><p>Out of 18-layer combinations that were implemented, 6 provided significant results and are illustrated in table 1.</p><p>Out of the 6 significant results obtained, the 2-layer network composed of the phrasal overlap and word mover's distance measures obtained the highest Pearson and Spearman correlation scores with 0.9269 and 0.9312, respectively. The three-layered network built up of the phrasal overlap, euclidean distance, and word mover's distance measures comes in at a close 0.9011 and 0.9198 Pearson and Spearman correlation, respectively.</p><p>As shown in this table, since MSE is the average squared difference between the predicted similarity score and the actual ground truth, the best possible value would be 0. As such, the experimented 2-layered network of the phrasal overlap and word mover's distance dimensions bears an optimal MSE of 0.041. The same principle applies to RMSE, which is the Standard deviation of the prediction errors. This renders the lowest possible value of 0.201 for the Multi-layered network model as the optimal performance index metric.</p><p>R2 is the coefficient of determination that observes how close the data are fitted to the regression line or decision threshold. Thereby, the first model achieves the highest R2 out of all the other combinations. Explained variance (EV) is a metric of the difference between the model's data and actual results. Alternatively, it is the portion of the model's overall variance that is clarified by variables that are present rather than those that occur due to error variance. Therefore, the higher the EV, the stronger the association. As such, the first network structure renders a respectable 0.534 EV that is greater than the other models. The Mean Absolute Error (MAE) is the average of absolute errors between paired observations (two sentences). With a value of 0.145, the 2-layered model attains the optimal value. Accordingly, building a multi-layer network architecture with a minimum number of optimal layers mapped to varying similarity measures has proven to be efficient in computing the overall similarity between sentence pairs as opposed to increasing the number of layers. This observation aligns with the concept of achieving high performance with a minimum number of hidden layers in a neural network <ref type="bibr" target="#b22">[23]</ref>.</p><p>In choosing appropriate similarity measures, the introduction of a syntactic and semantic measure, rather than solely using semantic similarity measures, demonstrated enhanced performance scores. <ref type="table" target="#tab_1">Table 2</ref> shows the proposed model MNet-Sim compared against the baseline models. Our proposed multi-layer network model to assess semantic similarity attains a respectable Pearson Correlation of 0.927 and Spearman correlation of 0.931. Lying within the range of ?0.50 and ?1, MNet-Sim shows a strong positive correlation between the predicted similarity scores and the ground truths. As such, the proposed model is shown to have demonstrated better performance than most of the state-of-the-art models in predicting semantic similarity.</p><p>Hence, this research observes that the generation of a multi-layered network with the proposed node similarity computation method detailed in section 4.2 shows optimal performance in identifying the similarity between sentences.</p><p>This model can be applied to several Natural Language Processing applications such as plagiarism detection, information extraction, sentiment detection <ref type="bibr">[24]</ref>, text or document categorization, text or document summarization, topic modeling, chatbot applications, machine translation, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Limitations and Future Work</head><p>With regard to the extension of this model, the multilayered structure permits the inclusion of new variables as additional layers where the dimensional measures for the optimum layer combinations are application-dependent. For example, in the context of the document and its sentences need to be considered as influential factors in computing the similarity, the context can be modeled as nodes of vectors or objects with appropriately weighted edges, and the network can be further extended with more dimensions as required. However, rapidly increasing the number of layers has been demonstrated to lower the model's performance and shows that keeping the number of layers at a minimum, mapped to an optimal combination of similarity measures, is efficient as the model's complexity is controlled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>Semantic Similarity of text has evolved over the years from word-to-word, vector-based, and context-based assessment. This research introduces a multi-layered semantic similarity network model called MNet-Sim that efficiently evaluates the semantic similarity between sentence pairs. Here, the modeling of sentence vectors in the form of nodes in a multi-layer network allows the sentences to be simultaneously analyzed upon different similarity measures, which are then aggregated through a node similarity computation formula proposed in this paper. The proposed model was evaluated and tested against established state-of-the-art models and is shown to have demonstrated better performance scores in assessing sentence similarity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Evaluation Results of different combinations of layers in the multi-layer network</head><label>1</label><figDesc></figDesc><table><row><cell>Metric</cell><cell>2 Layers</cell><cell>3 Layers</cell><cell>2 Layers</cell><cell>3 Layers</cell><cell>3 Layers</cell><cell>4 Layers</cell></row><row><cell></cell><cell>(po + wmd)</cell><cell>(po+ed+wmd)</cell><cell>(cs + po)</cell><cell>(cs + po +</cell><cell>(cs + po +</cell><cell>(cs + po +</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ed)</cell><cell>wmd)</cell><cell>ed + wmd)</cell></row><row><cell>Pearson Corr</cell><cell>0.9269</cell><cell>0. 9011</cell><cell>0. 8684</cell><cell>0. 7854</cell><cell>0. 7421</cell><cell>0. 6821</cell></row><row><cell cols="2">Spearman Corr 0. 9312</cell><cell>0. 9198</cell><cell>0. 8855</cell><cell>0. 8439</cell><cell>0. 8144</cell><cell>0. 7946</cell></row><row><cell>MSE</cell><cell>0. 041</cell><cell>0. 094</cell><cell>0. 053</cell><cell>0. 095</cell><cell>0. 088</cell><cell>0. 098</cell></row><row><cell>RMSE</cell><cell>0. 201</cell><cell>0. 306</cell><cell>0. 230</cell><cell>0. 309</cell><cell>0. 296</cell><cell>0. 313</cell></row><row><cell>R2</cell><cell>0. 029</cell><cell>-1. 245</cell><cell>-0. 263</cell><cell>-1. 281</cell><cell>-1. 103</cell><cell>-1. 344</cell></row><row><cell>EV</cell><cell>0. 534</cell><cell>0. 045</cell><cell>0. 409</cell><cell>0. 031</cell><cell>0. 098</cell><cell>0. 007</cell></row><row><cell>ME</cell><cell>0. 684</cell><cell>0. 979</cell><cell>0. 725</cell><cell>0. 983</cell><cell>0. 931</cell><cell>0. 996</cell></row><row><cell>MAE</cell><cell>0. 145</cell><cell>0. 232</cell><cell>0. 168</cell><cell>0. 234</cell><cell>0. 224</cell><cell>0.238</cell></row><row><cell cols="3">cs: cosine similarity; po: phrasal overlap;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ed: euclidean distance; wmd: word mover's distance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : The proposed model, MNet-Sim, compared against the state-of-the-art models. Model Pearson Correlation</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Spearman</cell></row><row><cell></cell><cell></cell><cell>Correlation</cell></row><row><cell>SMART</cell><cell>0. 929</cell><cell>0. 925</cell></row><row><cell>StructBERT</cell><cell>0. 928</cell><cell>0. 924</cell></row><row><cell>Mnet-Sim</cell><cell>0. 927</cell><cell>0. 931</cell></row><row><cell>(Proposed</cell><cell></cell><cell></cell></row><row><cell>model)</cell><cell></cell><cell></cell></row><row><cell>T5-11B</cell><cell>0. 925</cell><cell>0. 921</cell></row><row><cell>T5-3B</cell><cell>0. 906</cell><cell>0. 898</cell></row><row><cell>RealFormer</cell><cell>0. 901</cell><cell>0. 899</cell></row><row><cell>T5-Large</cell><cell>0. 899</cell><cell>0. 886</cell></row><row><cell>T5-Small</cell><cell>0. 856</cell><cell>0. 850</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">SemEval Dataset: https://alt.qcri.org/semeval2016/task2/ 2 NetworkX: https://networkx.github.io/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Therefore, the local layer similarity of nodes A and B in the Phrasal Overlap layer in the multi-layer network, ( , )It can be calculated as shown here below in equation <ref type="bibr" target="#b17">(18)</ref>.</p><p>( , ) = ( , ) + ? ( , ) 1 + ? ( , ) _ (18) Here, the weight of the similarity and the weighted edges of the connections between the adjacent nodes will consider the phrasal overlap scores instead of the cosine similarity that is shown in equations <ref type="bibr" target="#b15">(16)</ref> and <ref type="bibr" target="#b16">(17)</ref>. The same principle is applied to the other three layers and is represented as shown in equation <ref type="bibr" target="#b18">(19)</ref>  </p><p>In the case where more layers with different similarity measures are available in the multi-layer network architecture, the local layer similarity for that layer will be calculated in the same above manner applying that layer's corresponding similarity measure as the edge weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) Step 2: Computing the inter-layer weight</head><p>Once each layer is established, and the respective sentences' local layer similarities are computed as suggested in step 1 above, an inter-layer weighting between the layers in the multi-layer network will be computed.</p><p>We calculate this inter-layer weight, ? , using the Pairwise Jensen-Shannon divergence measure, which normalizes the Kullback-Leibler divergence <ref type="bibr" target="#b21">[22]</ref>, taking the distribution of the individual layers.</p><p>In the case of only two layers being considered as the dimensions of the network, the inter-layer weight will be a direct computation of the Pairwise Jensen-Shannon divergence between the two layers. If more layers build up the multi-layer network, the Jensen-Shannon divergence for each paired layer permutation is computed and averaged to obtain an overall inter-layer weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c) Step 3: Computing the overall sentence similarity using the aggregated layers</head><p>Having obtained the local layer similarities of the sentences for each layer and the inter-layer weight ? using the pairwise Jensen-Shannon divergence, an overall similarity score for the sentences A and B can be computed using the following equation <ref type="bibr" target="#b19">(20)</ref>. </p><p>Here, is any layer in the multi-layered network and hence, ( , ) corresponds to ( , ) , ( , ) , ( , ) , ( , ) ,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentence similarity based on semantic nets and corpus statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zuhair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keeley</forename><surname>O&amp;apos;shea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1138" to="1150" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Corpus-based and knowledge-based measures of text semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aaai</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="775" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Smart: Robust and efficient finetuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03437</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04577</idno>
		<title level="m">Structbert: Incorporating language structures into pre-training for deep language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illiapolosukhin</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified textto-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Realformer: Transformer likes residual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhargav</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<idno>arXiv-2012</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv eprints pages</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Application of semantic similarity calculation based on knowledge graph for personalized study recommendation service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxian</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Educational Sciences: Theory &amp; Practice</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An approach for measuring semantic similarity between Wikipedia concepts using multiple inheritances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Muhammad Jawad Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangjian</forename><surname>Hassan Wasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">102188</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluating text coherence based on a semantic similarity graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Wiragotama Putra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takenobu</forename><surname>Tokunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for Natural Language Processing</title>
		<meeting>TextGraphs-11: the Workshop on Graph-based Methods for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving chronological sentence ordering by a precedence relation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">750</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decision threshold adjustment in class prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-A</forename><surname>Jj Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-H</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAR and QSAR in Environmental Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="352" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analysis of neighbourhoods in multi-layered dynamic social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Br?dka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katarzyna</forename><surname>Przemys?awkazienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Musia?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skibicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="582" to="596" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cosine normalization: Using cosine similarity instead of the dot product in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xiaohexue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantitative linguistics approach to interlanguage development: a study based on the guangwai-Lancaster Chinese learner corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingua</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="page">102736</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Generalized word shift graphs: A method for visualizing and explaining pairwise comparisons between texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">Sheridan</forename><surname>Danforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dodds</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02250</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effects of hidden layers on the efficiency of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Uzair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noreen</forename><surname>Jamil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 23rd International Multitopic Conference (INMIC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sentiment Detection Using Fish Optimization Genetic Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dr</forename><surname>Sukhlalsangule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phulre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engineering Trends and Technology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="140" to="145" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
