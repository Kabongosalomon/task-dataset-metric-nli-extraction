<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Permutation invariance and uncertainty in multitemporal image super-resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Diego</forename><surname>Valsesia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Enrico</forename><surname>Magli</surname></persName>
						</author>
						<title level="a" type="main">Permutation invariance and uncertainty in multitemporal image super-resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-multitemporal super-resolution</term>
					<term>convolutional neural networks</term>
					<term>self-attention</term>
					<term>uncertainty estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances have shown how deep neural networks can be extremely effective at super-resolving remote sensing imagery, starting from a multitemporal collection of lowresolution images. However, existing models have neglected the issue of temporal permutation, whereby the temporal ordering of the input images does not carry any relevant information for the super-resolution task and causes such models to be inefficient with the, often scarce, ground truth data that available for training. Thus, models ought not to learn feature extractors that rely on temporal ordering. In this paper, we show how building a model that is fully invariant to temporal permutation significantly improves performance and data efficiency. Moreover, we study how to quantify the uncertainty of the super-resolved image so that the final user is informed on the local quality of the product. We show how uncertainty correlates with temporal variation in the series, and how quantifying it further improves model performance. Experiments on the Proba-V challenge dataset show significant improvements over the state of the art without the need for self-ensembling, as well as improved data efficiency, reaching the performance of the challenge winner with just 25% of the training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Mapping the Earth with high resolution imagery is critical for a wide range of applications including environmental monitoring, urban mapping, disaster assessment, military surveillance, and many more. At the same time, instruments onboard of satellites face constraints such as payload sizes, downlink bandwidth, etc. that can limit the spatial resolution of the images they acquire, or the temporal availability of highresolution (HR) products. Super-resolution (SR) techniques address the problem of estimating HR images from one or more low-resolution (LR) images. The availability of multiple images of the same scene is particularly useful since small geometric displacements allow the images to carry complementary information, that, when suitably combined by means of SR methods, can significantly increase the spatial resolution. In the context of remote sensing, there are several ways to obtain multiple images of the same scene, e.g., they can be acquired by a spacecraft during multiple orbits, or by multiple satellites imaging the same scene at different times, or may be obtained at the same time with different sensors. The most challenging scenario is the multitemporal one, as the content of the scene may change due to a variety of reasons, such as change in illumination, occlusions due to clouds, human activity, etc. Significant progress has recently been made on multitemporal image SR, also spurred by the Proba-V challenge by the European Space Agency <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. A curated dataset and a set of standard testing conditions allowed quick development of models tackling this difficult task and showed how deep learning models can effectively overcome difficulties such as unknown imaging model and temporal variation.</p><p>At the same time, we find that current models for multitemporal SR lack an important ingredient, which is the invariance to temporal ordering. Due to the unpredictability of change in the temporal series of LR images, no assumption can be made about patterns arising from a specific ordering of the images. In order to better understand this concept, let us compare it to a case where ordering matters, which is exploiting multiple spectral bands. In that case, each band has a specific physical meaning and ordering matters because there exist stable correlation patterns among bands. This is not the case when dealing with the temporal instead of spectral dimension, and any temporal permutation of the input LR images should always result in the same SR image. Therefore, we seek to build a model that is explicitly invariant to temporal permutation. Capturing this important prior allows to build a more robust and efficient model because it does not need to learn this property from the (possibly limited) data.</p><p>Moreover, in contrast with the LR images, for which established quality assessment criteria are typically available, the multitemporal SR process raises a problem related to the quality of the generated SR images, and, in particular, the degree of confidence to which a SR pixel has been reconstructed properly. If significant temporal variation is present, how can one trust the content of the SR scene to properly represent reality? We raise this issue for the first time in the context of deep multitemporal SR models and propose a technique that estimates an uncertainty value for each pixel in the SR image. We show that this uncertainty map correlates with temporal variations and with the true error signal. This uncertainty map can be made available to final users to judge the reliability of regions of the SR product.</p><p>Our main novel contributions in this paper can be thus summarized as:</p><p>? a new architecture for the multitemporal SR problem, called PIUnet (Permutation Invariance and Uncertainty network), which is invariant to temporal permutations and enables higher data efficiency, requiring smaller datasets for training; ? a method to estimate the aleatoric uncertainty of the SR image, consisting of an architectural design built in PIUnet and an ad-hoc training procedure;</p><p>arXiv:2105.12409v1 [eess.IV] 26 May 2021</p><p>? significant improvements over state-of-the-art on the Proba-V challenge dataset, in terms of both quality of the SR images and computational efficiency, since we do not require expensive temporal self-ensembles; ? a flexible model that can process an arbitrary number of input LR images in a stable manner and without ensembling or architecture redesigns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Image super-resolution has received great attention and a broad literature is available. However, the majority of works focus on single-image super-resolution (SISR) and a comparatively smaller number address the more challenging multi-image setting, and even fewer consider realistic LR degradations and multi-temporal change.</p><p>SISR has been addressed by means of interpolation-based techniques, optimization-based methods <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b5">[6]</ref> and, more recently, learning-based methods relying on deep neural networks <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b13">[14]</ref>. While SISR is interesting because of the limited amount of available information to solve the HR reconstruction problem, MISR offers a unique set of challenges and requires solutions that go beyond simple extensions of SISR works.</p><p>The first work on MISR by Tsai and Huang <ref type="bibr" target="#b14">[15]</ref> used a frequency-domain technique to combine multiple downsampled images with subpixel displacements. However, frequencydomain algorithms do not allow to easily incorporate prior knowledge about HR images, and thus several spatial-domain MISR techniques were proposed over the years, including nonuniform interpolation <ref type="bibr" target="#b15">[16]</ref>, iterative back-projection (IBP) <ref type="bibr" target="#b16">[17]</ref>, projection onto convex sets (POCS) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, sparse coding <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, and other regularized methods <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. In particular, IBP <ref type="bibr" target="#b16">[17]</ref> enjoyed some success and works by improving the initial SR guess by back-projecting the difference between simulated LR images and actual LR images to the SR image, and iteratively attempting at inverting the forward imaging process. However, IBP is ultimately limited by the inability to deal with unknown or very difficult to model image degradation processes, as well as the difficulty in including image priors. Regularized methods generate the SR image by solving an optimization problem where a regularization cost can encode sophisticated image priors to improve performance. Among those, the bilateral total variation (BTV) method <ref type="bibr" target="#b21">[22]</ref> exploits a combination of the total variation regularizer and the bilateral filter to create a robust edgepreserving prior.</p><p>Model-based techniques such as the aforementioned ones are limited by the ability to accurately describe the forward imaging system and by the handcrafted prior used to capture the properties of real images. On the other hand, learningbased techniques directly use the data to overcome these modeling challenges and learn system models and image priors from observations. Deep neural networks are the tool of choice for such methods and recent years have shown progress in using deep learning for MISR. In particular, in the context of video SR <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, convolutional neural networks (CNNs) have been developed to simultaneously perform motion compensation and SR frame generation. For instance, the dynamic upsampling filters (DUF) method proposed in <ref type="bibr" target="#b26">[27]</ref> estimates input-dependent interpolation filters for each pixel in the frames. Other applications of MISR can be found in burst photography <ref type="bibr" target="#b27">[28]</ref> where accurate registration and motion blur compensation play an important role. Recently, the Proba-V SR challenge <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, issued by the European Space Agency, stimulated research for MISR approaches in the remote sensing context. The new dataset is particularly interesting for the development of new methods, as it allows to deal with realistic image degradation, registration problems, and robustness to temporal variation in the scenes, both among LR images and between LR and HR. The challenge winner DeepSUM <ref type="bibr" target="#b28">[29]</ref> proposed a modular CNN composed of a SISR part, a module performing dynamic registration from the feature space and a fusion module based on 3D convolution. The architecture was further improved in DeepSUM++ <ref type="bibr" target="#b29">[30]</ref> by using non-local operations in the form of graph-convolutional layers. The challenge runner-up HighRes-Net <ref type="bibr" target="#b30">[31]</ref> proposed a recursive fusion strategy , also including a dynamic registration module. The current state-of-the-art is represented by the RAMS model <ref type="bibr" target="#b31">[32]</ref>, which exploits feature attention at multiple stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD A. Overview</head><p>We propose to tackle multitemporal image super-resolution by means of a novel neural network design, called PIUnet, addressing Permutation Invariance and Uncertainty estimation. An overview is presented in <ref type="figure" target="#fig_0">Fig. 1</ref>. The network input is a set with an arbitrary number of LR images. This is in contrast with other existing techniques which are designed for a fixed number of images. The LR images are assumed to be roughly registered with each other by means of preprocessing.</p><p>The key features of PIUnet can be found in its invariance to permutations of the LR inputs and in the estimation of both the SR image and a corresponding pixel-by-pixel uncertainty value. This is achieved by means of two parallel heads which project the feature space built by the backbone to the two sets of information. Notice that the SR head uses pixel shuffling <ref type="bibr" target="#b32">[33]</ref> as upsampling technique and only estimates the residual from bilinear upsampling and averaging of the LR inputs, while the uncertainty head does not use this global skip connection. The next sections describe in detail how to achieve permutation invariance and how to estimate the uncertainty map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Invariance to temporal permutation</head><p>In this section, we are going to discuss the main modules that allow the model to be invariant to permutations of the LR images in the temporal dimension. Before that, we need to introduce some terminology from group theory, and, in particular, the notions of equivariant and invariant functions.</p><formula xml:id="formula_0">Definition III.1 (Equivariance). A function f : X ? Y is said to be equivariant to the actions g of a group G if f (g ? x) = g ? f (x)</formula><p>for all x ? X, g ? G.  </p><formula xml:id="formula_1">Definition III.2 (Invariance). A function f : X ? Y is said to be invariant to the actions g of a group G if f (g ? x) = f (x)</formula><p>for all x ? X, g ? G.</p><p>In our case, we are dealing with the permutation group and its actions are all the possible temporal permutations of the input images. If we have an invariant function, the output will always be the same, no matter the permutation of the input, while for equivariant functions we will get an output that is exactly a permuted version of the output we would get without the input permutation.</p><p>Invariance is a powerful property to exploit because it embeds robustness into the model, and allows to capture prior knowledge about the data properties, that would otherwise be very difficult to learn from the data. In our problem, multitemporal invariance is particularly desirable, as the SR image does not depend on any specific temporal ordering, and it would not make sense to assume that there exist ordered temporal patterns that always recur on every image set. Indeed, prior work on multitemporal super-resolution <ref type="bibr" target="#b31">[32]</ref> attempted at capturing temporal invariance, but by means of data augmentation, where multiple permutations are fed to the model at training time, rather than by means of model operations. This augmentation approach is, at best, able to learn a weak invariance, as the model has to fully learn from the data that permutations correspond to the same output, rather than explicitly building this knowledge in the model operations. As an example, the fact that the training of the current state-of-the-art RAMS model <ref type="bibr" target="#b31">[32]</ref> does not capture invariance is testified by the significant improvements obtained by test-time ensembling, where predictions corresponding to multiple temporal permutations are averaged.</p><p>Building a model out of invariant layers can be challenging because only a limited number of functions that are invariant to permutations exist, and since they are typically simple functions (e.g., the average) they may not be sufficiently expressive to build complex features. A simple technique to build an invariant model is, therefore, to use a sequence of equivariant operations, which are easier to design, followed by a global invariant function. A trivial way to do that could be to independently process all the LR images with the same neural network and then combine the results with an invariant function, such as the mean. However, this is highly suboptimal because it does not exploit the correlation between images to build better feature spaces in the hidden layers. At the same time, using 2D convolution treating time as feature channels or 3D convolution would not provide equivariant operations, since the weights used by these layers assume a temporal ordering and any permutation would return a different result.</p><p>Therefore, we propose to use self-attention <ref type="bibr" target="#b33">[34]</ref> in the temporal dimension as a permutation-equivariant operation that, at the same time, allows to effectively combine the information from the multiple time instants, exploiting their crosscorrelations. We remark that, despite the name, self-attention is a significantly different operation from classic feature attention <ref type="bibr" target="#b34">[35]</ref>. Self-attention projects its input feature vector to three different subspaces, using three learnable matrices, to generate the so-called key, query, value vectors and the cross-correlation matrix between key and query is used as transformation matrix to weigh the temporal components in the value vectors and generate the output. More formally, given the representation of a pixel X ? R T ?F , characterized by F features and T temporal channels, the self-attention operation computes:</p><formula xml:id="formula_2">Q = XW q , K = XW k , V = XW v Y = softmax QK ? T ? V = AV</formula><p>where the softmax function is applied row-wise, and W q , W k , W v are learnable matrices. Notice how this operation may look like a classic linear layer in the sense that the input is transformed by a matrix A ? R T ?T , mixing the temporal channels, but the key difference is that A is computed as a function of the input itself, rather than being constituted of trainable values. It is easy to check that a permutation of the T temporal channels, results in a permutation of the columns of A, ensuring the overall equivariance of the operation. This operation is performed for all pixels of all the images in our batch. We remark that self-attention has a quadratic complexity O(T 2 ) in terms of computation and memory due to the QK cross-correlations, but since we apply it to the temporal dimension, the value of T is typically fairly small (e.g., in the Proba-V dataset, T = 9), ensuring efficient implementations. This operation provides us a building block to be used whenever we want to mix the temporal channels. Referring to <ref type="figure" target="#fig_0">Fig.1</ref>, we typically use 2D convolutions shared across the temporal channels to extract spatial features, and then use selfattention to temporally combine those features. Notice how sharing the 2D convolutions across the temporal dimension is crucial to maintaining equivariance to temporal permutation. Based on this idea, we also design a novel module to compute classic residual feature attention <ref type="bibr" target="#b34">[35]</ref>, called Temporally-Equivariant Feature Attention (TEFA), shown in <ref type="figure">Fig. 2</ref>, whose repetition serves as the backbone of our model. The classic residual feature attention <ref type="bibr" target="#b34">[35]</ref> extracts a scalar value that is used to weigh each feature map. Our proposed TEFA module extends the idea to deal with the extra temporal dimension and computes attention scores to weigh the feature channels by extracting spatial and temporal features in an equivariant way, by means of the aforementioned shared 2D convolutions and temporal self-attention, and averaging them over space and time.</p><p>We also propose a temporally-equivariant extension of the RegNet module presented in DeepSUM <ref type="bibr" target="#b28">[29]</ref>, called TERN and shown in <ref type="figure">Fig. 3</ref>. The goal of the original RegNet was to dynamically compute small K ? K spatial kernels from the input features to be used as filters over the input itself. They served as adaptive filters that could implement registration filters, or, in general, spatial interpolators that could refine the registration of the multitemporal images, exploiting the powerful feature space of the network. For more details, we refer the reader to <ref type="bibr" target="#b28">[29]</ref>. The original formulation was not perfectly equivariant as it relied on an explicit ordering where the first temporal image was taken as a reference and concatenated to each of the others to be processed in pairs as channels in a convolutional layer. In TERN, we overcome this limitation by exploiting self-attention to cross-correlate features over the temporal dimension and infer the values of the spatial kernels. Just like RegNet, TERN computes a different K ? K spatial filter for each temporal image, while the filter is shared across multiple feature maps.</p><p>Finally, when we consider the proposed architecture from its input to the output of the TERN module, we can notice that it is equivariant to temporal permutations. In order to make the overall model invariant, we simply average the output of TERN along the temporal axis.</p><p>We also remark that the proposed model does not have any constraint on the temporal dimension, i.e., the same model could be used for any number of multitemporal images, which could be especially useful if, once deployed, fewer images were available. This is not the case for other existing methods, including DeepSUM <ref type="bibr" target="#b28">[29]</ref> and RAMS <ref type="bibr" target="#b31">[32]</ref>, which have architectures that have hard constraints on using exactly 9 multitemporal images, while only HighResNet <ref type="bibr" target="#b30">[31]</ref> allows this flexibility thanks to their recursive fusion approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Uncertainty estimation</head><p>In this section, we propose a technique to assess a measure of confidence on the super-resolved image. This is important from the perspective of a final user of the SR product, whom we can inform about the degree of confidence a certain region of an image has to have been super-resolved properly.</p><p>We focus on characterizing the aleatoric uncertainty [36] on the SR image. This kind of uncertainty is due to stochastic perturbations in the input data, typically noise, or, in this case, also temporal variations of the scene. Characterizing aleatoric uncertainty allows us to determine whether a portion of the image was poorly super-resolved due to variability in the input LR images.</p><p>We use a heteroscedastic model for aleatoric uncertainty, essentially modeling each SR pixel as a random variable whose distribution can change on a pixel-by-pixel basis. The parameters of these distributions are then directly estimated by the neural network as its outputs. Training then uses the negative log likelihood (NLL) as loss function to be minimized. Most of the works on regression problems with neural networks <ref type="bibr" target="#b36">[37]</ref> model the samples as Normal random variables, thus the corresponding NLL can be seen as a generalization of the mean squared error (MSE) loss. However, it has been observed <ref type="bibr" target="#b37">[38]</ref> that, in many image restoration problems, the L1 loss typically outperforms the MSE loss, even if the evaluation metric is the tightly related PSNR. This has also been observed in the context of multitemporal super-resolution <ref type="bibr" target="#b31">[32]</ref> and could be explained by the robustness of the L1 metric to outliers. We therefore seek an extension of the L1 loss to train our network, and this leads to modeling the pixel distribution as a Laplacian:</p><formula xml:id="formula_3">p(x i ) = 1 2? i exp ? |x i ? ? i | ? i E [x i ] = ? i , Var [x i ] = 2? 2 i .</formula><p>The goal of the neural network is to output ? i , which will be the pixels in our SR image, and ? i which is proportional to the standard deviation and will therefore be our aleatoric uncertainty. This is done with two parallel heads, as shown in <ref type="figure" target="#fig_0">Fig.1</ref>, splitting after the temporal averaging operation. In practice, ? = log ? is estimated for numerical stability, resulting in the following loss function:</p><formula xml:id="formula_4">L = ? 1 N B b,i log p(x i ) = 1 N B b i ? (b) i + e ?? (b) i |x HR(b) i ? ? (b) i |<label>(1)</label></formula><p>for i = 1, . . . , N pixels and b = 1, . . . , B images. It is often the case that the HR ground truth is not registered with the SR image. In that case, it is common practice <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> to use the minimum value of L computed for all possible registration shifts as loss function.</p><p>We remark that estimating the uncertainty and using Eq. (1) as loss function actually serves a dual purpose. Not only it provides information on reliability of the SR image, but it also improves model performance with respect to the L1 loss. In fact, the contribution of the variance serves as a regularizer against excessively confident predictions, leading to higher quality solutions, as shown in the experiments in Sec. IV-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSIONS</head><p>In this section, we discuss the experimental performance of the proposed method by focusing on the Proba-V dataset from the corresponding SR challenge. We present comparisons with the state-of-the-art models, including the most recent ones developed after the challenge. We also analyze the properties of the proposed method in terms of label efficiency, the impact of the Laplacian NLL loss, and the information provided by the uncertainty map. Code is available online: https://github. com/diegovalsesia/piunet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setting</head><p>Our experiments employ the Proba-V super-resolution dataset <ref type="bibr" target="#b0">[1]</ref>, released by the European Space Agency in the context of a challenge <ref type="bibr" target="#b1">[2]</ref>. The unique feature of this dataset is the availability of both LR and HR captured by the same satellite. In fact, Proba-V is able to acquire LR images at 300m resolution daily, but also HR images at 100m resolution with a longer revisit time of 5 days. The availability of real acquisitions for both LR and HR images makes for an interesting case study for SR techniques, since it avoids synthetic degradation of the HR data to generate the LR ones, which often results in simplistic degradations and distorts model performance. This is also well-suited for supervised learning techniques, which are able to fully learn an inverse to the complex and unknown degradation mapping. However, the higher revisit time for HR data means that only a limited amount of images could be available, once cloud cover and limits to temporal change are taken into account, so it is important to study efficient models that can perform well with few data. The dataset provides single-band Level 2A (radiometrically and geometrically corrected Top-of-Atmosphere reflectance) images divided in a near-infrared (NIR) and visible (RED) categories. At least 9 LR images, acquired over the course of 30 days, are available for each HR scene. A total of 396 NIR scenes and 415 RED scenes are available for training, with additional 170 NIR and 176 RED scenes available for validation with known ground truth. The pixel size of the LR images is 128 ? 128, while the HR images are 384 ? 384.</p><p>We preprocess the data by selecting only the LR images having cloud coverage lower than 15% according to the provided clearance masks. We fix the number of used LR images per scene to 9 to match the standard setting used in other works. However, notice that the proposed model does not have any constraint on the size of the temporal dimension. We also normalize the images by subtracting the average intensity over the training set and dividing by the standard deviation. LR images are registered to each other by means of crosscorrelation, assuming a translational model. Training uses LR patches of size 32 ? 32 extracted from all spatial locations in the available images and augmented with rotations, while testing directly processes the full pixel size, since the model is fully-convolutional. Separate models are trained for NIR and RED.</p><p>A feature size F = 42 is used throughout the model, except for the linear layers in TEFA which form a bottleneck reducing to F = 5 features before returning to 42. A total of 16 TEFA modules and 1 TERN module are used. The spatial kernels computed by TERN have size 5 ? 5. The total number of trainable parameters is slightly under a million, and it is comparable with existing works. Training minimizes the NLL loss (Eq. (1)) for approximately 500 epochs, using the Adam optimizer <ref type="bibr" target="#b38">[39]</ref>. The NLL loss is made insensitive to misregistration between SR and HR and to absolute image brightness using the same approach as described in <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_5">L = min u,v?[0,6] L u,v L u,v = 1 N B j i ? (j) i + e ?? (j) i |x HR(u,v,j) i ?? (j) i | ? (j) i = ? (j) i + b (j) b (j) = 1 m (j) i 1 j x HR(u,v,j) i m (j) i ? ? (j) i m (j) i where m (j)</formula><p>i is the clearance mask for image j, and u, v indicate the amount of horizontal and vertical shift applied to the HR image. The evaluation metric is the corrected PSNR, as used in earlier works, which accounts for shifts between the SR and HR images, and is insensitive to absolute brightness: cPSNR = max u,v?[0,6] 10 log 10</p><formula xml:id="formula_6">(2 16 ? 1) 2 MSE u,v MSE u,v = x HR(u,v) m ? (x SR m + b m)) 2 2 m 1 ,</formula><p>with denoting elementwise product.   The learning rate is 10 ?4 and it is reduced to 2 ? 10 ?5 for the final epochs. A batch size equal to 24 was used. Training required approximately 2 days on a Titan RTX GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with state of the art</head><p>In this section, we compare PIUnet with a number of approaches, representing the state of the art for the multitemporal SR task. We include both model-based techniques and recent neural network methods to ensure a complete overview. The baseline ("Bicubic") consists in bicubic upsampling, followed by pixel-domain registration and temporal averaging. For what concerns model-based approaches, we consider IBP <ref type="bibr" target="#b16">[17]</ref> and BTV <ref type="bibr" target="#b21">[22]</ref>. IBP takes as input an initial guess corresponding to our Bicubic baseline and the spatial shifts related to the LR images using phase correlation algorithm. BTV also uses the same initial guess for the bilateral total variation minimization problem. Regarding neural network approaches, DUF <ref type="bibr" target="#b26">[27]</ref> is a technique from the video SR literature, using dynamic filters, a concept similar to what we use in TERN, but on a pixel-by-pixel basis; HighResNet <ref type="bibr" target="#b30">[31]</ref> is the runner-up in the ESA challenge, and has a unique setting in which images in the LR set are recursively fused; DeepSUM <ref type="bibr" target="#b28">[29]</ref> is the challenge winner and DeepSUM++ <ref type="bibr" target="#b29">[30]</ref> an evolution with non-local operators based on graph convolution; RAMS <ref type="bibr" target="#b31">[32]</ref> is the current state-of-the-art, heavily exploiting feature attention mechanisms. <ref type="table" target="#tab_1">Table I</ref> show quantitative results in terms of cPSNR and SSIM for the various methods. Notice that the table reports "(ens.)" for results from published material obtained with self-ensembling techniques. In particular, DeepSUM (ens.) used ensembles over five subsets of 9 images, while RAMS (ens.) averaged the results of 20 temporal permutations of the input images. Highlighting the use of self-ensembling is important as it increases computational complexity, requiring  more processing time and/or memory, with respect to a singleshot model. The results show that PIUnet outperforms the other methods, and, interestingly, even the ensembled version of RAMS.</p><p>We are also interested in testing the label efficiency of the methods that require training, i.e., what performance can be achieved if the training set only has a limited number of image with HR ground truth. The results are reported in <ref type="figure" target="#fig_2">Fig.  4</ref> which shows that PIUnet has significantly higher efficiency, by providing higher quality SR images even with constrained training sets. In particular, notice how just 100 scenes are required to reach approximately the same performance as DeepSUM (ens.).</p><p>Qualitative results are reported in Figs. <ref type="bibr">5, 6, 7.</ref> In particular, in <ref type="figure" target="#fig_3">Fig.5</ref>, notice how the output uncertainty map estimated by the PIUnet neural network correlates with the absolute error between the SR and HR images. <ref type="figure" target="#fig_4">Figs. 6, 7</ref> show increased sharpness and higher fidelity with respect to the HR image achieved by the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis and Ablation experiments 1) Number of input images:</head><p>We remark that PIUnet is capable of processing an arbitrary number of input images without resorting to self-ensembling techniques, which is a major limitation of state-of-the-art approaches like DeepSUM <ref type="bibr" target="#b28">[29]</ref> and RAMS <ref type="bibr" target="#b31">[32]</ref>. Moreover, those approaches cannot process a set of images which is smaller than nine without modifications to the architectures, due to the assumptions made on sizes in unpadded convolutional layers. It can be expected that PIUnet reaches optimal performance close to the number of images used during training. However, we expect a graceful behaviour as function of T . This is shown in <ref type="figure" target="#fig_6">Fig.  8</ref>, where we can see that the best performance is achieved when the number of test images is close to (but not always at) the number of training images. The figure also shows that finetuning for a few iterations with the target number of images can improve performance, at the cost of degrading it for significantly different input sizes. Building a model with a   broader optimality window so that finetuning is not required can be an interesting direction for future work.</p><p>2) Impact of TERN module: We evaluate the impact of TERN to the final performance of the model, and report it in <ref type="table" target="#tab_1">Table II</ref>. This experiments replaces the TERN module with an extra TEFA module to keep the number of parameters approximately constant. We can notice that TERN provides significant performance improvements, especially on the NIR band.</p><p>3) Computational complexity: <ref type="table" target="#tab_1">Table III</ref> reports some results on computational complexity of the proposed method with respect to state-of-the-art. We can see that PIUnet is significantly faster and with lower memory requirements than DeepSUM. The baseline version of RAMS is slightly faster and with comparable memory requirements. However, as shown in the previous section, we are able to outperform the temporally ensembled version of RAMS, which has a significant penalty in terms of complexity, as time or memory roughly scale linearly with the number of permutations in the ensemble. In particular, we report results for two ways of running the ensemble, i.e., serially, thus trading time for memory, or in parallel, trading memory for time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Uncertainty estimation</head><p>In this experiment, we first show that using the NLL loss improves performance over the classic L1 loss. <ref type="table" target="#tab_1">Table IV</ref> shows the results obtained with the two losses for the same network architecture. We can see that the information on the variance of the super-resolved pixel helps convergence by regularizing high-confidence predictions and results in superior performance. This would already be a significant reason to use such loss, but an additional benefit is the availability of the uncertainty values on a pixel-by-pixel basis for the SR product.</p><p>We can show how this uncertainty map can be used as guide to predict the unreliability of the SR image in areas with significant temporal variation. <ref type="figure">Fig. 9</ref> shows an example of this concept. We focus on a crop of a scene displaying some temporal variation in the river bed and surrounding areas. Notice how the variance of the SR pixels is higher in the corresponding areas, highlighting to the user that the SR image might be less reliable due to the high content variability in the available images.</p><p>A quantitative way to determine whether uncertainty is correctly estimated is to check how it correlates with the true error map. This can be done by means of sparsification plots <ref type="bibr" target="#b39">[40]</ref>. These plots are generated by first sorting all the pixels by decreasing uncertainty and then removing a progressively larger fraction of those with high uncertainty. If the uncertainty estimate correlates with the error signal, a quality metric measured on the remaining pixels should show improvements as more pixels are removed. Viceversa, if the uncertainty values were random, the curve would be a constant, as it does not provide informative ways of removing pixels to improve quality. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the sparsification curve obtained for the proposed method, and compared with random selection and with an oracle that uses the true error signal to sort the pixels (for all variants, the brightness bias and shifts are computed before sparsification and kept fixed). The curves indeed confirm that the uncertainty map carries meaningful information. Future techniques addressing uncertainty estimation can compare sparsification curves to show improvements on the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We showed the importance of invariance to temporal permutation when building deep models for multitemporal image super-resolution. Building equivariant layers by means of selfattention and using a globally invariant operation close to the output allows to significantly improve performance over this challenging task and removes the computationally-expensive self-ensembling operation. We also showed how to estimate the uncertainty of the SR image, so that the final user can be guided in trusting the generated product, and that this further improves model performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>PIUnet architecture. The model processes a stack of LR images and has two outputs, the top one being an uncertainty map and the bottom one the SR image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Temporally-equivariant feature attention block. Temporally-equivariant RegNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Label efficiency. Performance on the validation set as function of the number of HR scenes available for training. Dashed lines represent temporal ensembles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>NIR validation imgset0792. Left to right: HR, SR, SR uncertainty, ground truth absolute error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>NIR validation imgset0792 detail. Left: HR image. Right from top left to bottom right: one among LR images, bicubic (47.71 dB/0.9874), IBP (48.46 dB/0.9892), DeepSUM (50.82 dB/0.9933), RAMS (51.24 dB/0.9939), PIUnet (51.81 dB/0.9946).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>RED validation imgset0353 detail. Left: HR image. Right from top left to bottom right: one among LR images, bicubic (47.34 dB/0.9882), IBP (47.57 dB/0.9889), DeepSUM (48.78 dB/0.9913), RAMS (50.22 dB/0.9938), PIUnet (51.78 dB/0.9946).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Performance on the validation set as function of number of input LR images. Notice how the trained model gracefully handles any input size, but optimal performance may require finetuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Temporal variation is captured by the SR uncertainty map. NIR validation imgset0975. From left to right: four of the LR images, SR image, SR uncertainty map, HR image. Sparsification curves for the proposed method against random and oracle. The improvement over random shows that the output uncertainty map actually correlates with the error signal and provides valuable information on the local quality of the SR product.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Quantitative performance -cPSNR (dB) and cSSIM</figDesc><table><row><cell></cell><cell cols="4">Bicubic IBP [17] BTV [22] DUF [27]</cell><cell cols="4">HighResNet DeepSUM DeepSUM++ RAMS [32] [31] (ens.) [29] (ens.) [30]</cell><cell cols="2">RAMS [32] PIUnet (ens.)</cell></row><row><cell>NIR cPSNR</cell><cell>45.44</cell><cell>45.96</cell><cell>45.93</cell><cell>47.06</cell><cell>47.55</cell><cell>47.84</cell><cell>47.93</cell><cell>48.23</cell><cell>48.51</cell><cell>48.72</cell></row><row><cell>NIR cSSIM</cell><cell>0.9771</cell><cell>0.9778</cell><cell>0.9794</cell><cell>0.9842</cell><cell>0.9855</cell><cell>0.9858</cell><cell>0.9862</cell><cell>0.9875</cell><cell>0.9880</cell><cell>0.9883</cell></row><row><cell>RED cPSNR</cell><cell>47.34</cell><cell>48.21</cell><cell>48.12</cell><cell>49.36</cell><cell>49.75</cell><cell>50.00</cell><cell>50.08</cell><cell>50.17</cell><cell>50.44</cell><cell>50.62</cell></row><row><cell>RED cSSIM</cell><cell>0.9840</cell><cell>0.9865</cell><cell>0.9861</cell><cell>0.9842</cell><cell>0.9904</cell><cell>0.9908</cell><cell>0.9912</cell><cell>0.9913</cell><cell>0.9917</cell><cell>0.9921</cell></row><row><cell></cell><cell>49</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>48.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(dB)</cell><cell>47.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cPSNR</cell><cell>47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>46.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PIUnet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>RAMS (ens.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>46</cell><cell></cell><cell></cell><cell>RAMS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DeepSUM (ens.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DeepSUM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>45.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">No. of training scenes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">: Impact of TERN (cPSNR)</cell></row><row><cell></cell><cell>TERN</cell><cell>TEFA</cell></row><row><cell>NIR</cell><cell cols="2">48.72 dB 48.43 dB</cell></row><row><cell>RED</cell><cell cols="2">50.62 dB 50.55 dB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Computational complexity</cell></row><row><cell></cell><cell>runtime</cell><cell>memory</cell></row><row><cell>DeepSUM</cell><cell>484 ms</cell><cell>5420 MB</cell></row><row><cell>RAMS</cell><cell>102 ms</cell><cell>1250 MB</cell></row><row><cell cols="3">RAMS (ens.) 1642 ms / 1075 ms 1250 MB / 5340 MB</cell></row><row><cell>PIUnet</cell><cell>181 ms</cell><cell>1280 MB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Training loss comparison (cPSNR)</figDesc><table><row><cell></cell><cell>L1 loss</cell><cell>NLL loss</cell></row><row><cell>NIR</cell><cell>48.41 dB</cell><cell>48.72 dB</cell></row><row><cell cols="2">RED 48.53 dB</cell><cell>48.62 dB</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Super-resolution of probav images using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?rtens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Izzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krzic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astrodynamics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="387" to="402" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">PROBA-V Super Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Kelvins</surname></persName>
		</author>
		<ptr target="https://kelvins.esa.int/proba-v-super-resolution" />
		<imprint/>
	</monogr>
	<note>Advanced Concepts</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A total variation regularization based super-resolution reconstruction algorithm for digital video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing (JASP)</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">74585</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Edge-directed single-image super-resolution via adaptive gradient magnitude selfinterpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1289" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single image super-resolution with non-local means and steering kernel regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4544" to="4556" />
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single image super-resolution with multiscale similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1648" to="1659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="page" from="8" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1673" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), D. Fleet</title>
		<editor>B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image superresolution</title>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiframe image restoration and registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Vision and Image Processing</title>
		<meeting><address><addrLine>Greenwich, CT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>JAI Press</publisher>
			<date type="published" when="1984" />
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="317" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High resolution image formation from low resolution frames using delaunay triangulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lertrattanapanich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Bose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1427" to="1441" />
			<date type="published" when="2002-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models and Image Processing (CVGIP)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-resolution image recovery from imageplane arrays, using convex projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oskoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1715" to="1726" />
			<date type="published" when="1989-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-resolution image reconstruction from a low-resolution image sequence in the presence of time-varying motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Patti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Ibrahim</forename><surname>Sezan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murat Tekalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st International Conference on Image Processing (ICIP)</title>
		<meeting>1st International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<date type="published" when="1994-11" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="343" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Double sparsity for multi-frame super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-frame image super resolution based on sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toshiyuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hideitsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noboru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="64" to="78" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast and robust multiframe super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1327" to="1344" />
			<date type="published" when="2004-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kernel regression for image processing and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="349" to="366" />
			<date type="published" when="2007-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Super-resolution reconstruction algorithm to modis remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video superresolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging (TCI)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1611.05250</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep video superresolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep burst superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10997</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepsum: Deep neural network for super-resolution of unregistered multitemporal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Molini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3644" to="3656" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepsum++: Non-local deep neural network for super-resolution of unregistered multitemporal images</title>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2020 -2020 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="609" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-image super-resolution for remote sensing using deep recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Arefin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>St-Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalaitzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2020</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="816" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-image super resolution of remotely sensed images using residual attention deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Salvetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mazzia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khaliq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiaberge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5580" to="5590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a confidence measure for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1107" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
