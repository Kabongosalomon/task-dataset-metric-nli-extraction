<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GLF-CR: SAR-Enhanced Cloud Removal with Global-Local Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data Science in Earth Observation</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Remote Sensing Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ebel</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data Science in Earth Observation</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data Science in Earth Observation</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Remote Sensing Technology Institute</orgName>
								<orgName type="institution" key="instit2">German Aerospace Center</orgName>
								<address>
									<postCode>82234</postCode>
									<settlement>We?ling</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GLF-CR: SAR-Enhanced Cloud Removal with Global-Local Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>A R T I C L E I N F O</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Cloud removal Data fusion SAR Transformer</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>The challenge of the cloud removal task can be alleviated with the aid of Synthetic Aperture Radar (SAR) images that can penetrate cloud cover. However, the large domain gap between optical and SAR images as well as the severe speckle noise of SAR images may cause significant interference in SAR-based cloud removal, resulting in performance degeneration. In this paper, we propose a novel global-local fusion based cloud removal (GLF-CR) algorithm to leverage the complementary information embedded in SAR images. Exploiting the power of SAR information to promote cloud removal entails two aspects. The first, global fusion, guides the relationship among all local optical windows to maintain the structure of the recovered region consistent with the remaining cloudfree regions. The second, local fusion, transfers complementary information embedded in the SAR image that corresponds to cloudy areas to generate reliable texture details of the missing regions, and uses dynamic filtering to alleviate the performance degradation caused by speckle noise. Extensive evaluation demonstrates that the proposed algorithm can yield high quality cloud-free images and outperform state-of-the-art cloud removal algorithms with a gain about 1.7 dB in terms of PSNR on SEN12MS-CR dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Earth observation through satellites plays a vital role in understanding the world, and has attracted attention from a wide range of communities <ref type="bibr" target="#b40">(Xia et al., 2018;</ref><ref type="bibr" target="#b28">Requena-Mesa et al., 2021;</ref><ref type="bibr" target="#b13">Girard et al., 2021)</ref>. However, optical satellite images are often contaminated by clouds, which obstruct the view of the surface underneath, as shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>. A study conducted by the MODIS instrument shows that the overall global cloudiness is roughly 67% and the cloud fraction over land is about 55% <ref type="bibr" target="#b17">(King et al., 2013)</ref>. Thus, cloud removal becomes an indispensable pre-processing step for applications relying on data streams of continuous monitoring <ref type="bibr" target="#b5">(Ebel et al., 2021)</ref>. Due to the erasure of textures in cloud-covered regions, the task of cloud removal is severely ill-posed. Benefiting from Synthetic Aperture Radar (SAR) <ref type="bibr" target="#b2">(Bamler, 2000)</ref> (as shown in <ref type="figure" target="#fig_0">Fig. 1b</ref>), which is not affected by clouds due to its advantage of strong penetrability and measures the backscatter, the challenge of cloud removal can be essentially alleviated. However, the recovery of high-quality cloud-free images with the aid of SAR images is nevertheless a challenging problem due to the following issues:</p><p>? Domain Gap. SAR and optical images reveal different characteristics of observed objects due to their different imaging mechanisms, and thus a large domain gap exists between them <ref type="bibr" target="#b30">(Schmitt et al., 2017;</ref><ref type="bibr" target="#b22">Liu and Lei, 2018)</ref>. Transferring the complementary information A few SAR-based cloud removal methods to learn to transfer the concatenation of multi-modal images to cloudfree images have been proposed <ref type="bibr" target="#b12">(Gao et al., 2020;</ref><ref type="bibr" target="#b26">Meraner et al., 2020;</ref><ref type="bibr" target="#b5">Ebel et al., 2021)</ref>. However, the pixel-topixel translation does not take into account the long-range varying contextual information of the cloud-free regions, leading to texture and structure discrepancies. Furthermore, this concatenation method only partially explores the interactions or correlations between optical and SAR data, in which complementary information cannot be effectively transferred. Moreover, simply stacking multi-modal images is susceptible to speckle noise, which hinders the cloud removal performance.</p><p>To tackle the issues and limitations above, we propose a novel global-local fusion-based cloud removal (GLF-CR) algorithm by exploring the full potential of SAR image. It has been shown that SAR images help to recover texture details by compensating for the missing information in cloudy regions <ref type="bibr" target="#b26">(Meraner et al., 2020)</ref>. In addition, since a SAR image is not obscured by clouds, it contains reliable global contextual information that can provide valuable guidance for capturing global interactions between contexts GLF-CR (c) result of DSen2-CR <ref type="bibr" target="#b26">(Meraner et al., 2020)</ref>; (d) result of our proposed GLF-CR; (e) cloud-free image. The GLF-CR can restore images with more details and fewer artifacts. The size of each image is 1, 000 ? 1, 000 pixels.</p><p>to maintain global consistency with the remaining cloudfree regions. Specifically, GLF-CR contains two parallel backbones developed for optical and SAR image representation learning, where SAR features are used in a hierarchical manner to compensate for the loss of information. Inspired by Transformer architectures <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> that can capture global interactions between contexts, we propose a SAR-guided global context interaction (SGCI) block in which SAR features are used to guide the interactions of global optical feature. Furthermore, a SAR-based local feature compensation (SLFC) block is proposed to transfer complementary information from the corresponding regions in the SAR features to the optical features, where dynamic filtering is used to handle speckle noise. Consequently, the proposed algorithm can generate knowledgeable features with comprehensive information, thereby yielding high-quality cloud-free images. To sum up, the contributions of this work are three-fold:</p><p>? We propose a novel SAR-based cloud removal algorithm, GLF-CR. It incorporates the contribution of SAR to restoring reliable texture details and maintaining global consistency, thus enabling the region occluded by cloud cover to be effectively reconstructed.</p><p>? We propose a SAR-guided global context interaction (SGCI) block, in which the SAR feature is used to guide the global interactions between contexts in order to ensure that the structure of the recovered cloudfree region is consistent with the remaining cloud-free regions.</p><p>? We propose a SAR-guided local feature compensation (SLFC) block to enhance the transference of complementary information embedded in the SAR image while avoiding the influence of speckle noise, and thus generate more reliable texture details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Cloud Removal. Cloud removal aims to reconstruct the missing information caused by clouds in optical satellite imagery. Early attempts address this problem by assuming the corrupted regions and the remaining regions share the same statistical and geometrical structures. They view cloud removal as an inpainting task and use the information around the corrupted regions to predict the missing data <ref type="bibr" target="#b4">(Chan and Shen, 2001;</ref><ref type="bibr" target="#b25">Maalouf et al., 2009)</ref>. Many recent studies learn the mapping between cloudy and cloud-free images by benefiting from the remarkable generative capabilities of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b33">Singh and Komodakis (2018)</ref>; <ref type="bibr" target="#b38">Wen et al. (2021)</ref>; <ref type="bibr" target="#b51">Zi et al. (2022)</ref>. These methods fail to make accurate inferences when the corrupted region occupies a large portion of the image. To mitigate this problem, a series of studies make use of multispectral data to recover the missing information <ref type="bibr" target="#b31">(Shen et al., 2013;</ref><ref type="bibr" target="#b42">Xu et al., 2015;</ref><ref type="bibr" target="#b8">Enomoto et al., 2017)</ref>. For example, McGANs <ref type="bibr" target="#b8">(Enomoto et al., 2017)</ref> and CR-GAN-PM  utilize additional near-infrared (NIR) images, which process higher penetrability through clouds, to improve visibility. However, as the thickness of clouds increases, all the land signals in the optical bands are obstructed. Consequently, multitemporalbased approaches have been proposed to restore the missing information with data from other time periods <ref type="bibr" target="#b29">(Scarpa et al., 2018;</ref><ref type="bibr" target="#b32">Shen et al., 2019;</ref><ref type="bibr" target="#b44">Zhang et al., 2021;</ref><ref type="bibr" target="#b11">Gao et al., 2021;</ref><ref type="bibr" target="#b7">Ebel et al., 2022)</ref>. However, when encountering continual cloudy days, cloud-free reference data from an adjacent period is largely unavailable. Synthetic Aperture Radar (SAR) images are cloudpenetrable and thus provide missing information due to optically impenetrable clouds <ref type="bibr" target="#b2">(Bamler, 2000)</ref>. There is promising potential in SAR-to-optical image translation. Some researchers have tried to generate optical images directly from SAR <ref type="bibr" target="#b3">(Bermudez et al., 2018;</ref><ref type="bibr" target="#b10">Fuentes Reyes et al., 2019)</ref>. However, since SAR lacks spectrally resolved measurements, there are domain-specific potentials and peculiarities that cannot be compensated. It is challenging to guarantee the quality of the generated optical image translated from a SAR image. Recently, a few studies have explored the means of SAR-optical data fusion, exploiting the synergistic properties of the two imaging systems to guide cloud removal. <ref type="bibr" target="#b26">Meraner et al. (2020)</ref> concatenate the SAR image to the input optical image and use a deep residual neural network to predict the target cloud-free optical image. <ref type="bibr" target="#b12">Gao et al. (2020)</ref> utilize a two-step approach, first translating the SAR image into a simulated optical image, and then concatenating the simulated optical image, the SAR image, and the optical image corrupted by clouds to reconstruct the corrupted regions using the generative adversarial network (GAN). Experiments have verified the usefulness of SARoptical data fusion, but its gain is limited because the concatenation approach has limited ability to utilize the complementary information from the SAR image. To boost the gain that comes with the additional SAR information, we propose a novel cloud removal algorithm, GLF-CR, which incorporates the contribution of SAR to restoring reliable texture details and maintaining global consistency to compensate for information loss in cloudy regions. Image Restoration. Cloud removal is essentially an image restoration task in which a high-quality clean image is reconstructed from a low-quality, degraded counterpart. Recent advances in image restoration follow convolutional neural network (CNN), and numerous CNN-based models have been proposed to improve restoration performance <ref type="bibr" target="#b45">(Zhang et al., 2018a</ref><ref type="bibr" target="#b36">Wang et al., 2021)</ref>. Global context plays an important role in local pixel-wise recovery. However, convolution is not effective for long-range dependency modeling under the principle of local processing <ref type="bibr" target="#b19">(Liang et al., 2021)</ref>. To ensure visually consistent restoration results, a series of research focuses on the attention mechanism to obtain global dependency information. <ref type="bibr" target="#b37">Wang et al. (2019)</ref> exploit a two-round four-directional IRNN architecture to accumulate global contextual information. <ref type="bibr" target="#b48">Zheng et al. (2019)</ref> introduce a short+long term attention layer to ensure appearance consistency in the image domain. Recently, Transformer that employs a self-attention mechanism to capture global interactions between contexts <ref type="bibr" target="#b24">(Liu et al., 2021c)</ref> has been proposed and shows promising performance in image restoration <ref type="bibr" target="#b19">(Liang et al., 2021)</ref>. While the task of SAR-enhanced cloud removal studied in this paper needs to integrate both the information from the degraded image itself and the information from auxiliary SAR image, which is more challenging.</p><p>Most existing cloud removal methods are carried out by extending the input channels of the popular CNN architectures. For example, <ref type="bibr">McGAN (Enomoto et al., 2017)</ref> extends the input channels of the conditional Generative Adversarial Networks (cGANs) so that they are compatible with multispectral images. DSen2CR <ref type="bibr" target="#b26">(Meraner et al., 2020)</ref> is derived from the EDSR network <ref type="bibr" target="#b20">(Lim et al., 2017)</ref>, and concatenates the SAR's channels and the other channels of the input optical image as input. These architectures are usually designed for tasks like super-resolution and motion deblurring, where the local information from the original low-quality image is only partially lost. For the cloud removal task, all the local information in the area covered by thick clouds is missing because the clouds completely corrupt the reflectance signal. Thus, the cloud removal methods extended from these architectures have limited ability to fully utilize the spatial consistency between the cloudy and the neighboring cloud-free regions. In comparison, the architecture presented in this work is designed to integrate the global context information under the guidance of the SAR image. Multi-Modal Data Fusion. Commonly used fusion strategies include element-wise multiplication/addition or concatenation between different types of features <ref type="bibr" target="#b34">(Sun et al., 2019;</ref><ref type="bibr" target="#b9">Fu et al., 2020;</ref><ref type="bibr" target="#b41">Xu et al., 2021)</ref>; this multi-modal data fusion yields limited performance gain <ref type="bibr" target="#b39">(Wu and Han, 2018;</ref><ref type="bibr" target="#b1">Audebert et al., 2018;</ref><ref type="bibr" target="#b21">Liu et al., 2021a)</ref>. To better exploit the complementary information of the auxiliary data, <ref type="bibr" target="#b15">Hazirbas et al. (2016)</ref> propose FuseNet for semantic segmentation. FuseNet contains two branches to extract features from the RGB and depth images, and constantly fuses them via element-wise summation. <ref type="bibr" target="#b21">Liu et al. (2021a)</ref> propose an information aggregation distribution module for crowd counting, which consists of two branches for modality-specific representation learning (i.e., RGB and thermal image) and an additional branch for modalityshared representation learning. It dynamically enhances the modality-shared and modality-specific representations with a dual information propagation mechanism. These methods increase the utilization of complementary information of auxiliary data. Nevertheless, little consideration has been given to SAR-optical data fusion for cloud removal, the specific challenges of which are addressed and resolved in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Statement</head><p>Given a cloudy image defined over ? ? ? + ? with ? and ? respectively denoting the cloud-covered and cloudfree regions, the task of cloud removal aims at restoring the cloud-covered region of the image, i.e., ? . Generally, this task is severely ill-posed due to the missing information caused by clouds in optical satellite observations. Inpainting. The basic strategy is to infer the cloud-covered region ? from the cloud-free part ? , and thus it can be considered as inpainting task, i.e.,</p><formula xml:id="formula_0">? = INP ( ? ; ( )),<label>(1)</label></formula><p>where INP is an inpainting operator conditioned by latent structures of the whole image, i.e., ( ). Specifically, ( ) represents priors of images, e.g., smoothness, nonlocal similarities, or learned features embedding from data, with which the task of cloud removal is tractable. However, latent structures of ( ) are not generally holistic or are even unavailable in a cloud removal task when the cloud-covered region is dominant, leading to the failure of reconstruction if only a cloudy image is utilized. Translation. The SAR image is cloud free and can provide a valuable source that compensates for the information missing from the cloudy region. Inspired by the great success in style transfer work achieved by deep learning, existing SAR-based cloud removal methods mainly translate the SAR image to an optical image to remove clouds pixel-by-pixel:</p><formula xml:id="formula_1">? = TRF ( ? ; ( , )),<label>(2)</label></formula><p>where TRF is a transfer operator conditioned by the inherent relationship between SAR image and optical image , i.e., ( , ). Specifically, ( , ) represents the cross-modality transferring, which is usually learned from the dataset using the generative adversarial network (GAN) by feeding the stack of multi-modal data channels. However, the pixelby-pixel translation does not take the spatial consistency between the cloudy and neighboring cloud-free regions into consideration. It consequently leads to the failure to maintain global consistency. Moreover, its method of stacking the channels of SAR and optical images is somewhat straightforward but only partially explores interactions or correlations between multi-modal data. It thus leads to limited performance improvement despite the assistance of the SAR images. And it is further influenced by the speckle noise in the SAR images, leading to reconstruction error.</p><p>Thus the main obstacles to boosting cloud removal performance are two-fold.</p><p>? The network should effectively transfer the complementary information from SAR image ? to the optical image while overcoming the influence of its speckle noise to generate reliable texture details.</p><p>? The surface information from the cloud-free region ? should be considered to maintain the structure of the recovered cloud-free region consistent with the remaining cloud-free regions.</p><p>Global-Local Fusion. Thus the task of SAR-enhanced cloud removal is to develop an operator fusion conditioned by both the inherent relationship between SAR and optical images and latent structures of the whole image, i.e.,</p><formula xml:id="formula_2">? = fusion ( ? , ? ; ( ? ), ( , )),<label>(3)</label></formula><p>where ( ? ) is the non-local context information of the cloudy image learned under the guidance of SAR image.</p><p>Since the SAR image is not affected by cloud cover, it can provide valuable guidance for capturing global interactions between contexts, so as to maintain the structure of the recovered cloud-free region consistent with the remaining cloud-free regions. ( , ) in fusion is different from its counterpart in TRF , which incorporates the information of the SAR image by stacking its channels to the optical image. We propose instead a more effective fusion strategy to transfer the complementary information from the corresponding region in the SAR image, so as to generate more reliable texture details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>The overall framework of the proposed GLF-CR algorithm is illustrated in <ref type="figure">Fig. 2</ref>. It is a two-stream network in which the SAR feature is hierarchically fused into the optical feature to compensate for information loss in cloudy regions. Exploiting the power of SAR information to promote cloud removal entails two aspects: global fusion, to guide the relationship among all local optical windows with the SGCI block; and local fusion, to transfer the SAR feature corresponding to cloudy areas with the SLFC block. Specifically, a cloudy image and its corresponding SAR image are first fed into different branches to extract modality-specific features? 0 and? 0 with the shallow feature extraction (SFE) block,</p><formula xml:id="formula_3">0 = ( ),? 0 = ( ),<label>(4)</label></formula><p>where (?) and (?) denote the functions to extract the shallow features of the cloudy image and the SAR image, respectively. Then,? 0 and? 0 are fed into functions composited from the SGCI and SLFC block to obtain knowledgeable features with comprehensive information. More specifically, the intermediate features</p><formula xml:id="formula_4">{? 1 ,? 1 }, {? 2 ,? 2 }, ..., {? ,? } are abtained a? ,? = ( (? ?1 ,? ?1 )),<label>(5)</label></formula><p>where (?) and (?) denote the functions of the SGCI block and the SLFC block, respectively. The purpose of the SGCI block is local feature extraction and crosswindow feature interaction, where the SAR feature is used to guide the relationship among all local optical windows. Each SGCI block is followed by an SLFC block, which is designed to fuse the complementary information from the corresponding area in a SAR image into the optical feature of a cloudy area. More details about these two blocks will be given in Secs. 4.2 and 4.3. Finally, the high-quality cloud-free image ? is reconstructed by aggregating all the intermediate optical features,</p><formula xml:id="formula_5">? = + ([? 1 ,? 2 , ...,? ]),<label>(6)</label></formula><p>where denotes the function of cloud-free image reconstruction, and [? 1 ,? 2 , ...,? ] refers to the concatenation of the intermediate optical features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SAR-Guided Global Context Interaction</head><p>The SGCI block, whose detail is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, has two parallel streams for the input optical and SAR features. Each stream adopts dense connections in an approach similar to the residual dense block (RDB) <ref type="bibr" target="#b46">(Zhang et al., 2018b)</ref>, which is able to extract abundant local features via dense connected convolutional layers. As previously mentioned, SAR image clearly contributes to compensating for the missing information about cloudy regions, but not for the specific properties of optical images. Nevertheless, the cloud-free regions are conducive to the specific properties. The use of global texture information is necessary for the cloud removal task. Inspired by Transformer's ability to efficiently propagate information across the entire image to accumulate longrange varying contextual information, a Swin Transformer GLF-CR <ref type="figure">Fig. 2</ref>: Overview of the proposed global-local fusion based cloud removal (GLF-CR) algorithm. It is a two-stream network in which the SAR feature is hierarchically fused into the optical feature to compensate for information loss in cloudy areas. Exploiting the power of SAR information to promote cloud removal entails two aspects: global fusion, to guide the relationship among all local optical windows based on the SAR-guided global context interaction (SGCI) block; and local fusion, to transfer the SAR feature corresponding to cloudy areas based on the SAR-based local feature compensation (SLFC) block. layer (STL) <ref type="bibr" target="#b24">(Liu et al., 2021c)</ref> is added after each local convolutional layer for cross-window feature interaction.</p><p>The STL first partitions the input feature into nonoverlapping ? windows, then computes the standard self-attention separately for each window. Specifically, a local window optical/SAR feature</p><formula xml:id="formula_6">? ? ? 2 ? is linearly transformed to query ? ? ? 2 ? , key ? ? ? 2 ? , and value ? ? ? 2 ? ,</formula><p>where is the dimension of the query or key. The attention weight matrix is computed as follows:</p><formula xml:id="formula_7">= ? + , = ? + ,<label>(7)</label></formula><p>where is the learnable relative positional encoding. The essence of this attention matrix is the weight of a particular region that is absorbing information from other regions. For a cloudy region, due to the information loss, it is difficult to estimate its interactions with cloud-free regions. For the same region in a SAR image, its interactions with other regions can be estimated easily, which provides valuable guidance for the interactions between cloud-free and cloudy regions in the optical image. Thus, we transfer the attention map of the SAR image to refine the attention map of the optical image, i.e., we use to improve . We first obtain the attention map of the optical and SAR features and by Eq. (7). Then we compute the difference between and and obtain . Finally, we apply a gating function to adaptively refine :</p><formula xml:id="formula_8">= + ? ( ),<label>(8)</label></formula><p>where (?) is the gating function fed with the residual term and ? denotes the element-wise multiplication operation. The optical and SAR output are computed as: This module considers the relationship among all local window optical features under the guidance of the SAR feature, denoted in this paper as global fusion.</p><formula xml:id="formula_9">= Softmax(? ) , = Softmax( ) . (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">SAR-based Local Feature Compensation</head><p>The detail of the SLFC block is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Because the SAR image is corrupted by severe speckle noise, we utilize dynamic filtering for SAR features before information transference. Standard convolution filters are shared across all pixels in an image, while the dynamic filters vary from pixel to pixel. Therefore, the dynamic filters can handle the spatial variance issue <ref type="bibr" target="#b16">(Jia et al., 2016;</ref><ref type="bibr" target="#b49">Zhou et al., 2019)</ref>, thus helping to suppress the spatially unevenly speckle noise. Specifically, a filter is dynamically generated for each position in the SAR feature using the Dynamic Filter Generation (DFG) module. The DFG module takes the concatenation of the optical and SAR features ( , ) ? ? ? ?2 as input. The dimension of the generated filter ? is ? ? 2 and is reshaped into a five-dimensional filter. Then, for each position (?, , ) in the SAR feature ? ? ? ? , a specific local filter ? (?, , ) ? ? ? is applied to the region centered around (?, , ) a?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLF-CR</head><formula xml:id="formula_10">(?, , ) = ? (?, , ) * (?, , ),<label>(10)</label></formula><p>where * denotes the convolution operation. After transforming the extracted SAR feature using the dynamic filter to improve tolerance of speckle noise, we propagate the complementary information from the SAR feature to refine the optical feature, in the same way that the attention map is refined. We compute the difference between the optical and SAR features to obtain the residual information ? =? ? , and apply a gating function to transfer the complementary information,</p><formula xml:id="formula_11">= + ? ? ( ? ).<label>(11)</label></formula><p>To better exploit interactions among elements of the optical and SAR features for a further performance gain, we adopt a dual information propagation mechanism, i.e., updating the SAR feature as well. We compute the difference between the SAR feature and the updated optical feature ? = ?? , and also propagate the information through use of a gating function,</p><formula xml:id="formula_12">= + ? ? ( ? ).<label>(12)</label></formula><p>The enhanced optical and SAR features are then introduced into the next SGCI for further representation learning. This module considers the information transference between local features, denoted as local fusion in this paper. In an empirical manner, the batch size is set to 12 and the maximum epoch of training iterations is set to 30. The Adam optimizer is used and the learning rate starts at 10 ?4 , which decays by 50% every five epochs. By trading off the performance and complexity of the model, the number of the SGCI and SLFC blocks is set to 6; the number of dense connections in each stream of the SGCI block is set to 5; the window size and the attention head number for the STL layer are set to 8 and 8, respectively; and the size of the dynamic filter is set to 5. The codes, models, and more results are released at:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>https://github.com/xufangchn/GLF-CR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons with State-of-the-art Methods</head><p>We compare the proposed GLF-CR networks to stateof-the-art cloud removal methods, including multi-spectral based approaches, SpA GAN <ref type="bibr" target="#b27">(Pan, 2020)</ref>, the SAR-tooptical image translation approach, SAR2OPT <ref type="bibr" target="#b3">(Bermudez et al., 2018)</ref>, and SAR-optical data fusion based approaches, SAR-Opt-cGAN <ref type="bibr" target="#b14">(Grohnfeldt et al., 2018)</ref>, Simulation-Fusion GAN <ref type="bibr" target="#b12">(Gao et al., 2020)</ref> and DSen2-CR <ref type="bibr" target="#b26">(Meraner et al., 2020)</ref>. SpA GAN takes all channels of the input optical image as input. It uses the spatial attention network (SPANet)  as a generator to model the map from a cloudy image to a cloudless image. SAR2OPT performs SAR-to-optical translation by takeing the U-Net as the generator, not relying on any (cloudy) optical satellite information. SAR-Opt-cGAN and DSen2-CR both leverage the SAR image as a form of prior to guide the reconstruction process under thick, optically impenetrable clouds. The SAR's channels are simply concatenated to the other channels of the input optical image to predict the full spectrum of optical bands. SAR-Opt-cGAN is extended from U-Net, while DSen2-CR is extended from the EDSR network <ref type="bibr" target="#b20">(Lim et al., 2017)</ref>. Simulation-Fusion GAN first translates the SAR image into simulated optical data, then takes the concatenation of the simulated optical image, SAR and the corrupted optical image as input for prediction.</p><p>To validate the superiority of GLF-CR in leveraging the power of SAR images, we also refer to the fusion strategy in SAR-Opt-cGAN and DSen2-CR to train the proposed network, by using concatenation, denoted as Concat. We concatenate the SAR's channels and optical image's channels as input, and remove the branch for SAR feature learning, the attention map update in the SGCI blocks, and the SLFC blocks. The quantitative results are presented in <ref type="table" target="#tab_1">Table 1</ref>. The proposed GLF-CR network brings remarkable improvements compared to state-of-the-art methods. We choose 3 scenes to evaluate qualitative results, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. For each scene, from top-left to bottom-right are respectively the cloudy image, the SAR image, the results from SpA GAN, SAR2OPT, SAR-Opt-cGAN, Simulation-Fusion GAN, DSen2-CR, Concat and GLF-CR, and the cloud-free image. We find that the proposed GLF-CR network achieves the best visualization performance. Detailed analyses are presented below.</p><p>We first compare the cloud removal performance of SAR-based methods to the conventional method, SpA GAN. As the SAR image encodes rich geometrical information about cloud-covered regions, it facilitates the ground object construction. SpA GAN, which relies solely on cloudy optical images, are less effective than SAR-based cloud removal methods. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, it fails to tackle the thick cloud removal and generates undesirable artifacts, especially for cloud-covered regions.</p><p>We next compare the cloud removal performance of the SAR-to-optical image translation approach, SAR2OPT to the SAR-optical data fusion based approaches. SAR2OPT, which relies solely on SAR images, can reconstruct prominent geometric characteristics related to roads, crop fields, etc. But it suffers from content vanishing because the specific potentials and peculiarities of optical images cannot be fully compensated from the SAR images. Moreover, a distinct difference in the color distribution of SAR2OPT's reconstruction results and ground truth can be observed. SAR-Opt-cGAN adopts the same generator architecture as SAR2OPT while taking both the cloudy optical image and the SAR image as input. However, it performs worse than SAR2OPT which only takes the SAR images as input. And as shown in the second scene of <ref type="figure" target="#fig_3">Fig. 5</ref>, the SAR image clearly emphasizes the surface's physical properties. SAR-Opt-cGAN fails to reconstruct it while SAR2OPT does. It demonstrates the challenge of taking advantage of multimodal data fusion while avoiding the performance degradation caused by the undesirable effects in each modality. Simulation-Fusion GAN suffers from the performance degradation caused by the undesirable effects in simulated optical image besides the cloudy optical and SAR images, and also has poor color fidelity. To some extent, DSen2-CR alleviates the performance degradation caused by the undesirable effects by utilizing a tailored generator. However, its gain is still limited.</p><p>Our methods perform favorably when compared with DSen2-CR, which exploits the inherent advantage of SAR image. Among them, Concat adopts the same approach as SAR-Opt-cGAN and DSen2-CR to utilize the complementary information embedded in SAR images. It achieves higher performance than SAR-Opt-cGAN and DSen2-CR, as shown in <ref type="table" target="#tab_1">Table 1</ref>. Unlike the approach of SAR-Opt-cGAN and DSen2-CR, Concat contains global context interactions, which takes the information embedded in neighboring cloud-free regions into consideration, thus performing better in terms of global consistent structure. But those methods still leave distinct clouds or blur some image textures, which reflects the limitations of the concatenation method. Furthermore, It can be observed that the proposed GLF-CR network outperforms other methods by a large margin. It can restore images with more details and fewer artifacts, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. These significant improvements demonstrate that the proposed method can better use the complementary information embedded in SAR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis on Different Cloud Cover Levels</head><p>We further compare the proposed GLF-CR networks to state-of-the-art cloud removal methods on different cloud cover levels. We evaluate the performance of cloud removal on the images with cloud cover of 0% to 20%, 20% to 40%, 40% to 60%, 60% to 80%, and 80% to 100%, and show the comparison results in terms of the PSNR, SSIM, SAM, and MAE quality metrics in <ref type="figure">Fig. 6</ref>. The proposed methods perform favorably when compared with state-ofthe-art methods on all cloud cover levels.</p><p>It is observed that the overall performance of multispectralbased approaches, SpA GAN, is negatively correlated with the cloud cover level. With the higher cloud cover level, GLF-CR <ref type="figure">Fig. 6</ref>: Quantitative comparisons of proposed GLF-Nets to state-of-the-art methods on different cloud cover levels in terms of the PSNR, SSIM, SAM, and MAE quality metrics. they get less prior information and thus perform worse. And the performance of the SAR-to-optical image translation approach, SAR2OPT, is not related to the cloud cover level.</p><p>SAR-Opt-cGAN and Simulation-Fusion GAN utilize the prior information from both cloudy images and SAR images. It suffers the performance degradation caused by the undesirable effects in both modalities. When the cloud cover is low, it is not as good as the multispectral-based methods due to the interference from additional SAR image or simulated optical image from SAR image. When the cloud cover is high, it is not as good as the SAR-to-optical image translation approach due to the interference from clouds.</p><p>DSen2-CR alleviates the performance degradation to some extent, and thus outperforms the single-modalitybased methods. Concat adopts the same fusion strategy in DSen2-CR to utilize the complementary information embedded in SAR images while takes the information embedded in neighboring cloud-free regions into consideration, thus its performance is more superior to that of DSen2-CR when more prior information from cloud-free regions is available. And the proposed method is superior in exploiting the power of SAR information in addition to considering the information embedded in neighboring cloud-free regions, and thus steadily outperforms DSen2-CR on all cloud cover levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>The proposed GLF-CR network improves the performance of SAR-based cloud removal by incorporating global fusion to guide the relationship among all local optical windows with SAR features and local fusion to transfer the SAR feature corresponding to cloudy areas to compensate for the missing information. To determine what contributes to the superior performance of the proposed approach, we analyze the effectiveness of each component by comparing a few variants with and without the use of SAR image (SAR), Concatenation fusion (Concat), STL layer (STL), global fusion (GF), and dynamic filter (DF). The qualitative and qualitative results are shown in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure">Fig. 7</ref>, and the results on different cloud cover levels is shown in <ref type="table">Table 8</ref>. From the table and the figure, we can draw the following conclusions: Importance of SAR Image. We validate the importance of the SAR image by training the GLF-CR network without SAR images, denoted as w/o SAR. Since the input is a single source signal, i.e., the cloudy optical image itself, a single-stream network is adopted and no fusion strategy is used. As shown in <ref type="figure" target="#fig_4">Fig. 8</ref>, it performs comparable to the networks employing SAR images when the cloud cover level is low. However, when the cloud cover level gets higher, the performance gap between the networks with and without SAR images gets larger. And as shown in <ref type="figure">Fig. 7</ref>, w/o SAR tends to generate over-smoothed effects for cloud-covered regions, while the networks with SAR images can recover texture details. This demonstrates that the rich complementary information encoded in SAR images can effectively improve the cloud removal performance. Limitation of Concatenation Fusion. Compared with w/o SAR, Concat only adds two channels to the input to utilize the SAR image. The gain of utilizing the concatenation fusion is 0.17dB, while the proposed GLF-CR network obtains a gain of 0.71dB. As observed from <ref type="figure" target="#fig_4">Fig. 8</ref>, when the proportion of cloud-free regions is higher, the performance gap between Concat and GLF-CR is larger, since the proposed GLF-CR network can better exploit the power of SAR information  compared with the concatenation fusion. <ref type="figure">Fig. 7</ref> shows that the proposed GLF-CR network can recover more complete texture structure and obtain better visual effects. Effectiveness of Global Interactions. Capturing global interactions between contexts plays a vital role in maintaining global consistent structure. We train the GLF-CR network by removing the STL layers in the SGCI blocks, denoted as w/o STL. It can be observed that the proposed GLF-CR method which captures the global interactions between contexts can improve cloud removal performance effectively. It recovers clearer and more complete structure for the land in the second and fourth scenes in <ref type="figure">Fig. 7</ref>. Effectiveness of SAR-Guided Global Interactions. We further validate the effectiveness of guiding the global interactions of optical features with SAR features. We train the GLF-CR network by reserving the STL layer but not using the SAR feature to guide the global optical interactions, denoted as w/o GF. Compared with w/o STL, it can be observed that w/o GF has only a slight performance improvement in terms of SAM, despite using additional STL layers to maintain the spatial consistency, since estimating the interactions from the cloudy optical image itself will introduce some error. As shown in the third scene in <ref type="figure">Fig. 7</ref>, w/o GF generates undesirable artifacts. As the SAR image is not affected by cloud cover, it can provide valuable guidance for capturing global interactions between contexts. This point can be validated by comparing the results of w/o GF and GLF-CR. It can be seen that guiding the global interactions of optical features with SAR features can effectively improve the performance of cloud removal and make the structure of the predicted cloud-free image more consistent with ground truth. Effectiveness of the Dynamic Filter. The proposed GLF-CR network uses dynamic filtering to handle the speckle noise of SAR images. To validate the effectiveness of the dynamic filter, we train the GLF-CR network by removing the dynamic filter in SLFC blocks, denoted as w/o DF. It can be seen from <ref type="figure" target="#fig_4">Fig. 8</ref> that the performance of w/o DF degrades more severely in terms of PSNR and MAE that measure the quality of reconstructed images than in terms of SSIM and SAM that quantify spectral and structural similarity. And it can be observed that the trends of w/o DF and GLF-CR relative to the cloud cover level are similar. As both methods adopt the same strategy to utilize the information of the cloud-free regions and SAR images, while the proposed GLF-CR network can alleviate the problem of speckle noise in the SAR image and generate clearer images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Performance on Challenging Conditions. Cloud removal is quite challenging when the image to be processed is completely cloudy. To see how the proposed method behaves in the challenging conditions, <ref type="figure" target="#fig_5">Fig. 9</ref> shows the results on the images where the ground information is almost obscured by clouds. It can be found that the proposed method can recover the approximate information of ground objects while with poor texture details. Since the images are completely cloudy, no cloud-free part can be accessed and only SAR information is available to reconstruct the cloud-free images. The quality of reconstructed cloud-free images depends entirely on the information embedded in the SAR image. While the SAR image fails to feature the different agricultural landscapes, as seen in the first scene in <ref type="figure" target="#fig_5">Fig. 9</ref>, the reconstructed cloud-free image loses the corresponding details. And since no spectral information is available, the spectral fidelity of the reconstructed cloud-free image degrades. Speckle Noise in SAR Data. The SAR data in SEN12MS-CR dataset is from the Level-1 GRD product, which has been multi-looked for reduced speckle. Notwithstanding, the multi-looked data still exhibits a high degree of speckle noise, as seen from <ref type="figure" target="#fig_0">Fig. 1, 5</ref> and 7, since speckle noise is multiplicative in nature and difficult to distinguish from the original signal. And, while commonly referred to as "speckle noise", speckle is not only noise but in some sense has an information content <ref type="bibr" target="#b0">(Argenti et al., 2013)</ref>. At this point, we do not consider an explicit despeckling preprocessing step, but implicitly handle the spatially varying speckle distribution by the dynamic filter embedded in the network. It is also possible to preprocess the SAR data with a despeckling technique before feeding it to the network. Therefore, we train the GLF-CR network by removing the dynamic filter in SLFC blocks while feeding the SAR data despeckled with a median filter. As shown in <ref type="table">Table.</ref> 3, we can see that preprocessing the SAR data with a despeckling technique can reduce the influence of speckle noise on cloud removal. While the proposed method implicitly mitigates the influence of speckle noise based on the dynamic filter and can achieve better performance. Geometric Distortion in SAR Data. It is well-known that there is an inherent geometric distortion in SAR data when the terrain is undulating, due to the sensor's sideways view. It will lead to the inconsistency between the information in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Registration error between the optical and SAR Data.</head><p>The registration error between the optical image and its corresponding SAR image is expected to affect the learning process. The data instructions given by ESA illustrate that the Sentinel-1 SAR L1 productions and the Sentinel-2 optical L1C productions have a co-registration accuracy of within 2 pixels. We set the size of the dynamic filter in the SLFC blocks to 5 for a larger receptive field, which allows the proposed model to work when tiny deviations exist between the SAR and optical images. Nuisances between Cloudy Reference Image and Cloud-Free Target Image. The cloud removal performance in the paper is assessed on the SEN12MS-CR dataset by comparing the prediction with the cloud-free image temporally close to the cloudy one. There are some inevitable nuisances determined by the sunlight condition, acquisition geometry, humidity, pollution, change of landscape, etc, while the SEN12MS-CR dataset is curated to minimize such cases. However, the inevitable nuisances are negligible for a relatively large-scale test split that is globally and seasonally sampled without any bias to specific sunlight condition, etc. It implies that models biased to specific condition won't have any unfair advantages on the test split. Overall, the influence of nuisances can be averaged out. It poses no concern about the fairness of benchmarking the proposed model on the considered dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLF-CR</head><p>In addition, we test the proposed method on images where the interval between the cloud-free and cloudy image is different. The date of input cloudy image is July 17, 2018, and we use the SAR image with the closest interval to cloudy image as auxiliary data, whose date is July 18, 2018. And the date of cloud-free images used for the assessment are July 30, 2018, August 11, 2018 and September 28, 2018, respectively. The results are shown in <ref type="table">Table.</ref> 4. We can observe that the proposed method performs better than the best baseline DSen2-CR overall, which is consistent with the results on the SEN12MS-CR dataset. It shows the feasibility of assessing the performances with temporally close cloudfree images. And we can observe that, when the interval between the reference cloud-free image used to calculate the value of the metrics and the cloudy image is larger, the methods performs worse in terms of the metrics. It indicates that the method is able to restore the surface information of the input cloudy image, and thus the cloud-free image with the larger interval to input cloudy image has less reference value.</p><p>Strict ground truth correspondence may only be guaranteed by generating synthetic cloud coverage superimposed on cloud-free observations, as done in <ref type="bibr" target="#b8">Enomoto et al. (2017)</ref> and <ref type="bibr" target="#b12">Gao et al. (2020)</ref>. However, the experimental results in <ref type="bibr" target="#b6">Ebel et al. (2020)</ref> has indicated that popular synthetic cloud simulation techniques suffer from severe limitations in approximation to the real data. The great performance on synthetic data may not necessarily translate to equal performance on real data. Hence we follow the approach of using real observations, despite acknowledgeable shortcomings at other ends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we propose a novel global-local fusion based cloud removal (GLF-CR) algorithm for high quality cloud-free image reconstruction. It boosts cloud removal performance from two aspects, on the one hand, it guides the relationship among all local optical windows with the SAR feature to fully utilize the spatial consistency between the cloudy and the neighboring cloud-free regions, and on the other hand, it enhances the utilization of SAR data to compensate for missing information while alleviating the performance degradation caused by speckle noise. Extensive experiments demonstrate that the power of the information embedded in neighboring cloud-free regions and corresponding SAR data over different cloud cover levels. The proposed method can achieve state-of-the-art performance on all different cloud cover levels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustrative example of SAR-based cloud removal on a large scale cloudy image. (a) Cloudy optical image; (b) SAR image;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Detail of the SAR-guided global context interaction (SGCI) block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Detail of the SAR-based local feature compensation (SLFC) block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative results of cloud removal for 3 different scenes. For each scene, from top-left to bottom-right are respectively the cloudy image, the SAR image, the result from SpA GAN, SAR2OPT, SAR-Opt-cGAN, Simulation-Fusion GAN (SF GAN), DSen2-CR, Concat, GLF-CR, and the cloud-free image. The size of each image is 128 ? 128 pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Quantitative ablation study on different cloud cover levels in terms of the PSNR, SSIM, SAM and MAE quality metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Example results of GLF-CR on the images completely obscured by clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Each sample consists of a triplet of an orthorectified, geo-referenced Sentinel-1 dual-pol SAR image, a Sentinel-2 cloud-free multi-spectral image, and a cloud-covered Sentinel-2 multi-spectral image where the observations of cloud-free and cloud-covered images are close in time. The size of each image is 256 ? 256 pixels. The VV and VH polarizations of the SAR images are clipped</figDesc><table><row><cell>to values [?25, 0] and [?32.5, 0], and rescaled to the range</cell></row><row><cell>[0, 1]. All bands of the optical images are clipped to values</cell></row><row><cell>[0, 10000], and rescaled to the range [0, 1] as well. We split</cell></row><row><cell>the 169 ROIS into 149 ROIs for training, 10 ROIs for vali-</cell></row><row><cell>dation, and 10 ROIs for test. To avoid overall performance</cell></row><row><cell>being biased towards specific cloud cover level, we calculate</cell></row><row><cell>the percentage of cloud cover of each image by utilizing</cell></row><row><cell>the cloud detection flowchart in Meraner et al. (2020) and</cell></row><row><cell>randomly select 800 samples from the samples with cloud</cell></row><row><cell>cover of 0% to 20%, 20% to 40%, 40% to 60%, 60% to 80%,</cell></row><row><cell>and 80% to 100% as the test set, respectively. Specifically,</cell></row><row><cell>the training, validation and test set consist of 101, 615,</cell></row><row><cell>8, 623 and 4, 000 samples respectively. The results of cloud</cell></row><row><cell>removal are evaluated with the normalized data based on</cell></row><row><cell>the peak signal-to-noise ratio (PSNR), structural similarity</cell></row><row><cell>index measure (SSIM), spectral angle mapper (SAM), and</cell></row><row><cell>mean absolute error (MAE).</cell></row><row><cell>Implementation Details. The proposed GLF-CR network is</cell></row><row><cell>implemented using publicly available Pytorch and trained in</cell></row><row><cell>an end-to-end manner supervised by L1 loss on 4 NVIDIA</cell></row><row><cell>TITAN V GPUs. We implement the gating functions in</cell></row><row><cell>Secs. 4.2 and 4.3 by employing a convolution layer as well</cell></row><row><cell>as a Softmax layer, and the dynamic filter generation (DFG)</cell></row><row><cell>module in Sec. 4.3 is constituted by a convolution layer fol-</cell></row><row><cell>lowed by two residual blocks. During training, we randomly</cell></row><row><cell>crop the samples into 128 ? 128 patches.</cell></row></table><note>5.1. Experimental Settings Dataset and Metrics. The experiments are conducted on the large-scale dataset SEN12MS-CR (Ebel et al., 2021), which is built from freely available data acquired by the Sentinel satellites in the Copernicus program. The dataset contains 122, 218 samples from 169 non-overlapping regions of in- terest (ROI) distributed over all inhabited continents during all meteorological seasons.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Quantitative comparisons of proposed GLF-Nets to state-of-the-art methods.</figDesc><table><row><cell>Method</cell><cell cols="2">Input Optical SAR</cell><cell cols="4">PSNR ? SSIM ? SAM ? MAE ?</cell></row><row><cell>SpA GAN (Pan, 2020)</cell><cell>?</cell><cell></cell><cell>24.8688</cell><cell>0.7533</cell><cell cols="2">16.0454 0.0444</cell></row><row><cell>SAR2OPT (Bermudez et al., 2018)</cell><cell></cell><cell>?</cell><cell>25.7223</cell><cell>0.7918</cell><cell cols="2">14.0501 0.0427</cell></row><row><cell>SAR-Opt-cGAN (Grohnfeldt et al., 2018)</cell><cell>?</cell><cell>?</cell><cell>25.2948</cell><cell>0.7594</cell><cell cols="2">14.4389 0.0441</cell></row><row><cell>Simulation-Fusion GAN (Gao et al., 2020)</cell><cell>?</cell><cell>?</cell><cell>24.5519</cell><cell>0.6947</cell><cell cols="2">15.5929 0.0455</cell></row><row><cell>DSen2-CR (Meraner et al., 2020)</cell><cell>?</cell><cell>?</cell><cell>27.3780</cell><cell>0.8705</cell><cell>8.5073</cell><cell>0.0319</cell></row><row><cell>Concat (Ours)</cell><cell>?</cell><cell>?</cell><cell>28.5324</cell><cell>0.8804</cell><cell>8.1088</cell><cell>0.0284</cell></row><row><cell>GLF-CR (Ours)</cell><cell>?</cell><cell>?</cell><cell>.</cell><cell>.</cell><cell>.</cell><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">Quantitative ablation study of proposed algorithm with and</cell></row><row><cell cols="5">without use of the SAR image (SAR), Concatenation fusion</cell></row><row><cell cols="5">(Concat), STL layer (STL), global fusion (GF), and dynamic</cell></row><row><cell>filter (DF).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">PSNR ? SSIM ? SAM ? MAE ?</cell></row><row><cell cols="2">w/o SAR 28.3657</cell><cell>0.8759</cell><cell>8.1783</cell><cell>0.0299</cell></row><row><cell>Concat</cell><cell>28.5324</cell><cell>0.8804</cell><cell>8.1088</cell><cell>0.0284</cell></row><row><cell>w/o STL</cell><cell>28.5079</cell><cell>0.8825</cell><cell>8.1783</cell><cell>0.0287</cell></row><row><cell>w/o GF</cell><cell>28.4983</cell><cell>0.8816</cell><cell>8.0595</cell><cell>0.0287</cell></row><row><cell>w/o DF</cell><cell>28.2867</cell><cell>0.8800</cell><cell>7.9853</cell><cell>0.0297</cell></row><row><cell>GLF-CR</cell><cell>.</cell><cell>.</cell><cell>.</cell><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Performance of proposed algorithm with use of despeckled SAR data.the SAR data and the actual state of the ground objects, adversely affecting the cloud removal performance. The experiments in this paper are conducted on the SEN12MS-CR dataset (to our best knowledge, the only open-source cloud removal dataset with SAR data), where the SAR data is provided by the Sentinel-1 satellites. Its resolution is 10 and thus does not show excessive distortion. Furthermore, depending on the large scale of the dataset, the proposed powerful model can address this aspect to some extent.</figDesc><table><row><cell>Method</cell><cell cols="4">PSNR ? SSIM ? SAM ? MAE ?</cell></row><row><cell cols="2">w/ despeckled SAR 28.5377</cell><cell>0.8818</cell><cell>8.0719</cell><cell>0.0286</cell></row><row><cell>w/o DF</cell><cell>28.2867</cell><cell>0.8800</cell><cell>7.9853</cell><cell>0.0297</cell></row><row><cell>GLF-CR</cell><cell>29.0793</cell><cell>0.8855</cell><cell>7.6455</cell><cell>0.0266</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Evaluating cloud removal performance using the cloud-free images with different intervals from cloudy images.</figDesc><table><row><cell>Interval</cell><cell>Method</cell><cell cols="2">PSNR ? SSIM ?</cell><cell>SAM ?</cell><cell>MAE ?</cell></row><row><cell>13 days</cell><cell cols="2">DSen2-CR 27.6299 GLF-CR 28.6470</cell><cell>0.8618 0.8707</cell><cell>6.9426 6.9005</cell><cell>0.0293 0.0260</cell></row><row><cell>25 days</cell><cell cols="2">DSen2-CR 26.3796 GLF-CR 26.9173</cell><cell>0.8403 0.8444</cell><cell>8.0728 8.6355</cell><cell>0.0334 0.0317</cell></row><row><cell>72 days</cell><cell cols="2">DSen2-CR 25.1544 GLF-CR 25.3110</cell><cell cols="3">0.8247 10.0843 0.0382 0.8324 10.6852 0.0378</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A tutorial on speckle reduction in synthetic aperture radar images. IEEE Geoscience and remote sensing magazine 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Argenti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lef?vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Principles of synthetic aperture radar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bamler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surveys in Geophysics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="147" to="157" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SAR to optical image synthesis for cloud removal with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Bermudez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Happ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A B</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>Feitosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences IV-1</title>
		<imprint>
			<biblScope unit="page" from="5" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ISPRS Annals of the Photogrammetry</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nontexture inpainting by curvature-driven diffusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="436" to="449" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multisensor data fusion for cloud removal in global and all-season Sentinel-2 imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meraner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="5866" to="5878" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cloud removal in unpaired Sentinel-2 imagery using cycle-consistent GAN and SAR-optical data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2065" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SEN12MS-CR-TS: A remote-sensing data set for multimodal multitemporal cloud removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Filmy cloud removal on satellite imagery with multispectral conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Enomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">JL-DCF: Joint learning and densely-cooperative fusion framework for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3052" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">SAR-to-optical image translation based on conditional generative adversarial networks-optimization, opportunities and limits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fuentes Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Merkle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Remote Sensing 11</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sentinel-2 cloud removal considering ground changes by fusing multitemporal SAR and optical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3998</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 13</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cloud removal with fusion of high resolution optical and SAR images using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="0191" />
		</imprint>
	</monogr>
	<note>Remote Sensing 12</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Polygonal building extraction by frame field learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smirnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5891" to="5900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A conditional generative adversarial network to fuse SAR and multispectral optical data for cloud removal from Sentinel-2 images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grohnfeldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1726" to="1729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FuseNet: Incorporating depth into semantic segmentation via fusion-based CNN architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial and temporal distribution of clouds observed by MODIS onboard the Terra and Aqua satellites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Platnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Menzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Hubanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="3826" to="3852" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Thin cloud removal in optical remote sensing images based on generative adversarial networks and physical model of cloud distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Molinier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="373" to="389" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SwinIR: Image restoration using Swin Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-modal collaborative representation learning and a large-scale RGBT benchmark for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4823" to="4833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Can SAR images and optical images transfer with each other?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7019" to="7022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MRDDANet: A multiscale residual dense dual attention network for SAR image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical vision Transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A bandelet-based inpainting technique for clouds removal from remotely sensed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maalouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Augereau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Maloigne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="2363" to="2371" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cloud removal in Sentinel-2 imagery using a deep residual neural network and SARoptical data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meraner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="333" to="346" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cloud removal for remote sensing imagery via spatial attention generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13015</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EarthNet2021: A large-scale dataset and challenge for Earth surface forecasting as a guided video prediction task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Requena-Mesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reichstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1132" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A CNN-based fusion method for feature extraction from Sentinel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scarpa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gargiulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gaetano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">236</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 10</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fusion of SAR and optical remote sensing data-challenges and recent trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tupin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5458" to="5461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Compressed sensing-based inpainting of Aqua Moderate Resolution Imaging Spectroradiometer band 6 using adaptive spectrum-weighted sparse Bayesian dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="894" to="906" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A spatiotemporal fusion based cloud removal method for remote sensing images with land cover changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aihemaiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="862" to="874" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cloud-GAN: Cloud removal for Sentinel-2 imagery using a cyclic consistent generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1772" to="1775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Leveraging crowdsourced GPS data for road extraction from aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7509" to="7518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning a single network for scale-arbitrary super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4801" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatial attentive single-image deraining with a high quality real rain dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12270" to="12279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Generative adversarial learning in YUV color space for thin cloud removal on satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1079</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 13</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-modal circulant fusion for video-to-language and backward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1029" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Motion deblurring with real events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2583" to="2592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Thin cloud removal based on signal transmission principles and spectral mixture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1659" to="1669" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Speckle noise suppression in SAR images using a three-step algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">3643</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Combined deep prior with low-rank tensor SVD for thick cloud removal in multitemporal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Residual dense network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2480" to="2495" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pluralistic image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1438" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatio-temporal filter adaptive network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2482" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep learning meets SAR: Concepts, models, pitfalls, and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Montazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bamler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="143" to="172" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Thin cloud removal for remote sensing images using a physical-model-based cyclegan with unpaired data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2021.3140033</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
