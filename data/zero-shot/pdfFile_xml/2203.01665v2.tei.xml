<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">?-DARTS: Beta-Decay Regularization for Differentiable Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ye</surname></persName>
							<email>yepeng20@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">BAIDU USA LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">?-DARTS: Beta-Decay Regularization for Differentiable Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS) has attracted increasingly more attention in recent years because of its capability to design deep neural network automatically. Among them, differential NAS approaches such as DARTS, have gained popularity for the search efficiency. However, they suffer from two main issues, the weak robustness to the performance collapse and the poor generalization ability of the searched architectures. To solve these two problems, a simple-but-efficient regularization method, termed as Beta-Decay, is proposed to regularize the DARTS-based NAS searching process. Specifically, Beta-Decay regularization can impose constraints to keep the value and variance of activated architecture parameters from too large. Furthermore, we provide in-depth theoretical analysis on how it works and why it works. Experimental results on NAS-Bench-201 show that our proposed method can help to stabilize the searching process and makes the searched network more transferable across different datasets. In addition, our search scheme shows an outstanding property of being less dependent on training time and data. Comprehensive experiments on a variety of search spaces and datasets validate the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural architecture search (NAS) has attracted lots of interests for its potential to automatize the process of architecture design. Previous reinforcement learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref> and evolutionary algorithm <ref type="bibr" target="#b24">[25]</ref> based methods usually incur massive computation overheads, which hinder their practical applications. To reduce the search cost, a variety of approaches are proposed, including performance estimation <ref type="bibr" target="#b16">[17]</ref>, network morphisms <ref type="bibr" target="#b1">[2]</ref> and one-shot architecture search <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>. In particular, one-shot methods resort to <ref type="bibr">Figure 1</ref>. Schematic illustration about (a) DARTS <ref type="bibr" target="#b20">[21]</ref> and our proposed ?-DARTS, (b) DARTS- <ref type="bibr" target="#b4">[5]</ref>. DARTS-adds an auxiliary skip connection with a decay rate ? skip to alleviate the performance collapse problem. ?-DARTS introduces the Beta-Decay regularization to improve both the robustness of the searching process and the generalization ability of the searched architecture. weight sharing technique, which only needs to train a supernet covering all candidate sub-networks once. Based on this weight sharing strategy, differentiable architecture search <ref type="bibr" target="#b20">[21]</ref> (namely DARTS, as shown in <ref type="figure">Fig. 1</ref>) relaxes the discrete operation selection problem to learn differentiable architecture parameters, which further improves the search efficiency by alternately optimizing supernet weights and architecture parameters.</p><p>Although differentiable method has the advantages of simplicity and computational efficiency, its robustness and architecture generalization challenges still needs to be fully resolved. Firstly, lots of studies have shown that DARTS frequently suffers from performance collapse, that is the searched architecture tends to accumulate parameter-free operations especially for skip connection, leading to the performance degradation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. To handle this robustness challenge, lots of instructive works are proposed: directly restricting the number of skip connections <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>; exploiting or regularizing relevant indicators such as the norm of Hessian regarding the architecture parameters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>; changing the searching and/or discretization process <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>; implicitly regularizing the learned architecture parameters <ref type="bibr" target="#b0">[1]</ref>. However, the explicit regularization of architecture parameters optimization receives little attention, as previous works (including above methods) adopt L2 or weight decay regularization by default on learnable architecture parameters (i.e., ?), without exploring solution along this direction. Secondly, several works have pointed out that the optimal architecture obtained on the specific dataset cannot guarantee its good performance on another dataset <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>, namely the architecture generalization challenge. To improve the generalization of searched model, AdaptNAS <ref type="bibr" target="#b18">[19]</ref> explicitly minimizes the generalization gap of architectures between domains via the idea of cross domain, MixSearch <ref type="bibr" target="#b21">[22]</ref> searches a generalizable architecture by mixing multiple datasets of different domains and tasks. However, both methods solve this issue by leveraging larger datasets, while how to use a single dataset to learn a generalized architecture remains challenging.</p><p>This paper is dedicated to simultaneously solve the above-mentioned two challenges in an efficient way. Inspired by the widely-used L2 <ref type="bibr" target="#b6">[7]</ref> or weight decay regularization <ref type="bibr" target="#b17">[18]</ref> approaches, we intend to design a customized regularization for DARTS-based methods, which can explicitly regularize the optimizing process of architecture parameters. However, different from the regularization on the learnable architecture parameter set, ? (before the nonlinear activation of softmax), commonly used in standard DARTS and its subsequent variants, we propose a novel and generic Beta-Decay regularization, imposing regularization on the activated architecture parameters ? (after softmax), where</p><formula xml:id="formula_0">? k = exp(? k ) |O| k =1 exp(? k )</formula><p>. On one hand, the proposed Beta-Decay regularization is very simple to implement, achieved with only additional one line of PyTorch code in DARTS (Alg 1). On the other hand, this simple implementation is grounded by in-depth theoretical support. We provide theoretical analysis to show that, Beta-decay regularization not only mitigates unfair competition advantage among operations and solve the domination problem of parameter-free operations, but also minimizes the Lipschitz constraint defined by architecture parameters and make sure the generalization ability of searched architecture. In addition, we mathematically and experimentally demonstrate that, commonly-used L2 or weight decay regularization on ? may not be effective or even counterproductive for improving robustness and generalization of DARTS.  <ref type="figure">Fig. 1</ref>. Extensive experiments on various search spaces (i.e. NAS-Bench-201, DARTS, NAS-Bench-1Shot1) and datasets (i.e. CIFAR-10, CIFAR-100, Ima-geNet) verify the effectiveness of our method. Besides, our search scheme shows the following outstanding properties:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 PyTorch Implementation in DARTS</head><p>? The search trajectories on NAS-Bench-201 and NAS-Bench-1Shot1 show that, the found architecture has continuously rising performance, and the search process can reach its optimal point at an early epoch. ? We only need to search once on the proxy dataset (i.e., CIFAR-10), but the searched architecture can obtain promising performance on various datasets (i.e., CIFAR-10, CIFAR-100 and ImageNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Robustness of DARTS</head><p>As DARTS is known to chronically suffer from the performance collapse issue caused by the domination of parameter-free operators, lots of works have dedicated to resolving it. P-DARTS <ref type="bibr" target="#b3">[4]</ref> and DARTS+ <ref type="bibr" target="#b19">[20]</ref> directly limit the number of skip connections. Such handcrafted rules are somewhat suspicious and may mistakenly reject good networks. R-DARTS <ref type="bibr" target="#b0">[1]</ref> finds that the Hessian eigenvalues can be regarded as an indicator for the collapse, and employs stronger regularization or augmentation on the training of supernet weights to reduce this value. Then SDARTS <ref type="bibr" target="#b2">[3]</ref> implicitly regularizes this indicator by adding perturbations to architecture parameters via random smoothing or adversarial attack. Both methods are indirect solutions and rely heavily on the quality of the indicator. FairDARTS <ref type="bibr" target="#b5">[6]</ref> avoids operation competition by weighting each operation via independent sigmoid function, which will be pushed to zero or one by an MSE loss. DropNAS <ref type="bibr" target="#b14">[15]</ref> proposes a grouped operation dropout for the co-adaption problem and matthew effect. DOTS <ref type="bibr" target="#b11">[12]</ref> further uses the group operation search scheme to decouple the operation and topology search. DARTS- <ref type="bibr" target="#b4">[5]</ref> factors out the optimization advantage of skip connection by adding an auxiliary one. However, these methods circumvent the domination effect of parameter-free operations by changing the searching and/or discretization process or adding extra parameters. Different from these works, we explore a more generic solution by explicitly regularizing the architecture parameters optimization, making original DARTS great again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generalization of DARTS</head><p>Improving the generalization ability of deep model has always been the focus of deep learning research. Recent works provide guarantee on model generalization by minimizing loss value and loss sharpness simultaneously <ref type="bibr" target="#b10">[11]</ref>. However, the model generalization is not only related to the network weights, but also determined by its architecture. To this end, several methods attempt to improve the generalization of architectures in the field of NAS. Adapt-NAS <ref type="bibr" target="#b18">[19]</ref> incorporates the idea of domain adaptation into the search process of DARTS, which can minimize the generalization gap of neural architectures between domains.</p><p>MixSearch <ref type="bibr" target="#b21">[22]</ref> uses a composited multi-domain multi-task dataset to search a generalizable architecture in a differentiable manner. On one hand, both above methods are built on the assumption of having multiple datasets, while our method is not built on multiple datasets. On the other hand, our focus is on regularizing architecture parameters, which is not investigated in AdaptNAS and MixSearch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation of DARTS</head><p>Following <ref type="bibr" target="#b33">[34]</ref>, DARTS searches the structure of normal cell and reduction cell to stack the full network. Typically, a cell is defined as a directed acyclic graph (DAG) with N nodes, where each node denotes a latent representation and the information between every two nodes is transformed by an edge. Each edge (i, j) contains several candidate operations, and DARTS applies continuous relaxation via the learnable architecture parameters set ? to mix the outputs of different operations, converting the discrete operation selection into a differentiable parameter optimization problem,</p><formula xml:id="formula_1">O (i,j) (x) = |O| k=1 ? (i,j) k O k (x), ? (i,j) k = exp ? (i,j) k |O| k =1 exp ? (i,j) k<label>(1)</label></formula><p>where x and? are the input and mixed output of an edge, O is the candidate operation set, and ? denotes the softmaxactivated architecture parameter set. In this way, we can perform architecture search in a differentiable manner by solving following bi-level optimization objective,</p><formula xml:id="formula_2">min ? L val (w * (?), ?) s.t. w * (?) = arg min w L train (w, ?)<label>(2)</label></formula><p>In practice, architecture parameters ? and network weights w are alternately updated on the validation and training datasets via gradient descent, and w * is approximated by one-step forward or current w [21].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Commonly-used Regularization</head><p>In this paper, we intend to improve the robustness and architecture generalization of DARTS by explicitly regularizing the optimizing process of architecture parameters. Thus, we begin with the default settings of previous methods, namely L2 or weight decay regularization on architecture parameters, ?. For convenience of analysing, we consider the single-step update of the architecture parameters,</p><formula xml:id="formula_3">? t+1 k ? ? t k ? ? ? ? ? ? k L val<label>(3)</label></formula><p>where ? ? and L val are the learning rate of architecture parameters and the corresponding loss respectively. For multistep updates, it can be transformed into a single-step update problem through step-wise recursive analysis. not affected by the amplitude of ? (to avoid invalid regularization and optimization difficulties). (2) F can reflect the relative amplitude of ? (to impose more penalty on larger amplitude). To satisfy the two requirements, we adopt the softmax to normalize ?,</p><formula xml:id="formula_4">F (? k ) = exp (? k ) |O| k =1 exp (? k )<label>(9)</label></formula><p>Then we introduce our proposed Beta-Decay regularization loss, whose gradients with respective to ? equals to F (?),</p><formula xml:id="formula_5">LBeta = log ? ? |O| k=1 e ? k ? ? = smoothmax ({? k })<label>(10)</label></formula><p>After that, substituting Eq. (9) into Eq. <ref type="formula">(8)</ref>, we can obtain following equation, which further accounts for the effect of Beta-Decay regularization,</p><formula xml:id="formula_6">? t+1 k ? t k = |O| k =1 exp ? t+1 k |O| k =1 exp exp(? t k )?exp(? t k ) |O| k =1 exp ? t k ??? exp ? t+1 k<label>(11)</label></formula><p>Observing the above formula, we can get following conclusions: (1) When ? is the largest, ? is the smallest and less than 1; when ? is the smallest, ? is the largest and greater than 1; and when ? is equal, ? = 1.</p><p>(2) In current iteration, ? decreases as ? increases. (3) ? is smaller when ? is larger, and ? is larger when ? is smaller. As a result, the variance of ? is constrained to be smaller, and the value of ? is constrained to be closer to its mean, achieving the effect similar to weight decay, thus called Beta-Decay regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Theoretical Analysis</head><p>Stronger Robustness. According to the theorem revealed by recent work <ref type="bibr" target="#b31">[32]</ref>, the convergence of network weights w can heavily rely on ? skip in the supernet. In details, supposing that there are three operations (convolution, skip connection and none) in the search space and the training loss is MSE, when fixing architecture parameters to optimize network weights via gradient descent, at one step the training  loss can be reduced by ratio (1 ? ? w ?/4) with a probability of at least 1 ? ?,where ? w is the corresponding learning rate and will be bounded by ?, and ? obeys</p><formula xml:id="formula_7">? ? h?2 i=0 ? (i,h?1) conv 2 i?1 t=0 ? (t,i) skip 2<label>(12)</label></formula><p>where h is the number of supernet layers. From Eq. <ref type="formula" target="#formula_1">(12)</ref>, we can see that ? depends more on ? skip than ? conv , which demonstrates that the supernet weights can converge much faster with large ? skip . However, by imposing Beta-Decay regularization, we can redefine Eq. (12) as follows</p><formula xml:id="formula_8">? ? h?2 i=0 ? (i,h?1) conv ? (i,h?1) conv 2 i?1 t=0 ? (i,h?1) skip ? (t,i) skip 2<label>(13)</label></formula><p>As mentioned before, ? becomes smaller when ? is larger and ? becomes larger when ? is smaller, which makes the convergence of network weights rely more on ? conv and less on ? skip . From the perspective of convergence theorem <ref type="bibr" target="#b31">[32]</ref>, the Beta-Decay regularization constrains the privilege of ? skip and ensures the fair competition among architecture parameters. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, DARTS with L2 or  weight decay regularization suffers from the performance collapse issue, while DARTS with Beta-Decay regularization has a stable search process. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, original DARTS is dominated by skip connections while ?-DARTS tends to favor parametric operators. Stronger Generalization. Referring to <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b9">[10]</ref>, Lipschitz constraint is commonly used to measure and improve the generalization ability of trained deep model. Specifically, suppose the function fitted by a deep model is f w (x) where x is the input, when x 1 ? x 2 is very small, a welltrained model should meet the following constraint.</p><formula xml:id="formula_9">f w (x 1 ) ? f w (x 2 ) ? C(w) ? x 1 ? x 2<label>(14)</label></formula><p>where C(w) is the Lipschitz constant. The smaller the constant is, the trained model will be less sensitive to input disturbances and have better generalization ability. Furthermore, we can extend this theory to differentiable architecture search. For simplicity, we consider a singlelayer neural network, and multi-layer neural network can be solved through step-wise recursive analysis. Suppose the single-layer network is mixed by the operation set F (x) = (f 1 (x), f 2 (x), f 3 (x)), with the corresponding architecture parameters ? = (? 1 , ? 2 , ? 3 ). According to the Cauchy's inequality, we can get the following inequality. <ref type="bibr" target="#b14">(15)</ref> where ? = ? 2 i can be regarded as Lipschitz constant and ? i = 1. As a result, the smaller the measure ? is, the supernet will be less sensitive to the impact of input on the operation set, and the searched architecture will have better generalization ability. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the model searched by ?-DARTS on CIFAR-10 can well generalize to the CIFAR-100 and ImageNet16 datasets and achieve ex-cellent results. As shown in <ref type="figure" target="#fig_2">Fig. 4 and Fig. 3</ref>, the architecture parameter distribution learned by ?-DARTS maintains a relative small standard deviation, making sure the generalization ability of the searched model.</p><formula xml:id="formula_10">?F T (x1) ? ?F T (x2) ? ? F T (x1) ? F T (x2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Commonly-used Regularization May Not Work</head><p>When using L2 regularization on ?, we can obtain its effect on ? according to Eq. (4) and Eq. (8), defined as</p><formula xml:id="formula_11">? t+1 k ? t+1 k = |O| k =1 exp ? t+1 k |O| k =1 exp N (? t k ) ? N (? t k ) ??? exp ? t+1 k<label>(16)</label></formula><p>Similarly, when using weight decay on ?, we can obtain its effect on ? according to Eq. (5) and Eq. (8), as follows</p><formula xml:id="formula_12">? t+1 k ? t+1 k = |O| k =1 exp ? t+1 k |O| k =1 exp ? t k ? ? t k ??? exp ? t+1 k<label>(17)</label></formula><p>From Eq. (16) and Eq. <ref type="formula" target="#formula_1">(17)</ref>, we can find that: (1) When the values in ? are all around 0, achieving the purpose of L2 and weight decay regularization, Alpha regularization has little effect on Beta; while when the values in ? are all not near 0, it means that both regularization do not work. (2) For L2 and weight decay regularization on ?, only when the median of ? is equal to 0, Alpha regularization has the same and correct effect as Beta regularization. (3) A large variance of ? is undesirable, which conflicts with the purpose of L2 and weight decay regularization, and makes the optimization process more sensitive to the hyperparameters ? and ? ? . In addition, we show the alpha statistical characteristics when searching with different regularization in <ref type="figure" target="#fig_4">Fig. 4</ref>, we can see that for L2 and weight decay regularization: <ref type="bibr" target="#b0">(1)</ref> The mean and median of ? continue to decrease and gradually move away from 0. (2) The standard deviation of ? <ref type="table">Table 1</ref>. Performance comparison on NAS-Bench-201 benchmark <ref type="bibr" target="#b8">[9]</ref>. Note that ?-DARTS only searches on CIFAR-10 dataset, but can robustly achieve new SOTA on CIFAR-10, CIFAR-100 and ImageNet16-120. Averaged on 4 independent runs of searching. increases monotonically. These mathematical and experimental results show that L2 or weight decay regularization commonly used in existing gradient-based methods are not identical to Beta regularization, and may not be effective or even counterproductive. As a comparison, with our proposed Beta-Decay regularization: (1) The mean and median of ? are basically equal. (2) When the standard deviation of ? increases to a certain extent, it will remain unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments on various search spaces (i.e. NAS-Bench-201, DARTS, NAS-Bench-1Shot1) and datasets (i.e. CIFAR-10, CIFAR-100, ImageNet) to verify the robustness and generalization of ?-DARTS, and we further give some experimental insights about DARTS' dependence on training and data. The overall process of ?-DARTS is summarized in Alg 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 ?-DARTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require:</head><p>Architecture parameters ?; Network weights w; Number of search epochs E; Regularization coefficient adjustment scheme ? e , e ? {1, 2, . . . , E}. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on NAS-Bench-201 Search Space</head><p>Settings. NAS-Bench-201 <ref type="bibr" target="#b8">[9]</ref> is the most widely used NAS benchmark analyzing various NAS methods. NAS-Bench-201 provides a DARTS-like search space, containing 4 internal nodes with 5 associated operations. The search space consists of 15,625 architectures, with the ground truth performance of CIFAR-10, CIFAR-100 and ImageNet16-120 of each architecture provided. On NAS-Bench-201, the searching settings are kept the same as DARTS on <ref type="bibr" target="#b8">[9]</ref>. Results. The comparison results are shown in <ref type="table">Table 1</ref>. We only search on CIFAR-10 and use the found genotype to query the performance of various datasets. For robustness, our 4 runs of searching under different random seeds always find the same optimal solution, which is very close to the optimal performance of NAS-Bench-201. Moreover, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the performance collapse issue is well solved and ?-DARTS has a stable search process. For generalization ability, we can see that the architecture found on CIFAR-10 achieves consistent new SOTA on CIFAR-10, CIFAR-100 and ImageNet. For dependency on training and data, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the search process reaches its optimal point at an early stage (i.e., before 20 epochs), on different datasets. Such results validate that ?-DARTS has the ability to find the optimal architecture rapidly. More interestingly, we find that the search process of different datasets reach the optimal point in different epochs, although they belong to the same run of searching on CIFAR-10. More similar results can be found in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on DARTS Search Space</head><p>Settings. Common DARTS search space <ref type="bibr" target="#b20">[21]</ref> is also popular for evaluating NAS methods. The search space consists of normal cell and reduction cell. Each cell has 4 intermediate nodes with 14 edges, and each edge is associated with 8 candidate operations. On DARTS search space, all the search settings are kept the same as DARTS since our method only introduces the simple regularization. For evaluation settings, the evaluation on CIFAR-10/100 follows DARTS <ref type="bibr" target="#b20">[21]</ref> and the evaluation on ImageNet follows P-DARTS <ref type="bibr" target="#b3">[4]</ref> and PC-DARTS <ref type="bibr" target="#b28">[29]</ref>. Results. The comparison results are shown in <ref type="table">Table 2</ref>. We search on CIFAR-10 or CIFAR-100 while evaluating the inferred architecture on CIFAR-10, CIFAR-100 and Ima-geNet. For robustness, the average results of multiple independent runs of ?-DARTS achieve the SOTA performance on both CIFAR-10 and CIFAR-100, namely 97.47?0.08% <ref type="table">Table 2</ref>. Comparison of SOTA models on CIFAR-10/100 (left) and ImageNet(right). For CIFAR-10/100, results in the top block are obtained by training the best searched model while the bottom block shows the average results of multiple runs of searching. ? denotes the results of independently searching 3 times on CIFAR-100 and evaluating on both CIFAR-10 and CIFAR-100, while ? denotes the results on CIFAR-10. Because of the difference on classifiers, the network parameters on CIFAR-100 is slightly more than that of CIFAR-10 (about 0.05M). For ImageNet, the top block denotes networks are directly searched on ImageNet (Img.), the middle block indicates architectures are searched via the idea of Cross Domain (CD.) using CIFAR-10 and part of ImageNet, models in the bottom block are transferred from the searching results of CIFAR-10 (C10) or CIFAR-100 (C100). * denotes the model is obtained on a different search space.  and 83.48?0.03%, without extra changes or any cost. For generalization ability, architectures found on CIFAR-100 can still yield a SOTA result of 97.49?0.07% on CIFAR-10, and models found on CIFAR-10 obtain a new SOTA of 83.76?0.22% on CIFAR-100, and networks found on both CIFAR-10 and CIFAR-100 datasets can achieve comparable results on ImageNet with those of directly searching on ImageNet or using cross domain method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Importance of Increased Weighting Scheme. We firstly explore the influence of different weighting schemes on ?-DARTS, including linear increased weighting scheme, constant weighting scheme and linear decay weighting scheme. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. As we can see, linear decay weighting scheme impedes the effect of regularization, constant weighting scheme is sensitive to the hyperparameter, while linear increased weighting scheme is not only effective but also insensitive to hyperparameter. Besides, combining with the results of <ref type="figure" target="#fig_0">Fig. 2</ref> that the performance on CIFAR-10, CIFAR-100, and Imagenet in single run of searching reach the optimal point in order, we conclude that linear increased regularization coefficient can further maximize the generalization ability of inferred model after the searching performance on current data is maximized, as evidenced by Eq. (11) and Eq. <ref type="bibr" target="#b14">(15)</ref>. Wide Range of The Optimal Weight. We further investigate the optimal max weight of linear increased weighting scheme. The results on CIFAR-10 of NAS-Bench-201 and search space 1 of NAS-Bench-1Shot1 are provided in <ref type="figure" target="#fig_6">Fig. 5</ref>. We can see that the best performance is achieved in a wide range of max weights, namely about <ref type="bibr">25-</ref>  <ref type="formula" target="#formula_1">(17)</ref>, we find that the normalized values of ? in Eq. (11) has the ability to make sure that the optimization process is not sensitive to the hyperparameter of ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussions</head><p>Non-uniqueness. Actually, the idea of Beta regularization is what really matters, and the way to realize it is nonunique. Here, we show two kinds of variants of Beta regu-  </p><p>In addition, by introducing a threshold, we can get the smoothmax between the threshold and each architecture parameter. We simply set the threshold as 0 in this paper, namely Beta-Zero loss.</p><p>LBeta?Zero = smoothmax 0, ? l k = ? log 1 + e ?? l k <ref type="bibr" target="#b18">(19)</ref> The results of DARTS with Beta-Global and Beta-Zero regularization loss are shown in <ref type="table">Table.</ref> 4. As we can see, both loss can promote original DARTS by a large margin, while Beta-Global loss that takes the same effect with Beta-Decay loss, can more stably obtain better results than Beta-Zero loss under different weighting schemes. Such results validate that regularizing ? is important, while the way to achieve it has a lot of room for exploration. Generality. Moreover, we utilize NAS-Bench-1Shot1 <ref type="bibr" target="#b29">[30]</ref> benchmark and SDARTS-RS <ref type="bibr" target="#b2">[3]</ref> baseline to demonstrate the generality of Beta-Decay regularization. NAS-Bench-1Shot1 contains 3 search spaces, which consist of 6,240, 29,160 and 363,648 architectures with the CIFAR-10 performance separately. On NAS-Bench-1Shot1, both the operator of each edge and the topology of the cell need to be determined. We show the search trajectory in <ref type="figure">Fig. 6</ref>. On one hand, ?-SDARTS-RS can yield much lower test/validation error than SDARTS-RS across different search spaces. On the other hand, the error of ?-SDARTS-RS keeps decreasing while the error of SDARTS-RS increases first and then decreases, validating the more stable search process of ?-SDARTS-RS. Besides, the search process of ?-SDARTS-RS also reaches its optimal point at an early stage (i.e., around 10 epochs), on different search spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we investigate the explicit regularization on the optimization of architecture parameters of DARTS in depth, which is typically ignored by previous works. Firstly, we identify that L2 or weight decay regularization on alpha commonly used by DARTS and its variants may not be effective or even counterproductive. Then, we propose a novel and generic Beta-Decay regularization loss, for improving DARTS-based methods without extra changes or cost. In addition, we theoretically and experimentally show Beta-Decay regularization can improve both the robustness and the generalization of DARTS. Besides, we find that the proposed search scheme is less dependent on training time and data. Extensive experiments on various search spaces and datasets validate the superiority of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Accuracy of different datasets of DARTS with L2, Weight Decay (WD) and Beta-Decay (BD) regularization on NAS-Bench-201 benchmark. The curve is smoothed with a coefficient of 0.5. Note that we only search once on CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>?-DARTS, total std=0.63 (a) DARTS, total std=3.1 (b) ?-DARTS, total std=0.63</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The beta distribution of normal cell learned by DARTS and ?-DARTS, on the original search space in CIFAR-10. The operator indexes 1/2/3 mean the max pool/avg pool/skip connect, while others are the parametric operators. The total std is calculated by the sum of the standard deviation of all edges under the edge independence assumption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The alpha statistical characteristics (i.e. mean, median and standard deviation) of different edges of each epoch when searching on NAS-Bench-201 benchmark with (a) L2 regularization, (b) weight decay regularization, and (c) Beta-Decay regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 : 3 : 4 :</head><label>134</label><figDesc>Construct a supernet and initialize architecture parameters ? and supernet weights w 2: For each e ? [1, E] do Update architecture parameters ? by descending ? ? L val + ? e L Beta Update network weights w by descending ? w L train 5: Derive the final architecture based on the learned ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(Figure 5 .</head><label>5</label><figDesc>a) ?-DARTS on NAS-Bench-201 (b) ?-SDARTS-RS on NAS-Bench-1Shot1 (a) ?-DARTS on NAS-Bench-201 (b) ?-SDARTS-RS on NAS-Bench-1Shot1 The effect of different max weight of linear increased weighting schemes on the searching results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 Figure 6 .</head><label>36</label><figDesc>Error of SDARTS-RS and ?-SDARTS-RS on 3 search spaces of NAS-Bench-1Shot1 [30]. The curve is smoothed with 0.5. larization loss. Recalling Eq. (10), we can naturally figure out an alternative, using the smoothmax of all architecture parameters on the entire supernet, namely Beta-Global loss.L Beta?Global = smoothmax ? 1 1 , ? ? ? , ? L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>L</head><label></label><figDesc>Beta = torch.mean(torch.logsumexp( self.model. arch parameters, dim=-1)) 2: loss = self. val loss(self.model, input valid, target valid)+?L Beta</figDesc><table><row><cell>DARTS with Beta-Decay regularization (?-DARTS) is</cell></row><row><cell>illustrated in</cell></row></table><note>1:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Influence of different weighting schemes on ?-DARTS.</figDesc><table><row><cell>Method</cell><cell>GPU (Days)</cell><cell cols="2">CIFAR-10 Params(M) Acc(%)</cell><cell cols="2">CIFAR-100 Params(M) Acc(%)</cell><cell>Method</cell><cell>GPU (Days)</cell><cell>Params (M)</cell><cell>FLOPs (M)</cell><cell>Top1 (%)</cell><cell>Top5 (%)</cell></row><row><cell>NASNet-A [33]</cell><cell>2000</cell><cell>3.3</cell><cell>97.35</cell><cell>3.3</cell><cell>83.18</cell><cell>MnasNet-92  *  (Img.) [26]</cell><cell>1667</cell><cell>4.4</cell><cell>388</cell><cell cols="2">74.8 92.0</cell></row><row><cell>DARTS(1st) [21]</cell><cell>0.4</cell><cell>3.4</cell><cell>97.00?0.14</cell><cell>3.4</cell><cell>82.46</cell><cell>FairDARTS  *  (Img.) [6]</cell><cell>3</cell><cell>4.3</cell><cell>440</cell><cell cols="2">75.6 92.6</cell></row><row><cell>DARTS(2nd) [21]</cell><cell>1</cell><cell>3.3</cell><cell>97.24?0.09</cell><cell>-</cell><cell>-</cell><cell>PC-DARTS(Img.) [29]</cell><cell>3.8</cell><cell>5.3</cell><cell>597</cell><cell cols="2">75.8 92.7</cell></row><row><cell>SNAS [28]</cell><cell>1.5</cell><cell>2.8</cell><cell>97.15?0.02</cell><cell>2.8</cell><cell>82.45</cell><cell>DOTS(Img.) [12]</cell><cell>1.3</cell><cell>5.3</cell><cell>596</cell><cell cols="2">76.0 92.8</cell></row><row><cell>GDAS [8]</cell><cell>0.2</cell><cell>3.4</cell><cell>97.07</cell><cell>3.4</cell><cell>81.62</cell><cell>DARTS- *  (Img.) [5]</cell><cell>4.5</cell><cell>4.9</cell><cell>467</cell><cell cols="2">76.2 93.0</cell></row><row><cell>P-DARTS [4]</cell><cell>0.3</cell><cell>3.4</cell><cell>97.50</cell><cell>3.6</cell><cell>82.51</cell><cell>AdaptNAS-S(CD.) [19]</cell><cell>1.8</cell><cell>5.0</cell><cell>552</cell><cell cols="2">74.7 92.2</cell></row><row><cell>PC-DARTS [29]</cell><cell>0.1</cell><cell>3.6</cell><cell>97.43?0.07</cell><cell>3.6</cell><cell>83.10</cell><cell>AdaptNAS-C(CD.) [19]</cell><cell>2.0</cell><cell>5.3</cell><cell>583</cell><cell cols="2">75.8 92.6</cell></row><row><cell>P-DARTS [4]</cell><cell>0.3</cell><cell>3.3?0.21</cell><cell>97.19?0.14</cell><cell>-</cell><cell>-</cell><cell>AmoebaNet-C(C10) [25]</cell><cell>3150</cell><cell>6.4</cell><cell>570</cell><cell cols="2">75.7 92.4</cell></row><row><cell>R-DARTS(L2) [1]</cell><cell>1.6</cell><cell>-</cell><cell>97.05?0.21</cell><cell>-</cell><cell>81.99?0.26</cell><cell>SNAS(C10) [28]</cell><cell>1.5</cell><cell>4.3</cell><cell>522</cell><cell cols="2">72.7 90.8</cell></row><row><cell>SDARTS-ADV [3]</cell><cell>1.3</cell><cell>3.3</cell><cell>97.39?0.02</cell><cell>-</cell><cell>-</cell><cell>P-DARTS(C100) [4]</cell><cell>0.3</cell><cell>5.1</cell><cell>577</cell><cell cols="2">75.3 92.5</cell></row><row><cell>DOTS [12]</cell><cell>0.3</cell><cell>3.5</cell><cell>97.51?0.06</cell><cell>4.1</cell><cell>83.52?0.13</cell><cell>SDARTS-ADV(C10) [3]</cell><cell>1.3</cell><cell>5.4</cell><cell>594</cell><cell cols="2">74.8 92.2</cell></row><row><cell>DARTS+PT [27]</cell><cell>0.8</cell><cell>3.0</cell><cell>97.39?0.08</cell><cell>-</cell><cell>-</cell><cell>DOTS(C10) [12]</cell><cell>0.3</cell><cell>5.2</cell><cell>581</cell><cell cols="2">75.7 92.6</cell></row><row><cell>DARTS-[5]</cell><cell>0.4</cell><cell>3.5?0.13</cell><cell>97.41?0.08</cell><cell>3.4</cell><cell>82.49?0.25</cell><cell>DARTS+PT(C10) [27]</cell><cell>0.8</cell><cell>4.6</cell><cell>-</cell><cell cols="2">74.5 92.0</cell></row><row><cell>?-DARTS  ?</cell><cell>0.4</cell><cell cols="4">3.78?0.08 97.49?0.07 3.83?0.08 83.48?0.03</cell><cell>?-DARTS(C100)</cell><cell>0.4</cell><cell>5.4</cell><cell>597</cell><cell cols="2">75.8 92.9</cell></row><row><cell>?-DARTS  ?</cell><cell>0.4</cell><cell cols="4">3.75?0.15 97.47?0.08 3.80?0.15 83.76?0.22</cell><cell>?-DARTS(C10)</cell><cell>0.4</cell><cell>5.5</cell><cell>609</cell><cell cols="2">76.1 93.0</cell></row><row><cell>Weighting Scheme</cell><cell cols="2">CIFAR-10 valid</cell><cell cols="2">CIFAR-10 test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0-15/25/50/100</cell><cell cols="5">91.21/91.55/91.55/91.55 93.83/94.36/94.36/94.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5/10/15/25</cell><cell cols="5">84.96/90.59/91.55/90.59 88.02/93.31/94.36/93.31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>25-15/10/5/0</cell><cell cols="5">90.59/87.30/73.58/39.77 93.31/90.65/76.88/54.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The results of different Beta regularization loss with different weighting schemes on NAS-Bench-201 benchmark. Note that we only search on CIFAR-10 dataset, and perform 2 runs of searching under different random seeds.</figDesc><table><row><cell>Methods</cell><cell>Weighting Scheme</cell><cell cols="2">CIFAR-10 valid</cell><cell>test</cell><cell cols="2">CIFAR-100 valid</cell><cell>test</cell><cell>ImageNet16-120 valid test</cell></row><row><cell>DARTS(1st) [21]</cell><cell>3.2</cell><cell>39.77?0.00</cell><cell cols="2">54.30?0.00</cell><cell>15.03?0.00</cell><cell cols="2">15.61?0.00</cell><cell>16.43?0.00</cell><cell>16.32?0.00</cell></row><row><cell>Beta-Global</cell><cell>0-25</cell><cell cols="7">91.55/91.55 94.36/94.36 73.49/73.49 73.51/73.51 46.37/46.37 46.34/46.34</cell></row><row><cell>Beta-Global</cell><cell>0-50</cell><cell cols="7">91.55/91.55 94.36/94.36 73.49/73.49 73.51/73.51 46.37/46.37 46.34/46.34</cell></row><row><cell>Beta-Global</cell><cell>0-75</cell><cell cols="7">91.55/91.55 94.36/94.36 73.49/73.49 73.51/73.51 46.37/46.37 46.34/46.34</cell></row><row><cell>Beta-Global</cell><cell>0-100</cell><cell cols="7">91.21/91.55 93.83/94.36 71.60/73.49 71.88/73.51 45.75/46.37 44.65/46.34</cell></row><row><cell>Beta-Zero</cell><cell>0-25</cell><cell cols="7">91.21/90.97 93.83/93.91 71.60/70.41 71.88/70.78 45.75/43.77 44.65/44.78</cell></row><row><cell>Beta-Zero</cell><cell>0-50</cell><cell cols="7">91.55/91.21 94.36/93.83 73.49/71.60 73.51/71.88 46.37/45.74 46.34/44.65</cell></row><row><cell>Beta-Zero</cell><cell>0-75</cell><cell cols="7">91.61/91.05 94.37/93.66 72.75/71.02 73.22/71.38 45.56/45.23 46.71/44.70</cell></row><row><cell>Beta-Zero</cell><cell>0-100</cell><cell cols="7">91.21/91.21 93.83/93.83 71.60/71.60 71.88/71.88 45.75/45.75 44.65/44.65</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">In standard DARTS and its subsequent variants, Adam optimizer with L2 regularization is commonly used for architecture parameters optimization. However, for adaptive gradient algorithms, the gradients of L2 regularization are normalized (N ) by their summed magnitudes, thus the penalty for each element is relatively even, which partly offsets the effect of L2 regularization<ref type="bibr" target="#b22">[23]</ref>, defined as? t+1 k ? ? t k ? ? ? ? ? ? k L val ? ? ? ?N ? t k(4)Considering that L2 regularization may not be effective in adaptive gradient algorithms and is not identical to weight decay regularization<ref type="bibr" target="#b22">[23]</ref>, without loss of generality, we also include architecture parameters optimization with weight decay regularization<ref type="bibr" target="#b13">[14]</ref> for comparison, defined as? t+1 k ? ? t k ? ? ? ? ? ? k L val ? ? ? ?? t k (5)3.3. Beta-Decay RegularizationSince the searching and discretization process of DARTS actually utilize softmax-activated architecture parameter set, ?, to represent the importance of each operator, we shall pay more attention to the explicit regularization on ?. As shown in Subsection 3.4, Beta regularization has the ability to improve the robustness and architecture generalization of DARTS, which further denotes its significance. Although important, Beta regularization is typically ignored by previous works. This paper is devoted to filling this gap. Similar to the idea of most regularization methods, the core purpose of Beta regularization is to constrain the value of Beta from changing too much, formulated as? t+1 k = ? t+1 k ? t k ? t+1 k(6)For simplicity, we use a ? function with ? as the independent variable to express the total influence of Beta regularization here. To realize above Beta regularization similar to weight decay through ?, we firstly study the influence of ? regularization on ?. Recalling Eq. (4) and Eq. (5), we can conclude a unified formula as:? t+1 k ? ? t k ? ? ? ? ? k L val ? ? ? ?F ? t k(7)Further, we substitute Eq.<ref type="bibr" target="#b6">(7)</ref> and Eq. (3) into Eq. (1) to get? t+1 k and ? t+1 k , and then divide the former by the latter.? t+1 k ? t+1 k = |O| k =1 exp ? t+1 k |O| k =1 exp F (? t k ) ? F (? t k ) ??? exp ? t+1 k(8)As we can see in Eq. (8), the mapping function F determines the influence of ? on ?. Thus, all we need is to look for a suitable mapping function, F . Intuitively, a satisfactory F should meet the following two points: (1) F is</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Thomas Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Path-level network transformation for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stabilizing differentiable architecture search via perturbation-based regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1554" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Darts-: robustly stepping out of performance collapse without indicators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01027</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fair darts: Eliminating unfair advantages in differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="465" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2653</idno>
		<title level="m">Mehryar Mohri, and Afshin Rostamizadeh. L2 regularization for learning kernels</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00326</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Finlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Oberman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09540</idno>
		<title level="m">Lipschitz regularized deep neural networks generalize and are adversarially robust</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01412</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dots: Decoupling operation and topology in differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Juan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Huan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Ping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="12311" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparing biases for minimal network construction with back-propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="185" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropnas: Grouped operation dropout for differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2326" to="2332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dsnas: Direct neural architecture search without parameter retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12084" to="12092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning curve prediction with bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adapting neural architectures between domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kechen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darts+</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">Improved differentiable architecture search with early stopping</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mixsearch: Searching for domain generalized medical image segmentation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13280</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08947</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04392</idno>
		<title level="m">Rethinking architecture selection in differentiable nas</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">Snas: stochastic neural architecture search</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05737</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Nas-bench-1shot1: Benchmarking and dissecting one-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10784</idno>
		<title level="m">idarts: Differentiable architecture search with stochastic implicit gradients</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16537,2020.4</idno>
		<title level="m">Theory-inspired path-regularized differential network architecture search</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
