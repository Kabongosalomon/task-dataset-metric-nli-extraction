<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Alignment-Uniformity aware Representation Learning for Zero-shot Video Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Pu</surname></persName>
							<email>shipu@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Platform Department</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaili</forename><surname>Zhao</surname></persName>
							<email>kailizhao@bupt.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Beijing University of Posts and Telecom</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Zheng</surname></persName>
							<email>moonzheng@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Platform Department</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Alignment-Uniformity aware Representation Learning for Zero-shot Video Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most methods tackle zero-shot video classification by aligning visual-semantic representations within seen classes, which limits generalization to unseen classes. To enhance model generalizability, this paper presents an endto-end framework that preserves alignment and uniformity properties for representations on both seen and unseen classes. Specifically, we formulate a supervised contrastive loss to simultaneously align visual-semantic features (i.e., alignment) and encourage the learned features to distribute uniformly (i.e., uniformity). Unlike existing methods that only consider the alignment, we propose uniformity to preserve maximal-info of existing features, which improves the probability that unobserved features fall around observed data. Further, we synthesize features of unseen classes by proposing a class generator that interpolates and extrapolates the features of seen classes. Besides, we introduce two metrics, closeness and dispersion, to quantify the two properties and serve as new measurements of model generalizability. Experiments show that our method significantly outperforms SoTA by relative improvements of 28.1% on UCF101 and 27.0% on HMDB51. Code is available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Mimicking human capability to recognize things never seen before, zero-shot video classification (ZSVC) only trains models on videos of seen classes and makes predictions on unobserved ones <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>. Correspondingly, existing ZSVC models map visual and semantic features into a unified representation, and hope the association can be generalized to unseen classes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54]</ref>. However, these methods learn associated representations within limited classes, thus facing the following two critical problems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>: (1) semantic-gap: manifolds inconsistency between visual and semantics features, and (2) * These authors contributed equally. 1 https://github.com/ShipuLoveMili/CVPR2022-AURL  <ref type="figure">Figure 1</ref>. Visual-semantic representations: Comparisons of the learned representations between the SoTA <ref type="bibr" target="#b2">[3]</ref> and our method. ? and represent visual and semantic features separately; colors are for different classes. Besides, we use two metrics to quantify feature qualities on alignment (closeness? better) and uniformity (dispersion? better). We observe that ours show better closeness within classes and more separations among semantic clusters. domain-shift: the representations learned from training sets are biased when applied to the target sets due to disjoint classes between two groups. In ZSVC, these two problems cause side effects on model generalizability.</p><p>Reviewing the literature, we observe that most methods focus on tackling the semantic-gap by learning alignmentaware representations, which ensure visual and semantic features of the same class close. To improve the alignment, MSE loss <ref type="bibr" target="#b2">[3]</ref>, ranking loss <ref type="bibr" target="#b13">[14]</ref>, and center loss <ref type="bibr" target="#b12">[13]</ref> are commonly used to optimize the similarity between visual and semantic features. Apart from the loss, improvements for alignment are attributed mainly to the designs of architectures. For instance, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref> first project global visual features to local object attributes, then optimize similarity between the attributes and final semantics. In contrast, URL <ref type="bibr" target="#b53">[54]</ref>, Action2Vec <ref type="bibr" target="#b13">[14]</ref>, and TARN <ref type="bibr" target="#b1">[2]</ref> directly align visual and final semantic features, which are improved via attention modules. Since video features are hard to learn, the above methods utilize pre-trained models to extract visual features. The recent model <ref type="bibr" target="#b2">[3]</ref> benefits from the efficient R(2+1)D module <ref type="bibr" target="#b42">[43]</ref> in video classification and achieves the state-of-the-art (SoTA) results in ZSVC. However, the SoTA <ref type="bibr" target="#b2">[3]</ref> neglects to learn semantic features; thus, it is still not a true end-to-end (e2e) framework for visual-semantic feature learning. We claim that e2e is critical for alignment since fixed visual/semantic features will bring obstacles to adjusting one to approach another.</p><p>Noteworthily, the latest MUFI <ref type="bibr" target="#b34">[35]</ref> and ER <ref type="bibr" target="#b5">[6]</ref> get down to addressing the domain-shift problem by involving more semantic information, thus consuming extra resources. In particular, MUFI <ref type="bibr" target="#b34">[35]</ref> augments semantics by training multi-stream models on multiple datasets. ER <ref type="bibr" target="#b5">[6]</ref> expands class names by annotating amount of augmented words crawled from the website. Freeing complex models or additional annotations, we will design a compact model that preserves maximal semantic info of existing classes while synthesizing features of unseen classes.</p><p>To tackle the two problems with one stone, we present an end-to-end framework that jointly preserves alignment and uniformity properties for representations on both seen and unseen classes. Here, alignment ensures closeness of visual-semantic features; uniformity encourages the features to distribute uniformly (maximal-info preserving), which improves the possibility that unseen features stand around seen features, mitigating the domain-shift implicitly. Specifically, we formulate a supervised contrastive loss as a combination of two separate terms: one regularizes alignment of features within classes, and the other guides uniformity between semantic clusters. To alleviate the domainshift explicitly, we generate new features of unseen synthetic classes by our class generator that interpolates and extrapolates features of seen classes. In addition, we introduce closeness and dispersion scores to quantify the two properties and provide new measurements of model generalizability. <ref type="figure">Fig. 1</ref> illustrates the representations of our method and the SoTA alternative <ref type="bibr" target="#b2">[3]</ref>. We train the two models on ten classes sampled from Kinetics-700 <ref type="bibr" target="#b4">[5]</ref> and map features on 3D hyperspheres. We observe that our representation shows better closeness within classes and preserves more dispersion between semantic clusters. Experiments validate that our method significantly outperforms SoTA by relative improvements of 28.1% on UCF101 and 27.0% on HMDB51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised video classification (SVC): SVC tackles general classes initially (e.g., YouTube-8M dataset <ref type="bibr" target="#b0">[1]</ref>), then specific to action recognition recently (e.g., large-scale Kinetics-700 dataset <ref type="bibr" target="#b4">[5]</ref>). Learning temporal features is the main task of SVC. In the beginning, video features are generated via NetVLAD <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref> that fuses static features of multiple frames. Then, temporal/motion features of videos are optimized directly. We categorize the methods into twostream 2D-CNN and 3D-CNN based. Two-stream models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref> extract spatial and temporal features by performing separate 2D-CNN modules. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref> extracts motion features by computing 2D-CNN features' difference between neighboring frames. Furthermore, <ref type="bibr" target="#b41">[42]</ref> proposes C3D to fuse spatial and temporal features via an independent 3D-CNN module. Even C3D helps achieve promising results <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41]</ref>, its large parameters bring burdens to model optimization. Instead, I3D <ref type="bibr" target="#b4">[5]</ref> and P3D <ref type="bibr" target="#b33">[34]</ref> design 3D-CNN-like modules by combining 1D temporal and 2D spatial filters. Recently, a more efficient R(2+1)D module <ref type="bibr" target="#b42">[43]</ref> has been widely used, which includes a pseudo-3D kernel (2D spatial + 1D temporal) in residual networks. In this paper, we apply our model in action recognition and perform R(2+1)D for better spatial-temporal feature extraction.</p><p>Zero-shot video classification (ZSVC): Existing ZSVC methods align visual and semantic features on a unified representation and hope the alignment can be generalized to unseen classes. Most methods design various frameworks to optimize the alignment. Similar to zero-shot image classification <ref type="bibr" target="#b48">[49]</ref>, some methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> learn video attributes first, then design stage-wise framework. Given input videos, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref> learn classifiers for video attributes, then compare the predicted attributes and final class names. However, they cost intensive annotations of video attributes. Instead, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref> utilize pre-trained object detectors to determine object-level class names, then compute similarities between object-level and final class names. Recent work directly computes the similarity between visual and semantic features, and their contributions focus on enhancing visual features. URL <ref type="bibr" target="#b53">[54]</ref>, TARN <ref type="bibr" target="#b1">[2]</ref> and Action2Vec <ref type="bibr" target="#b13">[14]</ref> extract spatial-temporal features using a pre-trained C3D and then improve the features via attention modules. The latest model <ref type="bibr" target="#b2">[3]</ref> learns visual embeddings by an efficient R(2+1)D module and achieves SoTA results. However, the above methods are not true end-to-end (e2e) models because those utilize Word2vec <ref type="bibr" target="#b29">[30]</ref> to extract semantic features. We will justify that lacking e2e learning weakens the alignment since fixed visual/semantic features will bring obstacles to adjusting one to approach another. Except for the above designs, MSE loss <ref type="bibr" target="#b2">[3]</ref>, ranking loss <ref type="bibr" target="#b13">[14]</ref>, and center loss <ref type="bibr" target="#b12">[13]</ref> are commonly used to regularize the alignment of features. In this paper, we propose a true e2e framework and formulate a supervised contrastive loss, which first considers both alignment and uniformity properties in ZSVC.</p><p>Representation learning: In self-supervised and zeroshot learning, representation learning learns features of observed data, which can extract helpful info when applied to downstream tasks. In self-supervised learning, given pairs of positive and negative images/videos, contrastive learning regularizes representations where positives stand close while negatives keep apart. The pioneering work, SimCLR <ref type="bibr" target="#b6">[7]</ref> utilizes data augmentation to generate positive instances and maintains a large batch for choosing relatively enough negatives. <ref type="bibr" target="#b32">[33]</ref> applies SimCLR to the video domain. To save memory, MoCo <ref type="bibr" target="#b14">[15]</ref> presents momentum update to cache a large number of negative instances, then <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref>   <ref type="figure">Figure 2</ref>. Architecture of AURL: From left to right, we map a video sequence I and the class name set Y to a unified representation (fv(g(I)), fs(c(Y))). During training, to learn representations of seen classes, we introduce LS to preserve alignment and uniformity properties. For synthetic unseen classes, we introduce LUS to learn the two properties on synthetic visual-semantic features (?, Z). To synthesize features of unseen classes, we first utilize LC to learn visual centers W , then propose Class Generator to transform W and existing semantics fs(c(Y)) into the representation (?, Z). During inference, we perform an NNS strategy to obtain the final class.</p><formula xml:id="formula_0">L S L C I Y g c c(Y) g(I) f v f s f v (g(I)) f s (c(Y))</formula><p>tends MoCo to video understanding. <ref type="bibr" target="#b52">[53]</ref> introduces feature transformation on existing samples to obtain broader and diversified positives and negatives, thus enhancing discrimination. However, the above models learn instance pairs from the same domain, e.g., images or videos. Instead, CLIP <ref type="bibr" target="#b35">[36]</ref> exploits features of two domains (i.e., images and texts) guided by a contrastive loss. Motivated by CLIP, the latest models in ZSVC, MUFI <ref type="bibr" target="#b34">[35]</ref> and ER <ref type="bibr" target="#b5">[6]</ref>, extend the self-supervised contrastive loss to a fully-supervised loss that contrasts visual and semantic features. However, MUFI and ER neglect the difference and similarities of the selfsupervised and supervised contrastive losses. Considering alignment and uniformity properties, we build connections between the two losses and analyze the advantages of the supervised loss. Besides, MUFI and ER both require extra resources to improve model generalizability. We propose a compact model using a class generator to explicitly synthesize new features of synthetic unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Alignment-Uniformity aware Representation Learning (AURL)</head><p>This section describes Alignment-Uniformity aware Representation Learning (AURL) involving a unified architecture, loss functions, class generator, and two novel metrics, followed by its training and inference strategy, and then discusses similarities and differences against alternatives. <ref type="figure">Fig. 2</ref> shows the AURL architecture. Given complete K class names Y = {y 1 , . . . , y K }, and an input video I of class y i ? Y (e.g., playing basketball), we by end-to-end learn visual and semantic embeddings. We introduce R(2+1)D <ref type="bibr" target="#b42">[43]</ref> as the backbone to generate visual features g(I), and utilize a video projector f v to implement 3-layer MLP projection (2 fc+bn+ReLU and 1 fc+bn), thus obtain visual embeddings f v (g(I)) ? R d . Parallelly, we perform Word2vec c <ref type="bibr" target="#b29">[30]</ref> to extract the initial word embeddings c(Y), then learn semantic embeddings f s (c(Y)) ? R K?d by a word projector f s that has one fc (#node=512) and the 3-layer MLP projection. For convenience, we note f v (g(I)) and f s (c(y i )) of i-th class as v yi and s yi for the below discussions. Compared with SoTA methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref> that only learn video parts, our AURL end-to-end trains the backbone, video and word projectors, providing more feature flexibility under the regularization of loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Alignment-uniformity aware Loss</head><p>Can alignment and uniformity properties be preserved in supervised contrastive loss? For self-supervised learning, <ref type="bibr" target="#b7">[8]</ref> claims that contrastive loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref> (see Eq. 1 and supplementary) preserves alignment and uniformity properties. Alignment indicates that positive samples should be mapped to nearby features and thus be invariant to unneeded noises. Uniformity <ref type="bibr" target="#b46">[47]</ref> means feature vectors should be roughly uniformly distributed on the unit hypersphere, thus bringing better generalization to downstream tasks.</p><formula xml:id="formula_1">L self =?log[ exp [?sim(f, f + )] f??N exp [?sim(f, f ? )] ].<label>(1)</label></formula><p>Here, (f, f + ), (f, f ? ) are positive and negative pairs of images/videos, N is negative set; sim means a similarity function (thus in [-1, +1]); and ? is a temperature parameter.</p><p>Closeness in alignment and maximal-info preserving in uniformity are also essential properties of the unified representation learning in ZSVC. However, existing work mainly focuses on the alignment of visual-semantic features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b53">54]</ref>, the uniformity that improves generalization has not been discussed yet. Here, by leveraging video labels, we formulate a supervised contrastive loss as the combination of alignment and uniformity terms:</p><formula xml:id="formula_2">L sup =?log[ exp [?sim(v yi , s yi )] yj ?Y exp [?sim(v yi , s yj )] ],<label>(2)</label></formula><formula xml:id="formula_3">=?SP ? [?sim(v yi , s yi ) alignment + 1 ? LSE(?sim(v yi , s yj ) yj ?Y\yi ) uniformity ],</formula><p>where, SP ? (x) = 1 ? log(1+exp(?x)),</p><formula xml:id="formula_4">LSE(x) = log( x?X exp(x))</formula><p>.</p><p>v yi and s yi are visual and semantic features of class y i , and the complete class set is Y. SP ? means the Soft-Plus function and LSE is LogSumExp. Since Eq. 2 favors sim(v yi , s yi ) larger, visual and semantic features of the same class will be aligned. The uniformity term tends to maximize the distances between features of different classes using a LogSumExp function, thus spreading features as much as possible. To sum up, our L sup preserves the alignment and uniformity properties simultaneously. L sup performs better: Comparing L sup with L self , we observe that L sup includes positive pair sim(v yi , s yi ) in the denominator. Even recent work MUFI <ref type="bibr" target="#b34">[35]</ref> and ER <ref type="bibr" target="#b5">[6]</ref> also utilize supervised contrastive loss, not only do they neglect the alignment and uniformity properties, but also miss the similarity and difference between the two losses. Here, we show that L sup maintains advantages of both L self and triplet loss <ref type="bibr" target="#b36">[37]</ref>. We derive upper bounds of L sup and L self as follows (the full derivation in supplementary):</p><formula xml:id="formula_5">L self ? ?(sim max ?sim(v yi , s yi ) + log(K ?1) ? ),<label>(3)</label></formula><formula xml:id="formula_6">L sup ? ? max[sim max ?sim(v yi , s yi )+ log(K ?1) ? , 0]+log(2),</formula><p>where K is the number of classes, sim max is the maximal similarity among all negative pairs (sim max = max yj ?Y\y sim(v yi , s yj )). For a fair comparison, we also reformulate L self with class labels and obtain its upper bound in Eq. 3. With the upper bounds, we summarize the advantages of L sup as follows: 1. When log(K?1) ? ? 2, the two upper bounds will be similar. At this time, L sup performs as well as L self in a representation learning task. 2. When 0 ? log(K?1) ? &lt;2, the upper bound of L sup has a similar form as triplet loss <ref type="bibr" target="#b36">[37]</ref> that facilitates intrinsic ability to perform hard positive/negative mining.</p><p>3. L sup preserves the summation over all negatives in the denominator, thus improving discrimination among classes <ref type="bibr" target="#b38">[39]</ref>, which has the same motivation with contrastive learning that makes the embedding distribution uniform by increasing the number of negatives <ref type="bibr" target="#b6">[7]</ref>. In this paper, we take advantage of L sup to regularize the representations of both seen and synthetic unseen classes.</p><p>L sup for seen and unseen classes: We learn L sup for both seen and unseen classes (see L contrast in Eq. 4 where we utilize cos as a cosine function and map features on the hypersphere.). Specifically, we learn L S on visual-semantic features (i.e., v yi and s yi ? R 1?d , y i ? Y) for seen classes set Y. From the formulation of L S , we jointly align features of the same class and introduce uniformity that encourages semantic clusters to spread as much as possible, improving the possibility that features of unseen classes fall around existing ones. To offer effective positive/negative visual and semantic pairs that enhance the feature embedding <ref type="bibr" target="#b52">[53]</ref>, we propose a class generator to generate visual and semantic features of synthetic classes U, which are considered as "unseen classes" in comparison with seen classes Y. To retain the alignment-uniformity properties, we utilize L US to regularize visual and semantic features of K u unseen classes (i.e., the synthetic features ? and Z ? R Ku?d ).</p><formula xml:id="formula_7">L contrast =L S + L US (4) =?log[ exp [?cos(v yi , s yi )] yj ?Y exp [?cos(v yi , s yj )] ]+ 1 K u ui?U ?log[ exp [?cos(? ui , Z ui )] uj ?U exp [?cos(? ui , Z uj )]</formula><p>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Class Generator</head><p>To synthesize visual and semantic pairs (?, Z), we propose a class generator that applies a uniformly sampled linear transformation to all pairs of visual/semantic features of seen classes. Especially, instead of using single visual feature as the transformed features, we select representative "visual centers" learned from a supervised video classifier that interprets the parameter matrix of fc layer as the centers, as commonly used in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. We propose to exploit L C as the classification loss, which is an angular softmax loss, helping push all visual features towards their visual centers on the unit hyperspher <ref type="bibr" target="#b47">[48]</ref>.</p><formula xml:id="formula_8">L C = ? log exp[? cos(v yi , w yi )] yj ?Y exp[? cos(v yi , w yj )] .<label>(5)</label></formula><p>Here, v yi and w yi indicate a single visual feature and the learned visual center, respectively. With the visual centers w yi and the corresponding semantic features s yi , we interand extra-polates (i.e., linearly combine) these features to fill in incomplete class points on the hypersphere:</p><formula xml:id="formula_9">? = M ? norm(W ),<label>(6)</label></formula><formula xml:id="formula_10">Z = M ? norm(s Y ), norm(W ) = [ w y1 w y1 , . . . , w y D w y D ], norm(s Y ) = [ s y1 s y1 , . . . , s y D s y D ].</formula><p>Here, ? and Z ? R Ku?d separately represent the synthetic visual centers and semantic features of unseen classes; K u represents the number of unseen classes and d is the feature dimension. Besides, we apply normalization on W and s Y (both are D ? d matrix) because learning on a unit hypersphere helps model optimization <ref type="bibr" target="#b46">[47]</ref>; D is a hyper-parameter that means how many classes are sampled for unseen-class generator. The matrix M ? R Ku?D is used for inter-and extra-polations, whose elements are randomly sampled from a uniform distribution U (?, 1), and ?1 ? ?&lt;1. ? is another hyper-parameter that controls the distributed range of the synthetic points. It is worth noting that the settings of hyper-parameters D and ? are non-trivial. For D, we prefer D ? d, i.e., the number of seen classes should be larger than the dimension of a hypersphere. Because for a full rank matrix W , a linear combination of the column vector of W can express any vector on the transformed space. We aim to generate as diverse unseen classes as possible to improve the possibility that the synthesized points can cover the classes in the test set. Thus, in experiments, we will select features of all seen classes for feature transformation. For ?, we choose positive values for interpolation where the synthetic clusters locate inside of seen points (see <ref type="figure" target="#fig_1">Fig. 3(b)</ref>), and gradually enlarge the cluster regions by decreasing ? where the negative value is for extrapolation. <ref type="figure" target="#fig_1">Fig. 3 (a)</ref> illustrates our Class Generator with D = 10, d = 3, ? = ?1 on the Kinetics-700 dataset <ref type="bibr" target="#b17">[18]</ref>. We can see our transformation not only provides unseen classes but also approaches test classes (e.g., UCF101 dataset <ref type="bibr" target="#b39">[40]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Closeness and Dispersion</head><p>To quantify the alignment and uniformity, we introduce two metrics: closeness and dispersion. Closeness measures the mean distance of features within the same class, reflecting the alignment of visual and semantic features.</p><formula xml:id="formula_11">Closeness = 1 K yi?Y [ 1 N yi Ny i n=1</formula><p>(1 ? cos (v n yi , s n yi ))], <ref type="bibr" target="#b6">(7)</ref> where, N yi is the # of training videos of class y i . Besides, to evaluate the uniformity/separation of semantic clusters, we adopt minimal distances among all clusters to compute dispersion. Here, we consider all visual features within the same class as a semantic cluster instead of using one single semantic vector s yi . For example,v yi is the mean of visual features of the class y i , and indicates one semantic cluster.</p><formula xml:id="formula_12">Dispersion = 1 K yi?Y min y k ?Y\yi (1 ? cos(v yi ,v y k )). (8)</formula><p>The experiments in Sec. 4.2 show that models tested with higher accuracy preserve the lower closeness and higher dispersion in representations. We conclude our two metrics can serve as new measurements of model generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training &amp; Inference</head><p>Training: We end-to-end train visual and semantic features and jointly learn the contrastive loss L contrast and classification loss L C , thus obtaining the following overall loss:</p><formula xml:id="formula_13">L AURL =L S + L US + L C .<label>(9)</label></formula><p>We will justify our end-to-end training is critical for alignment and uniformity properties, and validate our compact model with L AURL outperforms SoTA alternatives. Inference: we train AURL on source dataset I with K seen classes Y = {y 1 , . . . , y K }, and evaluate the model on target dataset I t with T unseen classes Y t = {y t 1 , . . . , y t T }. In this paper, we follow the strict problem setting in <ref type="bibr" target="#b2">[3]</ref>, which requires training classes Y have no overlap with test classes Y t . Mathematically, we re-write the requirement as:</p><formula xml:id="formula_14">?y ? Y, min y t ?Y t (1 ? cos(c y , c y t )) &gt; ?,<label>(10)</label></formula><p>where c i means Word2vec features of class i, ? is the distance threshold. We utilize the Nearest Neighbor Search (NNS) strategy to obtain the final label of query video I t :</p><formula xml:id="formula_15">argmax y t ?Y t cos(f v (g(I t )), s y t ).<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Comparisons with Related Work</head><p>The closest studies to our AURL are SoTA <ref type="bibr" target="#b2">[3]</ref>, MUFI <ref type="bibr" target="#b34">[35]</ref>, and ER <ref type="bibr" target="#b5">[6]</ref>. <ref type="table" target="#tab_1">Table 1</ref> summarizes their similarities and differences. SoTA only utilizes MSE loss to regularize feature alignment within seen classes, limiting model generalizability. MUFI and ER both implicitly increase semantic info to improve the generalization. MUFI trains multi-stream models across multiple datasets. ER crawls and annotates a number of web words to expand existing class names. Unlike that MUFI and ER both require extra resources, our AURL is a compact model to utilize the uniformity that helps preserve maximal info of existing features, and introduce a class generator to synthesize more semantics explicitly. Even MUFI and ER adopt the supervised contrastive loss, they neglect how alignment and uniformity properties affect ZSVC. Besides, our AURL follows the strict label requirement (in Eq. 10) that classes of training and test sets are far away from each other, which manifests the nature of ZSVC. At last, compared with ER that only trains the last fc layers, AURL utilizes a true e2e training strategy that is critical to realize the two properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Datasets: We train our AURL on the Kinetics-700 dataset <ref type="bibr" target="#b17">[18]</ref> and evaluate it on UCF101 <ref type="bibr" target="#b39">[40]</ref> and HMDB51 <ref type="bibr" target="#b20">[21]</ref> datasets. The Kinetics-700 provides download links of YouTube videos annotated with 700 categories of human actions. We collect 555,774 videos using these links. The UCF101 contains 13,320 videos with 101 actions and the HMDB51 has 6,767 videos annotated with 51 actions.</p><p>Training protocol: For fair comparisons with the SoTA <ref type="bibr" target="#b2">[3]</ref>, we select the training videos in Kinetics-700 whose classes have non-overlap with UCF101 and HMDB51 as described in Eq. 10, and set the same ? = 0.05, thus obtain-ing 662 classes. AURL is inductive zero-shot learning, thus does not include any test data during training.</p><p>Evaluation protocol: Existing ZSVC methods adopt various evaluation protocols to report experimental results. For complete comparisons, we perform three protocols: 1, 3, and N test splits. 1 test split reports an accuracy on all videos of UCF101 or HMDB51 set. 3 test splits reports an average accuracy by averaging 3 accuracies that are separately evaluated on 3 test sets provided by the UCF101 or HMDB51. N test splits also reports the average by averaging N accuracies that are obtained by running N (10 in our method) times testing, in each, m classes are randomly selected (m=50 for UCF101 and m=25 for HMDB51).</p><p>Implementation details: We adopt one or multiple video clips as one input video of models. We follow the same SoTA settings <ref type="bibr" target="#b2">[3]</ref> (i.e., 1 or 25 video clips and 16 frames/clip with size of 1?16?112?112?3) for fair comparisons. If multiple video clips are used, we take the mean of multiple visual embeddings as the representative embedding. Besides, if a class name contains multiple words, we average the corresponding Word2vec features to represent the class prototype. For the AURL architecture, we set feature dimension of the R(2+1)D backbone as 512 (i.e., g(I) ? R 512 ) and dimension of Word2vec as 300 (i.e., c(Y) ? R K?300 ), and set the number of nodes in 3-layer MLP of the projector as 2048, 2048, and 2048 separately. During training, we empirically set K u as 662, ? as 10, D as 662, and ? as 0. We deploy the training on 8 Nvidia Tesla V100 GPUs. We set batch size as 256 and synchronize all batch normalization across GPUs following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>. We implement experiments using PyTorch and Horovod. SGD is our optimizer and a learning rate of 0.05 with a cosine decay schedule <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref> is adopted. Then, we set the weight decay as 0.0001 and the SGD momentum as 0.9. The number of training iterations is 58,500 which takes 45 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>To analyze AURL, we performed extensive ablations that were trained on the Kinetics-700 and evaluated on UCF101 and HMDB51 using 1 video clip and 1 test split of evaluation protocols. <ref type="table" target="#tab_2">Table 2</ref> summarizes the quantitative results. <ref type="figure" target="#fig_3">Fig. 4</ref> visualizes the visual-semantic representation of ablations by sampling 10 classes from Kinetics-700 dataset and setting the features as 3-D for better visualization. We will justify: (1) our model that preserves alignment-uniformity properties performs better than the SoTA method <ref type="bibr" target="#b2">[3]</ref> that focuses on alignment only; (2) end-to-end (e2e) training is critical to realize the two properties; (3) our AURL involving the class generator performs the best. From the justifications, we will show that our closeness and dispersion metrics can serve as new measurements of model generalizability. Here, we take the architecture of the SoTA <ref type="bibr" target="#b2">[3]</ref> as our Base model (i.e., base backbone + fc only for video parts).  Per (1), we compare Base w/ MSE (i.e., the SoTA) and Base w/ L S . From the accuracy in <ref type="table" target="#tab_2">Table 2</ref>, Base w/ L S largely improves the results by (14.8%, 35.1?40.3) on UCF101 and by (13.6%, 21.3?24.2) on HMDB. Comparing the learned representation in <ref type="figure" target="#fig_3">Fig. 4</ref> (a) w/ MSE and (b) w/ L S , we observe the semantic clusters of (b) spread more than (a), but the alignment within classes gets worse, for example, visual and semantic features are not calibrated for classes "skipping rope" and "abseiling". Similarly, we find the same trend in closeness and dispersion metrics shown in <ref type="figure" target="#fig_3">Fig. 4</ref> and <ref type="table" target="#tab_2">Table 2</ref>, where closeness gets worse (0.029?0.065, 0.30?0.45) but dispersion becomes much better (0.330?0.414, 0.09?0.29). We can see our Base w/ L S presesrving higher uniformity in the trained representation can achieve better generalization when making inference on the test set, even scarify a little alignment. Per (2), we involve e2e training strategy (i.e., base backbone + video projector for video parts; word projector for semantic parts) to the Base w/ L S , and get the Base w/ L S + e2e, which further improves the accuracy from 40.3 to 43.2 on UCF-101 and from 24.2 to 26.2 on HMDB. Not surprisingly, we observe the alignment is tuned better and uniformity is maintained in good quality, thus obtaining a better trade-off. Referring to <ref type="figure" target="#fig_3">Fig. 4 (b)</ref> and (c), we see Base w/ L S + e2e encourages better uniformity that semantic clusters are relatively distributed uniformly across the hypersphere while achieves a satisfying alignment that visual and semantic features are apparently aligned (see classes "skipping rope" and "abseiling" again for comparisons). The similar trends also occur in closeness and dispersion metrics, i.e., (0.024 and 0.340; 0.30 and 0.29) in <ref type="figure" target="#fig_3">Fig. 4</ref> and <ref type="table" target="#tab_2">Table 2</ref>. We conclude that e2e is critical for adjusting features to meet the regularizations of alignment and uniformity. Per (3), we apply L US to unseen classes coupling with the class generator (CG), i.e., our AURL. Compared AURL with Base w/ L S + e2e, AURL steadily improves 2.8% on UCF and 4.6% on HMDB, achieving the best accuracy. Quantitatively, closeness and dispersion reach the best scores, such as (0.29, 0.32) in <ref type="table" target="#tab_2">Table 2</ref> and (0.020, 0.354) in <ref type="figure" target="#fig_3">Fig. 4</ref>. From the representation of <ref type="figure" target="#fig_3">Fig. 4 (d)</ref>, we see the semantic clusters cover most regions of the hypersphere, which improves the possibility that unseen features fall around existing points, thus bringing a better generalization. Furthermore, we remove CG from AURL (i.e., AURL w/o CG) to validate the effectiveness of the class generator.</p><p>Comparing AURL and AURL w/o CG, we find that the performances of AURL w/o CG on UCF and HMDB both decrease, and the accuracy on HMDB even degrades lower than Base w/ L S + e2e. Thus, we conclude the CG is a critical module to enhance the generalization. Last but not least, from the above justifications, we summarize that our models consistently improve the accuracy by involving the proposed modules; closeness/dispersion measured on the learned representations have agreements with the accuracy evaluated on test sets, providing model evaluations even prior to making inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with the Closest SoTA</head><p>The closest SoTA to our AURL is the recent work <ref type="bibr" target="#b2">[3]</ref>, which utilizes a compact model that achieves the SoTA results even under a strict setting (i.e., Eq. 10). <ref type="table" target="#tab_1">Table 1</ref> summarizes the similarity and difference between the SoTA and our AURL. <ref type="table" target="#tab_3">Table 3</ref> shows the comprehensive comparisons quantitatively. We reported the SoTA results using the same settings and the authors' released code. For comprehensive comparisons, we include various evaluation protocols including Pre-training, Video clips, and Test splits. Pretraining means that SoTA fine-tunes the pre-trained models on the SUN dataset <ref type="bibr" target="#b49">[50]</ref>. From the comparisons, we see our  <ref type="table" target="#tab_4">Table 4</ref> shows the comparisons with the alternatives. The SoTA <ref type="bibr" target="#b2">[3]</ref> and our AURL with mean the two methods follow the strict label requirement in Eq.10. From the results, we observe that our AURL surpasses all the alternatives in various challenging situations. Specifically, we summarize the challenges: first, fewer test splits are harder testing situations, e.g., for SAOE <ref type="bibr" target="#b26">[27]</ref>, 3 vs. 10 splits corresponds to 32.8 vs. 40.4 on UCF; second, strict label requirement ( ) serves more difficult situation, e.g., our AURL w/ 10 (the more) test splits achieves even worse results than AURL w/ 3 splits; third, some methods acquire extra training datasets (e.g., Kinetics + extra 5 datasets trained in MUFI <ref type="bibr" target="#b34">[35]</ref>), additional semantic classes (e.g., web words used in ER <ref type="bibr" target="#b5">[6]</ref> ), and even training videos sampled from the same domain as the test set (e.g., tr/te are both UCF or HMDB in TARN <ref type="bibr" target="#b1">[2]</ref>, Act2Vec <ref type="bibr" target="#b13">[14]</ref>, PSGNN <ref type="bibr" target="#b12">[13]</ref>, and ER <ref type="bibr" target="#b5">[6]</ref>), which provide more difficulties to be competed against for other methods. Correspondingly, we find the superiority of our AURL as below: (1) AURL w/ 1 (the fewest) test split outperforms most methods w/ 3 or 50 splits, e.g.,  Finally, we conduct AURL with pre-extracted features (trained only on video and word projectors). We observe AURL w/ pre-extracted features achieves comparable performance with the e2e AURL -(59.5, 38.2) vs. (58.0, 39.0) on UCF and HMDB. This suggests AURL can achieve high performance without carefully finetuning video features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with the Alternatives</head><p>Limitations and possible solutions. Even our AURL achieves promising results, there are still two problems to be concerned. (1) Uniformity of visual features within classes could be included to further increase info-preserving, introducing contrastive learning between videos may be a possible solution. (2) It will be helpful to study how the class generator affects the overall optimization during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper learns representation awareness of both alignment and uniformity properties for seen and unseen classes. We reformulate a supervised contrastive loss to jointly align visual-semantic features and encourage semantic clusters to distribute uniformly. To explicitly synthesize features of unseen semantics, we propose a class generator that performs feature transformation on features of seen classes. Besides, we introduce closeness and dispersion to quantify the two properties, providing new measurements for generalization. Extensive ablations justify the effectiveness of each module in our model. Comparisons with the SoTA alternatives validate our model reaches the new SoTA results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of Class Generator: (a) Synthetic classes ? are generated by linearly combining features of seen classes ( with colors); means test classes. (b) Feature transformation w/ different ? synthesizes semantics covering various size of regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Ablations: The representations of ablations w/ MSE loss, our LS, e2e training, and the AURL. ? and represent visual and semantic features separately; colors are for different classes. Here, we randomly sample 10 classes from Kinetics-700 for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(46.8, 31.7) of AURL vs. (36.3, -) of OPCL and (43.0, 32.6) of PSGNN on (UCF, HMDB) dataset; (2) AURL w/ more strict requirements but w/o extra datasets competes against all the SoTA alternatives, e.g., (58.0, 39.0) of AURL vs. (51.8, 35.3) of ER, and (56.3, 31.0) of MUFI on (UCF, HMDB) dataset. To sum up, our AURL reaches the new SoTA in ZSVC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>ex- abseiling acting in play ? playing basketball ? yoga zumba K class names Video sequence</head><label></label><figDesc></figDesc><table><row><cell>R(2+1)D Backbone</cell><cell>Video Projector</cell><cell>Visual Centers</cell><cell>Class Generator</cell></row><row><cell></cell><cell></cell><cell>NNS</cell><cell>L US</cell></row><row><cell>Word2vec</cell><cell>Word Projector</cell><cell>Representations for</cell><cell>Class Generator Representations for</cell></row><row><cell></cell><cell></cell><cell>seen classes</cell><cell>unseen classes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons between AURL and alternative methods. AUL: alignment-uniformity learning, ERF: extra resources free, UCG: unseen class generator, SLR: strict label requirement.</figDesc><table><row><cell>Methods</cell><cell>ET</cell><cell>AUL</cell><cell>ERF</cell><cell>UCG</cell><cell>SLR</cell></row><row><cell>SoTA [3]</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell></row><row><cell>MUFI [35]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>ER [6]</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>AURL (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">*ET: end-to-end trainable,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablations of our modules using 1 video clip under the 1 test split protocol (? =0.05). Red numbers indicate the best. Closeness? better, dispersion? better, and top-1 accuracy ? better.</figDesc><table><row><cell>Method</cell><cell>L S e2e</cell><cell>L US +CG</cell><cell>Clo-se.</cell><cell>Dis-per.</cell><cell>UCF top-1</cell><cell>HMDB top-1</cell></row><row><cell>Base w/ MSE</cell><cell></cell><cell></cell><cell cols="2">0.30 0.09</cell><cell cols="2">35.1 21.3</cell></row><row><cell>Base w/ L S</cell><cell></cell><cell></cell><cell cols="2">0.45 0.29</cell><cell cols="2">40.3 24.2</cell></row><row><cell>Base w/ L S + e2e</cell><cell></cell><cell></cell><cell cols="2">0.30 0.29</cell><cell cols="2">43.2 26.2</cell></row><row><cell>AURL (ours)</cell><cell></cell><cell></cell><cell cols="2">0.29 0.32</cell><cell cols="2">44.4 27.4</cell></row><row><cell>AURL w/o CG</cell><cell></cell><cell cols="3">H H CG 0.33 0.32</cell><cell cols="2">43.7 25.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with the closest SoTA [3] on both UCF and HMDB datasets. Red numbers indicate the best. Video clip, 10 Test split) by (28.1, 27.0)% increases on UCF top-1 and HMDB top-1. To conclude, AURL outperforms the SoTA by a large margin.</figDesc><table><row><cell>Method</cell><cell>Pre-training</cell><cell>Video clips</cell><cell>Test splits</cell><cell>UCF top-1</cell><cell>UCF top-5</cell><cell>HMDB top-1</cell><cell>HMDB top-5</cell></row><row><cell>SoTA</cell><cell></cell><cell>1 1</cell><cell>10 10</cell><cell cols="3">43.0 68.2 27.0 45.6 73.1 28.1</cell><cell>54.4 51.8</cell></row><row><cell>AURL</cell><cell></cell><cell>1</cell><cell>10</cell><cell cols="3">55.1 79.3 34.3</cell><cell>65.1</cell></row><row><cell>SoTA</cell><cell></cell><cell>25 25</cell><cell>10 10</cell><cell cols="3">48.0 74.2 31.2 49.2 77.0 32.6</cell><cell>58.3 57.1</cell></row><row><cell>AURL</cell><cell></cell><cell>25</cell><cell>10</cell><cell cols="3">58.0 82.0 39.0</cell><cell>69.5</cell></row><row><cell>SoTA</cell><cell></cell><cell>1 1</cell><cell>1 1</cell><cell cols="3">35.1 56.4 21.3 36.8 61.7 23.0</cell><cell>42.2 41.3</cell></row><row><cell>AURL</cell><cell></cell><cell>1</cell><cell>1</cell><cell cols="3">44.4 70.0 27.4</cell><cell>53.2</cell></row><row><cell>SoTA</cell><cell></cell><cell>25 25</cell><cell>1 1</cell><cell cols="3">37.6 62.5 26.9 39.8 65.6 27.2</cell><cell>49.8 47.4</cell></row><row><cell>AURL</cell><cell></cell><cell>25</cell><cell>1</cell><cell cols="3">46.8 73.1 31.7</cell><cell>58.9</cell></row><row><cell cols="8">AURL consistently surpasses the SoTA under each evalua-</cell></row><row><cell cols="8">tion protocol. Specifically, the smallest improvements hap-</cell></row><row><cell cols="8">pen at (25 Video clips, 1 Test splits) by (17.6, 16.5)% im-</cell></row><row><cell cols="8">provements on UCF top-1 and HMDB top-1, and the largest</cell></row><row><cell cols="2">comes at (1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparisons with SoTA alternatives on both UCF and HMDB datasets. Results of alternatives were obtained from original papers, and the higher, the better. Red and blue numbers indicate the best and second best. means using ? =0.05 in Eq.10.</figDesc><table><row><cell></cell><cell>Test</cell><cell>Train</cell><cell>UCF</cell><cell>Train</cell><cell>HMDB</cell></row><row><cell>Method</cell><cell>splits</cell><cell>dataset</cell><cell>top-1</cell><cell>dataset</cell><cell>top-1</cell></row><row><cell>SoTA [3]</cell><cell>1</cell><cell cols="2">Kinetics 37.6</cell><cell>Kinetics</cell><cell>26.9</cell></row><row><cell>AURL</cell><cell>1</cell><cell cols="2">Kinetics 46.8</cell><cell>Kinetics</cell><cell>31.7</cell></row><row><cell>Obj2act [16]</cell><cell>3</cell><cell>-</cell><cell>30.3</cell><cell>-</cell><cell>15.6</cell></row><row><cell>SAOE [27]</cell><cell>3</cell><cell>-</cell><cell>32.8</cell><cell>-</cell><cell>-</cell></row><row><cell>OPCL [13]</cell><cell>3</cell><cell>-</cell><cell>36.3</cell><cell>-</cell><cell>-</cell></row><row><cell>MUFI [35]</cell><cell>3</cell><cell cols="2">Kinetics+ 56.3</cell><cell cols="2">Kinetics+ 31.0</cell></row><row><cell>AURL</cell><cell>3</cell><cell cols="2">Kinetics 60.9</cell><cell>Kinetics</cell><cell>40.4</cell></row><row><cell>TARN [2]</cell><cell>30</cell><cell>UCF</cell><cell>23.2</cell><cell>HMDB</cell><cell>19.5</cell></row><row><cell>Act2Vec [14]</cell><cell>-</cell><cell>UCF</cell><cell>22.1</cell><cell>HMDB</cell><cell>23.5</cell></row><row><cell>SAOE [27]</cell><cell>10</cell><cell>-</cell><cell>40.4</cell><cell>-</cell><cell>-</cell></row><row><cell>PSGNN [13]</cell><cell>50</cell><cell>UCF</cell><cell>43.0</cell><cell>HMDB</cell><cell>32.6</cell></row><row><cell>OPCL [13]</cell><cell>10</cell><cell>-</cell><cell>47.3</cell><cell>-</cell><cell>-</cell></row><row><cell>SoTA [3]</cell><cell>10</cell><cell cols="2">Kinetics 48.0</cell><cell>Kinetics</cell><cell>32.7</cell></row><row><cell>DASZL [19]</cell><cell>10</cell><cell>-</cell><cell>48.9</cell><cell>-</cell><cell>-</cell></row><row><cell>ER [6]</cell><cell>50</cell><cell>UCF</cell><cell>51.8</cell><cell>HMDB</cell><cell>35.3</cell></row><row><cell>AURL</cell><cell>10</cell><cell cols="2">Kinetics 58.0</cell><cell>Kinetics</cell><cell>39.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tarn: Temporal attentive relation network for few-shot and zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><surname>Bishay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Zoumpourlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09021</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking zero-shot video classification: End-to-end training for realistic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Zhdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Elaborative rehearsal for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Intriguing properties of contrastive losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02803,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explicit interaction model towards text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transductive multi-view zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2332" to="2345" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning attributes equals multi-source domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to model relationships for zero-shot video classification. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3476" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ac-tion2vec: A crossmodal embedding approach to action learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meera</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00484</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Objects2action: Classifying and localizing actions without any video example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Daszl: Dynamic action signatures for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Soo Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Peven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofei</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Schwertfeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02722</idno>
		<title level="m">Video contrastive learning with global context</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nextvlad: An efficient neural network to aggregate frame-level features for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongcheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial-aware object embeddings for zero-shot localization and classification of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Object priors for classifying and localizing unseen actions. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Thong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Videomoco: Contrastive video representation learning with temporally adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05859</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Boosting video representation learning with multi-faceted integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurlIPs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gate-shift networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep cosine metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2251" to="2265" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-task zero-shot action recognition with prioritised data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M Hospedales</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visual data synthesis via gan for zero-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenglong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02982</idno>
		<title level="m">Improving contrastive learning by visualizing feature transformation</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards universal representation for unseen action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
